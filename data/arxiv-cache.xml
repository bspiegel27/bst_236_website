<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-16T00:54:47Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-15T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">110622</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.10486v1</id>
    <updated>2025-04-14T17:59:58Z</updated>
    <published>2025-04-14T17:59:58Z</published>
    <title>DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar
  Relighting</title>
    <summary>  Creating relightable and animatable human avatars from monocular videos is a
rising research topic with a range of applications, e.g. virtual reality,
sports, and video games. Previous works utilize neural fields together with
physically based rendering (PBR), to estimate geometry and disentangle
appearance properties of human avatars. However, one drawback of these methods
is the slow rendering speed due to the expensive Monte Carlo ray tracing. To
tackle this problem, we proposed to distill the knowledge from implicit neural
fields (teacher) to explicit 2D Gaussian splatting (student) representation to
take advantage of the fast rasterization property of Gaussian splatting. To
avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We
also propose novel part-wise ambient occlusion probes for shadow computation.
Shadow prediction is achieved by querying these probes only once per pixel,
which paves the way for real-time relighting of avatars. These techniques
combined give high-quality relighting results with realistic shadow effects.
Our experiments demonstrate that the proposed student model achieves comparable
or even better relighting results with our teacher model while being 370 times
faster at inference time, achieving a 67 FPS rendering speed.
</summary>
    <author>
      <name>Zeren Jiang</name>
    </author>
    <author>
      <name>Shaofei Wang</name>
    </author>
    <author>
      <name>Siyu Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures, Project pages:
  https://jzr99.github.io/DNF-Avatar/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10453v1</id>
    <updated>2025-04-14T17:40:18Z</updated>
    <published>2025-04-14T17:40:18Z</published>
    <title>Anchors no more: Using peculiar velocities to constrain $H_0$ and the
  primordial Universe without calibrators</title>
    <summary>  We develop a novel approach to constrain the Hubble parameter $H_0$ and the
primordial power spectrum amplitude $A_\mathrm{s}$ using supernovae type Ia
(SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we
can model their distance and their covariance as a function of cosmological
parameters without the need of calibrators like Cepheids; this yields a new
independent probe of the large-scale structure based on SNIa data without
distance anchors. Crucially, we implement a differentiable pipeline in JAX,
including efficient emulators and affine sampling, reducing inference time from
years to hours on a single GPU. We first validate our method on mock datasets,
demonstrating that we can constrain $H_0$ and $\log 10^{10}A_\mathrm{s}$ within
$\sim10\%$ using $\sim10^3$ SNIa. We then test our pipeline with SNIa from an
$N$-body simulation, obtaining $7\%$-level unbiased constraints on $H_0$ with a
moderate noise level. We finally apply our method to Pantheon+ data,
constraining $H_0$ at the $10\%$ level without Cepheids when fixing
$A_\mathrm{s}$ to its $\it{Planck}$ value. On the other hand, we obtain
$15\%$-level constraints on $\log 10^{10}A_\mathrm{s}$ in agreement with
$\it{Planck}$ when including Cepheids in the analysis. In light of upcoming
observations of low redshift SNIa from the Zwicky Transient Facility and the
Vera Rubin Legacy Survey of Space and Time, surveys for which our method will
develop its full potential, we make our code publicly available.
</summary>
    <author>
      <name>Davide Piras</name>
    </author>
    <author>
      <name>Francesco Sorrenti</name>
    </author>
    <author>
      <name>Ruth Durrer</name>
    </author>
    <author>
      <name>Martin Kunz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 5 figures, comments welcome. Code available at
  https://github.com/dpiras/veloce</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10449v1</id>
    <updated>2025-04-14T17:38:25Z</updated>
    <published>2025-04-14T17:38:25Z</published>
    <title>M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models</title>
    <summary>  Effective reasoning is crucial to solving complex mathematical problems.
Recent large language models (LLMs) have boosted performance by scaling
test-time computation through long chain-of-thought reasoning. However,
transformer-based models are inherently limited in extending context length due
to their quadratic computational complexity and linear memory requirements. In
this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built
on the Mamba architecture, which allows memory-efficient inference. Our
approach leverages a distillation process from existing reasoning models and is
further enhanced through RL training. Experimental results on the AIME and MATH
benchmarks show that M1 not only outperforms previous linear RNN models but
also matches the performance of state-of-the-art Deepseek R1 distilled
reasoning models at a similar scale. We also compare our generation speed with
a highly performant general purpose inference engine, vLLM, and observe more
than a 3x speedup compared to a same size transformer. With throughput speedup,
we are able to achieve higher accuracy compared to DeepSeek R1 distilled
transformer reasoning models under a fixed generation time budget using
self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model
and provide a more effective approach to scaling test-time generation using
self-consistency or long chain of thought reasoning.
</summary>
    <author>
      <name>Junxiong Wang</name>
    </author>
    <author>
      <name>Wen-Ding Li</name>
    </author>
    <author>
      <name>Daniele Paliotta</name>
    </author>
    <author>
      <name>Daniel Ritter</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <author>
      <name>Tri Dao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available https://github.com/jxiw/M1</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10400v1</id>
    <updated>2025-04-14T16:51:10Z</updated>
    <published>2025-04-14T16:51:10Z</published>
    <title>Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone</title>
    <summary>  This work quantitatively evaluates the performance of event-based vision
systems (EVS) against conventional RGB-based models for action prediction in
collision avoidance on an FPGA accelerator. Our experiments demonstrate that
the EVS model achieves a significantly higher effective frame rate (1 kHz) and
lower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the
RGB-based model, particularly when tested on out-of-distribution data. The EVS
model also exhibits superior robustness in selecting optimal evasion maneuvers.
In particular, in distinguishing between movement and stationary states, it
achieves a 59 percentage point advantage in precision (78% vs. 19%) and a
substantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility
of the RGB model to overfitting. Further analysis in different combinations of
spatial classes confirms the consistent performance of the EVS model in both
test data sets. Finally, we evaluated the system end-to-end and achieved a
latency of approximately 2.14 ms, with event aggregation (1 ms) and inference
on the processing unit (0.94 ms) accounting for the largest components. These
results underscore the advantages of event-based vision for real-time collision
avoidance and demonstrate its potential for deployment in resource-constrained
environments.
</summary>
    <author>
      <name>Pietro Bonazzi</name>
    </author>
    <author>
      <name>Christian Vogt</name>
    </author>
    <author>
      <name>Michael Jost</name>
    </author>
    <author>
      <name>Lyes Khacef</name>
    </author>
    <author>
      <name>Federico Paredes-Vallés</name>
    </author>
    <author>
      <name>Michele Magno</name>
    </author>
    <link href="http://arxiv.org/abs/2504.10400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10397v1</id>
    <updated>2025-04-14T16:45:52Z</updated>
    <published>2025-04-14T16:45:52Z</published>
    <title>Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?</title>
    <summary>  Objective: This study investigates the potential of Large Language Models
(LLMs) as an alternative to human expert elicitation for extracting structured
causal knowledge and facilitating causal modeling in biometric and healthcare
applications.
  Material and Methods: LLM-generated causal structures, specifically Bayesian
networks (BNs), were benchmarked against traditional statistical methods (e.g.,
Bayesian Information Criterion) using healthcare datasets. Validation
techniques included structural equation modeling (SEM) to verifying
relationships, and measures such as entropy, predictive accuracy, and
robustness to compare network structures.
  Results and Discussion: LLM-generated BNs demonstrated lower entropy than
expert-elicited and statistically generated BNs, suggesting higher confidence
and precision in predictions. However, limitations such as contextual
constraints, hallucinated dependencies, and potential biases inherited from
training data require further investigation.
  Conclusion: LLMs represent a novel frontier in expert elicitation for
probabilistic causal modeling, promising to improve transparency and reduce
uncertainty in the decision-making using such models.
</summary>
    <author>
      <name>Olha Shaposhnyk</name>
    </author>
    <author>
      <name>Daria Zahorska</name>
    </author>
    <author>
      <name>Svetlana Yanushkevich</name>
    </author>
    <link href="http://arxiv.org/abs/2504.10397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10388v1</id>
    <updated>2025-04-14T16:32:17Z</updated>
    <published>2025-04-14T16:32:17Z</published>
    <title>Inferring genotype-phenotype maps using attention models</title>
    <summary>  Predicting phenotype from genotype is a central challenge in genetics.
Traditional approaches in quantitative genetics typically analyze this problem
using methods based on linear regression. These methods generally assume that
the genetic architecture of complex traits can be parameterized in terms of an
additive model, where the effects of loci are independent, plus (in some cases)
pairwise epistatic interactions between loci. However, these models struggle to
analyze more complex patterns of epistasis or subtle gene-environment
interactions. Recent advances in machine learning, particularly attention-based
models, offer a promising alternative. Initially developed for natural language
processing, attention-based models excel at capturing context-dependent
interactions and have shown exceptional performance in predicting protein
structure and function. Here, we apply attention-based models to quantitative
genetics. We analyze the performance of this attention-based approach in
predicting phenotype from genotype using simulated data across a range of
models with increasing epistatic complexity, and using experimental data from a
recent quantitative trait locus mapping study in budding yeast. We find that
our model demonstrates superior out-of-sample predictions in epistatic regimes
compared to standard methods. We also explore a more general multi-environment
attention-based model to jointly analyze genotype-phenotype maps across
multiple environments and show that such architectures can be used for
"transfer learning" - predicting phenotypes in novel environments with limited
training data.
</summary>
    <author>
      <name>Krishna Rijal</name>
    </author>
    <author>
      <name>Caroline M. Holmes</name>
    </author>
    <author>
      <name>Samantha Petti</name>
    </author>
    <author>
      <name>Gautam Reddy</name>
    </author>
    <author>
      <name>Michael M. Desai</name>
    </author>
    <author>
      <name>Pankaj Mehta</name>
    </author>
    <link href="http://arxiv.org/abs/2504.10388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10380v1</id>
    <updated>2025-04-14T16:25:36Z</updated>
    <published>2025-04-14T16:25:36Z</published>
    <title>Lorentzian Gromov-Hausdorff convergence and pre-compactness</title>
    <summary>  To goal of the paper is to introduce a convergence \`a la Gromov-Hausdorff
for Lorentzian spaces, building on $\epsilon$-nets consisting of causal
diamonds and relying only on the time separation function. This yields a
geometric notion of convergence, which can be applied to synthetic Lorentzian
spaces (Lorentzian pre-length spaces) or smooth spacetimes. Among the main
results, we prove a Lorentzian counterpart of the celebrated Gromov's
pre-compactness theorem for metric spaces, where controlled covers by balls are
replaced by controlled covers by diamonds. This yields a geometric
pre-compactness result for classes of globally hyperbolic spacetimes,
satisfying a uniform doubling property on Cauchy hypersurfaces and a suitable
control on the causality. The final part of the paper establishes several
applications: we show that Chru\'sciel-Grant approximations are an instance of
the Lorentzian Gromov-Hausdorff convergence here introduced, we prove that
timelike sectional curvature bounds are stable under such a convergence, we
introduce timelike blow-up tangents and discuss connections with the main
conjecture of causal set theory.
</summary>
    <author>
      <name>Andrea Mondino</name>
    </author>
    <author>
      <name>Clemens Sämann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">62 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="28A75, 51K10, 53C23, 53C50, 53B30, 53C80, 83C99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10372v1</id>
    <updated>2025-04-14T16:20:48Z</updated>
    <published>2025-04-14T16:20:48Z</published>
    <title>Simple physical systems as a reference for multivariate information
  dynamics</title>
    <summary>  Understanding a complex system entails capturing the non-trivial collective
phenomena that arise from interactions between its different parts. Information
theory is a flexible and robust framework to study such behaviours, with
several measures designed to quantify and characterise the interdependencies
among the system's components. However, since these estimators rely on the
statistical distributions of observed quantities, it is crucial to examine the
relationships between information-theoretic measures and the system's
underlying mechanistic structure. To this end, here we present an
information-theoretic analytical investigation of an elementary system of
interactive random walkers subject to Gaussian noise. Focusing on partial
information decomposition, causal emergence, and integrated information, our
results help us develop some intuitions on their relationship with the physical
parameters of the system. For instance, we observe that uncoupled systems can
exhibit emergent properties, in a way that we suggest may be better described
as ''statistically autonomous''. Overall, we observe that in this simple
scenario information measures align more reliably with the system's mechanistic
properties when calculated at the level of microscopic components, rather than
their coarse-grained counterparts, and over timescales comparable with the
system's intrinsic dynamics. Moreover, we show that approaches that separate
the contributions of the system's dynamics and steady-state distribution (e.g.
via causal perturbations) may help strengthen the interpretation of
information-theoretic analyses.
</summary>
    <author>
      <name>Alberto Liardi</name>
    </author>
    <author>
      <name>Madalina I. Sas</name>
    </author>
    <author>
      <name>George Blackburne</name>
    </author>
    <author>
      <name>William J. Knottenbelt</name>
    </author>
    <author>
      <name>Pedro A. M. Mediano</name>
    </author>
    <author>
      <name>Henrik Jeldtoft Jensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures + supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10361v1</id>
    <updated>2025-04-14T16:08:34Z</updated>
    <published>2025-04-14T16:08:34Z</published>
    <title>Root-$T\bar{T}$ Deformations On Causal Self-Dual Electrodynamics
  Theories</title>
    <summary>  The self-dual condition, which ensures invariance under electromagnetic
duality, manifests as a partial differential equation in nonlinear
electromagnetism theories. The general solution to this equation is expressed
in terms of an auxiliary field, $\tau$, and Courant-Hilbert functions,
$\ell(\tau)$, which depend on $\tau$. Recent studies have shown that
duality-invariant nonlinear electromagnetic theories fulfill the principle of
causality under the conditions $\frac{\partial \ell}{\partial \tau} \ge 1$ and
$\frac{\partial^2 \ell}{\partial \tau^2} \ge 0$.
  In this paper, we investigate theories with two coupling constants that also
comply with the principle of causality. We demonstrate that these theories
possess a new universal representation of the root-$T\bar{T}$ operator.
Additionally, we derive marginal and irrelevant flow equations for the
logarithmic causal self-dual electrodynamics and identify a symmetry referred
to as $\alpha$-symmetry, which is present in all these models.
</summary>
    <author>
      <name>Hossein Babaei-Aghbolagh</name>
    </author>
    <author>
      <name>Komeil Babaei Velni</name>
    </author>
    <author>
      <name>Song He</name>
    </author>
    <author>
      <name>Zahra Pezhman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10352v1</id>
    <updated>2025-04-14T16:03:21Z</updated>
    <published>2025-04-14T16:03:21Z</published>
    <title>Pseudo-Autoregressive Neural Codec Language Models for Efficient
  Zero-Shot Text-to-Speech Synthesis</title>
    <summary>  Recent zero-shot text-to-speech (TTS) systems face a common dilemma:
autoregressive (AR) models suffer from slow generation and lack duration
controllability, while non-autoregressive (NAR) models lack temporal modeling
and typically require complex designs. In this paper, we introduce a novel
pseudo-autoregressive (PAR) codec language modeling approach that unifies AR
and NAR modeling. Combining explicit temporal modeling from AR with parallel
generation from NAR, PAR generates dynamic-length spans at fixed time steps.
Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR
for initial generation followed by NAR refinement. In the first stage, PAR
progressively generates speech tokens along the time dimension, with each step
predicting all positions in parallel but only retaining the left-most span. In
the second stage, low-confidence tokens are iteratively refined in parallel,
leveraging the global contextual information. Experiments demonstrate that
PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on
large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech
test-clean set in terms of speech quality, speaker similarity, and
intelligibility, while achieving up to ten times faster inference speed. Audio
samples are available at https://anonymous-palle.github.io.
</summary>
    <author>
      <name>Yifan Yang</name>
    </author>
    <author>
      <name>Shujie Liu</name>
    </author>
    <author>
      <name>Jinyu Li</name>
    </author>
    <author>
      <name>Yuxuan Hu</name>
    </author>
    <author>
      <name>Haibin Wu</name>
    </author>
    <author>
      <name>Hui Wang</name>
    </author>
    <author>
      <name>Jianwei Yu</name>
    </author>
    <author>
      <name>Lingwei Meng</name>
    </author>
    <author>
      <name>Haiyang Sun</name>
    </author>
    <author>
      <name>Yanqing Liu</name>
    </author>
    <author>
      <name>Yan Lu</name>
    </author>
    <author>
      <name>Kai Yu</name>
    </author>
    <author>
      <name>Xie Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM MM 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
