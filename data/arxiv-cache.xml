<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-15T00:54:55Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-14T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">110489</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.08719v1</id>
    <updated>2025-04-11T17:33:32Z</updated>
    <published>2025-04-11T17:33:32Z</published>
    <title>SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language
  Modeling</title>
    <summary>  We present a decoder-only Transformer architecture that robustly generalizes
to sequence lengths substantially longer than those seen during training. Our
model, SWAN-GPT, interleaves layers without positional encodings (NoPE) and
sliding-window attention layers equipped with rotary positional encodings
(SWA-RoPE). Experiments demonstrate strong performance on sequence lengths
significantly longer than the training length without the need for additional
long-context training. This robust length extrapolation is achieved through our
novel architecture, enhanced by a straightforward dynamic scaling of attention
scores during inference. In addition, SWAN-GPT is more computationally
efficient than standard GPT architectures, resulting in cheaper training and
higher throughput. Further, we demonstrate that existing pre-trained
decoder-only models can be efficiently converted to the SWAN architecture with
minimal continued training, enabling longer contexts. Overall, our work
presents an effective approach for scaling language models to longer contexts
in a robust and efficient manner.
</summary>
    <author>
      <name>Krishna C. Puvvada</name>
    </author>
    <author>
      <name>Faisal Ladhak</name>
    </author>
    <author>
      <name>Santiago Akle Serrano</name>
    </author>
    <author>
      <name>Cheng-Ping Hsieh</name>
    </author>
    <author>
      <name>Shantanu Acharya</name>
    </author>
    <author>
      <name>Somshubra Majumdar</name>
    </author>
    <author>
      <name>Fei Jia</name>
    </author>
    <author>
      <name>Samuel Kriman</name>
    </author>
    <author>
      <name>Simeng Sun</name>
    </author>
    <author>
      <name>Dima Rekesh</name>
    </author>
    <author>
      <name>Boris Ginsburg</name>
    </author>
    <link href="http://arxiv.org/abs/2504.08719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08718v1</id>
    <updated>2025-04-11T17:30:46Z</updated>
    <published>2025-04-11T17:30:46Z</published>
    <title>EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage</title>
    <summary>  Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate
human pose, hand gesture, and facial expression from monocular images. Existing
methods predominantly rely on Transformer-based architectures, which suffer
from quadratic complexity in self-attention, leading to substantial
computational overhead, especially in multi-person scenarios. Recently, Mamba
has emerged as a promising alternative to Transformers due to its efficient
global modeling capability. However, it remains limited in capturing
fine-grained local dependencies, which are essential for precise EHPS. To
address these issues, we propose EMO-X, the Efficient Multi-person One-stage
model for multi-person EHPS. Specifically, we explore a Scan-based Global-Local
Decoder (SGLD) that integrates global context with skeleton-aware local
features to iteratively enhance human tokens. Our EMO-X leverages the superior
global modeling capability of Mamba and designs a local bidirectional scan
mechanism for skeleton-aware local refinement. Comprehensive experiments
demonstrate that EMO-X strikes an excellent balance between efficiency and
accuracy. Notably, it achieves a significant reduction in computational
complexity, requiring 69.8% less inference time compared to state-of-the-art
(SOTA) methods, while outperforming most of them in accuracy.
</summary>
    <author>
      <name>Haohang Jian</name>
    </author>
    <author>
      <name>Jinlu Zhang</name>
    </author>
    <author>
      <name>Junyi Wu</name>
    </author>
    <author>
      <name>Zhigang Tu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.08718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08716v1</id>
    <updated>2025-04-11T17:29:35Z</updated>
    <published>2025-04-11T17:29:35Z</published>
    <title>ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on
  Transformer Encoder Models Performance</title>
    <summary>  Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce
architectural advancements aimed at improving efficiency and performance.
Although the authors of ModernBERT report improved performance over DeBERTaV3
on several benchmarks, the lack of disclosed training data and the absence of
comparisons using a shared dataset make it difficult to determine whether these
gains are due to architectural improvements or differences in training data. In
this work, we conduct a controlled study by pretraining ModernBERT on the same
dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of
model design. Our results show that the previous model generation remains
superior in sample efficiency and overall benchmark performance, with
ModernBERT's primary advantage being faster training and inference speed.
However, the new proposed model still provides meaningful architectural
improvements compared to earlier models such as BERT and RoBERTa. Additionally,
we observe that high-quality pre-training data accelerates convergence but does
not significantly improve final performance, suggesting potential benchmark
saturation. These findings show the importance of disentangling pretraining
data from architectural innovations when evaluating transformer models.
</summary>
    <author>
      <name>Wissam Antoun</name>
    </author>
    <author>
      <name>Benoît Sagot</name>
    </author>
    <author>
      <name>Djamé Seddah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08714v1</id>
    <updated>2025-04-11T17:24:58Z</updated>
    <published>2025-04-11T17:24:58Z</published>
    <title>Generating Fine Details of Entity Interactions</title>
    <summary>  Images not only depict objects but also encapsulate rich interactions between
them. However, generating faithful and high-fidelity images involving multiple
entities interacting with each other, is a long-standing challenge. While
pre-trained text-to-image models are trained on large-scale datasets to follow
diverse text instructions, they struggle to generate accurate interactions,
likely due to the scarcity of training data for uncommon object interactions.
This paper introduces InterActing, an interaction-focused dataset with 1000
fine-grained prompts covering three key scenarios: (1) functional and
action-based interactions, (2) compositional spatial relationships, and (3)
multi-subject interactions. To address interaction generation challenges, we
propose a decomposition-augmented refinement procedure. Our approach,
DetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose
interactions into finer-grained concepts, uses a VLM to critique generated
images, and applies targeted interventions within the diffusion process in
refinement. Automatic and human evaluations show significantly improved image
quality, demonstrating the potential of enhanced inference strategies. Our
dataset and code are available at https://concepts-ai.com/p/detailscribe/ to
facilitate future exploration of interaction-rich image generation.
</summary>
    <author>
      <name>Xinyi Gu</name>
    </author>
    <author>
      <name>Jiayuan Mao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://concepts-ai.com/p/detailscribe/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08700v1</id>
    <updated>2025-04-11T17:07:09Z</updated>
    <published>2025-04-11T17:07:09Z</published>
    <title>Don't torque like that. Measuring compact object magnetic fields with
  analytic torque models</title>
    <summary>  Context. Changes of the rotational period observed in various magnetized
accreting sources are generally attributed to the interaction between the
in-falling plasma and the large-scale magnetic field of the accretor. A number
of models have been proposed to link these changes to the mass accretion rate,
based on different assumptions on the relevant physical processes and system
parameters. For X-ray binaries with neutron stars, with the help of precise
measurements of the spin periods provided by current instrumentation, these
models render a way to infer such parameters as the strength of the dipolar
field and a distance to the system. Often, the obtained magnetic field strength
values contradict those from other methods used to obtain magnetic field
estimates.
  Aims. We want to compare the results of several of the proposed accretion
models. To this end an example application of these models to data is
performed.
  Methods. We reformulate the set of disk accretion torque models in a way that
their parametrization are directly comparable. The application of the
reformulated models is discussed and demonstrated using Fermi/GBM and Swift/BAT
monitoring data covering several X-ray outbursts of the accreting pulsar 4U
0115+63.
  Results. We find that most of the models under consideration are able to
describe the observations to a high degree of accuracy and with little
indication for one model being preferred over the others. Yet, derived
parameters from those models show a large spread. Specifically the magnetic
field strength ranges over one order of magnitude for the different models.
This indicates that the results are heavily influenced by systematic
uncertainties.
</summary>
    <author>
      <name>J. J. R. Stierhof</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karl Remeis-Sternwarte and Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universität Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>E. Sokolova-Lapa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karl Remeis-Sternwarte and Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universität Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>K. Berger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karl Remeis-Sternwarte and Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universität Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>G. Vasilopoulos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Physics, National and Kapodistrian University of Athens</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Accelerating Systems &amp; Applications Athens</arxiv:affiliation>
    </author>
    <author>
      <name>P. Thalhammer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karl Remeis-Sternwarte and Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universität Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>N. Zalot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karl Remeis-Sternwarte and Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universität Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>R. Ballhausen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Maryland College Park, Department of Astronomy</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NASA Goddard Space Flight Center, Astrophysics Science Division</arxiv:affiliation>
    </author>
    <author>
      <name>I. El Mellah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Departmento de Física, Universidad de Santiago de Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Center for Interdisciplinary Research in Astrophysics and Space Exploration Santiago</arxiv:affiliation>
    </author>
    <author>
      <name>C. Malacaria</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INAF-Osservatorio Astronomico di Roma</arxiv:affiliation>
    </author>
    <author>
      <name>R. E. Rothschild</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Astronomy and Astrophysics, University of California San Diego</arxiv:affiliation>
    </author>
    <author>
      <name>P. Kretschmar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">European Space Astronomy Centre Madrid</arxiv:affiliation>
    </author>
    <author>
      <name>K. Pottschmidt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NASA Goddard Space Flight Center, Astrophysics Science Division</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Center for Space Sciences and Technology, University of Maryland Baltimore County</arxiv:affiliation>
    </author>
    <author>
      <name>J. Wilms</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karl Remeis-Sternwarte and Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universität Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in A&amp;A, 20 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08655v1</id>
    <updated>2025-04-11T15:58:46Z</updated>
    <published>2025-04-11T15:58:46Z</published>
    <title>TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous
  Racing</title>
    <summary>  Perception within autonomous driving is nearly synonymous with Neural
Networks (NNs). Yet, the domain of autonomous racing is often characterized by
scaled, computationally limited robots used for cost-effectiveness and safety.
For this reason, opponent detection and tracking systems typically resort to
traditional computer vision techniques due to computational constraints. This
paper introduces TinyCenterSpeed, a streamlined adaptation of the seminal
CenterPoint method, optimized for real-time performance on 1:10 scale
autonomous racing platforms. This adaptation is viable even on OBCs powered
solely by Central Processing Units (CPUs), as it incorporates the use of an
external Tensor Processing Unit (TPU). We demonstrate that, compared to
Adaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in
scaled autonomous racing, TinyCenterSpeed not only improves detection and
velocity estimation by up to 61.38% but also supports multi-opponent detection
and estimation. It achieves real-time performance with an inference time of
just 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold.
</summary>
    <author>
      <name>Neil Reichlin</name>
    </author>
    <author>
      <name>Nicolas Baumann</name>
    </author>
    <author>
      <name>Edoardo Ghignone</name>
    </author>
    <author>
      <name>Michele Magno</name>
    </author>
    <link href="http://arxiv.org/abs/2504.08655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08654v1</id>
    <updated>2025-04-11T15:58:31Z</updated>
    <published>2025-04-11T15:58:31Z</published>
    <title>The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose
  Estimation</title>
    <summary>  Forecasting hand motion and pose from an egocentric perspective is essential
for understanding human intention. However, existing methods focus solely on
predicting positions without considering articulation, and only when the hands
are visible in the field of view. This limitation overlooks the fact that
approximate hand positions can still be inferred even when they are outside the
camera's view. In this paper, we propose a method to forecast the 3D
trajectories and poses of both hands from an egocentric video, both in and out
of the field of view. We propose a diffusion-based transformer architecture for
Egocentric Hand Forecasting, EgoH4, which takes as input the observation
sequence and camera poses, then predicts future 3D motion and poses for both
hands of the camera wearer. We leverage full-body pose information, allowing
other joints to provide constraints on hand motion. We denoise the hand and
body joints along with a visibility predictor for hand joints and a 3D-to-2D
reprojection loss that minimizes the error when hands are in-view. We evaluate
EgoH4 on the Ego-Exo4D dataset, combining subsets with body and hand
annotations. We train on 156K sequences and evaluate on 34K sequences,
respectively. EgoH4 improves the performance by 3.4cm and 5.1cm over the
baseline in terms of ADE for hand trajectory forecasting and MPJPE for hand
pose forecasting. Project page: https://masashi-hatano.github.io/EgoH4/
</summary>
    <author>
      <name>Masashi Hatano</name>
    </author>
    <author>
      <name>Zhifan Zhu</name>
    </author>
    <author>
      <name>Hideo Saito</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <link href="http://arxiv.org/abs/2504.08654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08651v1</id>
    <updated>2025-04-11T15:55:50Z</updated>
    <published>2025-04-11T15:55:50Z</published>
    <title>Application of machine learning models to predict the relationship
  between air pollution, ecosystem degradation, and health disparities and lung
  cancer in Vietnam</title>
    <summary>  Lung cancer is one of the major causes of death worldwide, and Vietnam is not
an exception. This disease is the second most common type of cancer globally
and the second most common cause of death in Vietnam, just after liver cancer,
with 23,797 fatal cases and 26,262 new cases, or 14.4% of the disease in 2020.
Recently, with rising disease rates in Vietnam causing a huge public health
burden, lung cancer continues to hold the top position in attention and care.
Especially together with climate change, under a variety of types of pollution,
deforestation, and modern lifestyles, lung cancer risks are on red alert,
particularly in Vietnam. To understand more about the severe disease sources in
Vietnam from a diversity of key factors, including environmental features and
the current health state, with a particular emphasis on Vietnam's distinct
socioeconomic and ecological context, we utilize large datasets such as patient
health records and environmental indicators containing necessary information,
such as deforestation rate, green cover rate, air pollution, and lung cancer
risks, that is collected from well-known governmental sharing websites. Then,
we process and connect them and apply analytical methods (heatmap, information
gain, p-value, spearman correlation) to determine causal correlations
influencing lung cancer risks. Moreover, we deploy machine learning (ML) models
(Decision Tree, Random Forest, Support Vector Machine, K-mean clustering) to
discover cancer risk patterns. Our experimental results, leveraged by the
aforementioned ML models to identify the disease patterns, are promising,
particularly, the models as Random Forest, SVM, and PCA are working well on the
datasets and give high accuracy (99%), however, the K means clustering has very
low accuracy (10%) and does not fit the datasets.
</summary>
    <author>
      <name>Ngoc Hong Tran</name>
    </author>
    <author>
      <name>Lan Kim Vien</name>
    </author>
    <author>
      <name>Ngoc-Thao Thi Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and Published in the Proceeding of the 2nd International
  Conference on "Green Solutions and Emerging Technologies for Sustainability"
  (GSETS 2025) ISBN: 978-604-76-3087-5</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08645v1</id>
    <updated>2025-04-11T15:45:17Z</updated>
    <published>2025-04-11T15:45:17Z</published>
    <title>Title block detection and information extraction for enhanced building
  drawings search</title>
    <summary>  The architecture, engineering, and construction (AEC) industry still heavily
relies on information stored in drawings for building construction,
maintenance, compliance and error checks. However, information extraction (IE)
from building drawings is often time-consuming and costly, especially when
dealing with historical buildings. Drawing search can be simplified by
leveraging the information stored in the title block portion of the drawing,
which can be seen as drawing metadata. However, title block IE can be complex
especially when dealing with historical drawings which do not follow existing
standards for uniformity. This work performs a comparison of existing methods
for this kind of IE task, and then proposes a novel title block detection and
IE pipeline which outperforms existing methods, in particular when dealing with
complex, noisy historical drawings. The pipeline is obtained by combining a
lightweight Convolutional Neural Network and GPT-4o, the proposed inference
pipeline detects building engineering title blocks with high accuracy, and then
extract structured drawing metadata from the title blocks, which can be used
for drawing search, filtering and grouping. The work demonstrates high accuracy
and efficiency in IE for both vector (CAD) and hand-drawn (historical)
drawings. A user interface (UI) that leverages the extracted metadata for
drawing search is established and deployed on real projects, which demonstrates
significant time savings. Additionally, an extensible domain-expert-annotated
dataset for title block detection is developed, via an efficient AEC-friendly
annotation workflow that lays the foundation for future work.
</summary>
    <author>
      <name>Alessio Lombardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Buro Happold, London</arxiv:affiliation>
    </author>
    <author>
      <name>Li Duan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Birmingham City University</arxiv:affiliation>
    </author>
    <author>
      <name>Ahmed Elnagar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Buro Happold, London</arxiv:affiliation>
    </author>
    <author>
      <name>Ahmed Zaalouk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Birmingham City University</arxiv:affiliation>
    </author>
    <author>
      <name>Khalid Ismail</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Birmingham City University</arxiv:affiliation>
    </author>
    <author>
      <name>Edlira Vakaj</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Birmingham City University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, 1 table. Accepted for publication in the 2025
  European Conference on Computing in Construction (EC3,
  https://ec-3.org/conference2025/)</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08641v1</id>
    <updated>2025-04-11T15:41:43Z</updated>
    <published>2025-04-11T15:41:43Z</published>
    <title>Training-free Guidance in Text-to-Video Generation via Multimodal
  Planning and Structured Noise Initialization</title>
    <summary>  Recent advancements in text-to-video (T2V) diffusion models have
significantly enhanced the visual quality of the generated videos. However,
even recent T2V models find it challenging to follow text descriptions
accurately, especially when the prompt requires accurate control of spatial
layouts or object trajectories. A recent line of research uses layout guidance
for T2V models that require fine-tuning or iterative manipulation of the
attention map during inference time. This significantly increases the memory
requirement, making it difficult to adopt a large T2V model as a backbone. To
address this, we introduce Video-MSG, a training-free Guidance method for T2V
generation based on Multimodal planning and Structured noise initialization.
Video-MSG consists of three steps, where in the first two steps, Video-MSG
creates Video Sketch, a fine-grained spatio-temporal plan for the final video,
specifying background, foreground, and object trajectories, in the form of
draft video frames. In the last step, Video-MSG guides a downstream T2V
diffusion model with Video Sketch through noise inversion and denoising.
Notably, Video-MSG does not need fine-tuning or attention manipulation with
additional memory during inference time, making it easier to adopt large T2V
models. Video-MSG demonstrates its effectiveness in enhancing text alignment
with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V
generation benchmarks (T2VCompBench and VBench). We provide comprehensive
ablation studies about noise inversion ratio, different background generators,
background object detection, and foreground object segmentation.
</summary>
    <author>
      <name>Jialu Li</name>
    </author>
    <author>
      <name>Shoubin Yu</name>
    </author>
    <author>
      <name>Han Lin</name>
    </author>
    <author>
      <name>Jaemin Cho</name>
    </author>
    <author>
      <name>Jaehong Yoon</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website: https://video-msg.github.io; The first three authors
  contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
