<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-11-06T00:56:46Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-11-05T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">125404</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2511.02828v1</id>
    <updated>2025-11-04T18:56:27Z</updated>
    <published>2025-11-04T18:56:27Z</published>
    <title>Searching Within Galaxies for the Earliest Signs of Quenching With
  Spatially Resolved Star Formation Histories in UVCANDELS</title>
    <summary>  Understanding the complicated processes that regulate star formation and
cause a galaxy to become quiescent is key to our comprehension of galaxy
evolution. We used nine well resolved star-forming z&lt;1 galaxies from the
UVCANDELS survey, where a total of 10 HST bands including UV follow up in
UVIS/F275W allow us to reconstruct the star formation histories (SFHs) of
regions across each galaxy. This approach provides a powerful tool to explore
the spatio-temporal connection between star formation and galaxy evolution. The
spatial and temporal profiles of stellar mass and star formation rate surface
density were obtained from the SFHs of these regions. We measure scaling
relations and projected radial profiles of regions within each galaxy at the
time of observation and at 1 Gyr lookback time, noting possible trends in the
evolution. By comparing the change in star formation over time we can infer the
timing and location of star formation and see early signs of star formation
shut off before quenching occurs. We compared the star formation rate density
-- stellar mass density scaling relations for individual galaxies as they
evolve from 1 Gyr lookback time. The correlation lines pivot around a
log-stellar mass surface density of 7.25 [$M_\odot$ $kpc^{-2}$] may be evidence
of a self-regulating process on these scales. Radial profiles of galaxy Log
sSFR show an overall decrease over 1 Gyr, but five galaxies show a greater
change in Log sSFR at the outskirts than the center indicating a possible early
onset of quenching in these galaxies.
</summary>
    <author>
      <name>Charlotte Olsen</name>
    </author>
    <author>
      <name>Eric Gawiser</name>
    </author>
    <author>
      <name>Charlotte Welker</name>
    </author>
    <author>
      <name>Harry Teplitz</name>
    </author>
    <author>
      <name>Kartheik Iyer</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Marc Rafelski</name>
    </author>
    <author>
      <name>Rogier A. Windhorst</name>
    </author>
    <author>
      <name>Anton Koekemoer</name>
    </author>
    <author>
      <name>Anahita Alavi</name>
    </author>
    <author>
      <name>Ben Sunnquist</name>
    </author>
    <author>
      <name>Norman Grogin</name>
    </author>
    <author>
      <name>Yicheng Guo</name>
    </author>
    <author>
      <name>Christopher J. Conselice</name>
    </author>
    <author>
      <name>L. Y. Aaron Yung</name>
    </author>
    <author>
      <name>Kalina Nedkova</name>
    </author>
    <author>
      <name>Bahram Mobasher</name>
    </author>
    <author>
      <name>Ray A. Lucas</name>
    </author>
    <author>
      <name>Vihang Mehta</name>
    </author>
    <author>
      <name>Y. Sophia Dai</name>
    </author>
    <author>
      <name>Jonathan P. Gardner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 28 figures. Submitted to ApJ</arxiv:comment>
    <link href="http://arxiv.org/abs/2511.02828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02808v1</id>
    <updated>2025-11-04T18:30:09Z</updated>
    <published>2025-11-04T18:30:09Z</published>
    <title>Reliable Parameter Inference for the Epoch of Reionization using
  Balanced Neural Ratio Estimation</title>
    <summary>  We present an application of the Balanced Neural Ratio Estimation (BNRE)
algorithm to improve the statistical validity of parameter estimates used to
characterize the Epoch of Reionization, where the common assumption of a
multivariate Gaussian likelihood leads to overconfident and biased posterior
distributions. Using a two-parameter model of the Ly$\alpha$ forest
autocorrelation function, we show that BNRE yields posterior distributions that
are significantly better calibrated than those obtained under the Gaussian
likelihood assumption, as verified through the Test of Accuracy with Random
Points (TARP) and Simulation-Based Calibration (SBC) diagnostics. These results
demonstrate the potential of Simulation-Based Inference (SBI) methods, and in
particular BNRE, to provide statistically robust parameter constraints within
existing astrophysical modeling frameworks.
</summary>
    <author>
      <name>Diego González-Hernández</name>
    </author>
    <author>
      <name>Molly Wolfson</name>
    </author>
    <author>
      <name>Joseph F. Hennawi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning and the Physical Sciences Workshop, NeurIPS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2511.02808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02802v1</id>
    <updated>2025-11-04T18:25:17Z</updated>
    <published>2025-11-04T18:25:17Z</published>
    <title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular
  Foundation Models</title>
    <summary>  Tabular foundation models represent a growing paradigm in structured data
learning, extending the benefits of large-scale pretraining to tabular domains.
However, their adoption remains limited due to heterogeneous preprocessing
pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the
absence of standardized evaluation for deployment-oriented metrics such as
calibration and fairness. We present TabTune, a unified library that
standardizes the complete workflow for tabular foundation models through a
single interface. TabTune provides consistent access to seven state-of-the-art
models supporting multiple adaptation strategies, including zero-shot
inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient
fine-tuning (PEFT). The framework automates model-aware preprocessing, manages
architectural heterogeneity internally, and integrates evaluation modules for
performance, calibration, and fairness. Designed for extensibility and
reproducibility, TabTune enables consistent benchmarking of adaptation
strategies of tabular foundation models. The library is open source and
available at https://github.com/Lexsi-Labs/TabTune .
</summary>
    <author>
      <name>Aditya Tanna</name>
    </author>
    <author>
      <name>Pratinav Seth</name>
    </author>
    <author>
      <name>Mohamed Bouadi</name>
    </author>
    <author>
      <name>Utsav Avaiya</name>
    </author>
    <author>
      <name>Vinay Kumar Sankarapu</name>
    </author>
    <link href="http://arxiv.org/abs/2511.02802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02785v1</id>
    <updated>2025-11-04T18:06:30Z</updated>
    <published>2025-11-04T18:06:30Z</published>
    <title>Enhancing Federated Learning Privacy with QUBO</title>
    <summary>  Federated learning (FL) is a widely used method for training machine learning
(ML) models in a scalable way while preserving privacy (i.e., without
centralizing raw data). Prior research shows that the risk of exposing
sensitive data increases cumulatively as the number of iterations where a
client's updates are included in the aggregated model increase. Attackers can
launch membership inference attacks (MIA; deciding whether a sample or client
participated), property inference attacks (PIA; inferring attributes of a
client's data), and model inversion attacks (MI; reconstructing inputs),
thereby inferring client-specific attributes and, in some cases, reconstructing
inputs. In this paper, we mitigate risk by substantially reducing per client
exposure using a quantum computing-inspired quadratic unconstrained binary
optimization (QUBO) formulation that selects a small subset of client updates
most relevant for each training round. In this work, we focus on two threat
vectors: (i) information leakage by clients during training and (ii)
adversaries who can query or obtain the global model. We assume a trusted
central server and do not model server compromise. This method also assumes
that the server has access to a validation/test set with global data
distribution. Experiments on the MNIST dataset with 300 clients in 20 rounds
showed a 95.2% per-round and 49% cumulative privacy exposure reduction, with
147 clients' updates never being used during training while maintaining in
general the full-aggregation accuracy or even better. The method proved to be
efficient at lower scale and more complex model as well. A CINIC-10
dataset-based experiment with 30 clients resulted in 82% per-round privacy
improvement and 33% cumulative privacy.
</summary>
    <author>
      <name>Andras Ferenczi</name>
    </author>
    <author>
      <name>Sutapa Samanta</name>
    </author>
    <author>
      <name>Dagen Wang</name>
    </author>
    <author>
      <name>Todd Hodges</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2511.02785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02772v1</id>
    <updated>2025-11-04T17:58:47Z</updated>
    <published>2025-11-04T17:58:47Z</published>
    <title>Quantum Theory Can Decohere from a Causally-Indefinite Post-Quantum
  Theory</title>
    <summary>  We find a process satisfying the axioms of hyper-decoherence which produces
standard quantum theory from the theory of quantum boxes (higher-order quantum
theory with the non-signalling tensor product). This hyper-decoherence map
evades the no-go theorem of Lee and Selby by relaxing constraints on signalling
to the past and the uniqueness of purifications. We discuss some natural
opposing conclusions: that the existence of this map might be evidence of a
genuine hyper-decoherence process producing causal quantum theory from its
causally-indefinite higher-order theory; or that it serves as an indication
that the axioms of hyper-decoherence might need careful re-consideration,
especially regarding the subtle albeit central role that purity plays.
</summary>
    <author>
      <name>James Hefford</name>
    </author>
    <author>
      <name>Matt Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/2511.02772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02769v1</id>
    <updated>2025-11-04T17:56:00Z</updated>
    <published>2025-11-04T17:56:00Z</published>
    <title>STAR-VAE: Latent Variable Transformers for Scalable and Controllable
  Molecular Generation</title>
    <summary>  The chemical space of drug-like molecules is vast, motivating the development
of generative models that must learn broad chemical distributions, enable
conditional generation by capturing structure-property representations, and
provide fast molecular generation. Meeting the objectives depends on modeling
choices, including the probabilistic modeling approach, the conditional
generative formulation, the architecture, and the molecular input
representation. To address the challenges, we present STAR-VAE
(Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder),
a scalable latent-variable framework with a Transformer encoder and an
autoregressive Transformer decoder. It is trained on 79 million drug-like
molecules from PubChem, using SELFIES to guarantee syntactic validity. The
latent-variable formulation enables conditional generation: a property
predictor supplies a conditioning signal that is applied consistently to the
latent prior, the inference network, and the decoder. Our contributions are:
(i) a Transformer-based latent-variable encoder-decoder model trained on
SELFIES representations; (ii) a principled conditional latent-variable
formulation for property-guided generation; and (iii) efficient finetuning with
low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation
with limited property and activity data. On the GuacaMol and MOSES benchmarks,
our approach matches or exceeds baselines, and latent-space analyses reveal
smooth, semantically structured representations that support both unconditional
exploration and property-aware generation. On the Tartarus benchmarks, the
conditional model shifts docking-score distributions toward stronger predicted
binding. These results suggest that a modernized, scale-appropriate VAE remains
competitive for molecular generation when paired with principled conditioning
and parameter-efficient finetuning.
</summary>
    <author>
      <name>Bum Chul Kwon</name>
    </author>
    <author>
      <name>Ben Shapira</name>
    </author>
    <author>
      <name>Moshiko Raboh</name>
    </author>
    <author>
      <name>Shreyans Sethi</name>
    </author>
    <author>
      <name>Shruti Murarka</name>
    </author>
    <author>
      <name>Joseph A Morrone</name>
    </author>
    <author>
      <name>Jianying Hu</name>
    </author>
    <author>
      <name>Parthasarathy Suryanarayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2511.02769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02764v1</id>
    <updated>2025-11-04T17:45:19Z</updated>
    <published>2025-11-04T17:45:19Z</published>
    <title>Peer effect analysis with latent processes</title>
    <summary>  I study peer effects that arise from irreversible decisions in the absence of
a standard social equilibrium. I model a latent sequence of decisions in
continuous time and obtain a closed-form expression for the likelihood, which
allows to estimate proposed causal estimands. The method avoids regression on
conditional expectations or linear-in-means regression -- and thus
reflection-type problems (Manski, 1993) or simultaneity issues -- by modeling
the (unobserved) realized direction of causality, whose probability is
identified. Under a parsimonious parametric specification, I introduce a peer
effect parameter meant to capture the causal influence of first-movers on their
peers. Various forms of peer effect heterogeneity can be accommodated.
Parameters are shown to be consistently estimated by maximum likelihood methods
and lend themselves to standard inference.
</summary>
    <author>
      <name>Vincent Starck</name>
    </author>
    <link href="http://arxiv.org/abs/2511.02764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02755v1</id>
    <updated>2025-11-04T17:35:17Z</updated>
    <published>2025-11-04T17:35:17Z</published>
    <title>Controlling Performance and Budget of a Centralized Multi-agent LLM
  System with Reinforcement Learning</title>
    <summary>  Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.
</summary>
    <author>
      <name>Bowen Jin</name>
    </author>
    <author>
      <name>TJ Collins</name>
    </author>
    <author>
      <name>Donghan Yu</name>
    </author>
    <author>
      <name>Mert Cemri</name>
    </author>
    <author>
      <name>Shenao Zhang</name>
    </author>
    <author>
      <name>Mengyu Li</name>
    </author>
    <author>
      <name>Jay Tang</name>
    </author>
    <author>
      <name>Tian Qin</name>
    </author>
    <author>
      <name>Zhiyang Xu</name>
    </author>
    <author>
      <name>Jiarui Lu</name>
    </author>
    <author>
      <name>Guoli Yin</name>
    </author>
    <author>
      <name>Jiawei Han</name>
    </author>
    <author>
      <name>Zirui Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2511.02755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02754v1</id>
    <updated>2025-11-04T17:35:12Z</updated>
    <published>2025-11-04T17:35:12Z</published>
    <title>DANIEL: A Distributed and Scalable Approach for Global Representation
  Learning with EHR Applications</title>
    <summary>  Classical probabilistic graphical models face fundamental challenges in
modern data environments, which are characterized by high dimensionality,
source heterogeneity, and stringent data-sharing constraints. In this work, we
revisit the Ising model, a well-established member of the Markov Random Field
(MRF) family, and develop a distributed framework that enables scalable and
privacy-preserving representation learning from large-scale binary data with
inherent low-rank structure. Our approach optimizes a non-convex surrogate loss
function via bi-factored gradient descent, offering substantial computational
and communication advantages over conventional convex approaches. We evaluate
our algorithm on multi-institutional electronic health record (EHR) datasets
from 58,248 patients across the University of Pittsburgh Medical Center (UPMC)
and Mass General Brigham (MGB), demonstrating superior performance in global
representation learning and downstream clinical tasks, including relationship
detection, patient phenotyping, and patient clustering. These results highlight
a broader potential for statistical inference in federated, high-dimensional
settings while addressing the practical challenges of data complexity and
multi-institutional integration.
</summary>
    <author>
      <name>Zebin Wang</name>
    </author>
    <author>
      <name>Ziming Gan</name>
    </author>
    <author>
      <name>Weijing Tang</name>
    </author>
    <author>
      <name>Zongqi Xia</name>
    </author>
    <author>
      <name>Tianrun Cai</name>
    </author>
    <author>
      <name>Tianxi Cai</name>
    </author>
    <author>
      <name>Junwei Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2511.02754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.02749v1</id>
    <updated>2025-11-04T17:22:49Z</updated>
    <published>2025-11-04T17:22:49Z</published>
    <title>Using Span Queries to Optimize for Cache and Attention Locality</title>
    <summary>  Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.
</summary>
    <author>
      <name>Paul Castro</name>
    </author>
    <author>
      <name>Nick Mitchell</name>
    </author>
    <author>
      <name>Nathan Ordonez</name>
    </author>
    <author>
      <name>Thomas Parnell</name>
    </author>
    <author>
      <name>Mudhakar Srivatsa</name>
    </author>
    <author>
      <name>Antoni Viros i Martin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2511.02749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2511.02749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
