<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-03T00:54:02Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-02T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">111554</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.00661v1</id>
    <updated>2025-05-01T17:02:27Z</updated>
    <published>2025-05-01T17:02:27Z</published>
    <title>On the generalization of language models from in-context learning and
  finetuning: a controlled study</title>
    <summary>  Large language models exhibit exciting capabilities, yet can show
surprisingly narrow generalization from finetuning -- from failing to
generalize to simple reversals of relations they are trained on, to missing
logical deductions that can be made from trained information. These failures to
generalize from fine-tuning can hinder practical application of these models.
However, language models' in-context learning shows different inductive biases,
and can generalize better in some of these cases. Here, we explore these
differences in generalization between in-context- and fine-tuning-based
learning. To do so, we constructed several novel datasets to evaluate and
improve models' ability to generalize from finetuning data. The datasets are
constructed to isolate the knowledge in the dataset from that in pretraining,
to create clean tests of generalization. We expose pretrained large models to
controlled subsets of the information in these datasets -- either in context,
or through fine-tuning -- and evaluate their performance on test sets that
require various types of generalization. We find overall that in data-matched
settings, in-context learning can generalize more flexibly than fine-tuning
(though we also find some qualifications of prior findings, such as cases when
fine-tuning can generalize to reversals embedded in a larger structure of
knowledge). We build on these findings to propose a method to enable improved
generalization from fine-tuning: adding in-context inferences to finetuning
data. We show that this method improves generalization across various splits of
our datasets and other benchmarks. Our results have implications for
understanding the inductive biases of different modes of learning in language
models, and practically improving their performance.
</summary>
    <author>
      <name>Andrew K. Lampinen</name>
    </author>
    <author>
      <name>Arslan Chaudhry</name>
    </author>
    <author>
      <name>Stephanie C. Y. Chan</name>
    </author>
    <author>
      <name>Cody Wild</name>
    </author>
    <author>
      <name>Diane Wan</name>
    </author>
    <author>
      <name>Alex Ku</name>
    </author>
    <author>
      <name>JÃ¶rg Bornschein</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <author>
      <name>Murray Shanahan</name>
    </author>
    <author>
      <name>James L. McClelland</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00657v1</id>
    <updated>2025-05-01T16:59:36Z</updated>
    <published>2025-05-01T16:59:36Z</published>
    <title>Joint inference for gravitational wave signals and glitches using a
  data-informed glitch model</title>
    <summary>  Gravitational wave data are often contaminated by non-Gaussian noise
transients, glitches, which can bias the inference of astrophysical signal
parameters. Traditional approaches either subtract glitches in a pre-processing
step, or a glitch model can be included from an agnostic wavelet basis (e.g.
BayesWave). In this work, we introduce a machine-learning-based approach to
build a parameterised model of glitches. We train a normalising flow on known
glitches from the Gravity Spy catalogue, constructing an informative prior on
the glitch model. By incorporating this model into the Bayesian inference
analysis with Bilby, we estimate glitch and signal parameters simultaneously.
We demonstrate the performance of our method through bias reduction, glitch
identification and Bayesian model selection on real glitches. Our results show
that this approach effectively removes glitches from the data, significantly
improving source parameter estimation and reducing bias.
</summary>
    <author>
      <name>Ann-Kristin Malz</name>
    </author>
    <author>
      <name>John Veitch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to PRD</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00651v1</id>
    <updated>2025-05-01T16:54:21Z</updated>
    <published>2025-05-01T16:54:21Z</published>
    <title>Open-Source LLM-Driven Federated Transformer for Predictive IoV
  Management</title>
    <summary>  The proliferation of connected vehicles within the Internet of Vehicles (IoV)
ecosystem presents critical challenges in ensuring scalable, real-time, and
privacy-preserving traffic management. Existing centralized IoV solutions often
suffer from high latency, limited scalability, and reliance on proprietary
Artificial Intelligence (AI) models, creating significant barriers to
widespread deployment, particularly in dynamic and privacy-sensitive
environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular
systems remains underexplored, especially concerning prompt optimization and
effective utilization in federated contexts. To address these challenges, we
propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel
framework that leverages open-source LLMs for predictive IoV management. FPoTT
introduces a dynamic prompt optimization mechanism that iteratively refines
textual prompts to enhance trajectory prediction. The architecture employs a
dual-layer federated learning paradigm, combining lightweight edge models for
real-time inference with cloud-based LLMs to retain global intelligence. A
Transformer-driven synthetic data generator is incorporated to augment training
with diverse, high-fidelity traffic scenarios in the Next Generation Simulation
(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing
EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data
while maintaining high performance on synthetic datasets. These results
underscore the potential of open-source LLMs in enabling secure, adaptive, and
scalable IoV management, offering a promising alternative to proprietary
solutions in smart mobility ecosystems.
</summary>
    <author>
      <name>Yazan Otoum</name>
    </author>
    <author>
      <name>Arghavan Asad</name>
    </author>
    <author>
      <name>Ishtiaq Ahmad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint version; submitted for academic peer review</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00635v1</id>
    <updated>2025-05-01T16:20:16Z</updated>
    <published>2025-05-01T16:20:16Z</published>
    <title>SOMA: a novel sampler for exchangeable variables</title>
    <summary>  The problem of sampling exchangeable random variables arises in many Bayesian
inference tasks, especially in data imputation given a privatized summary
statistics. These permutation-invariant joint distributions often have
dependency structures that make sampling challenging. Component-wise sampling
strategies, such as Metropolis-within-Gibbs, can mix slowly because they
consider only comparing a proposed point with one component at a time. In this
work, we propose a novel Single-Offer-Multiple-Attempts (SOMA) sampler that is
tailored to sampling permutation invariant distributions. The core intuition of
SOMA is that a proposed point unsuitable to replace one component might still
be a good candidate to replace some other component in the joint distribution.
SOMA first makes a singer offer, and then simultaneously considers attempts to
replace each component of the current state with the single offer, before
making the final acceptance or rejection decision. We provide an acceptance
lower bound of SOMA and, using a coupling method, derive the convergence rate
upper bound of SOMA in the two-dimensional case. We validate theoretical
findings with numerical simulations, including a demonstration on
differentially private Bayesian linear regression.
</summary>
    <author>
      <name>Yifei Xiong</name>
    </author>
    <author>
      <name>Nianqiao Phyllis Ju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00632v1</id>
    <updated>2025-05-01T16:16:47Z</updated>
    <published>2025-05-01T16:16:47Z</published>
    <title>Detecting Modeling Bias with Continuous Time Flow Models on Weak Lensing
  Maps</title>
    <summary>  Simulation-based inference provides a powerful framework for extracting rich
information from nonlinear scales in current and upcoming cosmological surveys,
and ensuring its robustness requires stringent validation of forward models. In
this work, we recast forward model validation as an out-of-distribution (OoD)
detection problem within the framework of machine learning (ML)-based
simulation-based inference (SBI). We employ probability density as the metric
for OoD detection, and compare various density estimation techniques,
demonstrating that field-level probability density estimation via continuous
time flow models (CTFM) significantly outperforms feature-level approaches that
combine scattering transform (ST) or convolutional neural networks (CNN) with
normalizing flows (NFs), as well as NF-based field-level estimators, as
quantified by the area under the receiver operating characteristic curve
(AUROC). Our analysis shows that CTFM not only excels in detecting OoD samples
but also provides a robust metric for model selection. Additionally, we
verified CTFM maintains consistent efficacy across different cosmologies while
mitigating the inductive biases inherent in NF architectures. Although our
proof-of-concept study employs simplified forward modeling and noise settings,
our framework establishes a promising pathway for identifying unknown
systematics in the cosmology datasets.
</summary>
    <author>
      <name>Kangning Diao</name>
    </author>
    <author>
      <name>Biwei Dai</name>
    </author>
    <author>
      <name>Uros Seljak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 8 figures, 2 tables, comments welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00603v1</id>
    <updated>2025-05-01T15:35:01Z</updated>
    <published>2025-05-01T15:35:01Z</published>
    <title>Can LLMs Help Improve Analogical Reasoning For Strategic Decisions?
  Experimental Evidence from Humans and GPT-4</title>
    <summary>  This study investigates whether large language models, specifically GPT4, can
match human capabilities in analogical reasoning within strategic decision
making contexts. Using a novel experimental design involving source to target
matching, we find that GPT4 achieves high recall by retrieving all plausible
analogies but suffers from low precision, frequently applying incorrect
analogies based on superficial similarities. In contrast, human participants
exhibit high precision but low recall, selecting fewer analogies yet with
stronger causal alignment. These findings advance theory by identifying
matching, the evaluative phase of analogical reasoning, as a distinct step that
requires accurate causal mapping beyond simple retrieval. While current LLMs
are proficient in generating candidate analogies, humans maintain a comparative
advantage in recognizing deep structural similarities across domains. Error
analysis reveals that AI errors arise from surface level matching, whereas
human errors stem from misinterpretations of causal structure. Taken together,
the results suggest a productive division of labor in AI assisted
organizational decision making where LLMs may serve as broad analogy
generators, while humans act as critical evaluators, applying the most
contextually appropriate analogies to strategic problems.
</summary>
    <author>
      <name>Phanish Puranam</name>
    </author>
    <author>
      <name>Prothit Sen</name>
    </author>
    <author>
      <name>Maciej Workiewicz</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00591v1</id>
    <updated>2025-05-01T15:25:23Z</updated>
    <published>2025-05-01T15:25:23Z</published>
    <title>Explainable AI in Spatial Analysis</title>
    <summary>  This chapter discusses the opportunities of eXplainable Artificial
Intelligence (XAI) within the realm of spatial analysis. A key objective in
spatial analysis is to model spatial relationships and infer spatial processes
to generate knowledge from spatial data, which has been largely based on
spatial statistical methods. More recently, machine learning offers scalable
and flexible approaches that complement traditional methods and has been
increasingly applied in spatial data science. Despite its advantages, machine
learning is often criticized for being a black box, which limits our
understanding of model behavior and output. Recognizing this limitation, XAI
has emerged as a pivotal field in AI that provides methods to explain the
output of machine learning models to enhance transparency and understanding.
These methods are crucial for model diagnosis, bias detection, and ensuring the
reliability of results obtained from machine learning models. This chapter
introduces key concepts and methods in XAI with a focus on Shapley value-based
approaches, which is arguably the most popular XAI method, and their
integration with spatial analysis. An empirical example of county-level voting
behaviors in the 2020 Presidential election is presented to demonstrate the use
of Shapley values and spatial analysis with a comparison to multi-scale
geographically weighted regression. The chapter concludes with a discussion on
the challenges and limitations of current XAI techniques and proposes new
directions.
</summary>
    <author>
      <name>Ziqi Li</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00571v1</id>
    <updated>2025-05-01T14:55:22Z</updated>
    <published>2025-05-01T14:55:22Z</published>
    <title>Hypothesis-free discovery from epidemiological data by automatic
  detection and local inference for tree-based nonlinearities and interactions</title>
    <summary>  In epidemiological settings, Machine Learning (ML) is gaining popularity for
hypothesis-free discovery of risk (or protective) factors. Although ML is
strong at discovering non-linearities and interactions, this power is currently
compromised by a lack of reliable inference. Although local measures of feature
effect can be combined with tree ensembles, uncertainty quantifications for
these measures remain only partially available and oftentimes unsatisfactory.
We propose RuleSHAP, a framework for using rule-based, hypothesis-free
discovery that combines sparse Bayesian regression, tree ensembles and Shapley
values in a one-step procedure that both detects and tests complex patterns at
the individual level. To ease computation, we derive a formula that computes
marginal Shapley values more efficiently for our setting. We demonstrate the
validity of our framework on simulated data. To illustrate, we apply our
machinery to data from an epidemiological cohort to detect and infer several
effects for high cholesterol and blood pressure, such as nonlinear interaction
effects between features like age, sex, ethnicity, BMI and glucose level.
</summary>
    <author>
      <name>Giorgio Spadaccini</name>
    </author>
    <author>
      <name>Marjolein Fokkema</name>
    </author>
    <author>
      <name>Mark A. van de Wiel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Main body: 29 pages, 7 figures; Supplementary material: 39 pages, 14
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00570v1</id>
    <updated>2025-05-01T14:53:12Z</updated>
    <published>2025-05-01T14:53:12Z</published>
    <title>FreqKV: Frequency Domain Key-Value Compression for Efficient Context
  Window Extension</title>
    <summary>  Extending the context window in large language models (LLMs) is essential for
applications involving long-form content generation. However, the linear
increase in key-value (KV) cache memory requirements and the quadratic
complexity of self-attention with respect to sequence length present
significant challenges during fine-tuning and inference. Existing methods
suffer from performance degradation when extending to longer contexts. In this
work, we introduce a novel context extension method that optimizes both
fine-tuning and inference efficiency. Our method exploits a key observation: in
the frequency domain, the energy distribution of the KV cache is primarily
concentrated in low-frequency components. By filtering out the high-frequency
components, the KV cache can be effectively compressed with minimal information
loss. Building on this insight, we propose an efficient compression technique,
FreqKV, that iteratively compresses the increasing KV cache to a fixed size in
the frequency domain, applicable to both fine-tuning and inference. FreqKV
introduces no additional parameters or architectural modifications. With
minimal fine-tuning, LLMs can learn to leverage the limited cache that is
compressed in the frequency domain and extend the context window efficiently.
Experiments on various long context language modeling and understanding tasks
demonstrate the efficiency and efficacy of the proposed method.
</summary>
    <author>
      <name>Jushi Kai</name>
    </author>
    <author>
      <name>Boyi Zeng</name>
    </author>
    <author>
      <name>Yixuan Wang</name>
    </author>
    <author>
      <name>Haoli Bai</name>
    </author>
    <author>
      <name>Bo Jiang</name>
    </author>
    <author>
      <name>Zhouhan Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00562v1</id>
    <updated>2025-05-01T14:40:07Z</updated>
    <published>2025-05-01T14:40:07Z</published>
    <title>TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</title>
    <summary>  Learning to solve complex tasks with signal temporal logic (STL)
specifications is crucial to many real-world applications. However, most
previous works only consider fixed or parametrized STL specifications due to
the lack of a diverse STL dataset and encoders to effectively extract temporal
logic information for downstream tasks. In this paper, we propose TeLoGraF,
Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)
encoder and flow-matching to learn solutions for general STL specifications. We
identify four commonly used STL templates and collect a total of 200K
specifications with paired demonstrations. We conduct extensive experiments in
five simulation environments ranging from simple dynamical models in the 2D
space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped
navigation. Results show that our method outperforms other baselines in the STL
satisfaction rate. Compared to classical STL planning algorithms, our approach
is 10-100X faster in inference and can work on any system dynamics. Besides, we
show our graph-encoding method's capability to solve complex STLs and
robustness to out-distribution STL specifications. Code is available at
https://github.com/mengyuest/TeLoGraF
</summary>
    <author>
      <name>Yue Meng</name>
    </author>
    <author>
      <name>Chuchu Fan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICML2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
