<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-26T00:56:02Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-25T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">119478</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.16577v1</id>
    <updated>2025-08-22T17:59:40Z</updated>
    <published>2025-08-22T17:59:40Z</published>
    <title>MV-RAG: Retrieval Augmented Multiview Diffusion</title>
    <summary>  Text-to-3D generation approaches have advanced significantly by leveraging
pretrained 2D diffusion priors, producing high-quality and 3D-consistent
outputs. However, they often fail to produce out-of-domain (OOD) or rare
concepts, yielding inconsistent or inaccurate results. To this end, we propose
MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images
from a large in-the-wild 2D database and then conditions a multiview diffusion
model on these images to synthesize consistent and accurate multiview outputs.
Training such a retrieval-conditioned model is achieved via a novel hybrid
strategy bridging structured multiview data and diverse 2D image collections.
This involves training on multiview data using augmented conditioning views
that simulate retrieval variance for view-specific reconstruction, alongside
training on sets of retrieved real-world 2D images using a distinctive held-out
view prediction objective: the model predicts the held-out view from the other
views to infer 3D consistency from 2D data. To facilitate a rigorous OOD
evaluation, we introduce a new collection of challenging OOD prompts.
Experiments against state-of-the-art text-to-3D, image-to-3D, and
personalization baselines show that our approach significantly improves 3D
consistency, photorealism, and text adherence for OOD/rare concepts, while
maintaining competitive performance on standard benchmarks.
</summary>
    <author>
      <name>Yosef Dayani</name>
    </author>
    <author>
      <name>Omer Benishu</name>
    </author>
    <author>
      <name>Sagie Benaim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://yosefdayani.github.io/MV-RAG</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.16577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16573v1</id>
    <updated>2025-08-22T17:56:01Z</updated>
    <published>2025-08-22T17:56:01Z</published>
    <title>ORCA: Mitigating Over-Reliance for Multi-Task Dwell Time Prediction with
  Causal Decoupling</title>
    <summary>  Dwell time (DT) is a critical post-click metric for evaluating user
preference in recommender systems, complementing the traditional click-through
rate (CTR). Although multi-task learning is widely adopted to jointly optimize
DT and CTR, we observe that multi-task models systematically collapse their DT
predictions to the shortest and longest bins, under-predicting the moderate
durations. We attribute this moderate-duration bin under-representation to
over-reliance on the CTR-DT spurious correlation, and propose ORCA to address
it with causal-decoupling. Specifically, ORCA explicitly models and subtracts
CTR's negative transfer while preserving its positive transfer. We further
introduce (i) feature-level counterfactual intervention, and (ii) a
task-interaction module with instance inverse-weighting, weakening CTR-mediated
effect and restoring direct DT semantics. ORCA is model-agnostic and easy to
deploy. Experiments show an average 10.6% lift in DT metrics without harming
CTR. Code is available at
https://github.com/Chrissie-Law/ORCA-Mitigating-Over-Reliance-for-Multi-Task-Dwell-Time-Prediction-with-Causal-Decoupling.
</summary>
    <author>
      <name>Huishi Luo</name>
    </author>
    <author>
      <name>Fuzhen Zhuang</name>
    </author>
    <author>
      <name>Yongchun Zhu</name>
    </author>
    <author>
      <name>Yiqing Wu</name>
    </author>
    <author>
      <name>Bo Kang</name>
    </author>
    <author>
      <name>Ruobing Xie</name>
    </author>
    <author>
      <name>Feng Xia</name>
    </author>
    <author>
      <name>Deqing Wang</name>
    </author>
    <author>
      <name>Jin Dong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3746252.3760898</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3746252.3760898" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a short paper at CIKM 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.16573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16556v1</id>
    <updated>2025-08-22T17:23:25Z</updated>
    <published>2025-08-22T17:23:25Z</published>
    <title>Spherical latent space models for social network analysis</title>
    <summary>  This article introduces a spherical latent space model for social network
analysis, embedding actors on a hypersphere rather than in Euclidean space as
in standard latent space models. The spherical geometry facilitates the
representation of transitive relationships and community structure, naturally
captures cyclical patterns, and ensures bounded distances, thereby mitigating
degeneracy issues common in traditional approaches. Bayesian inference is
performed via Markov chain Monte Carlo methods to estimate both latent
positions and other model parameters. The approach is demonstrated using two
benchmark social network datasets, yielding improved model fit and
interpretability relative to conventional latent space models.
</summary>
    <author>
      <name>Juan Sosa</name>
    </author>
    <author>
      <name>Carlos Nosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 8 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.16556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16553v1</id>
    <updated>2025-08-22T17:21:56Z</updated>
    <published>2025-08-22T17:21:56Z</published>
    <title>TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a
  Milling Machine</title>
    <summary>  In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.
</summary>
    <author>
      <name>Tim Langer</name>
    </author>
    <author>
      <name>Matthias Widra</name>
    </author>
    <author>
      <name>Volkhard Beyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.16553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; I.5.4; C.5.3; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16552v1</id>
    <updated>2025-08-22T17:21:21Z</updated>
    <published>2025-08-22T17:21:21Z</published>
    <title>Data Gluttony: Epistemic Risks, Dependent Testing and Data Reuse in
  Large Datasets</title>
    <summary>  Large-scale registries have collected vast amounts of data which has enabled
investigators to efficiently conduct studies of observational data. Common
practice is for investigators to use all data meeting the inclusion criteria of
their study to perform their analysis. We term this common practice data
gluttony. It has apparent formal justification insofar as this approach
maximizes per-study power. But this comes at a cost: data reuse affects the
shape of the tail distribution of inferential errors. Using the theory of risk
orderings we demonstrate how positively dependent testing procedures result in
strictly riskier distributions of inferential error.
  We identify two remedies to this state of affairs: research portfolio
optimization and what we term data temperance. Research portfolio optimization
requires that we formulate the enterprise of inference in a utility theoretic
framework: associated to each hypothesis to be evaluated is some utility
dependent on its truth as well as the impact of the statistical decision
rendered on the basis of the data. Under certain models of data governance,
this approach can be used to optimally allocate data usage across multiple
inferential tasks. On the other hand, data temperance is a more flexible
strategy for managing the distribution of inferential errors. Data temperance
is the principle that an investigator use only as much data as is necessary to
perform the task at hand. This is possible due to the diminishing marginal
returns in power and precision in sample size. We analyze the effectiveness of
data temperance at reducing the dependence across testing and develop a theory
of the capacity of a static database to sustain large numbers of inferential
tasks with low probability of inducing pairwise dependent testing procedures.
</summary>
    <author>
      <name>Reid Dale</name>
    </author>
    <author>
      <name>Jordan Rodu</name>
    </author>
    <author>
      <name>Maria E. Currie</name>
    </author>
    <author>
      <name>Mike Baiocchi</name>
    </author>
    <link href="http://arxiv.org/abs/2508.16552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16513v1</id>
    <updated>2025-08-22T16:36:52Z</updated>
    <published>2025-08-22T16:36:52Z</published>
    <title>Extreme Lithium Depletion in Solar Twins: Challenging Non-Standard
  Mixing Models</title>
    <summary>  Lithium (Li) is a powerful tracer of stellar mixing, gradually depleted in
solar twins by non-standard transport below the convective zone. Here, we
identify six new solar twins with exceptionally low Li levels that are not
explained by current non-standard mixing models and, together with our
previously reported anomalous solar twin HIP 8522, suggest a distinct
population marked by a violent evolutionary past. Employing high-resolution
spectra ($R=60,000 - 165,000$), we infer precise stellar parameters and
chemical compositions, including Li abundances. We consider possible scenarios
generating enhanced mixing, including planetary engulfment, blue straggler
stars (BSSs), and early episodic accretion. Our planet engulfment simulations
indicate that only one star may have engulfed an exoplanet, rapidly depleting
Li via thermohaline convection. In the BSS scenario, radial velocity data rule
out binary mass transfer, revealing no stellar companions but instead two new
exoplanets. If these stars are field BSSs, a binary merger is likely though
uncertain given that current BSS models focus mostly on stars in open clusters.
Using pre-main-sequence episodic accretion models, we find that solar-mass
stars can experience enhanced Li depletion without significant beryllium (Be)
depletion. This is consistent with the Be abundances measured in two of our
stars and represents the most plausible scenario, pending Be measurements for
the remaining stars. These unique stars, together with HIP 8522, represent
exceptional cases for testing stellar evolution models and probing internal
mixing processes in Sun-like stars.
</summary>
    <author>
      <name>Isabelle Winnick</name>
    </author>
    <author>
      <name>Jhon Yana Galarza</name>
    </author>
    <author>
      <name>Henrique Reggiani</name>
    </author>
    <author>
      <name>Thiago Ferreira</name>
    </author>
    <author>
      <name>Isabelle Baraffe</name>
    </author>
    <author>
      <name>Diego Lorenzo-Oliveira</name>
    </author>
    <author>
      <name>Micaela Oyague</name>
    </author>
    <author>
      <name>Rita Valle</name>
    </author>
    <author>
      <name>Renzo Trujillo Diaz</name>
    </author>
    <author>
      <name>Nathan Leigh</name>
    </author>
    <author>
      <name>Matias Flores Trivigno</name>
    </author>
    <author>
      <name>Ricardo Lopez-Valdivia</name>
    </author>
    <author>
      <name>Gabriela Carvalho Silva</name>
    </author>
    <author>
      <name>Eder Martioli</name>
    </author>
    <author>
      <name>Helio Perottoni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 12 figures, 9 tables, submitted to ApJ</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.16513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16491v1</id>
    <updated>2025-08-22T16:15:53Z</updated>
    <published>2025-08-22T16:15:53Z</published>
    <title>Tracking flat bands via phonon-mediated interband scattering</title>
    <summary>  Flat-band (FB) materials have emerged as promising platforms for exploring
exotic quantum phases. While numerous candidates have recently been identified
through spectroscopic techniques such as angle-resolved photoemission
spectroscopy, central challenges remain on how to tune FBs towards the Fermi
level $E_F$ and to understand their impact on low-energy excitations probed in
electronic transport experiments. Here, we show that, by attributing the
temperature dependence of the electrical resistivity at elevated temperatures
to electron-phonon interband scattering, one can infer the position of FBs near
$E_F$ across diverse material classes. As charge carriers scatter off phonons,
interband transitions into FB states lead to distinctive sub- or superlinear
resistivity at elevated temperatures, governed by the proximity of the FB to
$E_F$. Our phenomenological model captures these universal transport behaviors
observed across several recently studied FB compounds and offers a simple,
broadly applicable method for detecting flat bands.
</summary>
    <author>
      <name>Fabian Garmroudi</name>
    </author>
    <author>
      <name>Xinlin Yan</name>
    </author>
    <author>
      <name>Silke Paschen</name>
    </author>
    <author>
      <name>Sean M. Thomas</name>
    </author>
    <author>
      <name>Eric D. Bauer</name>
    </author>
    <author>
      <name>Andrej Pustogow</name>
    </author>
    <author>
      <name>Priscila F. S. Rosa</name>
    </author>
    <link href="http://arxiv.org/abs/2508.16491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16479v1</id>
    <updated>2025-08-22T15:51:33Z</updated>
    <published>2025-08-22T15:51:33Z</published>
    <title>Disentangled Multi-modal Learning of Histology and Transcriptomics for
  Cancer Characterization</title>
    <summary>  Histopathology remains the gold standard for cancer diagnosis and prognosis.
With the advent of transcriptome profiling, multi-modal learning combining
transcriptomics with histology offers more comprehensive information. However,
existing multi-modal approaches are challenged by intrinsic multi-modal
heterogeneity, insufficient multi-scale integration, and reliance on paired
data, restricting clinical applicability. To address these challenges, we
propose a disentangled multi-modal framework with four contributions: 1) To
mitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into
tumor and microenvironment subspaces using a disentangled multi-modal fusion
module, and introduce a confidence-guided gradient coordination strategy to
balance subspace optimization. 2) To enhance multi-scale integration, we
propose an inter-magnification gene-expression consistency strategy that aligns
transcriptomic signals across WSI magnifications. 3) To reduce dependency on
paired data, we propose a subspace knowledge distillation strategy enabling
transcriptome-agnostic inference through a WSI-only student model. 4) To
improve inference efficiency, we propose an informative token aggregation
module that suppresses WSI redundancy while preserving subspace semantics.
Extensive experiments on cancer diagnosis, prognosis, and survival prediction
demonstrate our superiority over state-of-the-art methods across multiple
settings. Code is available at
https://github.com/helenypzhang/Disentangled-Multimodal-Learning.
</summary>
    <author>
      <name>Yupei Zhang</name>
    </author>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Anran Liu</name>
    </author>
    <author>
      <name>Lequan Yu</name>
    </author>
    <author>
      <name>Chao Li</name>
    </author>
    <link href="http://arxiv.org/abs/2508.16479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16464v1</id>
    <updated>2025-08-22T15:30:40Z</updated>
    <published>2025-08-22T15:30:40Z</published>
    <title>What makes an entity salient in discourse?</title>
    <summary>  Entities in discourse vary broadly in salience: main participants, objects
and locations are noticeable and memorable, while tangential ones are less
important and quickly forgotten, raising questions about how humans signal and
infer relative salience. Using a graded operationalization of salience based on
summary-worthiness in multiple summaries of a discourse, this paper explores
data from 24 spoken and written genres of English to extract a multifactorial
complex of overt and implicit linguistic cues, such as recurring subjecthood or
definiteness, discourse relations and hierarchy across utterances, as well as
pragmatic functional inferences based on genre and communicative intent.
Tackling the question 'how is the degree of salience expressed for each and
every entity mentioned?' our results show that while previous approaches to
salience all correlate with our salience scores to some extent, no single
generalization is without exceptions, and the phenomenon cuts across all levels
of linguistic representation.
</summary>
    <author>
      <name>Amir Zeldes</name>
    </author>
    <author>
      <name>Jessica Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2508.16464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16463v1</id>
    <updated>2025-08-22T15:25:40Z</updated>
    <published>2025-08-22T15:25:40Z</published>
    <title>Modular Embedding Recomposition for Incremental Learning</title>
    <summary>  The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.
</summary>
    <author>
      <name>Aniello Panariello</name>
    </author>
    <author>
      <name>Emanuele Frascaroli</name>
    </author>
    <author>
      <name>Pietro Buzzega</name>
    </author>
    <author>
      <name>Lorenzo Bonicelli</name>
    </author>
    <author>
      <name>Angelo Porrello</name>
    </author>
    <author>
      <name>Simone Calderara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 36th British Machine Vision Conference (BMVC 2025),
  Sheffield, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.16463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.16463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
