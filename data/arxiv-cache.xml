<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-10T00:52:47Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-09T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">123147</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.07318v1</id>
    <updated>2025-10-08T17:59:55Z</updated>
    <published>2025-10-08T17:59:55Z</published>
    <title>Artificial Hippocampus Networks for Efficient Long-Context Modeling</title>
    <summary>  Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.
</summary>
    <author>
      <name>Yunhao Fang</name>
    </author>
    <author>
      <name>Weihao Yu</name>
    </author>
    <author>
      <name>Shu Zhong</name>
    </author>
    <author>
      <name>Qinghao Ye</name>
    </author>
    <author>
      <name>Xuehan Xiong</name>
    </author>
    <author>
      <name>Lai Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/ByteDance-Seed/AHN</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.07318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07312v1</id>
    <updated>2025-10-08T17:58:41Z</updated>
    <published>2025-10-08T17:58:41Z</published>
    <title>h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement
  Learning</title>
    <summary>  Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.
</summary>
    <author>
      <name>Sumeet Ramesh Motwani</name>
    </author>
    <author>
      <name>Alesia Ivanova</name>
    </author>
    <author>
      <name>Ziyang Cai</name>
    </author>
    <author>
      <name>Philip Torr</name>
    </author>
    <author>
      <name>Riashat Islam</name>
    </author>
    <author>
      <name>Shital Shah</name>
    </author>
    <author>
      <name>Christian Schroeder de Witt</name>
    </author>
    <author>
      <name>Charles London</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint, 31 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.07312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07309v2</id>
    <updated>2025-10-09T02:27:56Z</updated>
    <published>2025-10-08T17:57:35Z</published>
    <title>Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the
  Business Domain</title>
    <summary>  In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.
</summary>
    <author>
      <name>Yue Li</name>
    </author>
    <author>
      <name>Ran Tao</name>
    </author>
    <author>
      <name>Derek Hommel</name>
    </author>
    <author>
      <name>Yusuf Denizay DÃ¶nder</name>
    </author>
    <author>
      <name>Sungyong Chang</name>
    </author>
    <author>
      <name>David Mimno</name>
    </author>
    <author>
      <name>Unso Eun Seo Jo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures, under review for ACL ARR; typos corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.07309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07293v1</id>
    <updated>2025-10-08T17:50:16Z</updated>
    <published>2025-10-08T17:50:16Z</published>
    <title>AudioMarathon: A Comprehensive Benchmark for Long-Context Audio
  Understanding and Efficiency in Audio LLMs</title>
    <summary>  Processing long-form audio is a major challenge for Large Audio Language
models (LALMs). These models struggle with the quadratic cost of attention
($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio
benchmarks are built mostly from short clips and do not evaluate models in
realistic long context settings. To address this gap, we introduce
AudioMarathon, a benchmark designed to evaluate both understanding and
inference efficiency on long-form audio. AudioMarathon provides a diverse set
of tasks built upon three pillars: long-context audio inputs with durations
ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of
2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,
sound, and music, and complex reasoning that requires multi-hop inference. We
evaluate state-of-the-art LALMs and observe clear performance drops as audio
length grows. We also study acceleration techniques and analyze the trade-offs
of token pruning and KV cache eviction. The results show large gaps across
current LALMs and highlight the need for better temporal reasoning and
memory-efficient architectures. We believe AudioMarathon will drive the audio
and multimodal research community to develop more advanced audio understanding
models capable of solving complex audio tasks.
</summary>
    <author>
      <name>Peize He</name>
    </author>
    <author>
      <name>Zichen Wen</name>
    </author>
    <author>
      <name>Yubo Wang</name>
    </author>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>Xiaoqian Liu</name>
    </author>
    <author>
      <name>Jiajie Huang</name>
    </author>
    <author>
      <name>Zehui Lei</name>
    </author>
    <author>
      <name>Zhuangcheng Gu</name>
    </author>
    <author>
      <name>Xiangqi Jin</name>
    </author>
    <author>
      <name>Jiabing Yang</name>
    </author>
    <author>
      <name>Kai Li</name>
    </author>
    <author>
      <name>Zhifei Liu</name>
    </author>
    <author>
      <name>Weijia Li</name>
    </author>
    <author>
      <name>Cunxiang Wang</name>
    </author>
    <author>
      <name>Conghui He</name>
    </author>
    <author>
      <name>Linfeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 23 figures, the code is available at
  \url{https://github.com/DabDans/AudioMarathon}</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.07293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07287v1</id>
    <updated>2025-10-08T17:46:11Z</updated>
    <published>2025-10-08T17:46:11Z</published>
    <title>Modified Fronsdal coordinates for maximally extended Schwarzschild
  spacetime</title>
    <summary>  We introduce a coordinate system that complements the Kruskal--Szekeres
extension. Like the standard construction, it covers the maximally extended
Schwarzschild manifold in its entirety, while offering an additional advantage
of expressing the areal radius as an explicit function of the new coordinates.
Its main limitation, however, is that radial null geodesics are no longer
represented as 45-degree lines in the Kruskal plane, making the causal
structure more difficult to interpret. Nevertheless, the new system offers a
compelling aesthetic trade-off: among all known maximally extended systems -
including those of Kruskal-Szekeres, Israel, Fronsdal, Novikov, and Synge - it
exhibits the highest degree of symmetry with respect to Schwarzschild's
original r- and t-coordinate lines. It trades the regular pattern of Kruskal's
light cones for a symmetric nesting arrangement of the two-dimensional spheres.
The proposed extension sheds new light on the closely related Fronsdal's
six-dimensional embedding construction, and clarifies the deep connection that
exists between the most important implicit (Kruskal-Szekeres) and explicit
(Israel's) procedures for maximal extension of the Schwarzschild geometry that
is well known to those working in the field but rarely presented in textbooks
on general relativity.
</summary>
    <author>
      <name>Andrei Galiautdinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.07287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07283v1</id>
    <updated>2025-10-08T17:43:36Z</updated>
    <published>2025-10-08T17:43:36Z</published>
    <title>Content-Adaptive Inference for State-of-the-art Learned Video
  Compression</title>
    <summary>  While the BD-rate performance of recent learned video codec models in both
low-delay and random-access modes exceed that of respective modes of
traditional codecs on average over common benchmarks, the performance
improvements for individual videos with complex/large motions is much smaller
compared to scenes with simple motion. This is related to the inability of a
learned encoder model to generalize to motion vector ranges that have not been
seen in the training set, which causes loss of performance in both coding of
flow fields as well as frame prediction and coding. As a remedy, we propose a
generic (model-agnostic) framework to control the scale of motion vectors in a
scene during inference (encoding) to approximately match the range of motion
vectors in the test and training videos by adaptively downsampling frames. This
results in down-scaled motion vectors enabling: i) better flow estimation;
hence, frame prediction and ii) more efficient flow compression. We show that
the proposed framework for content-adaptive inference improves the BD-rate
performance of already state-of-the-art low-delay video codec DCVC-FM by up to
41\% on individual videos without any model fine tuning. We present ablation
studies to show measures of motion and scene complexity can be used to predict
the effectiveness of the proposed framework.
</summary>
    <author>
      <name>Ahmet Bilican</name>
    </author>
    <author>
      <name>M. AkÄ±n YÄ±lmaz</name>
    </author>
    <author>
      <name>A. Murat Tekalp</name>
    </author>
    <link href="http://arxiv.org/abs/2510.07283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07259v1</id>
    <updated>2025-10-08T17:21:43Z</updated>
    <published>2025-10-08T17:21:43Z</published>
    <title>The cosmic web's Lyman-$Î±$ glow at $z \approx 2.5$; varying
  hydrodynamic models, dust, and wide-field, narrow-band imaging detection</title>
    <summary>  The diffuse glow of the cosmic web in Lyman-$\alpha$ emission has long been
predicted, yet remained elusive to direct wide field detection. We present
theoretical calculations that, when compared with recent observations made
using the Condor Array Telescope in New Mexico reported in Lanzetta et al.
2024, point to its discovery at $z \approx 2.5$. Synthetic Lyman-$\alpha$
surface brightness maps are constructed from five state-of-the-art hydrodynamic
simulations (Illustris-TNG, SIMBA, EAGLE, CROCODILE, and Sherwood),
incorporating dust attenuation, star formation, collisional excitation, and
recombination physics. Our cosmic web Lyman-$\alpha$ surface brightness
predictions are consistent with the UV excess detected at high significance in
the recent deep, wide field, narrow-band imaging Condor data. The calculations
presented here thus demonstrate that diffuse Lyman-$\alpha$ emission is
observable with current (and next-generation) wide field low surface brightness
facilities, opening the path to direct cartographic mapping of the cosmic web.
These findings mark a turning point: for the first time, cosmology moves beyond
inference from absorption and high-density peaks, into panoramic imaging of the
faint intergalactic scaffolding that underpins structure formation in the
Universe.
</summary>
    <author>
      <name>Oleksii Sokoliuk</name>
    </author>
    <author>
      <name>John K. Webb</name>
    </author>
    <author>
      <name>Kenneth M. Lanzetta</name>
    </author>
    <author>
      <name>Michael M. Shara</name>
    </author>
    <author>
      <name>Stefan Gromoll</name>
    </author>
    <author>
      <name>James S. Bolton</name>
    </author>
    <author>
      <name>Robert F. Carswell</name>
    </author>
    <author>
      <name>Gaspar Galaz</name>
    </author>
    <author>
      <name>CÃ©dric Ledoux</name>
    </author>
    <author>
      <name>Gaspare Lo Curto</name>
    </author>
    <author>
      <name>Alain Smette</name>
    </author>
    <author>
      <name>David Valls-Gabaud</name>
    </author>
    <author>
      <name>Anja von der Linden</name>
    </author>
    <author>
      <name>Frederick M. Walter</name>
    </author>
    <author>
      <name>Joris Witstok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.07259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07257v1</id>
    <updated>2025-10-08T17:20:53Z</updated>
    <published>2025-10-08T17:20:53Z</published>
    <title>Test-Time Graph Search for Goal-Conditioned Reinforcement Learning</title>
    <summary>  Offline goal-conditioned reinforcement learning (GCRL) trains policies that
reach user-specified goals at test time, providing a simple, unsupervised,
domain-agnostic way to extract diverse behaviors from unlabeled, reward-free
datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
agents due to temporal credit assignment and error accumulation, and the
offline setting amplifies these effects. To alleviate this issue, we introduce
Test-Time Graph Search (TTGS), a lightweight planning approach to solve the
GCRL task. TTGS accepts any state-space distance or cost signal, builds a
weighted graph over dataset states, and performs fast search to assemble a
sequence of subgoals that a frozen policy executes. When the base learner is
value-based, the distance is derived directly from the learned goal-conditioned
value function, so no handcrafted metric is needed. TTGS requires no changes to
training, no additional supervision, no online interaction, and no privileged
information, and it runs entirely at inference. On the OGBench benchmark, TTGS
improves success rates of multiple base learners on challenging locomotion
tasks, demonstrating the benefit of simple metric-guided test-time planning for
offline GCRL.
</summary>
    <author>
      <name>Evgenii Opryshko</name>
    </author>
    <author>
      <name>Junwei Quan</name>
    </author>
    <author>
      <name>Claas Voelcker</name>
    </author>
    <author>
      <name>Yilun Du</name>
    </author>
    <author>
      <name>Igor Gilitschenski</name>
    </author>
    <link href="http://arxiv.org/abs/2510.07257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07255v1</id>
    <updated>2025-10-08T17:20:27Z</updated>
    <published>2025-10-08T17:20:27Z</published>
    <title>ARGscape: A modular, interactive tool for manipulation of spatiotemporal
  ancestral recombination graphs</title>
    <summary>  Ancestral recombination graphs (ARGs) encode the complete genealogical
history of a population of recombining lineages. ARGs, and their succinct
representation, tree sequences, are increasingly central to modern population
genetics methods, yet building an intuition for ARGs remains challenging. This
is particularly true when analyzing ancestry in a geographic context, as there
is a critical lack of dedicated, interactive tools capable of visualizing ARGs
as spatiotemporal objects. To address this gap, we introduce ARGscape, an
interactive platform for simulating, analyzing, and visualizing ARGs across
space and time. ARGscape provides a user-friendly graphical interface featuring
dynamic 2- and 3-dimensional visualizations to explore ARGs through space and
time, as well as a novel "spatial diff" visualization for quantitative
comparison of geographic inference methods. ARGscape is an innovative, unified
framework that seamlessly integrates leading command-line, Python, and R-based
tools for ARG simulation, manipulation, and use in spatiotemporal inference
into both graphical and command-line interfaces. By integrating these various
functionalities, ARGscape facilitates novel data exploration and hypothesis
generation, while lowering the barrier to entry for spatiotemporal ARG analysis
in both research and education use-cases. ARGscape is built with a Python
FastAPI backend and a React/TypeScript frontend. It is freely available as a
live demo at https://www.argscape.com and as a Python package on PyPI (pip
install argscape). The source code and documentation are available on GitHub at
https://github.com/chris-a-talbot/argscape.
</summary>
    <author>
      <name>Christopher Talbot</name>
    </author>
    <author>
      <name>Gideon Bradburd</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages; 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.07255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07239v1</id>
    <updated>2025-10-08T17:06:20Z</updated>
    <published>2025-10-08T17:06:20Z</published>
    <title>Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided
  LoRA Experts</title>
    <summary>  Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.
</summary>
    <author>
      <name>Christos Ziakas</name>
    </author>
    <author>
      <name>Nicholas Loo</name>
    </author>
    <author>
      <name>Nishita Jain</name>
    </author>
    <author>
      <name>Alessandra Russo</name>
    </author>
    <link href="http://arxiv.org/abs/2510.07239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
