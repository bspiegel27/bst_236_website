<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-30T01:02:44Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117753</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.21053v1</id>
    <updated>2025-07-28T17:59:57Z</updated>
    <published>2025-07-28T17:59:57Z</published>
    <title>Flow Matching Policy Gradients</title>
    <summary>  Flow-based generative models, including diffusion models, excel at modeling
continuous distributions in high-dimensional spaces. In this work, we introduce
Flow Policy Optimization (FPO), a simple on-policy reinforcement learning
algorithm that brings flow matching into the policy gradient framework. FPO
casts policy optimization as maximizing an advantage-weighted ratio computed
from the conditional flow matching loss, in a manner compatible with the
popular PPO-clip framework. It sidesteps the need for exact likelihood
computation while preserving the generative capabilities of flow-based models.
Unlike prior approaches for diffusion-based reinforcement learning that bind
training to a specific sampling method, FPO is agnostic to the choice of
diffusion or flow integration at both training and inference time. We show that
FPO can train diffusion-style policies from scratch in a variety of continuous
control tasks. We find that flow-based models can capture multimodal action
distributions and achieve higher performance than Gaussian policies,
particularly in under-conditioned settings.
</summary>
    <author>
      <name>David McAllister</name>
    </author>
    <author>
      <name>Songwei Ge</name>
    </author>
    <author>
      <name>Brent Yi</name>
    </author>
    <author>
      <name>Chung Min Kim</name>
    </author>
    <author>
      <name>Ethan Weber</name>
    </author>
    <author>
      <name>Hongsuk Choi</name>
    </author>
    <author>
      <name>Haiwen Feng</name>
    </author>
    <author>
      <name>Angjoo Kanazawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">See our blog post: https://flowreinforce.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.21053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21051v1</id>
    <updated>2025-07-28T17:59:55Z</updated>
    <published>2025-07-28T17:59:55Z</published>
    <title>Certification of nonobjective information by Popescu-Rohrlich box
  fraction and distinguishing quantum theory</title>
    <summary>  It is demonstrated that identifying information-theoretic limitations of
quantum Bell nonlocality alone cannot completely distinguish quantum theory
from generalized nonsignaling theories. To this end, an information-theoretic
concept of certifying nonobjective information by the Popescu-Rohrlich box
fraction is employed. Furthermore, in the aforementioned demonstration, a
partial answer to the question of what distinguishes quantum theory from
generalized nonsignaling theories emerges beyond the one provided by the
principle of information causality alone. This is accomplished by demonstrating
that postquantum models identified by the information causality are isolated by
the emergence of the Popescu-Rohrlich box fraction of nonobjective information
in Bell-local boxes of a generalized nonsignaling theory, over the two other
generalized nonsignaling theories that have simplicial local state spaces.
</summary>
    <author>
      <name>Chellasamy Jebarathinam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.21051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21040v1</id>
    <updated>2025-07-28T17:56:34Z</updated>
    <published>2025-07-28T17:56:34Z</published>
    <title>Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps:
  An Interpretation and Potential Improvements</title>
    <summary>  We propose a probabilistic interpretation of transformers as unrolled
inference steps assuming a probabilistic Laplacian Eigenmaps model from the
ProbDR framework. Our derivation shows that at initialisation, transformers
perform "linear" dimensionality reduction. We also show that within the
transformer block, a graph Laplacian term arises from our arguments, rather
than an attention matrix (which we interpret as an adjacency matrix). We
demonstrate that simply subtracting the identity from the attention matrix (and
thereby taking a graph diffusion step) improves validation performance on a
language model and a simple vision transformer.
</summary>
    <author>
      <name>Aditya Ravuri</name>
    </author>
    <author>
      <name>Neil D. Lawrence</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Initial version</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.21040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21009v1</id>
    <updated>2025-07-28T17:22:10Z</updated>
    <published>2025-07-28T17:22:10Z</published>
    <title>Memorization in Fine-Tuned Large Language Models</title>
    <summary>  This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.
</summary>
    <author>
      <name>Danil Savine</name>
    </author>
    <author>
      <name>Muni Sreenivas Pydi</name>
    </author>
    <author>
      <name>Jamal Atif</name>
    </author>
    <author>
      <name>Olivier Capp√©</name>
    </author>
    <link href="http://arxiv.org/abs/2507.21009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.20994v1</id>
    <updated>2025-07-28T16:59:53Z</updated>
    <published>2025-07-28T16:59:53Z</published>
    <title>Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety
  to Vision in LVLM</title>
    <summary>  Large visual-language models (LVLMs) integrate aligned large language models
(LLMs) with visual modules to process multimodal inputs. However, the safety
mechanisms developed for text-based LLMs do not naturally extend to visual
modalities, leaving LVLMs vulnerable to harmful image inputs. To address this
cross-modal safety gap, we introduce security tensors - trainable input vectors
applied during inference through either the textual or visual modality. These
tensors transfer textual safety alignment to visual processing without
modifying the model's parameters. They are optimized using a curated dataset
containing (i) malicious image-text pairs requiring rejection, (ii) contrastive
benign pairs with text structurally similar to malicious queries, with the
purpose of being contrastive examples to guide visual reliance, and (iii)
general benign samples preserving model functionality. Experimental results
demonstrate that both textual and visual security tensors significantly enhance
LVLMs' ability to reject diverse harmful visual inputs while maintaining
near-identical performance on benign tasks. Further internal analysis towards
hidden-layer representations reveals that security tensors successfully
activate the language module's textual "safety layers" in visual inputs,
thereby effectively extending text-based safety to the visual modality.
</summary>
    <author>
      <name>Shen Li</name>
    </author>
    <author>
      <name>Liuyi Yao</name>
    </author>
    <author>
      <name>Wujia Niu</name>
    </author>
    <author>
      <name>Lan Zhang</name>
    </author>
    <author>
      <name>Yaliang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Codes and data are available at
  https://github.com/listen0425/Security-Tensors</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.20994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.20994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.20993v1</id>
    <updated>2025-07-28T16:52:31Z</updated>
    <published>2025-07-28T16:52:31Z</published>
    <title>Personalized Treatment Effect Estimation from Unstructured Data</title>
    <summary>  Existing methods for estimating personalized treatment effects typically rely
on structured covariates, limiting their applicability to unstructured data.
Yet, leveraging unstructured data for causal inference has considerable
application potential, for instance in healthcare, where clinical notes or
medical images are abundant. To this end, we first introduce an approximate
'plug-in' method trained directly on the neural representations of unstructured
data. However, when these fail to capture all confounding information, the
method may be subject to confounding bias. We therefore introduce two
theoretically grounded estimators that leverage structured measurements of the
confounders during training, but allow estimating personalized treatment
effects purely from unstructured inputs, while avoiding confounding bias. When
these structured measurements are only available for a non-representative
subset of the data, these estimators may suffer from sampling bias. To address
this, we further introduce a regression-based correction that accounts for the
non-uniform sampling, assuming the sampling mechanism is known or can be
well-estimated. Our experiments on two benchmark datasets show that the plug-in
method, directly trainable on large unstructured datasets, achieves strong
empirical performance across all settings, despite its simplicity.
</summary>
    <author>
      <name>Henri Arno</name>
    </author>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <link href="http://arxiv.org/abs/2507.20993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.20993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.20990v1</id>
    <updated>2025-07-28T16:50:33Z</updated>
    <published>2025-07-28T16:50:33Z</published>
    <title>PyBird-JAX: Accelerated inference in large-scale structure with
  model-independent emulation of one-loop galaxy power spectra</title>
    <summary>  We present $\texttt{PyBird-JAX}$, a differentiable, $\texttt{JAX}$-based
implementation of $\texttt{PyBird}$, using internal neural network emulators to
accelerate computationally costly operations for rapid large-scale structure
(LSS) analysis. $\texttt{PyBird-JAX}$ computes one-loop EFTofLSS predictions
for redshift-space galaxy power spectrum multipoles in 1.2 ms on a CPU and 0.2
ms on a GPU, achieving 3-4 orders of magnitude speed-up over $\texttt{PyBird}$.
The emulators take a compact spline-based representation of the input linear
power spectrum $P(k)$ as feature vectors, making the approach applicable to a
wide range of cosmological models. We rigorously validate its accuracy against
large-volume simulations and on BOSS data, including cosmologies not explicitly
represented in the training set. Leveraging automatic differentiation,
$\texttt{PyBird-JAX}$ supports Fisher forecasting, Taylor expansion of model
predictions, gradient-based searches, and vectorised ensemble sampling.
Interfaced with a variety of samplers and Boltzmann solvers,
$\texttt{PyBird-JAX}$ provides a high-performance, end-to-end inference
pipeline. Combined with a symbolic-$P(k)$ generator, a typical Stage-4 LSS MCMC
converges in minutes on a GPU. Our results demonstrate that
$\texttt{PyBird-JAX}$ delivers the precision and speed required for upcoming
LSS surveys, opening the door to accelerated cosmological inference with
minimal accuracy loss and no pretraining. In a companion paper [1], we put
$\texttt{PyBird-JAX}$ to use in achieving LSS marginalised constraints free
from volume projection effects through non-flat measures.
</summary>
    <author>
      <name>Alexander Reeves</name>
    </author>
    <author>
      <name>Pierre Zhang</name>
    </author>
    <author>
      <name>Henry Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 + 14 pages, 9 figures, 4 tables. $\texttt{PyBird-JAX}$ code is
  available at https://github.com/pierrexyz/pybird</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.20990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.20990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.20991v1</id>
    <updated>2025-07-28T16:50:33Z</updated>
    <published>2025-07-28T16:50:33Z</published>
    <title>Debiasing inference in large-scale structure with non-flat volume
  measures</title>
    <summary>  Increasingly large parameter spaces, used to more accurately model precision
observables in physics, can paradoxically lead to large deviations in the
inferred parameters of interest -- a bias known as volume projection effects --
when marginalising over many nuisance parameters. For posterior distributions
that admit a Laplace expansion, we show that this artefact of Bayesian
inference can be mitigated by defining expectation values with respect to a
non-flat volume measure, such that the posterior mean becomes unbiased on
average. We begin by finding a measure that ensures the mean is an unbiased
estimator of the mode. Although the mode itself, as we rediscover, is biased
under sample averaging, this choice yields the least biased estimator due to a
cancellation we clarify. We further explain why bias in marginal posteriors can
appear relatively large, yet remains correctable, when the number of nuisances
is large. To demonstrate our approach, we present mock analyses in large-scale
structure (LSS) wherein cosmological parameters are subject to large projection
effects (at the 1-2$\sigma$ level) under a flat measure, that are however
recovered at high fidelity ($&lt;0.1\sigma$) when estimated using non-flat
counterparts. Our cosmological analyses are enabled by $\texttt{PyBird-JAX}$, a
fast, differentiable pipeline for LSS developed in our companion paper [1].
</summary>
    <author>
      <name>Alexander Reeves</name>
    </author>
    <author>
      <name>Pierre Zhang</name>
    </author>
    <author>
      <name>Henry Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 + 9 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.20991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.20991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.20989v1</id>
    <updated>2025-07-28T16:47:48Z</updated>
    <published>2025-07-28T16:47:48Z</published>
    <title>Probing the Neutral Fraction of the Warm Ionized Medium via [NI] 5200</title>
    <summary>  Most of the ionized mass in the Milky Way is in the Warm Ionized Medium (WIM)
and not in the bright H~II regions. The WIM is traced by dispersion measure and
has been extensively studied in recombination lines (primarily, H$\alpha$) and
optical nebular lines (primarily, S+ and N+). The observations can be well
explained by a photo-ionized nebula with a low ionization parameter. It is
generally thought that the source of ionization (and heating) of the WIM is due
to Lyman continuum leaking from HII regions which are concentrated in the
Galactic plane. The rays of the diffuse Galactic Lyman-continuum radiation
field incident on the Warm Neutral Medium (WNM) are absorbed, forming an
ionized skin. In nebulae with low-ionization parameter the transition from
ionized gas to neutral gas is gradual, unlike the case for HII regions with
their sharp Stromgren spheres. The transition region is warm enough to excite
oxygen and nitrogen atoms to emit [OI] 6300,6363 and [NI] 5198,5200. Domgorgen
&amp; Mathis (1994) recognized the value of [OI] 6300 as a diagnostic of the
fraction of the diffuse continuum that is absorbed by the WNM and therefore
constrains the fraction of the diffuse Lyman continuum that escapes to the
halo. Unfortunately, observations of Galactic [OI] 6300 have been stymied by
bright [OI] 6300 airglow emission. [NI] 5200,5198 has been a historically less
popular probe because this doublet is less luminous than the oxygen doublet.
However, we point out that the [NI] airglow is two orders of magnitude smaller
than that of [OI]. Furthermore, even in the presence of comparable airglow, the
WIM [NI] emission can be inferred using the doublet intensity ratio for which a
medium-resolution spectrometer such as the Local Volume Mapper will suffice.
Separately, we note, in extragalactic systems, that [OI]6300/[NI]5200 is a
robust measure of the O/N abundance ratio.
</summary>
    <author>
      <name>S. R. Kulkarni</name>
    </author>
    <author>
      <name>S. Noll</name>
    </author>
    <author>
      <name>W. Kausch</name>
    </author>
    <author>
      <name>Soumyadeep Bhattacharjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages; 18 figures; 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.20989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.20989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.20985v1</id>
    <updated>2025-07-28T16:45:32Z</updated>
    <published>2025-07-28T16:45:32Z</published>
    <title>Behavioral Study of Dashboard Mechanisms</title>
    <summary>  Visualization dashboards are increasingly used in strategic settings like
auctions to enhance decision-making and reduce strategic confusion. This paper
presents behavioral experiments evaluating how different dashboard designs
affect bid optimization in reverse first-price auctions. Additionally, we
assess how dashboard designs impact the auction designer's ability to
accurately infer bidders' preferences within the dashboard mechanism framework.
We compare visualizations of the bid allocation rule, commonly deployed in
practice, to alternatives that display expected utility. We find that
utility-based visualizations significantly improve bidding by reducing
cognitive demands on bidders. However, even with improved dashboards, bidders
systematically under-shade their bids, driven by an implicit preference for
certain wins in uncertain settings. As a result, dashboard-based mechanisms
that assume fully rational or risk-neutral bidder responses to dashboards can
produce significant estimation errors when inferring private preferences, which
may lead to suboptimal allocations in practice. Explicitly modeling agents'
behavioral responses to dashboards substantially improves inference accuracy,
highlighting the need to align visualization design and econometric inference
assumptions in practice.
</summary>
    <author>
      <name>Paula Kayongo</name>
    </author>
    <author>
      <name>Jessica Hullman</name>
    </author>
    <author>
      <name>Jason Hartline</name>
    </author>
    <link href="http://arxiv.org/abs/2507.20985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.20985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
