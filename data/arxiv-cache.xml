<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-27T00:50:51Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-26T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">121821</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.21319v1</id>
    <updated>2025-09-25T16:19:06Z</updated>
    <published>2025-09-25T16:19:06Z</published>
    <title>RLBFF: Binary Flexible Feedback to bridge between Human Feedback &amp;
  Verifiable Rewards</title>
    <summary>  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at &lt;5% of the inference cost).
</summary>
    <author>
      <name>Zhilin Wang</name>
    </author>
    <author>
      <name>Jiaqi Zeng</name>
    </author>
    <author>
      <name>Olivier Delalleau</name>
    </author>
    <author>
      <name>Ellie Evans</name>
    </author>
    <author>
      <name>Daniel Egert</name>
    </author>
    <author>
      <name>Hoo-Chang Shin</name>
    </author>
    <author>
      <name>Felipe Soares</name>
    </author>
    <author>
      <name>Yi Dong</name>
    </author>
    <author>
      <name>Oleksii Kuchaiev</name>
    </author>
    <link href="http://arxiv.org/abs/2509.21319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21305v1</id>
    <updated>2025-09-25T15:19:39Z</updated>
    <published>2025-09-25T15:19:39Z</published>
    <title>Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors
  in LLMs</title>
    <summary>  Large language models (LLMs) often exhibit sycophantic behaviors -- such as
excessive agreement with or flattery of the user -- but it is unclear whether
these behaviors arise from a single mechanism or multiple distinct processes.
We decompose sycophancy into sycophantic agreement and sycophantic praise,
contrasting both with genuine agreement. Using difference-in-means directions,
activation additions, and subspace geometry across multiple models and
datasets, we show that: (1) the three behaviors are encoded along distinct
linear directions in latent space; (2) each behavior can be independently
amplified or suppressed without affecting the others; and (3) their
representational structure is consistent across model families and scales.
These results suggest that sycophantic behaviors correspond to distinct,
independently steerable representations.
</summary>
    <author>
      <name>Daniel Vennemeyer</name>
    </author>
    <author>
      <name>Phan Anh Duong</name>
    </author>
    <author>
      <name>Tiffany Zhan</name>
    </author>
    <author>
      <name>Tianyu Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2509.21305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21302v1</id>
    <updated>2025-09-25T15:17:11Z</updated>
    <published>2025-09-25T15:17:11Z</published>
    <title>Quantized Visual Geometry Grounded Transformer</title>
    <summary>  Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs), have made remarkable progress with the use of
large-scale transformers. Their prohibitive computational and memory costs
severely hinder real-world deployment. Post-Training Quantization (PTQ) has
become a common practice for compressing and accelerating models. However, we
empirically observe that PTQ faces unique obstacles when compressing
billion-scale VGGTs: the data-independent special tokens induce heavy-tailed
activation distributions, while the multi-view nature of 3D data makes
calibration sample selection highly unstable. This paper proposes the first
Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly. Second, we design Noise-Filtered Diverse Sampling, which
filters outliers via deep-layer statistics and constructs frame-aware diverse
calibration clusters to ensure stable quantization ranges. Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin. We highlight
that our 4-bit QuantVGGT can deliver a 3.7$\times$ memory reduction and
2.5$\times$ acceleration in real-hardware inference, while maintaining
reconstruction accuracy above 98\% of its full-precision counterpart. This
demonstrates the vast advantages and practicality of QuantVGGT in
resource-constrained scenarios. Our code is released in
https://github.com/wlfeng0509/QuantVGGT.
</summary>
    <author>
      <name>Weilun Feng</name>
    </author>
    <author>
      <name>Haotong Qin</name>
    </author>
    <author>
      <name>Mingqiang Wu</name>
    </author>
    <author>
      <name>Chuanguang Yang</name>
    </author>
    <author>
      <name>Yuqi Li</name>
    </author>
    <author>
      <name>Xiangqi Li</name>
    </author>
    <author>
      <name>Zhulin An</name>
    </author>
    <author>
      <name>Libo Huang</name>
    </author>
    <author>
      <name>Yulun Zhang</name>
    </author>
    <author>
      <name>Michele Magno</name>
    </author>
    <author>
      <name>Yongjun Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2509.21302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21301v1</id>
    <updated>2025-09-25T15:17:05Z</updated>
    <published>2025-09-25T15:17:05Z</published>
    <title>Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive
  Cross-Stage Parallelization</title>
    <summary>  This paper presents Nova, a real-time scheduling framework for serving
agentic vision-language models (VLMs) on a single GPU with balanced per-request
latency and overall request process throughput. Our design begins by enabling
effective pipelining across vision encode, LLM prefill, and LLM decode stages
of VLMs, by exploiting their heterogeneous resource demands during execution
and incorporating elastic GPU spatial partitioning among stages to maximally
utilize the compute and memory resources. Building on this, we introduce a
real-time scheduling algorithm that adaptively calibrates resource allocation
among stages based on a Pareto-optimal analysis of the latency-throughput
trade-off, allowing the system to sustain responsiveness and resource
efficiency under dynamic request loads. To further alleviate GPU memory
pressure, we design a lightweight weight offloading strategy for vision
encoders that preserves inference efficiency with minimized memory overhead.
Extensive evaluations on both synthetic and real-world agent workloads
demonstrate that Nova consistently outperforms the state-of-the-art baselines,
improving the maximum latency by up to 23.3%, while keeping competitive
throughput.
</summary>
    <author>
      <name>Yuhang Xu</name>
    </author>
    <author>
      <name>Shengzhong Liu</name>
    </author>
    <author>
      <name>Dong Zhang</name>
    </author>
    <author>
      <name>Bingheng Yan</name>
    </author>
    <author>
      <name>Fan Wu</name>
    </author>
    <author>
      <name>Guihai Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2509.21301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21260v1</id>
    <updated>2025-09-25T14:54:23Z</updated>
    <published>2025-09-25T14:54:23Z</published>
    <title>A Causality-Aware Spatiotemporal Model for Multi-Region and
  Multi-Pollutant Air Quality Forecasting</title>
    <summary>  Air pollution, a pressing global problem, threatens public health,
environmental sustainability, and climate stability. Achieving accurate and
scalable forecasting across spatially distributed monitoring stations is
challenging due to intricate multi-pollutant interactions, evolving
meteorological conditions, and region specific spatial heterogeneity. To
address this challenge, we propose AirPCM, a novel deep spatiotemporal
forecasting model that integrates multi-region, multi-pollutant dynamics with
explicit meteorology-pollutant causality modeling. Unlike existing methods
limited to single pollutants or localized regions, AirPCM employs a unified
architecture to jointly capture cross-station spatial correlations, temporal
auto-correlations, and meteorology-pollutant dynamic causality. This empowers
fine-grained, interpretable multi-pollutant forecasting across varying
geographic and temporal scales, including sudden pollution episodes. Extensive
evaluations on multi-scale real-world datasets demonstrate that AirPCM
consistently surpasses state-of-the-art baselines in both predictive accuracy
and generalization capability. Moreover, the long-term forecasting capability
of AirPCM provides actionable insights into future air quality trends and
potential high-risk windows, offering timely support for evidence-based
environmental governance and carbon mitigation planning.
</summary>
    <author>
      <name>Junxin Lu</name>
    </author>
    <author>
      <name>Shiliang Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.21260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21259v1</id>
    <updated>2025-09-25T14:53:36Z</updated>
    <published>2025-09-25T14:53:36Z</published>
    <title>Semantic Edge-Cloud Communication for Real-Time Urban Traffic
  Surveillance with ViT and LLMs over Mobile Networks</title>
    <summary>  Real-time urban traffic surveillance is vital for Intelligent Transportation
Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle
trajectories, and prevent collisions in smart cities. Deploying edge cameras
across urban environments is a standard practice for monitoring road
conditions. However, integrating these with intelligent models requires a
robust understanding of dynamic traffic scenarios and a responsive interface
for user interaction. Although multimodal Large Language Models (LLMs) can
interpret traffic images and generate informative responses, their deployment
on edge devices is infeasible due to high computational demands. Therefore, LLM
inference must occur on the cloud, necessitating visual data transmission from
edge to cloud, a process hindered by limited bandwidth, leading to potential
delays that compromise real-time performance. To address this challenge, we
propose a semantic communication framework that significantly reduces
transmission overhead. Our method involves detecting Regions of Interest (RoIs)
using YOLOv11, cropping relevant image segments, and converting them into
compact embedding vectors using a Vision Transformer (ViT). These embeddings
are then transmitted to the cloud, where an image decoder reconstructs the
cropped images. The reconstructed images are processed by a multimodal LLM to
generate traffic condition descriptions. This approach achieves a 99.9%
reduction in data transmission size while maintaining an LLM response accuracy
of 89% for reconstructed cropped images, compared to 93% accuracy with original
cropped images. Our results demonstrate the efficiency and practicality of ViT
and LLM-assisted edge-cloud semantic communication for real-time traffic
surveillance.
</summary>
    <author>
      <name>Murat Arda Onsu</name>
    </author>
    <author>
      <name>Poonam Lohan</name>
    </author>
    <author>
      <name>Burak Kantarci</name>
    </author>
    <author>
      <name>Aisha Syed</name>
    </author>
    <author>
      <name>Matthew Andrews</name>
    </author>
    <author>
      <name>Sean Kennedy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.21259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21251v1</id>
    <updated>2025-09-25T14:45:06Z</updated>
    <published>2025-09-25T14:45:06Z</published>
    <title>Instruction-tuned Self-Questioning Framework for Multimodal Reasoning</title>
    <summary>  The field of vision-language understanding has been actively researched in
recent years, thanks to the development of Large Language Models~(LLMs).
However, it still needs help with problems requiring multi-step reasoning, even
for very simple questions. Recent studies adopt LLMs to tackle this problem by
iteratively generating sub-questions and answers. However, there are
disadvantages such as 1) the fine-grained visual contents of images are not
available using LLMs that cannot read visual information, 2) internal
mechanisms are inaccessible and difficult to reproduce by using black-box LLMs.
To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,
which improves inference performance by generating image-aware informative
sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists
of a Questioner, Answerer, and Reasoner that share the same architecture.
Questioner and Answerer generate sub-questions and sub-answers to help infer
the main-question, and Reasoner performs reasoning on the main-question
considering the generated sub-question information. Our experiments show that
the proposed method SQ-InstructBLIP, which uses the generated sub-questions as
additional information when solving the VQA task, performs more accurate
reasoning than the previous works.
</summary>
    <author>
      <name>You-Won Jang</name>
    </author>
    <author>
      <name>Yu-Jung Heo</name>
    </author>
    <author>
      <name>Jaeseok Kim</name>
    </author>
    <author>
      <name>Minsu Lee</name>
    </author>
    <author>
      <name>Du-Seong Chang</name>
    </author>
    <author>
      <name>Byoung-Tak Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted to the "CLVL: 5th Workshop on Closing the
  Loop Between Vision and Language (ICCV 2023 CLVL workshop)."</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.21251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21250v1</id>
    <updated>2025-09-25T14:45:02Z</updated>
    <published>2025-09-25T14:45:02Z</published>
    <title>Federated Flow Matching</title>
    <summary>  Data today is decentralized, generated and stored across devices and
institutions where privacy, ownership, and regulation prevent centralization.
This motivates the need to train generative models directly from distributed
data locally without central aggregation. In this paper, we introduce Federated
Flow Matching (FFM), a framework for training flow matching models under
privacy constraints. Specifically, we first examine FFM-vanilla, where each
client trains locally with independent source and target couplings, preserving
privacy but yielding curved flows that slow inference. We then develop FFM-LOT,
which employs local optimal transport couplings to improve straightness within
each client but lacks global consistency under heterogeneous data. Finally, we
propose FFM-GOT, a federated strategy based on the semi-dual formulation of
optimal transport, where a shared global potential function coordinates
couplings across clients. Experiments on synthetic and image datasets show that
FFM enables privacy-preserving training while enhancing both the flow
straightness and sample quality in federated settings, with performance
comparable to the centralized baseline.
</summary>
    <author>
      <name>Zifan Wang</name>
    </author>
    <author>
      <name>Anqi Dong</name>
    </author>
    <author>
      <name>Mahmoud Selim</name>
    </author>
    <author>
      <name>Michael M. Zavlanos</name>
    </author>
    <author>
      <name>Karl H. Johansson</name>
    </author>
    <link href="http://arxiv.org/abs/2509.21250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21233v1</id>
    <updated>2025-09-25T14:34:34Z</updated>
    <published>2025-09-25T14:34:34Z</published>
    <title>RadioSED II: discovering the peaked spectrum radio sources in Stripe 82</title>
    <summary>  This paper is the second in a series presenting \textsc{RadioSED}, a Bayesian
inference framework for constructing, modelling and classifying radio spectral
energy distributions from publicly-available surveys. We focus here on the
application of our framework to SDSS Stripe 82. Not only do we recover all
eleven previously-published peaked spectrum sources from the literature within
this region, but we increase the number of known peaked spectrum sources here
by more than an order of magnitude. We investigate the variability properties
of our peaked spectrum sample, and find that overall they exhibit a low degree
of variability, consistent with previous samples of peaked spectrum active
galactic nuclei. The multiwavelength properties of these sources reveal that we
have selected a population comprising largely distant ($z \geq 1$), powerful
active galaxies. We find that the most compact jets are located preferentially
in quasar-type hosts, with galaxy-type hosts home to slightly more extended
radio structures. We discuss these findings in the context of current and
forthcoming large area radio surveys.
</summary>
    <author>
      <name>E. F. Kerrison</name>
    </author>
    <author>
      <name>E. M. Sadler</name>
    </author>
    <author>
      <name>V. A. Moss</name>
    </author>
    <author>
      <name>E. K. Mahony</name>
    </author>
    <author>
      <name>L. Driessen</name>
    </author>
    <author>
      <name>K. Ross</name>
    </author>
    <author>
      <name>K. Rose</name>
    </author>
    <author>
      <name>D. Dobie</name>
    </author>
    <author>
      <name>T. Murphy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 16 figures, 3 tables. Three appendices of online material.
  Accepted for publication in MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.21233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.21226v1</id>
    <updated>2025-09-25T14:30:42Z</updated>
    <published>2025-09-25T14:30:42Z</published>
    <title>Bayesian inference for velocity-jump models for movement</title>
    <summary>  The velocity-jump model is a specific type of piecewise deterministic Markov
process in which an individual's velocity is constant except at times that form
the events of some point process. It represents an interpretable
continuous-time version of the discrete-time `step and turn' models widely used
in analysing wildlife telemetry. In this paper, I derive a reversible jump
Markov chain Monte Carlo algorithm to carry out exact Bayesian inference for
velocity-jump models by reconstructing the trajectories between observations,
and illustrate its use in analysing real and simulated telemetry data. The
method uses a proposal distribution for updating velocities that is constructed
by approximating the movement model with a multivariate normal distribution and
then conditioning that distribution on the data. The velocity-jump models
considered can incorporate measurement error and Markov dependence between
successive velocities.
</summary>
    <author>
      <name>Paul G. Blackwell</name>
    </author>
    <link href="http://arxiv.org/abs/2509.21226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.21226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
