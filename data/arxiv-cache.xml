<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-01-15T01:00:27Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-01-15T01:00:30Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>130812</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2601.08831v1</id>
    <title>3AM: Segment Anything with Geometric Consistency in Videos</title>
    <updated>2026-01-13T18:59:54Z</updated>
    <link href="https://arxiv.org/abs/2601.08831v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08831v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T18:59:54Z</published>
    <arxiv:comment>Project page: https://jayisaking.github.io/3AM-Page/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yang-Che Sun</name>
    </author>
    <author>
      <name>Cheng Sun</name>
    </author>
    <author>
      <name>Chin-Yang Lin</name>
    </author>
    <author>
      <name>Fu-En Yang</name>
    </author>
    <author>
      <name>Min-Hung Chen</name>
    </author>
    <author>
      <name>Yen-Yu Lin</name>
    </author>
    <author>
      <name>Yu-Lun Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08800v1</id>
    <title>MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm</title>
    <updated>2026-01-13T18:38:18Z</updated>
    <link href="https://arxiv.org/abs/2601.08800v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08800v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node &amp; multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.
  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T18:38:18Z</published>
    <arxiv:comment>Submitted to ICDCS 2026</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <author>
      <name>Jinrui Jia</name>
    </author>
    <author>
      <name>Wenhao He</name>
    </author>
    <author>
      <name>Yong Zhang</name>
    </author>
    <author>
      <name>Fang Dong</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08770v1</id>
    <title>Memory DisOrder: Memory Re-orderings as a Timerless Side-channel</title>
    <updated>2026-01-13T17:59:28Z</updated>
    <link href="https://arxiv.org/abs/2601.08770v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08770v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>To improve efficiency, nearly all parallel processing units (CPUs and GPUs) implement relaxed memory models in which memory operations may be re-ordered, i.e., executed out-of-order. Prior testing work in this area found that memory re-orderings are observed more frequently when other cores are active, e.g., stressing the memory system, which likely triggers aggressive hardware optimizations.
  In this work, we present Memory DisOrder: a timerless side-channel that uses memory re-orderings to infer activity on other processes. We first perform a fuzzing campaign and show that many mainstream processors (X86/Arm/Apple CPUs, NVIDIA/AMD/Apple GPUs) are susceptible to cross-process signals. We then show how the vulnerability can be used to implement classic attacks, including a covert channel, achieving up to 16 bits/second with 95% accuracy on an Apple M3 GPU, and application fingerprinting, achieving reliable closed-world DNN architecture fingerprinting on several CPUs and an Apple M3 GPU. Finally, we explore how low-level system details can be exploited to increase re-orderings, showing the potential for a covert channel to achieve nearly 30K bits/second on X86 CPUs. More precise attacks can likely be developed as the vulnerability becomes better understood.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T17:59:28Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Sean Siddens</name>
    </author>
    <author>
      <name>Sanya Srivastava</name>
    </author>
    <author>
      <name>Reese Levine</name>
    </author>
    <author>
      <name>Josiah Dykstra</name>
    </author>
    <author>
      <name>Tyler Sorensen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08743v1</id>
    <title>TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL</title>
    <updated>2026-01-13T17:20:55Z</updated>
    <link href="https://arxiv.org/abs/2601.08743v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08743v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T17:20:55Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jinbo Su</name>
    </author>
    <author>
      <name>Yuxuan Hu</name>
    </author>
    <author>
      <name>Cuiping Li</name>
    </author>
    <author>
      <name>Hong Chen</name>
    </author>
    <author>
      <name>Jia Li</name>
    </author>
    <author>
      <name>Lintao Ma</name>
    </author>
    <author>
      <name>Jing Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08742v1</id>
    <title>Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents</title>
    <updated>2026-01-13T17:18:38Z</updated>
    <link href="https://arxiv.org/abs/2601.08742v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08742v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T17:18:38Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Xin Quan</name>
    </author>
    <author>
      <name>Jiafeng Xiong</name>
    </author>
    <author>
      <name>Marco Valentino</name>
    </author>
    <author>
      <name>André Freitas</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08724v1</id>
    <title>Kernel Learning for Regression via Quantum Annealing Based Spectral Sampling</title>
    <updated>2026-01-13T16:50:07Z</updated>
    <link href="https://arxiv.org/abs/2601.08724v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08724v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While quantum annealing (QA) has been developed for combinatorial optimization, practical QA devices operate at finite temperature and under noise, and their outputs can be regarded as stochastic samples close to a Gibbs--Boltzmann distribution. In this study, we propose a QA-in-the-loop kernel learning framework that integrates QA not merely as a substitute for Markov-chain Monte Carlo sampling but as a component that directly determines the learned kernel for regression. Based on Bochner's theorem, a shift-invariant kernel is represented as an expectation over a spectral distribution, and random Fourier features (RFF) approximate the kernel by sampling frequencies. We model the spectral distribution with a (multi-layer) restricted Boltzmann machine (RBM), generate discrete RBM samples using QA, and map them to continuous frequencies via a Gaussian--Bernoulli transformation. Using the resulting RFF, we construct a data-adaptive kernel and perform Nadaraya--Watson (NW) regression. Because the RFF approximation based on $\cos(\bmω^{\top}Δ\bm{x})$ can yield small negative values and cancellation across neighbors, the Nadaraya--Watson denominator $\sum_j k_{ij}$ may become close to zero. We therefore employ nonnegative squared-kernel weights $w_{ij}=k(\bm{x}_i,\bm{x}_j)^2$, which also enhances the contrast of kernel weights. The kernel parameters are trained by minimizing the leave-one-out NW mean squared error, and we additionally evaluate local linear regression with the same squared-kernel weights at inference. Experiments on multiple benchmark regression datasets demonstrate a decrease in training loss, accompanied by structural changes in the kernel matrix, and show that the learned kernel tends to improve $R^2$ and RMSE over the baseline Gaussian-kernel NW. Increasing the number of random features at inference further enhances accuracy.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T16:50:07Z</published>
    <arxiv:comment>15pages, 5 figures, 4 tables</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Yasushi Hasegawa</name>
    </author>
    <author>
      <name>Masayuki Ohzeki</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08698v1</id>
    <title>Double Strike: Breaking Approximation-Based Side-Channel Countermeasures for DNNs</title>
    <updated>2026-01-13T16:24:58Z</updated>
    <link href="https://arxiv.org/abs/2601.08698v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08698v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep neural networks (DNNs), which support services such as driving assistants and medical diagnoses, undergo lengthy and expensive training procedures. Therefore, the training's outcome - the DNN weights - represents a significant intellectual property asset to protect. Side-channel analysis (SCA) has recently appeared as an effective approach to recover this confidential asset from DNN implementations. In response, researchers have proposed to defend DNN implementations through classic side-channel countermeasures, at the cost of higher energy consumption, inference time, and resource utilisation. Following a different approach, Ding et al. (HOST'25) introduced MACPRUNING, a novel SCA countermeasure based on pruning, a performance-oriented Approximate Computing technique: at inference time, the implementation randomly prunes (or skips) non-important weights (i.e., with low contribution to the DNN's accuracy) of the first layer, exponentially increasing the side-channel resilience of the protected DNN implementation. However, the original security analysis of MACPRUNING did not consider a control-flow dependency intrinsic to the countermeasure design. This dependency may allow an attacker to circumvent MACPRUNING and recover the weights important to the DNN's accuracy. This paper describes a preprocessing methodology to exploit the above-mentioned control-flow dependency. Through practical experiments on a Chipwhisperer-Lite running a MACPRUNING-protected Multi-Layer Perceptron, we target the first 8 weights of each neuron and recover 96% of the important weights, demonstrating the drastic reduction in security of the protected implementation. Moreover, we show how microarchitectural leakage improves the effectiveness of our methodology, even allowing for the recovery of up to 100% of the targeted non-important weights. Lastly, by adapting our methodology [continue in pdf].</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T16:24:58Z</published>
    <arxiv:comment>To appear at IEEE International Symposium on Hardware Oriented Security and Trust (HOST) 2026. 10 pages, 7 figures, 4 algorithms, 1 code listing</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Lorenzo Casalino</name>
    </author>
    <author>
      <name>Maria Méndez Real</name>
    </author>
    <author>
      <name>Jean-Christophe Prévotet</name>
    </author>
    <author>
      <name>Rubén Salvador</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08694v1</id>
    <title>Precise measurement of the $Λ$-binding energy difference between $^3_Λ$H and $^4_Λ$H via decay-pion spectroscopy at MAMI</title>
    <updated>2026-01-13T16:17:56Z</updated>
    <link href="https://arxiv.org/abs/2601.08694v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08694v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We performed high-precision decay-pion spectroscopy of light $Λ$ hypernuclei at the Mainz Microtron (MAMI) using the A1 spectrometer facility. By measuring the monochromatic $π^-$ momentum from the two-body weak decay $^3_Λ\mathrm{H} \to {}^3\mathrm{He} + π^-$ and referencing it to the $^4_Λ\mathrm{H} \to {}^4\mathrm{He} + π^-$ decay, we determined the $Λ$ binding energy of $^3_Λ\mathrm{H}$ with unprecedented accuracy. The obtained value, $B_Λ(^3_Λ\mathrm{H}) = 0.523 \pm 0.013~(\mathrm{stat.}) \pm 0.075~(\mathrm{syst.})$~MeV, is consistent with the STAR result, but indicates a significantly deeper binding than inferred from earlier measurements. This result implies a stronger $Λ$-deuteron interaction and provides stringent constraints on hyperon-nucleon interactions.</summary>
    <category term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T16:17:56Z</published>
    <arxiv:primary_category term="nucl-ex"/>
    <author>
      <name>Ryoko Kino</name>
    </author>
    <author>
      <name>Sho Nagao</name>
    </author>
    <author>
      <name>Patrick Achenbach</name>
    </author>
    <author>
      <name>Satoshi N. Nakamura</name>
    </author>
    <author>
      <name>Josef Pochodzalla</name>
    </author>
    <author>
      <name>Takeru Akiyama</name>
    </author>
    <author>
      <name>Ralph Böhm</name>
    </author>
    <author>
      <name>Mirco Christmann</name>
    </author>
    <author>
      <name>Michael O. Distler</name>
    </author>
    <author>
      <name>Luca Doria</name>
    </author>
    <author>
      <name>Anselm Esser</name>
    </author>
    <author>
      <name>Julian Geratz</name>
    </author>
    <author>
      <name>Christian Helmel</name>
    </author>
    <author>
      <name>Matthias Hoek</name>
    </author>
    <author>
      <name>Tatsuhiro Ishige</name>
    </author>
    <author>
      <name>Masashi Kaneta</name>
    </author>
    <author>
      <name>Pascal Klag</name>
    </author>
    <author>
      <name>David Markus</name>
    </author>
    <author>
      <name>Harald Merkel</name>
    </author>
    <author>
      <name>Masaya Mizuno</name>
    </author>
    <author>
      <name>Ulrich Müller</name>
    </author>
    <author>
      <name>Kotaro Nishi</name>
    </author>
    <author>
      <name>Ken Nishida</name>
    </author>
    <author>
      <name>Kazuki Okuyama</name>
    </author>
    <author>
      <name>Jonas Pätschke</name>
    </author>
    <author>
      <name>Björn Sören Schlimme</name>
    </author>
    <author>
      <name>Concettina Sfienti</name>
    </author>
    <author>
      <name>Tianhao Shao</name>
    </author>
    <author>
      <name>Daniel Steger</name>
    </author>
    <author>
      <name>Marcell Steinen</name>
    </author>
    <author>
      <name>Liguang Tang</name>
    </author>
    <author>
      <name>Michaela Thiel</name>
    </author>
    <author>
      <name>Philipp Vonwirth</name>
    </author>
    <author>
      <name>Luca Wilhelm</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08693v1</id>
    <title>Stellar masses of optically dark galaxies: uncertainty introduced by the attenuation law and star-formation histories</title>
    <updated>2026-01-13T16:17:04Z</updated>
    <link href="https://arxiv.org/abs/2601.08693v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08693v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>JWST observations have suggested that some high-redshift galaxies may be ultra-massive, thereby challenging standard models of early galaxy formation and cosmology. We analyse the stellar masses using different modelling assumptions and with new data of three galaxies (S1, S2 and S3), whose NIRCam/grism redshifts were consistent with $z&gt;5$. These three optically dark galaxies have previously been reported to host exceptionally high stellar masses and star-formation rates, implying extremely high star-formation efficiencies. Recent NIRSpec/IFU observations for S1 indicate a spectroscopic redshift of $z_{\rm spec}=3.2461^{+0.0001}_{-0.0002}$, which is lower than previously reported. Using the Bayesian spectral energy distribution (SED) modelling tool \texttt{Prospector}, we investigate the impact of key model assumptions on stellar mass estimates, such as the choice of star-formation history (SFH) priors (constant versus rising SFH base for the non-parametric prior), the dust attenuation law, and the treatment of emission line fluxes. Our analysis yields revised stellar masses of $\log(M_{\star}/M_{\odot}) \approx 10.36^{+0.47}_{-0.32}, 10.95^{+0.11}_{-0.10}$ and $10.31^{+0.24}_{-0.19}$ for S1, S2, and S3, respectively. We find that adopting a rising SFH base prior results in lower inferred stellar masses compared to a constant SFH base prior. We identify a significant degeneracy between the dust attenuation curve slope, the amount of dust attenuation, and stellar mass. Our results highlight various systematics in SED modelling due to SFH priors and dust attenuation that can influence stellar mass estimates of heavily dust obscured sources. Nevertheless, even with these revised stellar mass estimates, two of the three galaxies remain among the most massive and actively star-forming systems at their respective redshifts, implying high star-formation efficiencies.</summary>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T16:17:04Z</published>
    <arxiv:comment>13 pages, 8 figures, to be submitted to MNRAS, comments are welcome</arxiv:comment>
    <arxiv:primary_category term="astro-ph.GA"/>
    <author>
      <name>Yash Lapasia</name>
    </author>
    <author>
      <name>Sandro Tacchella</name>
    </author>
    <author>
      <name>Francesco D'Eugenio</name>
    </author>
    <author>
      <name>Dávid Puskás</name>
    </author>
    <author>
      <name>Andrew J. Bunker</name>
    </author>
    <author>
      <name>A. Lola Danhaive</name>
    </author>
    <author>
      <name>Benjamin D. Johnson</name>
    </author>
    <author>
      <name>Roberto Maiolino</name>
    </author>
    <author>
      <name>Brant Robertson</name>
    </author>
    <author>
      <name>Charlotte Simmonds</name>
    </author>
    <author>
      <name>Irene Shivaei</name>
    </author>
    <author>
      <name>Christina C. Williams</name>
    </author>
    <author>
      <name>Christopher Willmer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.08671v1</id>
    <title>Ultrafast quantum optics with attosecond control</title>
    <updated>2026-01-13T15:47:48Z</updated>
    <link href="https://arxiv.org/abs/2601.08671v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.08671v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modern Quantum optics largely remains quasi-stationary, far from intrinsic optical field timescales. Ultrafast quantum optics seeks to generate, shape, and measure quantum states of light on femtosecond and attosecond timescales. Here we introduce a quantum light field squeezer (QLFS) that enables the generation and attosecond control of ultrafast broadband squeezed light. Using degenerate four-wave mixing in a quasi-collinear focusing geometry, our approach overcomes conventional broadband phase-matching limits, producing intensity- and phase-squeezed states directly from few-cycle laser pulses. Our ultrafast quantum optical metrology reveals a time-dependent squeezing distribution across individual half-cycles of the electric field. Incorporating this time-dependent squeezing into strong-field simulations shows that the temporal redistribution of quantum uncertainty reshapes the high-harmonic emission. Moreover, by tuning the relative pulse delay and phase-matching angle, we achieve attosecond precision in controlling the squeezing characteristics by visualizing inferred effective Wigner representations of the quantum light field. Beyond characterization, we demonstrate that the quantum light-induced tunneling-current noise is sensitive to the nonclassical intensity-noise statistics of the driving squeezed light, with sub-femtosecond control. Together, these results extend the generation, control, and effective phase-space representation of squeezed light into the ultrafast and attosecond regime, opening new avenues for quantum optics in strong-field and solid-state systems.</summary>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-13T15:47:48Z</published>
    <arxiv:comment>36 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="physics.optics"/>
    <author>
      <name>Mohamed Sennary</name>
    </author>
    <author>
      <name>Javier Rivera-Dean</name>
    </author>
    <author>
      <name>Maciej Lewenstein</name>
    </author>
    <author>
      <name>Mohammed Th. Hassan</name>
    </author>
  </entry>
</feed>
