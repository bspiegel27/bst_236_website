<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-19T01:00:12Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-19T01:00:13Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>128926</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.15713v1</id>
    <title>DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</title>
    <updated>2025-12-17T18:59:55Z</updated>
    <link href="https://arxiv.org/abs/2512.15713v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15713v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:59:55Z</published>
    <arxiv:comment>11 pages, 5 figures, conference or other essential info</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Lunbin Zeng</name>
    </author>
    <author>
      <name>Jingfeng Yao</name>
    </author>
    <author>
      <name>Bencheng Liao</name>
    </author>
    <author>
      <name>Hongyuan Tao</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15705v1</id>
    <title>Dynamic Rebatching for Efficient Early-Exit Inference with DREX</title>
    <updated>2025-12-17T18:55:45Z</updated>
    <link href="https://arxiv.org/abs/2512.15705v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15705v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:55:45Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Xuting Liu</name>
    </author>
    <author>
      <name>Daniel Alexander</name>
    </author>
    <author>
      <name>Siva Kesava Reddy Kakarla</name>
    </author>
    <author>
      <name>Behnaz Arzani</name>
    </author>
    <author>
      <name>Vincent Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15702v1</id>
    <title>End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</title>
    <updated>2025-12-17T18:53:29Z</updated>
    <link href="https://arxiv.org/abs/2512.15702v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15702v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:53:29Z</published>
    <arxiv:comment>Project Page: https://guoyww.github.io/projects/resampling-forcing/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yuwei Guo</name>
    </author>
    <author>
      <name>Ceyuan Yang</name>
    </author>
    <author>
      <name>Hao He</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Meng Wei</name>
    </author>
    <author>
      <name>Zhenheng Yang</name>
    </author>
    <author>
      <name>Weilin Huang</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15692v1</id>
    <title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
    <updated>2025-12-17T18:47:31Z</updated>
    <link href="https://arxiv.org/abs/2512.15692v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15692v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:47:31Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Jonas Pai</name>
    </author>
    <author>
      <name>Liam Achenbach</name>
    </author>
    <author>
      <name>Victoriano Montesinos</name>
    </author>
    <author>
      <name>Benedek Forrai</name>
    </author>
    <author>
      <name>Oier Mees</name>
    </author>
    <author>
      <name>Elvis Nava</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15689v1</id>
    <title>Error mitigation for logical circuits using decoder confidence</title>
    <updated>2025-12-17T18:45:27Z</updated>
    <link href="https://arxiv.org/abs/2512.15689v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15689v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Fault-tolerant quantum computers use decoders to monitor for errors and find a plausible correction. A decoder may provide a decoder confidence score (DCS) to gauge its success. We adopt a swim distance DCS, computed from the shortest path between syndrome clusters. By contracting tensor networks, we compare its performance to the well-known complementary gap and find that both reliably estimate the logical error probability (LEP) in a decoding window. We explore ways to use this to mitigate the LEP in entire circuits. For shallow circuits, we just abort if any decoding window produces an exceptionally low DCS: for a distance-13 surface code, rejecting a mere 0.1% of possible DCS values improves the entire circuit's LEP by more than 5 orders of magnitude. For larger algorithms comprising up to trillions of windows, DCS-based rejection remains effective for enhancing observable estimation. Moreover, one can use DCS to assign each circuit's output a unique LEP, and use it as a basis for maximum likelihood inference. This can reduce the effects of noise by an order of magnitude at no quantum cost; methods can be combined for further improvements.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:45:27Z</published>
    <arxiv:comment>16 pages (11 main, 5 appendix). 13 figures (8 main, 5 appendix)</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Maria Dincă</name>
    </author>
    <author>
      <name>Tim Chan</name>
    </author>
    <author>
      <name>Simon C. Benjamin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15676v1</id>
    <title>Data-driven controlled subgroup selection in clinical trials</title>
    <updated>2025-12-17T18:28:33Z</updated>
    <link href="https://arxiv.org/abs/2512.15676v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15676v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Subgroup selection in clinical trials is essential for identifying patient groups that react differently to a treatment, thereby enabling personalised medicine. In particular, subgroup selection can identify patient groups that respond particularly well to a treatment or that encounter adverse events more often. However, this is a post-selection inference problem, which may pose challenges for traditional techniques used for subgroup analysis, such as increased Type I error rates and potential biases from data-driven subgroup identification. In this paper, we present two methods for subgroup selection in regression problems: one based on generalised linear modelling and another on isotonic regression. We demonstrate how these methods can be used for data-driven subgroup identification in the analysis of clinical trials, focusing on two distinct tasks: identifying patient groups that are safe from manifesting adverse events and identifying patient groups with high treatment effect, while controlling for Type I error in both cases. A thorough simulation study is conducted to evaluate the strengths and weaknesses of each method, providing detailed insight into the sensitivity of the Type I error rate control to modelling assumptions.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:28:33Z</published>
    <arxiv:comment>37 pages, 10 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Manuel M. Müller</name>
    </author>
    <author>
      <name>Björn Bornkamp</name>
    </author>
    <author>
      <name>Frank Bretz</name>
    </author>
    <author>
      <name>Timothy I. Cannings</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Henry W. J. Reeve</name>
    </author>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <author>
      <name>Nikolaos Sfikas</name>
    </author>
    <author>
      <name>Fang Wan</name>
    </author>
    <author>
      <name>Konstantinos Sechidis</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15671v1</id>
    <title>When sufficiency is insufficient: the functional information bottleneck for identifying probabilistic neural representations</title>
    <updated>2025-12-17T18:23:54Z</updated>
    <link href="https://arxiv.org/abs/2512.15671v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15671v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The neural basis of probabilistic computations remains elusive, even amidst growing evidence that humans and other animals track their uncertainty. Recent work has proposed that probabilistic representations arise naturally in task-optimized neural networks trained without explicitly probabilistic inductive biases. However, prior work has lacked clear criteria for distinguishing probabilistic representations, those that perform transformations characteristic of probabilistic computation, from heuristic neural codes that merely reformat inputs. We propose a novel information bottleneck framework, the functional information bottleneck (fIB), that crucially evaluates a neural representation based not only on its statistical sufficiency but also on its minimality, allowing us to disambiguate heuristic from probabilistic coding. To demonstrate the power of this framework, we study a variety of task-optimized neural networks that had been suggested to develop probabilistic representations in earlier work: networks trained to perform static inference tasks (such as cue combination and coordinate transformation) or dynamic state estimation tasks (Kalman filtering). In contrast to earlier claims, our minimality requirement reveals that probabilistic representations fail to emerge in these networks: they do not develop minimal codes of Bayesian posteriors in their hidden layer activities, and instead rely on heuristic input recoding. Therefore, it remains an open question under which conditions truly probabilistic representations emerge in neural networks. More generally, our work provides a stringent framework for identifying probabilistic neural codes. Thus, it lays the foundation for systematically examining whether, how, and which posteriors are represented in neural circuits during complex decision-making tasks.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:23:54Z</published>
    <arxiv:comment>20 pages, 10 figures</arxiv:comment>
    <arxiv:primary_category term="q-bio.NC"/>
    <author>
      <name>Ishan Kalburge</name>
    </author>
    <author>
      <name>Máté Lengyel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15663v1</id>
    <title>Explaining the Reasoning of Large Language Models Using Attribution Graphs</title>
    <updated>2025-12-17T18:15:26Z</updated>
    <link href="https://arxiv.org/abs/2512.15663v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15663v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:15:26Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Chase Walker</name>
    </author>
    <author>
      <name>Rickard Ewetz</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15653v1</id>
    <title>Characterizing Mamba's Selective Memory using Auto-Encoders</title>
    <updated>2025-12-17T18:05:25Z</updated>
    <link href="https://arxiv.org/abs/2512.15653v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15653v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T18:05:25Z</published>
    <arxiv:comment>AACL 2025. Oral Presentation</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Tamanna Hossain</name>
    </author>
    <author>
      <name>Robert L. Logan</name>
    </author>
    <author>
      <name>Ganesh Jagadeesan</name>
    </author>
    <author>
      <name>Sameer Singh</name>
    </author>
    <author>
      <name>Joel Tetreault</name>
    </author>
    <author>
      <name>Alejandro Jaimes</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.15649v1</id>
    <title>VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</title>
    <updated>2025-12-17T17:58:35Z</updated>
    <link href="https://arxiv.org/abs/2512.15649v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.15649v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-17T17:58:35Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hongbo Zhao</name>
    </author>
    <author>
      <name>Meng Wang</name>
    </author>
    <author>
      <name>Fei Zhu</name>
    </author>
    <author>
      <name>Wenzhuo Liu</name>
    </author>
    <author>
      <name>Bolin Ni</name>
    </author>
    <author>
      <name>Fanhu Zeng</name>
    </author>
    <author>
      <name>Gaofeng Meng</name>
    </author>
    <author>
      <name>Zhaoxiang Zhang</name>
    </author>
  </entry>
</feed>
