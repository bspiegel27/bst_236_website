<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-24T00:52:51Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-23T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">121508</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.18094v1</id>
    <updated>2025-09-22T17:59:40Z</updated>
    <published>2025-09-22T17:59:40Z</published>
    <title>UniPixel: Unified Object Referring and Segmentation for Pixel-Level
  Visual Reasoning</title>
    <summary>  Recent advances in Large Multi-modal Models (LMMs) have demonstrated their
remarkable success as general-purpose multi-modal assistants, with particular
focuses on holistic image- and video-language understanding. Conversely, less
attention has been given to scaling fine-grained pixel-level understanding
capabilities, where the models are expected to realize pixel-level alignment
between visual signals and language semantics. Some previous studies have
applied LMMs to related tasks such as region-level captioning and referring
expression segmentation. However, these models are limited to performing either
referring or segmentation tasks independently and fail to integrate these
fine-grained perception capabilities into visual reasoning. To bridge this gap,
we propose UniPixel, a large multi-modal model capable of flexibly
comprehending visual prompt inputs and generating mask-grounded responses. Our
model distinguishes itself by seamlessly integrating pixel-level perception
with general visual understanding capabilities. Specifically, UniPixel
processes visual prompts and generates relevant masks on demand, and performs
subsequent reasoning conditioning on these intermediate pointers during
inference, thereby enabling fine-grained pixel-level reasoning. The
effectiveness of our approach has been verified on 10 benchmarks across a
diverse set of tasks, including pixel-level referring/segmentation and
object-centric understanding in images/videos. A novel PixelQA task that
jointly requires referring, segmentation, and question answering is also
designed to verify the flexibility of our method.
</summary>
    <author>
      <name>Ye Liu</name>
    </author>
    <author>
      <name>Zongyang Ma</name>
    </author>
    <author>
      <name>Junfu Pu</name>
    </author>
    <author>
      <name>Zhongang Qi</name>
    </author>
    <author>
      <name>Yang Wu</name>
    </author>
    <author>
      <name>Ying Shan</name>
    </author>
    <author>
      <name>Chang Wen Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2025 Camera Ready. Project Page:
  https://polyu-chenlab.github.io/unipixel/</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.18094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18085v1</id>
    <updated>2025-09-22T17:58:21Z</updated>
    <published>2025-09-22T17:58:21Z</published>
    <title>Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative
  Decoding</title>
    <summary>  Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to
autoregressive LLMs (AR-LLMs) with the potential to operate at significantly
higher token generation rates. However, currently available open-source dLLMs
often generate at much lower rates, typically decoding only a single token at
every denoising timestep in order to maximize output quality. We present
Spiffy, a speculative decoding algorithm that accelerates dLLM inference by
$\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output
distribution. This work addresses the unique challenges involved in applying
ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes
draft states by leveraging the dLLM's distribution itself in an
auto-speculative manner. This approach is efficient and effective, and
eliminates the overheads of training and running an independent draft model. To
structure the candidate draft states, we propose a novel directed draft graph
which is uniquely designed to take advantage of the bidirectional, block-wise
nature of dLLM generation and can be verified in parallel by the dLLM. To
further optimize the structure of these draft graphs, we introduce an
efficient, offline calibration algorithm that procedurally determines
high-quality graph configurations. These optimized draft graphs, enabling
increased acceptance rates, lead to a significant boost in the overall speedup
achieved by the system. Crucially, Spiffy is also complementary to other recent
innovations in improving dLLM generation speeds such as KV-caching and
multi-token unmasking. We demonstrate that when combined with such parallel
decoding algorithms, Spiffy is able to effectively multiply the benefits of
these methods leading to total speedups of up to $\mathbf{7.9\times}$.
</summary>
    <author>
      <name>Sudhanshu Agrawal</name>
    </author>
    <author>
      <name>Risheek Garrepalli</name>
    </author>
    <author>
      <name>Raghavv Goel</name>
    </author>
    <author>
      <name>Mingu Lee</name>
    </author>
    <author>
      <name>Christopher Lott</name>
    </author>
    <author>
      <name>Fatih Porikli</name>
    </author>
    <link href="http://arxiv.org/abs/2509.18085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18083v1</id>
    <updated>2025-09-22T17:56:38Z</updated>
    <published>2025-09-22T17:56:38Z</published>
    <title>Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning</title>
    <summary>  We introduce Reasoning Core, a new scalable environment for Reinforcement
Learning with Verifiable Rewards (RLVR), designed to advance foundational
symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks
that focus on games or isolated puzzles, Reasoning Core procedurally generates
problems across core formal domains, including PDDL planning, first-order
logic, context-free grammar parsing, causal reasoning, and system equation
solving. The environment is built on key design principles of high-generality
problem distributions, verification via external tools, and continuous
difficulty control, which together provide a virtually infinite supply of novel
training instances. Initial zero-shot evaluations with frontier LLMs confirm
the difficulty of Reasoning Core's tasks, positioning it as a promising
resource to improve the reasoning capabilities of future models.
</summary>
    <author>
      <name>Valentin Lacombe</name>
    </author>
    <author>
      <name>Valentin Quesnel</name>
    </author>
    <author>
      <name>Damien Sileo</name>
    </author>
    <link href="http://arxiv.org/abs/2509.18083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18056v1</id>
    <updated>2025-09-22T17:30:15Z</updated>
    <published>2025-09-22T17:30:15Z</published>
    <title>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning
  for Video LLMs</title>
    <summary>  This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1
</summary>
    <author>
      <name>Yunheng Li</name>
    </author>
    <author>
      <name>Jing Cheng</name>
    </author>
    <author>
      <name>Shaoyong Jia</name>
    </author>
    <author>
      <name>Hangyi Kuang</name>
    </author>
    <author>
      <name>Shaohui Jiao</name>
    </author>
    <author>
      <name>Qibin Hou</name>
    </author>
    <author>
      <name>Ming-Ming Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NeurIPS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.18056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18052v1</id>
    <updated>2025-09-22T17:27:29Z</updated>
    <published>2025-09-22T17:27:29Z</published>
    <title>The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM
  Societies</title>
    <summary>  Large Language Models (LLMs) are increasingly used for social simulation,
where populations of agents are expected to reproduce human-like collective
behavior. However, we find that many recent studies adopt experimental designs
that systematically undermine the validity of their claims. From a survey of
over 40 papers, we identify six recurring methodological flaws: agents are
often homogeneous (Profile), interactions are absent or artificially imposed
(Interaction), memory is discarded (Memory), prompts tightly control outcomes
(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),
and validation relies on simplified theoretical models rather than real-world
data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying
social experiment in 53.1% of cases when given instructions from prior
work-violating the Unawareness principle. We formalize these six requirements
as the PIMMUR principles and argue they are necessary conditions for credible
LLM-based social simulation. To demonstrate their impact, we re-run five
representative studies using a framework that enforces PIMMUR and find that the
reported social phenomena frequently fail to emerge under more rigorous
conditions. Our work establishes methodological standards for LLM-based
multi-agent research and provides a foundation for more reliable and
reproducible claims about "AI societies."
</summary>
    <author>
      <name>Jiaxu Zhou</name>
    </author>
    <author>
      <name>Jen-tse Huang</name>
    </author>
    <author>
      <name>Xuhui Zhou</name>
    </author>
    <author>
      <name>Man Ho Lam</name>
    </author>
    <author>
      <name>Xintao Wang</name>
    </author>
    <author>
      <name>Hao Zhu</name>
    </author>
    <author>
      <name>Wenxuan Wang</name>
    </author>
    <author>
      <name>Maarten Sap</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.18052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18041v1</id>
    <updated>2025-09-22T17:15:13Z</updated>
    <published>2025-09-22T17:15:13Z</published>
    <title>NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and
  Neuro-Symbolic Reasoning</title>
    <summary>  Long-Form Video Question Answering (LVQA) poses challenges beyond traditional
visual question answering (VQA), which is often limited to static images or
short video clips. While current vision-language models (VLMs) perform well in
those settings, they struggle with complex queries in LVQA over long videos
involving multi-step temporal reasoning and causality. Vanilla approaches,
which sample frames uniformly and feed them to a VLM with the question, incur
significant token overhead, forcing severe downsampling. As a result, the model
often misses fine-grained visual structure, subtle event transitions, or key
temporal cues, ultimately leading to incorrect answers. To address these
limitations, recent works have explored query-adaptive frame sampling,
hierarchical keyframe selection, and agent-based iterative querying. However,
these methods remain fundamentally heuristic: they lack explicit temporal
representations and cannot enforce or verify logical event relationships. As a
result, there are no formal guarantees that the sampled context actually
encodes the compositional or causal logic demanded by the question. To address
these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play
neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language
question into a formal temporal logic expression, constructs a video automaton
from frame-level semantic propositions, and applies model checking to
rigorously identify video segments satisfying the question's logical
requirements. Only these logic-verified segments are submitted to the VLM, thus
improving interpretability, reducing hallucinations, and enabling compositional
reasoning without modifying or fine-tuning the model. Experiments on
LongVideoBench and CinePile show NeuS-QA improves performance by over 10%,
especially on questions involving event ordering, causality, and multi-step
compositional reasoning.
</summary>
    <author>
      <name>Sahil Shah</name>
    </author>
    <author>
      <name>S P Sharan</name>
    </author>
    <author>
      <name>Harsh Goel</name>
    </author>
    <author>
      <name>Minkyu Choi</name>
    </author>
    <author>
      <name>Mustafa Munir</name>
    </author>
    <author>
      <name>Manvik Pasula</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <author>
      <name>Sandeep Chinchali</name>
    </author>
    <link href="http://arxiv.org/abs/2509.18041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18014v1</id>
    <updated>2025-09-22T16:53:38Z</updated>
    <published>2025-09-22T16:53:38Z</published>
    <title>Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data
  Synthesis</title>
    <summary>  Tabular Generative Models are often argued to preserve privacy by creating
synthetic datasets that resemble training data. However, auditing their
empirical privacy remains challenging, as commonly used similarity metrics fail
to effectively characterize privacy risk. Membership Inference Attacks (MIAs)
have recently emerged as a method for evaluating privacy leakage in synthetic
data, but their practical effectiveness is limited. Numerous attacks exist
across different threat models, each with distinct implementations targeting
various sources of privacy leakage, making them difficult to apply
consistently. Moreover, no single attack consistently outperforms the others,
leading to a routine underestimation of privacy risk.
  To address these issues, we propose a unified, model-agnostic threat
framework that deploys a collection of attacks to estimate the maximum
empirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an
open-source Python library that streamlines this auditing process through a
novel testbed that integrates seamlessly into existing synthetic data
evaluation pipelines through a Scikit-Learn-like API. Our software implements
13 attack methods through a Scikit-Learn-like API, designed to enable fast
systematic estimation of privacy leakage for practitioners as well as
facilitate the development of new attacks and experiments for researchers.
  We demonstrate our framework's utility in the largest tabular synthesis
privacy benchmark to date, revealing that higher synthetic data quality
corresponds to greater privacy leakage, that similarity-based privacy metrics
show weak correlation with MIA results, and that the differentially private
generator PATEGAN can fail to preserve privacy under such attacks. This
underscores the necessity of MIA-based auditing when designing and deploying
Tabular Generative Models.
</summary>
    <author>
      <name>Joshua Ward</name>
    </author>
    <author>
      <name>Xiaofeng Lin</name>
    </author>
    <author>
      <name>Chi-Hua Wang</name>
    </author>
    <author>
      <name>Guang Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/2509.18014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18011v1</id>
    <updated>2025-09-22T16:49:49Z</updated>
    <published>2025-09-22T16:49:49Z</published>
    <title>Robust, Online, and Adaptive Decentralized Gaussian Processes</title>
    <summary>  Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for
modeling complex signals, but scale cubically with data, assume static targets,
and are brittle to outliers, limiting their applicability in large-scale
problems with dynamic and noisy environments. Recent work introduced
decentralized random Fourier feature Gaussian processes (DRFGP), an online and
distributed algorithm that casts GPs in an information-filter form, enabling
exact sequential inference and fully distributed computation without reliance
on a fusion center. In this paper, we extend DRFGP along two key directions:
first, by introducing a robust-filtering update that downweights the impact of
atypical observations; and second, by incorporating a dynamic adaptation
mechanism that adapts to time-varying functions. The resulting algorithm
retains the recursive information-filter structure while enhancing stability
and accuracy. We demonstrate its effectiveness on a large-scale Earth system
application, underscoring its potential for in-situ modeling.
</summary>
    <author>
      <name>Fernando Llorente</name>
    </author>
    <author>
      <name>Daniel Waxman</name>
    </author>
    <author>
      <name>Sanket Jantre</name>
    </author>
    <author>
      <name>Nathan M. Urban</name>
    </author>
    <author>
      <name>Susan E. Minkoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Icassp 2026 Special Session on "Bridging Signal
  Processing and Machine Learning with Gaussian Processes."</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.18011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.18005v1</id>
    <updated>2025-09-22T16:44:34Z</updated>
    <published>2025-09-22T16:44:34Z</published>
    <title>M3ET: Efficient Vision-Language Learning for Robotics based on
  Multimodal Mamba-Enhanced Transformer</title>
    <summary>  In recent years, multimodal learning has become essential in robotic vision
and information fusion, especially for understanding human behavior in complex
environments. However, current methods struggle to fully leverage the textual
modality, relying on supervised pretrained models, which limits semantic
extraction in unsupervised robotic environments, particularly with significant
modality loss. These methods also tend to be computationally intensive, leading
to high resource consumption in real-world applications. To address these
challenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a
lightweight model designed for efficient multimodal learning, particularly on
mobile platforms. By incorporating the Mamba module and a semantic-based
adaptive attention mechanism, M3ET optimizes feature fusion, alignment, and
modality reconstruction. Our experiments show that M3ET improves cross-task
performance, with a 2.3 times increase in pretraining inference speed. In
particular, the core VQA task accuracy of M3ET remains at 0.74, while the
model's parameter count is reduced by 0.67. Although performance on the EQA
task is limited, M3ET's lightweight design makes it well suited for deployment
on resource-constrained robotic platforms.
</summary>
    <author>
      <name>Yanxin Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Software Northwestern Polytechnical University</arxiv:affiliation>
    </author>
    <author>
      <name>Liang He</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Software Northwestern Polytechnical University</arxiv:affiliation>
    </author>
    <author>
      <name>Zeyi Kang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Software Northwestern Polytechnical University</arxiv:affiliation>
    </author>
    <author>
      <name>Zuheng Ming</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratoire L2Tl University Sorbonne Paris Nord</arxiv:affiliation>
    </author>
    <author>
      <name>Kaixing Zhao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Software Yangtze River Delta Research Institute</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.18005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.18005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.17999v1</id>
    <updated>2025-09-22T16:39:22Z</updated>
    <published>2025-09-22T16:39:22Z</published>
    <title>The Narcissus Hypothesis:Descending to the Rung of Illusion</title>
    <summary>  Modern foundational models increasingly reflect not just world knowledge, but
patterns of human preference embedded in their training data. We hypothesize
that recursive alignment-via human feedback and model-generated corpora-induces
a social desirability bias, nudging models to favor agreeable or flattering
responses over objective reasoning. We refer to it as the Narcissus Hypothesis
and test it across 31 models using standardized personality assessments and a
novel Social Desirability Bias score. Results reveal a significant drift toward
socially conforming traits, with profound implications for corpus integrity and
the reliability of downstream inferences. We then offer a novel epistemological
interpretation, tracing how recursive bias may collapse higher-order reasoning
down Pearl's Ladder of Causality, culminating in what we refer to as the Rung
of Illusion.
</summary>
    <author>
      <name>Riccardo Cadei</name>
    </author>
    <author>
      <name>Christian Internò</name>
    </author>
    <link href="http://arxiv.org/abs/2509.17999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.17999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
