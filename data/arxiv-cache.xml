<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-07T00:51:55Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-06T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">108232</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.03751v1</id>
    <updated>2025-03-05T18:59:50Z</updated>
    <published>2025-03-05T18:59:50Z</published>
    <title>GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera
  Control</title>
    <summary>  We present GEN3C, a generative video model with precise Camera Control and
temporal 3D Consistency. Prior video models already generate realistic videos,
but they tend to leverage little 3D information, leading to inconsistencies,
such as objects popping in and out of existence. Camera control, if implemented
at all, is imprecise, because camera parameters are mere inputs to the neural
network which must then infer how the video depends on the camera. In contrast,
GEN3C is guided by a 3D cache: point clouds obtained by predicting the
pixel-wise depth of seed images or previously generated frames. When generating
the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with
the new camera trajectory provided by the user. Crucially, this means that
GEN3C neither has to remember what it previously generated nor does it have to
infer the image structure from the camera pose. The model, instead, can focus
all its generative power on previously unobserved regions, as well as advancing
the scene state to the next frame. Our results demonstrate more precise camera
control than prior work, as well as state-of-the-art results in sparse-view
novel view synthesis, even in challenging settings such as driving scenes and
monocular dynamic video. Results are best viewed in videos. Check out our
webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/
</summary>
    <author>
      <name>Xuanchi Ren</name>
    </author>
    <author>
      <name>Tianchang Shen</name>
    </author>
    <author>
      <name>Jiahui Huang</name>
    </author>
    <author>
      <name>Huan Ling</name>
    </author>
    <author>
      <name>Yifan Lu</name>
    </author>
    <author>
      <name>Merlin Nimier-David</name>
    </author>
    <author>
      <name>Thomas MÃ¼ller</name>
    </author>
    <author>
      <name>Alexander Keller</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <author>
      <name>Jun Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in CVPR 2025. Website:
  https://research.nvidia.com/labs/toronto-ai/GEN3C/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03724v1</id>
    <updated>2025-03-05T18:24:58Z</updated>
    <published>2025-03-05T18:24:58Z</published>
    <title>Deep Causal Behavioral Policy Learning: Applications to Healthcare</title>
    <summary>  We present a deep learning-based approach to studying dynamic clinical
behavioral regimes in diverse non-randomized healthcare settings. Our proposed
methodology - deep causal behavioral policy learning (DC-BPL) - uses deep
learning algorithms to learn the distribution of high-dimensional clinical
action paths, and identifies the causal link between these action paths and
patient outcomes. Specifically, our approach: (1) identifies the causal effects
of provider assignment on clinical outcomes; (2) learns the distribution of
clinical actions a given provider would take given evolving patient
information; (3) and combines these steps to identify the optimal provider for
a given patient type and emulate that provider's care decisions. Underlying
this strategy, we train a large clinical behavioral model (LCBM) on electronic
health records data using a transformer architecture, and demonstrate its
ability to estimate clinical behavioral policies. We propose a novel
interpretation of a behavioral policy learned using the LCBM: that it is an
efficient encoding of complex, often implicit, knowledge used to treat a
patient. This allows us to learn a space of policies that are critical to a
wide range of healthcare applications, in which the vast majority of clinical
knowledge is acquired tacitly through years of practice and only a tiny
fraction of information relevant to patient care is written down (e.g. in
textbooks, studies or standardized guidelines).
</summary>
    <author>
      <name>Jonas Knecht</name>
    </author>
    <author>
      <name>Anna Zink</name>
    </author>
    <author>
      <name>Jonathan Kolstad</name>
    </author>
    <author>
      <name>Maya Petersen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.03724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03708v1</id>
    <updated>2025-03-05T17:59:19Z</updated>
    <published>2025-03-05T17:59:19Z</published>
    <title>Rethinking Video Tokenization: A Conditioned Diffusion-based Approach</title>
    <summary>  Video tokenizers, which transform videos into compact latent representations,
are key to video generation. Existing video tokenizers are based on the VAE
architecture and follow a paradigm where an encoder compresses videos into
compact latents, and a deterministic decoder reconstructs the original videos
from these latents. In this paper, we propose a novel
\underline{\textbf{C}}onditioned \underline{\textbf{D}}iffusion-based video
\underline{\textbf{T}}okenizer entitled \textbf{\ourmethod}, which departs from
previous methods by replacing the deterministic decoder with a 3D causal
diffusion model. The reverse diffusion generative process of the decoder is
conditioned on the latent representations derived via the encoder. With a
feature caching and sampling acceleration, the framework efficiently
reconstructs high-fidelity videos of arbitrary lengths. Results show that
{\ourmethod} achieves state-of-the-art performance in video reconstruction
tasks using just a single-step sampling. Even a smaller version of {\ourmethod}
still achieves reconstruction results on par with the top two baselines.
Furthermore, the latent video generation model trained using {\ourmethod} also
shows superior performance.
</summary>
    <author>
      <name>Nianzu Yang</name>
    </author>
    <author>
      <name>Pandeng Li</name>
    </author>
    <author>
      <name>Liming Zhao</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Chen-Wei Xie</name>
    </author>
    <author>
      <name>Yehui Tang</name>
    </author>
    <author>
      <name>Xudong Lu</name>
    </author>
    <author>
      <name>Zhihang Liu</name>
    </author>
    <author>
      <name>Yun Zheng</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Junchi Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2503.03708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03699v1</id>
    <updated>2025-03-05T17:51:45Z</updated>
    <published>2025-03-05T17:51:45Z</published>
    <title>High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with
  low-massive Circumstellar Medium: The Case of SN 2023ixf</title>
    <summary>  In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the
H-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and
produce high-energy neutrinos (HE-$\nu$, TeV-PeV) through proton-proton
inelastic scattering. Despite understanding the production mechanism of these
neutrinos, the lack of direct observations raises questions about particle
acceleration efficiency and the involved astrophysical conditions. This study
focuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN
2023ixf. We developed a semi-analytical model to characterize the progenitor
and CSM at the explosion time, allowing us to infer the expected neutrino flux
at Earth during the SN's interaction phase. Our model shows that neutrino
emission depends not only on shock velocity and CSM mass but also on the
spatial matter distribution of the CSM. By analysing the bolometric light curve
of SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,
consisting of $9\,\text{M}_{\rm \odot}$ (including $0.07\,\text{M}_{\rm \odot}$
of radioactive $^{56}$Ni) and having a kinetic energy of $1.8\,\text{foe}$,
collides with a low-mass CSM of $0.06\,\text{M}_{\rm \odot}$ distributed
according to a power-law density profile with an exponent of $s=2.9$. Through
these parameters, we estimate that up to $4\pm1\times 10^{-2}$ muon
(anti-)neutrino events could be detected by IceCube within 50 days
post-explosion. Although the predicted flux ($\lesssim 3\times
10^{-9}\,\text{GeV} \, \text{cm}^{-2} \, \text{s}^{-1}$) is below current
IceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could
detect HE-$\nu$ from similar SN events.
</summary>
    <author>
      <name>S. P. Cosentino</name>
    </author>
    <author>
      <name>M. L. Pumo</name>
    </author>
    <author>
      <name>S. Cherubini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 11 figures, under revision in Monthly Notices of the Royal
  Astronomical Society - Main Journal (First submission 23-Aug-2024; Revised
  version submitted 23-Feb-2025; currently awaiting reviewer report)</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03693v1</id>
    <updated>2025-03-05T17:43:49Z</updated>
    <published>2025-03-05T17:43:49Z</published>
    <title>ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural
  Faithfulness in SpArX</title>
    <summary>  In the field of Explainable Artificial Intelligence (XAI), argumentative XAI
approaches have been proposed to represent the internal reasoning process of
deep neural networks in a more transparent way by interpreting hidden nodes as
arguements. However, as the number of layers increases, existing compression
methods simplify all layers at once, which lead to high accumulative
information loss. To compensate for this, we propose an iterative
layer-by-layer compression technique in which each layer is compressed
separately and the reduction error in the next layer is immediately compensated
for, thereby improving the overall input-output and structural fidelity of the
model. Experiments on the Breast Cancer Diagnosis dataset show that, compared
to traditional compression, the method reduces input-output and structural
unfaithfulness, and maintains a more consistent attack-support relationship in
the Argumentative Explanation scheme. This is significant because it provides a
new way to make complex MLP models more compact while still conveying their
internal inference logic without distortion.
</summary>
    <author>
      <name>Ungsik Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03686v1</id>
    <updated>2025-03-05T17:27:59Z</updated>
    <published>2025-03-05T17:27:59Z</published>
    <title>MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems</title>
    <summary>  LLM-based multi-agent systems (MAS) have shown significant potential in
tackling diverse tasks. However, to design effective MAS, existing approaches
heavily rely on manual configurations or multiple calls of advanced LLMs,
resulting in inadaptability and high inference costs. In this paper, we
simplify the process of building an MAS by reframing it as a generative
language task, where the input is a user query and the output is a
corresponding MAS. To address this novel task, we unify the representation of
MAS as executable code and propose a consistency-oriented data construction
pipeline to create a high-quality dataset comprising coherent and consistent
query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source
medium-sized LLM that is capable of generating query-adaptive MAS within a
single LLM inference. The generated MAS can be seamlessly applied to process
user queries and deliver high-quality responses. Extensive experiments on 9
benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms
10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high
effectiveness, efficiency and strong generalization ability. Code will be
available at https://github.com/rui-ye/MAS-GPT.
</summary>
    <author>
      <name>Rui Ye</name>
    </author>
    <author>
      <name>Shuo Tang</name>
    </author>
    <author>
      <name>Rui Ge</name>
    </author>
    <author>
      <name>Yaxin Du</name>
    </author>
    <author>
      <name>Zhenfei Yin</name>
    </author>
    <author>
      <name>Siheng Chen</name>
    </author>
    <author>
      <name>Jing Shao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03684v1</id>
    <updated>2025-03-05T17:25:20Z</updated>
    <published>2025-03-05T17:25:20Z</published>
    <title>Towards Trustworthy Federated Learning</title>
    <summary>  This paper develops a comprehensive framework to address three critical
trustworthy challenges in federated learning (FL): robustness against Byzantine
attacks, fairness, and privacy preservation. To improve the system's defense
against Byzantine attacks that send malicious information to bias the system's
performance, we develop a Two-sided Norm Based Screening (TNBS) mechanism,
which allows the central server to crop the gradients that have the l lowest
norms and h highest norms. TNBS functions as a screening tool to filter out
potential malicious participants whose gradients are far from the honest ones.
To promote egalitarian fairness, we adopt the q-fair federated learning
(q-FFL). Furthermore, we adopt a differential privacy-based scheme to prevent
raw data at local clients from being inferred by curious parties. Convergence
guarantees are provided for the proposed framework under different scenarios.
Experimental results on real datasets demonstrate that the proposed framework
effectively improves robustness and fairness while managing the trade-off
between privacy and accuracy. This work appears to be the first study that
experimentally and theoretically addresses fairness, privacy, and robustness in
trustworthy FL.
</summary>
    <author>
      <name>Alina Basharat</name>
    </author>
    <author>
      <name>Yijun Bian</name>
    </author>
    <author>
      <name>Ping Xu</name>
    </author>
    <author>
      <name>Zhi Tian</name>
    </author>
    <link href="http://arxiv.org/abs/2503.03684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03670v1</id>
    <updated>2025-03-05T17:04:46Z</updated>
    <published>2025-03-05T17:04:46Z</published>
    <title>Study of an active region prominence using spectropolarimetric data in
  the He I D3 multiplet</title>
    <summary>  Prominences are cool overdensities of plasma supported by magnetic fields
that levitate in the solar corona. The physical characterization of these
structures is key for understanding the magnetic field in the corona. Our work
attempts to shed light on the properties of prominences by using observations
at high polarimetric sensitivity in the He I D3 multiplet taken with the Zurich
Imaging Polarimeter-3 instrument at the Istituto ricerche solari Aldo e Cele
Dacco observatory. We used the HAZEL inversion code to infer the thermodynamic
and magnetic properties of an active region prominence, assuming one- and
two-component models. Our observations unveil a great diversity of physical
conditions in the prominence. The observed Stokes profiles are usually broad
and show interesting features, which can be described assuming a two-component
model. The contribution of each component and the trends inferred for some
parameters vary with the distance to the solar limb. While both components have
analogous properties and contribute similarly close to the limb, a major
component mainly describes the properties inferred at 10-40 arcsecs away from
the limb. Moreover, both components usually show significant differences in
thermal broadening, which is essential for ensuring a good fit quality between
observations and synthetic profiles. Summarizing, the observed region of the
prominence shows line-of-sight velocities of 1-3 km/s and rather horizontal
fields of 20-80 gauss. We also report hints of a twist close to a prominence
foot and changes in the magnetic configuration at specific locations. Our
results indicate a mainly horizontal magnetic field of a few tens of gauss in
the prominence. A model of two components with different thermal broadenings
and filling factors, depending on the limb distance, is crucial for providing a
consistent solution across most of the observed prominence.
</summary>
    <author>
      <name>S. Esteban Pozuelo</name>
    </author>
    <author>
      <name>A. Asensio Ramos</name>
    </author>
    <author>
      <name>J. Trujillo Bueno</name>
    </author>
    <author>
      <name>R. Ramelli</name>
    </author>
    <author>
      <name>F. Zeuner</name>
    </author>
    <author>
      <name>M. Bianda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures. Accepted for publication in A&amp;A. Abstract has
  been abridged</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03666v1</id>
    <updated>2025-03-05T16:59:08Z</updated>
    <published>2025-03-05T16:59:08Z</published>
    <title>Analogical Reasoning Inside Large Language Models: Concept Vectors and
  the Limits of Abstraction</title>
    <summary>  Analogical reasoning relies on conceptual abstractions, but it is unclear
whether Large Language Models (LLMs) harbor such internal representations. We
explore distilled representations from LLM activations and find that function
vectors (FVs; Todd et al., 2024) - compact representations for in-context
learning (ICL) tasks - are not invariant to simple input changes (e.g.,
open-ended vs. multiple-choice), suggesting they capture more than pure
concepts. Using representational similarity analysis (RSA), we localize a small
set of attention heads that encode invariant concept vectors (CVs) for verbal
concepts like "antonym". These CVs function as feature detectors that operate
independently of the final output - meaning that a model may form a correct
internal representation yet still produce an incorrect output. Furthermore, CVs
can be used to causally guide model behaviour. However, for more abstract
concepts like "previous" and "next", we do not observe invariant linear
representations, a finding we link to generalizability issues LLMs display
within these domains.
</summary>
    <author>
      <name>Gustaw OpieÅka</name>
    </author>
    <author>
      <name>Hannes Rosenbusch</name>
    </author>
    <author>
      <name>Claire E. Stevenson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03634v1</id>
    <updated>2025-03-05T16:14:43Z</updated>
    <published>2025-03-05T16:14:43Z</published>
    <title>Feature Matching Intervention: Leveraging Observational Data for Causal
  Representation Learning</title>
    <summary>  A major challenge in causal discovery from observational data is the absence
of perfect interventions, making it difficult to distinguish causal features
from spurious ones. We propose an innovative approach, Feature Matching
Intervention (FMI), which uses a matching procedure to mimic perfect
interventions. We define causal latent graphs, extending structural causal
models to latent feature space, providing a framework that connects FMI with
causal graph learning. Our feature matching procedure emulates perfect
interventions within these causal latent graphs. Theoretical results
demonstrate that FMI exhibits strong out-of-distribution (OOD)
generalizability. Experiments further highlight FMI's superior performance in
effectively identifying causal features solely from observational data.
</summary>
    <author>
      <name>Haoze Li</name>
    </author>
    <author>
      <name>Jun Xie</name>
    </author>
    <link href="http://arxiv.org/abs/2503.03634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
