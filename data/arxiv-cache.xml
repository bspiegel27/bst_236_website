<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-25T00:57:35Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-24T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">119405</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.15773v1</id>
    <updated>2025-08-21T17:59:57Z</updated>
    <published>2025-08-21T17:59:57Z</published>
    <title>Scaling Group Inference for Diverse and High-Quality Generation</title>
    <summary>  Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.
</summary>
    <author>
      <name>Gaurav Parmar</name>
    </author>
    <author>
      <name>Or Patashnik</name>
    </author>
    <author>
      <name>Daniil Ostashev</name>
    </author>
    <author>
      <name>Kuan-Chieh Wang</name>
    </author>
    <author>
      <name>Kfir Aberman</name>
    </author>
    <author>
      <name>Srinivasa Narasimhan</name>
    </author>
    <author>
      <name>Jun-Yan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website: https://www.cs.cmu.edu/~group-inference, GitHub:
  https://github.com/GaParmar/group-inference</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.15773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15774v1</id>
    <updated>2025-08-21T17:59:57Z</updated>
    <published>2025-08-21T17:59:57Z</published>
    <title>CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</title>
    <summary>  Visual diffusion models achieve remarkable progress, yet they are typically
trained at limited resolutions due to the lack of high-resolution data and
constrained computation resources, hampering their ability to generate
high-fidelity images or videos at higher resolutions. Recent efforts have
explored tuning-free strategies to exhibit the untapped potential
higher-resolution visual generation of pre-trained models. However, these
methods are still prone to producing low-quality visual content with repetitive
patterns. The key obstacle lies in the inevitable increase in high-frequency
information when the model generates visual content exceeding its training
resolution, leading to undesirable repetitive patterns deriving from the
accumulated errors. In this work, we propose CineScale, a novel inference
paradigm to enable higher-resolution visual generation. To tackle the various
issues introduced by the two types of video generation architectures, we
propose dedicated variants tailored to each. Unlike existing baseline methods
that are confined to high-resolution T2I and T2V generation, CineScale broadens
the scope by enabling high-resolution I2V and V2V synthesis, built atop
state-of-the-art open-source video generation frameworks. Extensive experiments
validate the superiority of our paradigm in extending the capabilities of
higher-resolution visual generation for both image and video models.
Remarkably, our approach enables 8k image generation without any fine-tuning,
and achieves 4k video generation with only minimal LoRA fine-tuning. Generated
video samples are available at our website:
https://eyeline-labs.github.io/CineScale/.
</summary>
    <author>
      <name>Haonan Qiu</name>
    </author>
    <author>
      <name>Ning Yu</name>
    </author>
    <author>
      <name>Ziqi Huang</name>
    </author>
    <author>
      <name>Paul Debevec</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CineScale is an extended work of FreeScale (ICCV 2025). Project Page:
  https://eyeline-labs.github.io/CineScale/, Code Repo:
  https://github.com/Eyeline-Labs/CineScale</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.15774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15772v1</id>
    <updated>2025-08-21T17:59:32Z</updated>
    <published>2025-08-21T17:59:32Z</published>
    <title>Visual Autoregressive Modeling for Instruction-Guided Image Editing</title>
    <summary>  Recent advances in diffusion models have brought remarkable visual fidelity
to instruction-guided image editing. However, their global denoising process
inherently entangles the edited region with the entire image context, leading
to unintended spurious modifications and compromised adherence to editing
instructions. In contrast, autoregressive models offer a distinct paradigm by
formulating image synthesis as a sequential process over discrete visual
tokens. Their causal and compositional mechanism naturally circumvents the
adherence challenges of diffusion-based methods. In this paper, we present
VAREdit, a visual autoregressive (VAR) framework that reframes image editing as
a next-scale prediction problem. Conditioned on source image features and text
instructions, VAREdit generates multi-scale target features to achieve precise
edits. A core challenge in this paradigm is how to effectively condition the
source image tokens. We observe that finest-scale source features cannot
effectively guide the prediction of coarser target features. To bridge this
gap, we introduce a Scale-Aligned Reference (SAR) module, which injects
scale-matched conditioning information into the first self-attention layer.
VAREdit demonstrates significant advancements in both editing adherence and
efficiency. On standard benchmarks, it outperforms leading diffusion-based
methods by 30\%+ higher GPT-Balance score. Moreover, it completes a
$512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the
similarly sized UltraEdit. The models are available at
https://github.com/HiDream-ai/VAREdit.
</summary>
    <author>
      <name>Qingyang Mao</name>
    </author>
    <author>
      <name>Qi Cai</name>
    </author>
    <author>
      <name>Yehao Li</name>
    </author>
    <author>
      <name>Yingwei Pan</name>
    </author>
    <author>
      <name>Mingyue Cheng</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Qi Liu</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source codes and models are available at
  https://github.com/HiDream-ai/VAREdit</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.15772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15766v1</id>
    <updated>2025-08-21T17:58:50Z</updated>
    <published>2025-08-21T17:58:50Z</published>
    <title>Discovering Hidden Algebraic Structures via Transformers with Rank-Aware
  Beam GRPO</title>
    <summary>  Recent efforts have extended the capabilities of transformers in logical
reasoning and symbolic computations. In this work, we investigate their
capacity for non-linear latent pattern discovery in the context of functional
decomposition, focusing on the challenging algebraic task of multivariate
polynomial decomposition. This problem, with widespread applications in science
and engineering, is proved to be NP-hard, and demands both precision and
insight. Our contributions are threefold: First, we develop a synthetic data
generation pipeline providing fine-grained control over problem complexity.
Second, we train transformer models via supervised learning and evaluate them
across four key dimensions involving scaling behavior and generalizability.
Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a
rank-aware reinforcement learning method suitable for hard algebraic problems.
Finetuning with BGRPO improves accuracy while reducing beam width by up to
half, resulting in approximately 75% lower inference compute. Additionally, our
model demonstrates competitive performance in polynomial simplification,
outperforming Mathematica in various cases.
</summary>
    <author>
      <name>Jaeha Lee</name>
    </author>
    <author>
      <name>Gio Huh</name>
    </author>
    <author>
      <name>Ning Su</name>
    </author>
    <author>
      <name>Tony Yue YU</name>
    </author>
    <link href="http://arxiv.org/abs/2508.15766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15761v1</id>
    <updated>2025-08-21T17:56:10Z</updated>
    <published>2025-08-21T17:56:10Z</published>
    <title>Waver: Wave Your Way to Lifelike Video Generation</title>
    <summary>  We present Waver, a high-performance foundation model for unified image and
video generation. Waver can directly generate videos with durations ranging
from 5 to 10 seconds at a native resolution of 720p, which are subsequently
upscaled to 1080p. The model simultaneously supports text-to-video (T2V),
image-to-video (I2V), and text-to-image (T2I) generation within a single,
integrated framework. We introduce a Hybrid Stream DiT architecture to enhance
modality alignment and accelerate training convergence. To ensure training data
quality, we establish a comprehensive data curation pipeline and manually
annotate and train an MLLM-based video quality model to filter for the
highest-quality samples. Furthermore, we provide detailed training and
inference recipes to facilitate the generation of high-quality videos. Building
on these contributions, Waver excels at capturing complex motion, achieving
superior motion amplitude and temporal consistency in video synthesis. Notably,
it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial
Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming
existing open-source models and matching or surpassing state-of-the-art
commercial solutions. We hope this technical report will help the community
more efficiently train high-quality video generation models and accelerate
progress in video generation technologies. Official page:
https://github.com/FoundationVision/Waver.
</summary>
    <author>
      <name>Yifu Zhang</name>
    </author>
    <author>
      <name>Hao Yang</name>
    </author>
    <author>
      <name>Yuqi Zhang</name>
    </author>
    <author>
      <name>Yifei Hu</name>
    </author>
    <author>
      <name>Fengda Zhu</name>
    </author>
    <author>
      <name>Chuang Lin</name>
    </author>
    <author>
      <name>Xiaofeng Mei</name>
    </author>
    <author>
      <name>Yi Jiang</name>
    </author>
    <author>
      <name>Zehuan Yuan</name>
    </author>
    <author>
      <name>Bingyue Peng</name>
    </author>
    <link href="http://arxiv.org/abs/2508.15761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15741v1</id>
    <updated>2025-08-21T17:31:20Z</updated>
    <published>2025-08-21T17:31:20Z</published>
    <title>Effective programming of a photonic processor with complex
  interferometric structure</title>
    <summary>  Reconfigurable photonics have rapidly become an invaluable tool for
information processing. Light-based computing accelerators are promising for
boosting neural network learning and inference and optical interconnects are
foreseen as a solution to the information transfer bottleneck in
high-performance computing. In this study, we demonstrate the successful
programming of a transformation implemented using a reconfigurable photonic
circuit with a non-conventional architecture. The core of most photonic
processors is an MZI-based architecture that establishes an analytical
connection between the controllable parameters and circuit transformation.
However, several architectures that are substantially more difficult to program
have improved robustness to fabrication defects. We use two algorithms that
rely on different initial datasets to reconstruct the circuit model of a
complex interferometer, and then program the required unitary transformation.
Both methods performed accurate circuit programming with an average fidelity
above 98\%. Our results provide a strong foundation for the introduction of
non-conventional interferometric architectures for photonic information
processing.
</summary>
    <author>
      <name>Ilya V. Kondratyev</name>
    </author>
    <author>
      <name>Kseniia N. Urusova</name>
    </author>
    <author>
      <name>Artem S. Argenchiev</name>
    </author>
    <author>
      <name>Nikita S. Klushnikov</name>
    </author>
    <author>
      <name>Sergei S. Kuzmin</name>
    </author>
    <author>
      <name>Nikolay N. Skryabin</name>
    </author>
    <author>
      <name>Alexander D. Golikov</name>
    </author>
    <author>
      <name>Vadim V. Kovalyuk</name>
    </author>
    <author>
      <name>Gregory N. Goltsman</name>
    </author>
    <author>
      <name>Ivan V. Dyakonov</name>
    </author>
    <author>
      <name>Stanislav S. Straupe</name>
    </author>
    <author>
      <name>Sergei P. Kulik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.15741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15734v1</id>
    <updated>2025-08-21T17:22:06Z</updated>
    <published>2025-08-21T17:22:06Z</published>
    <title>Measuring the environmental impact of delivering AI at Google Scale</title>
    <summary>  The transformative power of AI is undeniable - but as user adoption
accelerates, so does the need to understand and mitigate the environmental
impact of AI serving. However, no studies have measured AI serving
environmental metrics in a production environment. This paper addresses this
gap by proposing and executing a comprehensive methodology for measuring the
energy usage, carbon emissions, and water consumption of AI inference workloads
in a large-scale, AI production environment. Our approach accounts for the full
stack of AI serving infrastructure - including active AI accelerator power,
host system energy, idle machine capacity, and data center energy overhead.
Through detailed instrumentation of Google's AI infrastructure for serving the
Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24
Wh of energy - a figure substantially lower than many public estimates. We also
show that Google's software efficiency efforts and clean energy procurement
have driven a 33x reduction in energy consumption and a 44x reduction in carbon
footprint for the median Gemini Apps text prompt over one year. We identify
that the median Gemini Apps text prompt uses less energy than watching nine
seconds of television (0.24 Wh) and consumes the equivalent of five drops of
water (0.26 mL). While these impacts are low compared to other daily
activities, reducing the environmental impact of AI serving continues to
warrant important attention. Towards this objective, we propose that a
comprehensive measurement of AI serving environmental metrics is critical for
accurately comparing models, and to properly incentivize efficiency gains
across the full AI serving stack.
</summary>
    <author>
      <name>Cooper Elsworth</name>
    </author>
    <author>
      <name>Keguo Huang</name>
    </author>
    <author>
      <name>David Patterson</name>
    </author>
    <author>
      <name>Ian Schneider</name>
    </author>
    <author>
      <name>Robert Sedivy</name>
    </author>
    <author>
      <name>Savannah Goodman</name>
    </author>
    <author>
      <name>Ben Townsend</name>
    </author>
    <author>
      <name>Parthasarathy Ranganathan</name>
    </author>
    <author>
      <name>Jeff Dean</name>
    </author>
    <author>
      <name>Amin Vahdat</name>
    </author>
    <author>
      <name>Ben Gomes</name>
    </author>
    <author>
      <name>James Manyika</name>
    </author>
    <link href="http://arxiv.org/abs/2508.15734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15725v1</id>
    <updated>2025-08-21T17:08:16Z</updated>
    <published>2025-08-21T17:08:16Z</published>
    <title>The Approach of Sliced Inference in Systems of Stochastic Differential
  Equations with Comments on the Heston Model</title>
    <summary>  Stochastic differential equations have been an important tool in modeling
complex financial relations, equipped with the possibility of being
multidimensional to better oversee complexities inherent in finance. This
multidimensionality, however, comes with a larger parameter space to estimate.
Therefore, via a dimension reduction method, Sliced Inverse Regression, we aim
to reduce high-dimensional parameter space to a reduced feature space and aim
to estimate the parameters on this new featured space rather than using full
data structure to lower computational costs. For this study, we closely study
the Heston model, and remark our methodology of inference on this chosen model.
</summary>
    <author>
      <name>Ahmet Umur Özsoy</name>
    </author>
    <link href="http://arxiv.org/abs/2508.15725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15719v1</id>
    <updated>2025-08-21T16:57:33Z</updated>
    <published>2025-08-21T16:57:33Z</published>
    <title>Tutorial on the Probabilistic Unification of Estimation Theory, Machine
  Learning, and Generative AI</title>
    <summary>  Extracting meaning from uncertain, noisy data is a fundamental problem across
time series analysis, pattern recognition, and language modeling. This survey
presents a unified mathematical framework that connects classical estimation
theory, statistical inference, and modern machine learning, including deep
learning and large language models. By analyzing how techniques such as maximum
likelihood estimation, Bayesian inference, and attention mechanisms address
uncertainty, the paper illustrates that many AI methods are rooted in shared
probabilistic principles. Through illustrative scenarios including system
identification, image classification, and language generation, we show how
increasingly complex models build upon these foundations to tackle practical
challenges like overfitting, data sparsity, and interpretability. In other
words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian
classification, and deep learning all represent different facets of a shared
goal: inferring hidden causes from noisy and/or biased observations. It serves
as both a theoretical synthesis and a practical guide for students and
researchers navigating the evolving landscape of machine learning.
</summary>
    <author>
      <name>Mohammed Elmusrati</name>
    </author>
    <link href="http://arxiv.org/abs/2508.15719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.15692v1</id>
    <updated>2025-08-21T16:17:15Z</updated>
    <published>2025-08-21T16:17:15Z</published>
    <title>Effect Identification and Unit Categorization in the Multi-Score
  Regression Discontinuity Design with Application to LED Manufacturing</title>
    <summary>  The RDD (regression discontinuity design) is a widely used framework for
identification and estimation of causal effects at a cutoff of a single running
variable. Practical settings, in particular those encountered in production
systems, often involve decision-making defined by multiple thresholds and
criteria. Common MRD (multi-score RDD) approaches transform these to a
one-dimensional design, to employ identification and estimation results.
However, this practice can introduce non-compliant behavior. We develop
theoretical tools to identify and reduce some of this "fuzziness" when
estimating the cutoff-effect on compliers of sub-rules. We provide a sound
definition and categorization of unit behavior types for multi-dimensional
cutoff-rules, extending existing categorizations. We identify conditions for
the existence and identification of the cutoff-effect on complier in multiple
dimensions, and specify when identification remains stable after excluding
nevertaker and alwaystaker. Further, we investigate how decomposing
cutoff-rules into simpler parts alters the unit behavior. This allows
identification and removal of non-compliant units potentially improving
estimates. We validate our framework on simulated and real-world data from
opto-electronic semiconductor manufacturing. Our empirical results demonstrate
the usability for refining production policies. Particularly we show that our
approach decreases the estimation variance, highlighting the practical value of
the MRD framework in manufacturing.
</summary>
    <author>
      <name>Philipp Alexander Schwarz</name>
    </author>
    <author>
      <name>Oliver Schacht</name>
    </author>
    <author>
      <name>Sven Klaassen</name>
    </author>
    <author>
      <name>Johannes Oberpriller</name>
    </author>
    <author>
      <name>Martin Spindler</name>
    </author>
    <link href="http://arxiv.org/abs/2508.15692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.15692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
