<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-10T01:06:46Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-09T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">118492</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.05635v1</id>
    <updated>2025-08-07T17:59:44Z</updated>
    <published>2025-08-07T17:59:44Z</published>
    <title>Genie Envisioner: A Unified World Foundation Platform for Robotic
  Manipulation</title>
    <summary>  We introduce Genie Envisioner (GE), a unified world foundation platform for
robotic manipulation that integrates policy learning, evaluation, and
simulation within a single video-generative framework. At its core, GE-Base is
a large-scale, instruction-conditioned video diffusion model that captures the
spatial, temporal, and semantic dynamics of real-world robotic interactions in
a structured latent space. Built upon this foundation, GE-Act maps latent
representations to executable action trajectories through a lightweight,
flow-matching decoder, enabling precise and generalizable policy inference
across diverse embodiments with minimal supervision. To support scalable
evaluation and training, GE-Sim serves as an action-conditioned neural
simulator, producing high-fidelity rollouts for closed-loop policy development.
The platform is further equipped with EWMBench, a standardized benchmark suite
measuring visual fidelity, physical consistency, and instruction-action
alignment. Together, these components establish Genie Envisioner as a scalable
and practical foundation for instruction-driven, general-purpose embodied
intelligence. All code, models, and benchmarks will be released publicly.
</summary>
    <author>
      <name>Yue Liao</name>
    </author>
    <author>
      <name>Pengfei Zhou</name>
    </author>
    <author>
      <name>Siyuan Huang</name>
    </author>
    <author>
      <name>Donglin Yang</name>
    </author>
    <author>
      <name>Shengcong Chen</name>
    </author>
    <author>
      <name>Yuxin Jiang</name>
    </author>
    <author>
      <name>Yue Hu</name>
    </author>
    <author>
      <name>Jingbin Cai</name>
    </author>
    <author>
      <name>Si Liu</name>
    </author>
    <author>
      <name>Jianlan Luo</name>
    </author>
    <author>
      <name>Liliang Chen</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <author>
      <name>Maoqing Yao</name>
    </author>
    <author>
      <name>Guanghui Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://genie-envisioner.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.05635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05634v1</id>
    <updated>2025-08-07T17:59:43Z</updated>
    <published>2025-08-07T17:59:43Z</published>
    <title>Towards Generalizable Safety in Crowd Navigation via Conformal
  Uncertainty Handling</title>
    <summary>  Mobile robots navigating in crowds trained using reinforcement learning are
known to suffer performance degradation when faced with out-of-distribution
scenarios. We propose that by properly accounting for the uncertainties of
pedestrians, a robot can learn safe navigation policies that are robust to
distribution shifts. Our method augments agent observations with prediction
uncertainty estimates generated by adaptive conformal inference, and it uses
these estimates to guide the agent's behavior through constrained reinforcement
learning. The system helps regulate the agent's actions and enables it to adapt
to distribution shifts. In the in-distribution setting, our approach achieves a
96.93% success rate, which is over 8.80% higher than the previous
state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times
fewer intrusions into ground-truth human future trajectories. In three
out-of-distribution scenarios, our method shows much stronger robustness when
facing distribution shifts in velocity variations, policy changes, and
transitions from individual to group dynamics. We deploy our method on a real
robot, and experiments show that the robot makes safe and robust decisions when
interacting with both sparse and dense crowds. Our code and videos are
available on https://gen-safe-nav.github.io/.
</summary>
    <author>
      <name>Jianpeng Yao</name>
    </author>
    <author>
      <name>Xiaopan Zhang</name>
    </author>
    <author>
      <name>Yu Xia</name>
    </author>
    <author>
      <name>Zejin Wang</name>
    </author>
    <author>
      <name>Amit K. Roy-Chowdhury</name>
    </author>
    <author>
      <name>Jiachen Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9th Conference on Robot Learning (CoRL 2025); Project website:
  https://gen-safe-nav.github.io/. arXiv admin note: text overlap with
  arXiv:2407.17460</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.05634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05632v1</id>
    <updated>2025-08-07T17:59:31Z</updated>
    <published>2025-08-07T17:59:31Z</published>
    <title>Partial projected ensembles and spatiotemporal structure of information
  scrambling</title>
    <summary>  Thermalisation and information scrambling in out-of-equilibrium quantum
many-body systems are deeply intertwined: local subsystems dynamically approach
thermal density matrices while their entropies track information spreading.
Projected ensembles--ensembles of pure states conditioned on measurement
outcomes of complementary subsystems--provide higher-order probes of
thermalisation, converging at late times to universal maximum-entropy
ensembles. In this work, we introduce the partial projected ensemble (PPE) as a
framework to study how the spatiotemporal structure of scrambling is imprinted
on projected ensembles. The PPE consists of an ensemble of mixed states induced
on a subsystem by measurements on a spatially separated part of its complement,
tracing out the remainder, naturally capturing scenarios involving discarded
outcomes or noise-induced losses. We show that statistical fluctuations of the
PPE faithfully track the causal lightcone of information spreading, revealing
how scrambling dynamics are encoded in ensemble structure. In addition, we
demonstrate that the probabilities of bit-string probabilities (PoPs)
associated with the PPE exhibit distinct dynamical regimes and provide an
experimentally accessible probe of scrambling. Both PPE fluctuations and PoPs
display exponential sensitivity to the size of the discarded region, reflecting
exponential degradation of quantum correlations under erasure. We substantiate
these findings using the non-integrable kicked Ising chain, combining numerics
in the ergodic regime with exact results at its self-dual point. We extend our
analysis to a many-body localised (MBL) regime numerically, along with analytic
results for the $\ell$-bit model. The linear and logarithmic lightcones
characteristic of ergodic and MBL regimes emerge naturally from PPE dynamics,
establishing it as a powerful tool for probing scrambling and deep
thermalisation.
</summary>
    <author>
      <name>Saptarshi Mandal</name>
    </author>
    <author>
      <name>Pieter W. Claeys</name>
    </author>
    <author>
      <name>Sthitadhi Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.05632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05626v1</id>
    <updated>2025-08-07T17:58:42Z</updated>
    <published>2025-08-07T17:58:42Z</published>
    <title>Physically Controllable Relighting of Photographs</title>
    <summary>  We present a self-supervised approach to in-the-wild image relighting that
enables fully controllable, physically based illumination editing. We achieve
this by combining the physical accuracy of traditional rendering with the
photorealistic appearance made possible by neural rendering. Our pipeline works
by inferring a colored mesh representation of a given scene using monocular
estimates of geometry and intrinsic components. This representation allows
users to define their desired illumination configuration in 3D. The scene under
the new lighting can then be rendered using a path-tracing engine. We send this
approximate rendering of the scene through a feed-forward neural renderer to
predict the final photorealistic relighting result. We develop a differentiable
rendering process to reconstruct in-the-wild scene illumination, enabling
self-supervised training of our neural renderer on raw image collections. Our
method represents a significant step in bringing the explicit physical control
over lights available in typical 3D computer graphics tools, such as Blender,
to in-the-wild relighting.
</summary>
    <author>
      <name>Chris Careaga</name>
    </author>
    <author>
      <name>Yağız Aksoy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3721238.3730666</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3721238.3730666" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SIGGRAPH 2025, 10 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH Conference Papers, Year 2025, Article No. 105, Pages 1 -
  10</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2508.05626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05619v1</id>
    <updated>2025-08-07T17:57:12Z</updated>
    <published>2025-08-07T17:57:12Z</published>
    <title>The Missing Reward: Active Inference in the Era of Experience</title>
    <summary>  This paper argues that Active Inference (AIF) provides a crucial foundation
for developing autonomous AI agents capable of learning from experience without
continuous human reward engineering. As AI systems begin to exhaust
high-quality training data and rely on increasingly large human workforces for
reward design, the current paradigm faces significant scalability challenges
that could impede progress toward genuinely autonomous intelligence. The
proposal for an ``Era of Experience,'' where agents learn from self-generated
data, is a promising step forward. However, this vision still depends on
extensive human engineering of reward functions, effectively shifting the
bottleneck from data curation to reward curation. This highlights what we
identify as the \textbf{grounded-agency gap}: the inability of contemporary AI
systems to autonomously formulate, adapt, and pursue objectives in response to
changing circumstances. We propose that AIF can bridge this gap by replacing
external reward signals with an intrinsic drive to minimize free energy,
allowing agents to naturally balance exploration and exploitation through a
unified Bayesian objective. By integrating Large Language Models as generative
world models with AIF's principled decision-making framework, we can create
agents that learn efficiently from experience while remaining aligned with
human values. This synthesis offers a compelling path toward AI systems that
can develop autonomously while adhering to both computational and physical
constraints.
</summary>
    <author>
      <name>Bo Wen</name>
    </author>
    <link href="http://arxiv.org/abs/2508.05619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05615v1</id>
    <updated>2025-08-07T17:54:27Z</updated>
    <published>2025-08-07T17:54:27Z</published>
    <title>Test-Time Reinforcement Learning for GUI Grounding via Region
  Consistency</title>
    <summary>  Graphical User Interface (GUI) grounding, the task of mapping natural
language instructions to precise screen coordinates, is fundamental to
autonomous GUI agents. While existing methods achieve strong performance
through extensive supervised training or reinforcement learning with labeled
rewards, they remain constrained by the cost and availability of pixel-level
annotations. We observe that when models generate multiple predictions for the
same GUI element, the spatial overlap patterns reveal implicit confidence
signals that can guide more accurate localization. Leveraging this insight, we
propose GUI-RC (Region Consistency), a test-time scaling method that constructs
spatial voting grids from multiple sampled predictions to identify consensus
regions where models show highest agreement. Without any training, GUI-RC
improves accuracy by 2-3% across various architectures on ScreenSpot
benchmarks. We further introduce GUI-RCPO (Region Consistency Policy
Optimization), which transforms these consistency patterns into rewards for
test-time reinforcement learning. By computing how well each prediction aligns
with the collective consensus, GUI-RCPO enables models to iteratively refine
their outputs on unlabeled data during inference. Extensive experiments
demonstrate the generality of our approach: GUI-RC boosts
Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO
further improves it to 85.14% through self-supervised optimization. Our
approach reveals the untapped potential of test-time scaling and test-time
reinforcement learning for GUI grounding, offering a promising path toward more
robust and data-efficient GUI agents.
</summary>
    <author>
      <name>Yong Du</name>
    </author>
    <author>
      <name>Yuchen Yan</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Zhengxi Lu</name>
    </author>
    <author>
      <name>Chang Zong</name>
    </author>
    <author>
      <name>Weiming Lu</name>
    </author>
    <author>
      <name>Shengpei Jiang</name>
    </author>
    <author>
      <name>Yongliang Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://zju-real.github.io/gui-rcpo Code:
  https://github.com/zju-real/gui-rcpo</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.05615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05571v1</id>
    <updated>2025-08-07T17:02:23Z</updated>
    <published>2025-08-07T17:02:23Z</published>
    <title>Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in
  $\{\pm1, \pm i\}$</title>
    <summary>  Quantization-Aware Training (QAT) integrates quantization into the training
loop, enabling LLMs to learn robust low-bit representations, and is widely
recognized as one of the most promising research directions. All current QAT
research focuses on minimizing quantization error on full-precision models,
where the full-precision accuracy acts as an upper bound (accuracy ceiling). No
existing method has even attempted to surpass this ceiling. To break this
ceiling, we propose a new paradigm: raising the ceiling (full-precision model),
and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$,
the first 2-bit quantization framework for complex-valued LLMs. Specifically,
our method leverages the representational advantages of the complex domain to
boost full-precision accuracy. We map weights to the fourth roots of unity
$\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically
optimal 2-bit representation. Importantly, each quantized weight has either a
zero real or imaginary part, enabling multiplication-free inference using only
additions and element swaps. Experimental results show that Fairy$\pm i$
outperforms the ceiling of existing 2-bit quantization approaches in terms of
both PPL and downstream tasks, while maintaining strict storage and compute
efficiency. This work opens a new direction for building highly accurate and
practical LLMs under extremely low-bit constraints.
</summary>
    <author>
      <name>Feiyu Wang</name>
    </author>
    <author>
      <name>Guoan Wang</name>
    </author>
    <author>
      <name>Yihao Zhang</name>
    </author>
    <author>
      <name>Shengfan Wang</name>
    </author>
    <author>
      <name>Weitao Li</name>
    </author>
    <author>
      <name>Bokai Huang</name>
    </author>
    <author>
      <name>Shimao Chen</name>
    </author>
    <author>
      <name>Zihan Jiang</name>
    </author>
    <author>
      <name>Rui Xu</name>
    </author>
    <author>
      <name>Tong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.05571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05568v1</id>
    <updated>2025-08-07T17:00:47Z</updated>
    <published>2025-08-07T17:00:47Z</published>
    <title>X-VFL: A New Vertical Federated Learning Framework with Cross Completion
  and Decision Subspace Alignment</title>
    <summary>  Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.
</summary>
    <author>
      <name>Qinghua Yao</name>
    </author>
    <author>
      <name>Xiangrui Xu</name>
    </author>
    <author>
      <name>Zhize Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.05568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05545v1</id>
    <updated>2025-08-07T16:22:49Z</updated>
    <published>2025-08-07T16:22:49Z</published>
    <title>PRvL: Quantifying the Capabilities and Risks of Large Language Models
  for PII Redaction</title>
    <summary>  Redacting Personally Identifiable Information (PII) from unstructured text is
critical for ensuring data privacy in regulated domains. While earlier
approaches have relied on rule-based systems and domain-specific Named Entity
Recognition (NER) models, these methods fail to generalize across formats and
contexts. Recent advances in Large Language Models (LLMs) offer a promising
alternative, yet the effect of architectural and training choices on redaction
performance remains underexplored. LLMs have demonstrated strong performance in
tasks that require contextual language understanding, including the redaction
of PII in free-form text. Prior work suggests that with appropriate adaptation,
LLMs can become effective contextual privacy learners. However, the
consequences of architectural and training choices for PII Redaction remain
underexplored. In this work, we present a comprehensive analysis of LLMs as
privacy-preserving PII Redaction systems. We evaluate a range of LLM
architectures and training strategies for their effectiveness in PII Redaction.
Our analysis measures redaction performance, semantic preservation, and PII
leakage, and compares these outcomes against latency and computational cost.
The results provide practical guidance for configuring LLM-based redactors that
are accurate, efficient, and privacy-aware. To support reproducibility and
real-world deployment, we release PRvL, an open-source suite of fine-tuned
models, and evaluation tools for general-purpose PII Redaction. PRvL is built
entirely on open-source LLMs and supports multiple inference settings for
flexibility and compliance. It is designed to be easily customized for
different domains and fully operable within secure, self-managed environments.
This enables data owners to perform redactions without relying on third-party
services or exposing sensitive content beyond their own infrastructure.
</summary>
    <author>
      <name>Leon Garza</name>
    </author>
    <author>
      <name>Anantaa Kotal</name>
    </author>
    <author>
      <name>Aritran Piplai</name>
    </author>
    <author>
      <name>Lavanya Elluri</name>
    </author>
    <author>
      <name>Prajit Das</name>
    </author>
    <author>
      <name>Aman Chadha</name>
    </author>
    <link href="http://arxiv.org/abs/2508.05545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.05537v1</id>
    <updated>2025-08-07T16:13:24Z</updated>
    <published>2025-08-07T16:13:24Z</published>
    <title>Tractable Sharpness-Aware Learning of Probabilistic Circuits</title>
    <summary>  Probabilistic Circuits (PCs) are a class of generative models that allow
exact and tractable inference for a wide range of queries. While recent
developments have enabled the learning of deep and expressive PCs, this
increased capacity can often lead to overfitting, especially when data is
limited. We analyze PC overfitting from a log-likelihood-landscape perspective
and show that it is often caused by convergence to sharp optima that generalize
poorly. Inspired by sharpness aware minimization in neural networks, we propose
a Hessian-based regularizer for training PCs. As a key contribution, we show
that the trace of the Hessian of the log-likelihood-a sharpness proxy that is
typically intractable in deep neural networks-can be computed efficiently for
PCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer
that yields simple closed-form parameter updates for EM, and integrates
seamlessly with gradient based learning methods. Experiments on synthetic and
real-world datasets demonstrate that our method consistently guides PCs toward
flatter minima, improves generalization performance.
</summary>
    <author>
      <name>Hrithik Suresh</name>
    </author>
    <author>
      <name>Sahil Sidheekh</name>
    </author>
    <author>
      <name>Vishnu Shreeram M. P</name>
    </author>
    <author>
      <name>Sriraam Natarajan</name>
    </author>
    <author>
      <name>Narayanan C. Krishnan</name>
    </author>
    <link href="http://arxiv.org/abs/2508.05537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.05537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
