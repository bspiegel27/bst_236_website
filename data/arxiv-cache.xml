<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-25T01:00:04Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-25T01:00:12Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>129409</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.20612v1</id>
    <title>Making Large Language Models Efficient Dense Retrievers</title>
    <updated>2025-12-23T18:58:25Z</updated>
    <link href="https://arxiv.org/abs/2512.20612v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20612v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:58:25Z</published>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Yibin Lei</name>
    </author>
    <author>
      <name>Shwai He</name>
    </author>
    <author>
      <name>Ang Li</name>
    </author>
    <author>
      <name>Andrew Yates</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20605v1</id>
    <title>Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</title>
    <updated>2025-12-23T18:51:50Z</updated>
    <link href="https://arxiv.org/abs/2512.20605v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20605v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:51:50Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Seijin Kobayashi</name>
    </author>
    <author>
      <name>Yanick Schimpf</name>
    </author>
    <author>
      <name>Maximilian Schlegel</name>
    </author>
    <author>
      <name>Angelika Steger</name>
    </author>
    <author>
      <name>Maciej Wolczyk</name>
    </author>
    <author>
      <name>Johannes von Oswald</name>
    </author>
    <author>
      <name>Nino Scherre</name>
    </author>
    <author>
      <name>Kaitlin Maile</name>
    </author>
    <author>
      <name>Guillaume Lajoie</name>
    </author>
    <author>
      <name>Blake A. Richards</name>
    </author>
    <author>
      <name>Rif A. Saurous</name>
    </author>
    <author>
      <name>James Manyika</name>
    </author>
    <author>
      <name>Blaise Agüera y Arcas</name>
    </author>
    <author>
      <name>Alexander Meulemans</name>
    </author>
    <author>
      <name>João Sacramento</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20591v1</id>
    <title>LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</title>
    <updated>2025-12-23T18:38:25Z</updated>
    <link href="https://arxiv.org/abs/2512.20591v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20591v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value &lt; 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:38:25Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Changyi Lin</name>
    </author>
    <author>
      <name>Boda Huo</name>
    </author>
    <author>
      <name>Mingyang Yu</name>
    </author>
    <author>
      <name>Emily Ruppel</name>
    </author>
    <author>
      <name>Bingqing Chen</name>
    </author>
    <author>
      <name>Jonathan Francis</name>
    </author>
    <author>
      <name>Ding Zhao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20587v1</id>
    <title>Quantum Gates from Wolfram Model Multiway Rewriting Systems</title>
    <updated>2025-12-23T18:34:42Z</updated>
    <link href="https://arxiv.org/abs/2512.20587v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20587v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We show how representations of finite-dimensional quantum operators can be constructed using nondeterministic rewriting systems. In particular, we investigate Wolfram model multiway rewriting systems based on string substitutions. Multiway systems were proposed by S. Wolfram as generic model systems for multicomputational processes, emphasizing their significance as a foundation for modeling complexity, nondeterminism, and branching structures of measurement outcomes. Here, we investigate a specific class of multiway systems based on cyclic character strings with a neighborhood constraint - the latter called Leibnizian strings. We show that such strings exhibit a Fermi-Dirac distribution for expectation values of occupation numbers of character neighborhoods. A Leibnizian string serves as an abstraction of a $N$-fermion system. A multiway system of these strings encodes causal relations between rewriting events in a nondeterministic manner. The collection of character strings realizes a $\mathbb{Z}$-module with a symmetric $\mathbb{Z}$-bilinear form. For discrete spaces, this generalizes the notion of an inner product over a vector field. This admits a discrete analogue of the path integral and a $S$-matrix for multiway systems of Leibnizian strings. The elements of this $S$-matrix yield transition amplitudes between states of the multiway system based on an action defined over a sequence of Leibnizian strings. We then show that these $S$-matrices give explicit representations of quantum gates for qubits and qudits, and also circuits composed of such gates. We find that, as formal models of nondeterministic computation, rewriting systems of Leibnizian strings with causal structure encode representations of the CNOT, $π/8$, and Hadamard gates. Hence, using multiway systems one can represent quantum circuits for qubits.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.QA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:34:42Z</published>
    <arxiv:comment>39 pages, 15 figures</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Furkan Semih Dündar</name>
    </author>
    <author>
      <name>Xerxes D. Arsiwalla</name>
    </author>
    <author>
      <name>Hatem Elshatlawy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20586v1</id>
    <title>Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent</title>
    <updated>2025-12-23T18:32:17Z</updated>
    <link href="https://arxiv.org/abs/2512.20586v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20586v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p &gt; 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:32:17Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Humza Nusrat</name>
    </author>
    <author>
      <name>Luke Francisco</name>
    </author>
    <author>
      <name>Bing Luo</name>
    </author>
    <author>
      <name>Hassan Bagher-Ebadian</name>
    </author>
    <author>
      <name>Joshua Kim</name>
    </author>
    <author>
      <name>Karen Chin-Snyder</name>
    </author>
    <author>
      <name>Salim Siddiqui</name>
    </author>
    <author>
      <name>Mira Shah</name>
    </author>
    <author>
      <name>Eric Mellon</name>
    </author>
    <author>
      <name>Mohammad Ghassemi</name>
    </author>
    <author>
      <name>Anthony Doemer</name>
    </author>
    <author>
      <name>Benjamin Movsas</name>
    </author>
    <author>
      <name>Kundan Thind</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20581v1</id>
    <title>MERGE-RNA: a physics-based model to predict RNA secondary structure ensembles with chemical probing</title>
    <updated>2025-12-23T18:26:57Z</updated>
    <link href="https://arxiv.org/abs/2512.20581v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20581v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The function of RNA molecules is deeply related to their secondary structure, which determines which nucleobases are accessible for pairing. Most RNA molecules however function through dynamic and heterogeneous structural ensembles. Chemical probing methods (e.g., DMS probing) rely on selective chemical modification of accessible RNA nucleotides to infer base-pairing status, yet the resulting nucleotide-resolution data represent ensemble averages over dynamic RNA conformations. We present MERGE-RNA, a unified, physics-based framework that explicitly models the full experimental pipeline, from the thermodynamics of probe binding to the mutational profiling readout. By integrating measurements across probe concentrations and replicates, our model learns a small set of transferable and interpretable parameters together with minimal sequence-specific soft constraints. This enables the prediction of secondary structure ensembles that best explain the data and the detection of suboptmal structures involved in dynamic processes. We validate MERGE-RNA on diverse RNAs, showing that it achieves strong structural accuracy while preserving essential conformational heterogeneity. In a designed RNA for which we report new DMS data, MERGE-RNA detects transient intermediate states associated with strand displacement, dynamics that remain invisible to traditional methods.</summary>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:26:57Z</published>
    <arxiv:primary_category term="q-bio.BM"/>
    <author>
      <name>Giuseppe Sacco</name>
    </author>
    <author>
      <name>Jianhui Li</name>
    </author>
    <author>
      <name>Redmond P. Smyth</name>
    </author>
    <author>
      <name>Guido Sanguinetti</name>
    </author>
    <author>
      <name>Giovanni Bussi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20579v1</id>
    <title>Spin-induced quadrupole moment based test for eccentric binaries</title>
    <updated>2025-12-23T18:26:37Z</updated>
    <link href="https://arxiv.org/abs/2512.20579v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20579v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The spin-induced quadrupole moment-based test of black hole nature is routinely used to probe the true nature of detected binary signals, assuming a circular orbit. We extend the applicability of the method to binaries in eccentric orbits. Considering simulated signals of varying masses, spins, and signal strengths, we demonstrate how the systematic errors resulting from neglecting orbital eccentricity compare with the statistical errors, using a semi-analytic Fisher matrix-based formalism that accounts for both current and future detectors. Further, we quantify the systematic errors by developing a Bayesian inference framework for the current detector network. The inspiral-only aligned spin gravitational wave waveform model for eccentric binaries, TaylorF2Ecc, is employed. For the current detector network, neglecting an initial eccentricity of $e_0^{\rm inj}=0.1$ defined at $20\,\mathrm {Hz} $ can lead to a serious bias in binary parameter inference. Notably, a nearly equal-mass, moderately spinning binary black hole in an eccentric orbit can be identified as a non-black hole binary with extreme spins and asymmetric masses. We demonstrate the criticality of biased estimates that may arise when neglecting the orbital eccentricity while performing tests of black hole nature and discuss prospects.</summary>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:26:37Z</published>
    <arxiv:comment>10 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="gr-qc"/>
    <author>
      <name>N. V. Krishnendu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20578v1</id>
    <title>Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits</title>
    <updated>2025-12-23T18:21:32Z</updated>
    <link href="https://arxiv.org/abs/2512.20578v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20578v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:21:32Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Amirhosein Ghasemabadi</name>
    </author>
    <author>
      <name>Di Niu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20569v1</id>
    <title>Distilling to Hybrid Attention Models via KL-Guided Layer Selection</title>
    <updated>2025-12-23T18:12:22Z</updated>
    <link href="https://arxiv.org/abs/2512.20569v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20569v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T18:12:22Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yanhong Li</name>
    </author>
    <author>
      <name>Songlin Yang</name>
    </author>
    <author>
      <name>Shawn Tan</name>
    </author>
    <author>
      <name>Mayank Mishra</name>
    </author>
    <author>
      <name>Rameswar Panda</name>
    </author>
    <author>
      <name>Jiawei Zhou</name>
    </author>
    <author>
      <name>Yoon Kim</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20555v1</id>
    <title>A Manifestly Causal Approach to Quantum Field Theory</title>
    <updated>2025-12-23T17:55:35Z</updated>
    <link href="https://arxiv.org/abs/2512.20555v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20555v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We develop a probability-level, manifestly causal formalism for calculations in QFT. The approach involves an implicit summation over final states, which makes causality manifest since retarded propagators emerge naturally. This inclusive summation over final states may also offer insights into the cancellation of IR divergences in physical observables within gauge theories, in accordance with the BN and KLN theorems. To study this, we first conduct particle scattering calculations using conventional methods, determining the quark-antiquark production cross section at first-order in gluon corrections, with careful tracking and cancellation of both IR and UV divergences. We then apply the causal formalism to analogous processes in scalar field theory, introducing novel diagrams that represent algebraic terms at the probability level, akin to Feynman diagrams at the amplitude level. We present a list of rules that generate all probability-level diagrams for particle scattering processes in which one is fully inclusive over final states that contain no initial-state particles. We also investigate the Unruh effect through the lens of the causal formalism. We calculate the transition rate of an accelerating UdW detector coupled to a massive scalar field, from both the perspective of an inertial observer and an accelerating observer. We confirm that the two perspectives give the same transition rate, despite the Rindler observer describing the Minkowski vacuum state as a thermal bath of particles. Numerical results for the transition rate are presented, highlighting the transient effects caused by forcing the field to initially be in the Minkowski vacuum state. Finally, we review the literature regarding the response of an UdW detector on various trajectories in the spacetime of a (3+1)-D Schwarzschild black hole, with a view to extending the analysis in the future using our causal formalism.</summary>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-23T17:55:35Z</published>
    <arxiv:comment>PhD thesis, University of Manchester, 2025</arxiv:comment>
    <arxiv:primary_category term="hep-th"/>
    <author>
      <name>Ross Jenkinson</name>
    </author>
  </entry>
</feed>
