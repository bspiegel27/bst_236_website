<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-11-26T00:57:15Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-26T00:57:15Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>126997</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2511.19436v1</id>
    <title>VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection</title>
    <updated>2025-11-24T18:59:56Z</updated>
    <link href="https://arxiv.org/abs/2511.19436v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19436v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:59:56Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Qiang Wang</name>
    </author>
    <author>
      <name>Xinyuan Gao</name>
    </author>
    <author>
      <name>SongLin Dong</name>
    </author>
    <author>
      <name>Jizhou Han</name>
    </author>
    <author>
      <name>Jiangyang Li</name>
    </author>
    <author>
      <name>Yuhang He</name>
    </author>
    <author>
      <name>Yihong Gong</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19433v1</id>
    <title>Mixture of Horizons in Action Chunking</title>
    <updated>2025-11-24T18:59:51Z</updated>
    <link href="https://arxiv.org/abs/2511.19433v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19433v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:59:51Z</published>
    <arxiv:comment>15 pages, 14 figures</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Dong Jing</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Jiaqi Liu</name>
    </author>
    <author>
      <name>Weiliang Tang</name>
    </author>
    <author>
      <name>Zelong Sun</name>
    </author>
    <author>
      <name>Yunchao Yao</name>
    </author>
    <author>
      <name>Zhenyu Wei</name>
    </author>
    <author>
      <name>Yunhui Liu</name>
    </author>
    <author>
      <name>Zhiwu Lu</name>
    </author>
    <author>
      <name>Mingyu Ding</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19431v1</id>
    <title>Cloud4D</title>
    <updated>2025-11-24T18:59:37Z</updated>
    <link href="https://arxiv.org/abs/2511.19431v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19431v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($&lt;10\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:59:37Z</published>
    <arxiv:comment>NeurIPS 2025 Spotlight, project page: https://cloud4d.jacob-lin.com/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jacob Lin</name>
    </author>
    <author>
      <name>Edward Gryspeerdt</name>
    </author>
    <author>
      <name>Ronald Clark</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19418v1</id>
    <title>Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</title>
    <updated>2025-11-24T18:55:19Z</updated>
    <link href="https://arxiv.org/abs/2511.19418v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19418v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:55:19Z</published>
    <arxiv:comment>Project page: https://wakalsprojectpage.github.io/comt-website/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yiming Qin</name>
    </author>
    <author>
      <name>Bomin Wei</name>
    </author>
    <author>
      <name>Jiaxin Ge</name>
    </author>
    <author>
      <name>Konstantinos Kallidromitis</name>
    </author>
    <author>
      <name>Stephanie Fu</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Xudong Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19404v1</id>
    <title>Nonparametric Instrumental Variable Regression with Observed Covariates</title>
    <updated>2025-11-24T18:42:49Z</updated>
    <link href="https://arxiv.org/abs/2511.19404v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19404v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:42:49Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Zikai Shen</name>
    </author>
    <author>
      <name>Zonghao Chen</name>
    </author>
    <author>
      <name>Dimitri Meunier</name>
    </author>
    <author>
      <name>Ingo Steinwart</name>
    </author>
    <author>
      <name>Arthur Gretton</name>
    </author>
    <author>
      <name>Zhu Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19394v1</id>
    <title>BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation</title>
    <updated>2025-11-24T18:31:51Z</updated>
    <link href="https://arxiv.org/abs/2511.19394v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19394v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single "background" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.
  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:31:51Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Rachit Saluja</name>
    </author>
    <author>
      <name>Asli Cihangir</name>
    </author>
    <author>
      <name>Ruining Deng</name>
    </author>
    <author>
      <name>Johannes C. Paetzold</name>
    </author>
    <author>
      <name>Fengbei Liu</name>
    </author>
    <author>
      <name>Mert R. Sabuncu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19393v1</id>
    <title>Guesswork in the gap: the impact of uncertainty in the compact binary population on source classification</title>
    <updated>2025-11-24T18:31:39Z</updated>
    <link href="https://arxiv.org/abs/2511.19393v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19393v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The nature of the compact objects within the supposed "lower mass gap" remains uncertain. Observations of GW190814 and GW230529 highlight the challenges gravitational waves face in distinguishing neutron stars from black holes. Interpreting these systems is especially difficult because classifications depend simultaneously on measurement noise, compact binary population models, and equation of state (EOS) constraints on the maximum neutron star mass. We analyze 66 confident events from GWTC-3 to quantify how the probability of a component being a neutron star, P(NS), varies across the population. The effects are substantial, the dominant drivers of classification are the pairing preferences of neutron stars with other compact objects, and the neutron star spin distributions. The data reveals that P(NS) varies between 1% - 67% for GW230529's primary and between 51% - 100% for GW190425's primary. By contrast, P(NS) for GW190814's secondary varies by &lt;10%, demonstrating robustness from its high signal-to-noise ratio and small mass ratio. Analysis using EOS information tends to affect P(NS) through the inferred maximum neutron star mass rather than the maximum spin. As it stands, P(NS) remains sensitive to numerous population parameters, limiting its reliability and potentially leading to ambiguous classifications of future GW events.</summary>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:31:39Z</published>
    <arxiv:comment>26 pages, 12 figures, 5 tables</arxiv:comment>
    <arxiv:primary_category term="astro-ph.HE"/>
    <author>
      <name>Utkarsh Mali</name>
    </author>
    <author>
      <name>Reed Essick</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19390v1</id>
    <title>Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme</title>
    <updated>2025-11-24T18:30:04Z</updated>
    <link href="https://arxiv.org/abs/2511.19390v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19390v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:30:04Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Rudy Morel</name>
    </author>
    <author>
      <name>Francesco Pio Ramunno</name>
    </author>
    <author>
      <name>Jeff Shen</name>
    </author>
    <author>
      <name>Alberto Bietti</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Miles Cranmer</name>
    </author>
    <author>
      <name>Siavash Golkar</name>
    </author>
    <author>
      <name>Olexandr Gugnin</name>
    </author>
    <author>
      <name>Geraud Krawezik</name>
    </author>
    <author>
      <name>Tanya Marwah</name>
    </author>
    <author>
      <name>Michael McCabe</name>
    </author>
    <author>
      <name>Lucas Meyer</name>
    </author>
    <author>
      <name>Payel Mukhopadhyay</name>
    </author>
    <author>
      <name>Ruben Ohana</name>
    </author>
    <author>
      <name>Liam Parker</name>
    </author>
    <author>
      <name>Helen Qu</name>
    </author>
    <author>
      <name>François Rozet</name>
    </author>
    <author>
      <name>K. D. Leka</name>
    </author>
    <author>
      <name>François Lanusse</name>
    </author>
    <author>
      <name>David Fouhey</name>
    </author>
    <author>
      <name>Shirley Ho</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19381v1</id>
    <title>Asymptotic linear dependence and ellipse statistics for multivariate two-sample homogeneity test</title>
    <updated>2025-11-24T18:21:09Z</updated>
    <link href="https://arxiv.org/abs/2511.19381v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19381v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Statistical depth, which measures the center-outward rank of a given sample with respect to its underlying distribution, has become a popular and powerful tool in nonparametric inference. In this paper, we investigate the use of statistical depth in multivariate two-sample problems. We propose a new depth-based nonparametric two-sample test, which has the Chi-square(1) asymptotic distribution under the null hypothesis. Simulations and real-data applications highlight the efficacy and practical value of the proposed test.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:21:09Z</published>
    <arxiv:comment>19 pages; 9 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Chifeng Shen</name>
    </author>
    <author>
      <name>Yuejiao Fu</name>
    </author>
    <author>
      <name>Michael Chen</name>
    </author>
    <author>
      <name>Xiaoping Shi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.19379v1</id>
    <title>Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware</title>
    <updated>2025-11-24T18:19:42Z</updated>
    <link href="https://arxiv.org/abs/2511.19379v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.19379v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\mathcal{C} \approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\mathcal{C} \approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-24T18:19:42Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Srishti Gupta</name>
    </author>
    <author>
      <name>Yashasvee Taiwade</name>
    </author>
  </entry>
</feed>
