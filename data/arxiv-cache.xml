<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-19T00:57:09Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-18T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">119039</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.11624v1</id>
    <updated>2025-08-15T17:52:56Z</updated>
    <published>2025-08-15T17:52:56Z</published>
    <title>LoRAtorio: An intrinsic approach to LoRA Skill Composition</title>
    <summary>  Low-Rank Adaptation (LoRA) has become a widely adopted technique in
text-to-image diffusion models, enabling the personalisation of visual concepts
such as characters, styles, and objects. However, existing approaches struggle
to effectively compose multiple LoRA adapters, particularly in open-ended
settings where the number and nature of required skills are not known in
advance. In this work, we present LoRAtorio, a novel train-free framework for
multi-LoRA composition that leverages intrinsic model behaviour. Our method is
motivated by two key observations: (1) LoRA adapters trained on narrow domains
produce denoised outputs that diverge from the base model, and (2) when
operating out-of-distribution, LoRA outputs show behaviour closer to the base
model than when conditioned in distribution. The balance between these two
observations allows for exceptional performance in the single LoRA scenario,
which nevertheless deteriorates when multiple LoRAs are loaded. Our method
operates in the latent space by dividing it into spatial patches and computing
cosine similarity between each patch's predicted noise and that of the base
model. These similarities are used to construct a spatially-aware weight
matrix, which guides a weighted aggregation of LoRA outputs. To address domain
drift, we further propose a modification to classifier-free guidance that
incorporates the base model's unconditional score into the composition. We
extend this formulation to a dynamic module selection setting, enabling
inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio
achieves state-of-the-art performance, showing up to a 1.3% improvement in
ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises
effectively to multiple latent diffusion models.
</summary>
    <author>
      <name>Niki Foteinopoulou</name>
    </author>
    <author>
      <name>Ignas Budvytis</name>
    </author>
    <author>
      <name>Stephan Liwicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.11624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11622v1</id>
    <updated>2025-08-15T17:48:36Z</updated>
    <published>2025-08-15T17:48:36Z</published>
    <title>Deconfounding via Profiled Transfer Learning</title>
    <summary>  Unmeasured confounders are a major source of bias in regression-based effect
estimation and causal inference. In this paper, we advocate a new profiled
transfer learning framework, ProTrans, to address confounding effects in the
target dataset, when additional source datasets that possess similar
confounding structures are available. We introduce the concept of profiled
residuals to characterize the shared confounding patterns between source and
target datasets. By incorporating these profiled residuals into the target
debiasing step, we effectively mitigates the latent confounding effects. We
also propose a source selection strategy to enhance robustness of ProTrans
against noninformative sources. As a byproduct, ProTrans can also be utilized
to estimate treatment effects when potential confounders exist, without the use
of auxiliary features such as instrumental or proxy variables, which are often
challenging to select in practice. Theoretically, we prove that the resulting
estimated model shift from sources to target is confounding-free without any
assumptions imposed on the true confounding structure, and that the target
parameter estimation achieves the minimax optimal rate under mild conditions.
Simulated and real-world experiments validate the effectiveness of ProTrans and
support the theoretical findings.
</summary>
    <author>
      <name>Ziyuan Chen</name>
    </author>
    <author>
      <name>Yifan Jiang</name>
    </author>
    <author>
      <name>Jingyuan Liu</name>
    </author>
    <author>
      <name>Fang Yao</name>
    </author>
    <link href="http://arxiv.org/abs/2508.11622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11616v1</id>
    <updated>2025-08-15T17:29:06Z</updated>
    <published>2025-08-15T17:29:06Z</published>
    <title>Controlling Multimodal LLMs via Reward-guided Decoding</title>
    <summary>  As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.
</summary>
    <author>
      <name>Oscar Mañas</name>
    </author>
    <author>
      <name>Pierluca D'Oro</name>
    </author>
    <author>
      <name>Koustuv Sinha</name>
    </author>
    <author>
      <name>Adriana Romero-Soriano</name>
    </author>
    <author>
      <name>Michal Drozdzal</name>
    </author>
    <author>
      <name>Aishwarya Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ICCV 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.11616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11614v1</id>
    <updated>2025-08-15T17:24:36Z</updated>
    <published>2025-08-15T17:24:36Z</published>
    <title>Bulk viscous cosmological models with cosmological constant:
  Observational constraints</title>
    <summary>  We investigate whether viscous cold dark matter (vCDM) in a
$\Lambda$-dominated FLRW universe can alleviate the Hubble tension while
satisfying thermodynamic constraints, examining both flat and curved
geometries. We model vCDM with bulk viscosity $\zeta =
\zeta_0\,(\Omega_{vc}/\Omega_{vc0})^m$, where $m$ determines the viscosity
evolution and $\Omega_{vc}$ is the density parameter of vCDM. We explore two
particular scenarios: constant viscosity ($m=0$), and variable viscosity ($m$
free). Using Bayesian inference, we constrain these models with the latest
datasets: the Pantheon+ SN Ia sample (both with SH0ES calibration, PPS, and
without it, PP), $H(z)$ measurements from CC and BAO as separate datasets, and
a Gaussian prior on $H_0$ from 2022 SH0ES baseline, $H_0=73.04 \pm 1.04$
km/s/Mpc (R22 prior). We compare the models via information criteria such as
AIC, BIC, DIC, and Bayesian evidence. Our results reveal that the Hubble
tension persists, although it shows partial alleviation ($\sim 1\sigma$
tension) in all investigated scenarios when local measurements are included.
For the flat $m=0$ case, the joint analysis yields $H_0 =
71.05^{+0.62}_{-0.60}$ km/s/Mpc. Curved model initially favors $\Omega_{K0} &gt;
0$ (at more than $2\sigma$), but this preference shifts toward flatness once
the PPS+R22 prior are included. Notably, the current viscosity is constrained
to $\zeta_0 \sim 10^6$ Pa s in all scenarios, in agreement with the
thermodynamic requirements. Although model selection via BIC and Bayesian
evidence favors $\Lambda$CDM, AIC and DIC show mild support for viscous models
in some datasets. Bulk viscous models moderately improve fits but neither
resolve the Hubble tension nor outperform the $\Lambda$CDM model. To achieve
more robust constraints, future analyses should incorporate CMB observations,
which are expected to break parameter degeneracies involving $m$ and
$\tilde{\zeta}_0$.
</summary>
    <author>
      <name>R. Noemí Villalobos</name>
    </author>
    <author>
      <name>Yerko Vásquez</name>
    </author>
    <author>
      <name>Norman Cruz</name>
    </author>
    <author>
      <name>Carlos H. López-Caraballo</name>
    </author>
    <link href="http://arxiv.org/abs/2508.11614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11584v2</id>
    <updated>2025-08-18T05:11:18Z</updated>
    <published>2025-08-15T16:42:23Z</published>
    <title>Visual Perception Engine: Fast and Flexible Multi-Head Inference for
  Robotic Vision Tasks</title>
    <summary>  Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.
</summary>
    <author>
      <name>Jakub Łucki</name>
    </author>
    <author>
      <name>Jonathan Becktor</name>
    </author>
    <author>
      <name>Georgios Georgakis</name>
    </author>
    <author>
      <name>Rob Royce</name>
    </author>
    <author>
      <name>Shehryar Khattak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.11584v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11584v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11576v1</id>
    <updated>2025-08-15T16:33:14Z</updated>
    <published>2025-08-15T16:33:14Z</published>
    <title>Causality Matters: How Temporal Information Emerges in Video Language
  Models</title>
    <summary>  Video language models (VideoLMs) have made significant progress in multimodal
understanding. However, temporal understanding, which involves identifying
event order, duration, and relationships across time, still remains a core
challenge. Prior works emphasize positional encodings (PEs) as a key mechanism
for encoding temporal structure. Surprisingly, we find that removing or
modifying PEs in video inputs yields minimal degradation in the performance of
temporal understanding. In contrast, reversing the frame sequence while
preserving the original PEs causes a substantial drop. To explain this
behavior, we conduct substantial analysis experiments to trace how temporal
information is integrated within the model. We uncover a causal information
pathway: temporal cues are progressively synthesized through inter-frame
attention, aggregated in the final frame, and subsequently integrated into the
query tokens. This emergent mechanism shows that temporal reasoning emerges
from inter-visual token interactions under the constraints of causal attention,
which implicitly encodes temporal structure. Based on these insights, we
propose two efficiency-oriented strategies: staged cross-modal attention and a
temporal exit mechanism for early token truncation. Experiments on two
benchmarks validate the effectiveness of both approaches. To the best of our
knowledge, this is the first work to systematically investigate video temporal
understanding in VideoLMs, offering insights for future model improvement.
</summary>
    <author>
      <name>Yumeng Shi</name>
    </author>
    <author>
      <name>Quanyu Long</name>
    </author>
    <author>
      <name>Yin Wu</name>
    </author>
    <author>
      <name>Wenya Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2508.11576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11575v1</id>
    <updated>2025-08-15T16:31:12Z</updated>
    <published>2025-08-15T16:31:12Z</published>
    <title>Activate Me!: Designing Efficient Activation Functions for
  Privacy-Preserving Machine Learning with Fully Homomorphic Encryption</title>
    <summary>  The growing adoption of machine learning in sensitive areas such as
healthcare and defense introduces significant privacy and security challenges.
These domains demand robust data protection, as models depend on large volumes
of sensitive information for both training and inference. Fully Homomorphic
Encryption (FHE) presents a compelling solution by enabling computations
directly on encrypted data, maintaining confidentiality across the entire
machine learning workflow. However, FHE inherently supports only linear
operations, making it difficult to implement non-linear activation functions,
essential components of modern neural networks. This work focuses on designing,
implementing, and evaluating activation functions tailored for FHE-based
machine learning. We investigate two commonly used functions: the Square
function and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20
architectures with the CKKS scheme from the OpenFHE library. For ReLU, we
assess two methods: a conventional low-degree polynomial approximation and a
novel scheme-switching technique that securely evaluates ReLU under FHE
constraints. Our findings show that the Square function performs well in
shallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per
image. In contrast, deeper models like ResNet-20 benefit more from ReLU. The
polynomial approximation yields 83.8% accuracy with 1,145 seconds per image,
while our scheme-switching method improves accuracy to 89.8%, albeit with a
longer inference time of 1,697 seconds. These results underscore a critical
trade-off in FHE-based ML: faster activation functions often reduce accuracy,
whereas those preserving accuracy demand greater computational resources.
</summary>
    <author>
      <name>Nges Brian Njungle</name>
    </author>
    <author>
      <name>Michel A. Kinsy</name>
    </author>
    <link href="http://arxiv.org/abs/2508.11575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11542v1</id>
    <updated>2025-08-15T15:38:52Z</updated>
    <published>2025-08-15T15:38:52Z</published>
    <title>Nested Operator Inference for Adaptive Data-Driven Learning of
  Reduced-order Models</title>
    <summary>  This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.
</summary>
    <author>
      <name>Nicole Aretz</name>
    </author>
    <author>
      <name>Karen Willcox</name>
    </author>
    <link href="http://arxiv.org/abs/2508.11542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11538v1</id>
    <updated>2025-08-15T15:34:56Z</updated>
    <published>2025-08-15T15:34:56Z</published>
    <title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title>
    <summary>  Video reasoning segmentation (VRS) endeavors to delineate referred objects in
videos guided by implicit instructions that encapsulate human intent and
temporal logic. Previous approaches leverage large vision language models
(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.
However, this paradigm suffers from limited interpretability during inference
and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing
inspiration from seminal breakthroughs in reinforcement learning, we introduce
Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in
segmentation. Veason-R1 is trained through Group Relative Policy Optimization
(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we
curate high-quality CoT training data to instill structured reasoning
trajectories, bridging video-level semantics and frame-level spatial grounding,
yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO
fine-tuning encourages efficient exploration of the reasoning space by
optimizing reasoning chains. To this end, we incorporate a holistic reward
mechanism that synergistically enhances spatial alignment and temporal
consistency, bolstering keyframe localization and fine-grained grounding.
Comprehensive empirical evaluations demonstrate that Veason-R1 achieves
state-of-the-art performance on multiple benchmarks, surpassing prior art by
significant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),
while exhibiting robustness to hallucinations (+8.8 R). Our code and model
weights will be available at Veason-R1.
</summary>
    <author>
      <name>Sitong Gong</name>
    </author>
    <author>
      <name>Lu Zhang</name>
    </author>
    <author>
      <name>Yunzhi Zhuge</name>
    </author>
    <author>
      <name>Xu Jia</name>
    </author>
    <author>
      <name>Pingping Zhang</name>
    </author>
    <author>
      <name>Huchuan Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.11538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.11537v1</id>
    <updated>2025-08-15T15:34:49Z</updated>
    <published>2025-08-15T15:34:49Z</published>
    <title>MultiPark: Multimodal Parking Transformer with Next-Segment Prediction</title>
    <summary>  Parking accurately and safely in highly constrained spaces remains a critical
challenge. Unlike structured driving environments, parking requires executing
complex maneuvers such as frequent gear shifts and steering saturation. Recent
attempts to employ imitation learning (IL) for parking have achieved promising
results. However, existing works ignore the multimodal nature of parking
behavior in lane-free open space, failing to derive multiple plausible
solutions under the same situation. Notably, IL-based methods encompass
inherent causal confusion, so enabling a neural network to generalize across
diverse parking scenarios is particularly difficult. To address these
challenges, we propose MultiPark, an autoregressive transformer for multimodal
parking. To handle paths filled with abrupt turning points, we introduce a
data-efficient next-segment prediction paradigm, enabling spatial
generalization and temporal extrapolation. Furthermore, we design learnable
parking queries factorized into gear, longitudinal, and lateral components,
parallelly decoding diverse parking behaviors. To mitigate causal confusion in
IL, our method employs target-centric pose and ego-centric collision as
outcome-oriented loss across all modalities beyond pure imitation loss.
Evaluations on real-world datasets demonstrate that MultiPark achieves
state-of-the-art performance across various scenarios. We deploy MultiPark on a
production vehicle, further confirming our approach's robustness in real-world
parking environments.
</summary>
    <author>
      <name>Han Zheng</name>
    </author>
    <author>
      <name>Zikang Zhou</name>
    </author>
    <author>
      <name>Guli Zhang</name>
    </author>
    <author>
      <name>Zhepei Wang</name>
    </author>
    <author>
      <name>Kaixuan Wang</name>
    </author>
    <author>
      <name>Peiliang Li</name>
    </author>
    <author>
      <name>Shaojie Shen</name>
    </author>
    <author>
      <name>Ming Yang</name>
    </author>
    <author>
      <name>Tong Qin</name>
    </author>
    <link href="http://arxiv.org/abs/2508.11537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.11537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
