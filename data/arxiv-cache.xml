<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-07T00:56:47Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">114325</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.05344v1</id>
    <updated>2025-06-05T17:59:55Z</updated>
    <published>2025-06-05T17:59:55Z</published>
    <title>SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs</title>
    <summary>  Multimodal Large Language Models (MLLMs) are commonly derived by extending
pre-trained Large Language Models (LLMs) with visual capabilities. In this
work, we investigate how MLLMs process visual inputs by analyzing their
attention mechanisms. We reveal a surprising sparsity phenomenon: only a small
subset (approximately less than 5%) of attention heads in LLMs actively
contribute to visual understanding, termed visual heads. To identify these
heads efficiently, we design a training-free framework that quantifies
head-level visual relevance through targeted response analysis. Building on
this discovery, we introduce SparseMM, a KV-Cache optimization strategy that
allocates asymmetric computation budgets to heads in LLMs based on their visual
scores, leveraging the sparity of visual heads for accelerating the inference
of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the
particularity of visual, SparseMM prioritizes stress and retaining visual
semantics during decoding. Extensive evaluations across mainstream multimodal
benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency
trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%
memory reduction during generation while maintaining performance parity on
efficiency test. Our project is open sourced at
https://github.com/CR400AF-A/SparseMM.
</summary>
    <author>
      <name>Jiahui Wang</name>
    </author>
    <author>
      <name>Zuyan Liu</name>
    </author>
    <author>
      <name>Yongming Rao</name>
    </author>
    <author>
      <name>Jiwen Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2506.05344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05345v1</id>
    <updated>2025-06-05T17:59:55Z</updated>
    <published>2025-06-05T17:59:55Z</published>
    <title>Inference-Time Hyper-Scaling with KV Cache Compression</title>
    <summary>  Inference-time scaling trades efficiency for increased reasoning accuracy by
generating longer or more parallel sequences. However, in Transformer LLMs,
generation cost is bottlenecked by the size of the key-value (KV) cache, rather
than the number of generated tokens. Hence, we explore inference-time
hyper-scaling: by compressing the KV cache, we can generate more tokens within
the same compute budget and further improve the accuracy of scaled inference.
The success of this approach, however, hinges on the ability of compression
methods to preserve accuracy even at high compression ratios. To make
hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a
novel method for sparsifying KV caches that only requires 1K training steps to
achieve 8$\times$ compression, while maintaining better accuracy than
training-free sparse attention. Instead of prematurely discarding cached
tokens, DMS delays token eviction, implicitly merging representations and
preserving critical information. We demonstrate the effectiveness of
inference-time hyper-scaling with DMS on multiple families of LLMs, showing
that it boosts accuracy for comparable inference runtime and memory load. For
instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on
GPQA, and 9.6 on LiveCodeBench across compute budgets.
</summary>
    <author>
      <name>Adrian Łańcucki</name>
    </author>
    <author>
      <name>Konrad Staniszewski</name>
    </author>
    <author>
      <name>Piotr Nawrot</name>
    </author>
    <author>
      <name>Edoardo M. Ponti</name>
    </author>
    <link href="http://arxiv.org/abs/2506.05345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05341v1</id>
    <updated>2025-06-05T17:59:42Z</updated>
    <published>2025-06-05T17:59:42Z</published>
    <title>Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via
  Spatial Reasoning</title>
    <summary>  Realistic 3D indoor scene synthesis is vital for embodied AI and digital
content creation. It can be naturally divided into two subtasks: object
generation and layout generation. While recent generative models have
significantly advanced object-level quality and controllability, layout
generation remains challenging due to limited datasets. Existing methods either
overfit to these datasets or rely on predefined constraints to optimize
numerical layout that sacrifice flexibility. As a result, they fail to generate
scenes that are both open-vocabulary and aligned with fine-grained user
instructions. We introduce DirectLayout, a framework that directly generates
numerical 3D layouts from text descriptions using generalizable spatial
reasoning of large language models (LLMs). DirectLayout decomposes the
generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting
it into 3D space, and refining object placements. To enable explicit spatial
reasoning and help the model grasp basic principles of object placement, we
employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.
Additionally, we design CoT-Grounded Generative Layout Reward to enhance
generalization and spatial planning. During inference, DirectLayout addresses
asset-layout mismatches via Iterative Asset-Layout Alignment through in-context
learning. Extensive experiments demonstrate that DirectLayout achieves
impressive semantic consistency, generalization and physical plausibility.
</summary>
    <author>
      <name>Xingjian Ran</name>
    </author>
    <author>
      <name>Yixuan Li</name>
    </author>
    <author>
      <name>Linning Xu</name>
    </author>
    <author>
      <name>Mulin Yu</name>
    </author>
    <author>
      <name>Bo Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://directlayout.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05333v1</id>
    <updated>2025-06-05T17:59:24Z</updated>
    <published>2025-06-05T17:59:24Z</published>
    <title>Kinetics: Rethinking Test-Time Scaling Laws</title>
    <summary>  We rethink test-time scaling laws from a practical efficiency perspective,
revealing that the effectiveness of smaller models is significantly
overestimated. Prior work, grounded in compute-optimality, overlooks critical
memory access bottlenecks introduced by inference-time strategies (e.g.,
Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to
32B parameters, reveals a new Kinetics Scaling Law that better guides resource
allocation by incorporating both computation and memory access costs. Kinetics
Scaling Law suggests that test-time compute is more effective when used on
models above a threshold than smaller ones. A key reason is that in TTS,
attention, rather than parameter count, emerges as the dominant cost factor.
Motivated by this, we propose a new scaling paradigm centered on sparse
attention, which lowers per-token cost and enables longer generations and more
parallel samples within the same resource budget. Empirically, we show that
sparse attention models consistently outperform dense counterparts, achieving
over 60 points gains in low-cost regimes and over 5 points gains in high-cost
regimes for problem-solving accuracy on AIME, encompassing evaluations on
state-of-the-art MoEs. These results suggest that sparse attention is essential
for realizing the full potential of test-time scaling because, unlike training,
where parameter scaling saturates, test-time accuracy continues to improve
through increased generation. The code is available at
https://github.com/Infini-AI-Lab/Kinetics.
</summary>
    <author>
      <name>Ranajoy Sadhukhan</name>
    </author>
    <author>
      <name>Zhuoming Chen</name>
    </author>
    <author>
      <name>Haizhong Zheng</name>
    </author>
    <author>
      <name>Yang Zhou</name>
    </author>
    <author>
      <name>Emma Strubell</name>
    </author>
    <author>
      <name>Beidi Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2506.05333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05332v1</id>
    <updated>2025-06-05T17:59:04Z</updated>
    <published>2025-06-05T17:59:04Z</published>
    <title>Unleashing Hour-Scale Video Training for Long Video-Language
  Understanding</title>
    <summary>  Recent long-form video-language understanding benchmarks have driven progress
in video large multimodal models (Video-LMMs). However, the scarcity of
well-annotated long videos has left the training of hour-long Video-LLMs
underexplored. To close this gap, we present VideoMarathon, a large-scale
hour-long video instruction-following dataset. This dataset includes around
9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60
minutes per video. Specifically, it contains 3.3M high-quality QA pairs,
spanning six fundamental topics: temporality, spatiality, object, action,
scene, and event. Compared to existing video instruction datasets,
VideoMarathon significantly extends training video durations up to 1 hour, and
supports 22 diverse tasks requiring both short- and long-term video
comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and
efficient Video-LMM for hour-scale video-language modeling. It enables
hour-long video training and inference at 1-FPS sampling by leveraging a memory
augmentation module, which adaptively integrates user question-relevant and
spatiotemporal-informative semantics from a cached full video context. In our
experiments, Hour-LLaVA achieves the best performance on multiple long
video-language benchmarks, demonstrating the high quality of the VideoMarathon
dataset and the superiority of the Hour-LLaVA model.
</summary>
    <author>
      <name>Jingyang Lin</name>
    </author>
    <author>
      <name>Jialian Wu</name>
    </author>
    <author>
      <name>Ximeng Sun</name>
    </author>
    <author>
      <name>Ze Wang</name>
    </author>
    <author>
      <name>Jiang Liu</name>
    </author>
    <author>
      <name>Yusheng Su</name>
    </author>
    <author>
      <name>Xiaodong Yu</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <author>
      <name>Zicheng Liu</name>
    </author>
    <author>
      <name>Emad Barsoum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://videomarathon.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05325v1</id>
    <updated>2025-06-05T17:58:09Z</updated>
    <published>2025-06-05T17:58:09Z</published>
    <title>Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via
  Latent Alignment</title>
    <summary>  Quasiparticle interference (QPI) imaging is a powerful tool for probing
electronic structures in quantum materials, but extracting the single-scatterer
QPI pattern (i.e., the kernel) from a multi-scatterer image remains a
fundamentally ill-posed inverse problem. In this work, we propose the first
AI-based framework for QPI kernel extraction. We introduce a two-step learning
strategy that decouples kernel representation learning from
observation-to-kernel inference. In the first step, we train a variational
autoencoder to learn a compact latent space of scattering kernels. In the
second step, we align the latent representation of QPI observations with those
of the pre-learned kernels using a dedicated encoder. This design enables the
model to infer kernels robustly even under complex, entangled scattering
conditions. We construct a diverse and physically realistic QPI dataset
comprising 100 unique kernels and evaluate our method against a direct one-step
baseline. Experimental results demonstrate that our approach achieves
significantly higher extraction accuracy, and improved generalization to unseen
kernels.
</summary>
    <author>
      <name>Yingshuai Ji</name>
    </author>
    <author>
      <name>Haomin Zhuang</name>
    </author>
    <author>
      <name>Matthew Toole</name>
    </author>
    <author>
      <name>James McKenzie</name>
    </author>
    <author>
      <name>Xiaolong Liu</name>
    </author>
    <author>
      <name>Xiangliang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.05325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05321v1</id>
    <updated>2025-06-05T17:57:11Z</updated>
    <published>2025-06-05T17:57:11Z</published>
    <title>LSM-2: Learning from Incomplete Wearable Sensor Data</title>
    <summary>  Foundation models, a cornerstone of recent advancements in machine learning,
have predominantly thrived on complete and well-structured data. Wearable
sensor data frequently suffers from significant missingness, posing a
substantial challenge for self-supervised learning (SSL) models that typically
assume complete data inputs. This paper introduces the second generation of
Large Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel
SSL approach that learns robust representations directly from incomplete data
without requiring explicit imputation. AIM's core novelty lies in its use of
learnable mask tokens to model both existing ("inherited") and artificially
introduced missingness, enabling it to robustly handle fragmented real-world
data during inference. Pre-trained on an extensive dataset of 40M hours of
day-long multimodal sensor data, our LSM-2 with AIM achieves the best
performance across a diverse range of tasks, including classification,
regression and generative modeling. Furthermore, LSM-2 with AIM exhibits
superior scaling performance, and critically, maintains high performance even
under targeted missingness scenarios, reflecting clinically coherent patterns,
such as the diagnostic value of nighttime biosignals for hypertension
prediction. This makes AIM a more reliable choice for real-world wearable data
applications.
</summary>
    <author>
      <name>Maxwell A. Xu</name>
    </author>
    <author>
      <name>Girish Narayanswamy</name>
    </author>
    <author>
      <name>Kumar Ayush</name>
    </author>
    <author>
      <name>Dimitris Spathis</name>
    </author>
    <author>
      <name>Shun Liao</name>
    </author>
    <author>
      <name>Shyam A. Tailor</name>
    </author>
    <author>
      <name>Ahmed Metwally</name>
    </author>
    <author>
      <name>A. Ali Heydari</name>
    </author>
    <author>
      <name>Yuwei Zhang</name>
    </author>
    <author>
      <name>Jake Garrison</name>
    </author>
    <author>
      <name>Samy Abdel-Ghaffar</name>
    </author>
    <author>
      <name>Xuhai Xu</name>
    </author>
    <author>
      <name>Ken Gu</name>
    </author>
    <author>
      <name>Jacob Sunshine</name>
    </author>
    <author>
      <name>Ming-Zher Poh</name>
    </author>
    <author>
      <name>Yun Liu</name>
    </author>
    <author>
      <name>Tim Althoff</name>
    </author>
    <author>
      <name>Shrikanth Narayanan</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Mark Malhotra</name>
    </author>
    <author>
      <name>Shwetak Patel</name>
    </author>
    <author>
      <name>Yuzhe Yang</name>
    </author>
    <author>
      <name>James M. Rehg</name>
    </author>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Daniel McDuff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Xu and Narayanswamy are co-first authors. McDuff and Liu are co-last
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05320v1</id>
    <updated>2025-06-05T17:57:08Z</updated>
    <published>2025-06-05T17:57:08Z</published>
    <title>Generalizable, real-time neural decoding with hybrid state-space models</title>
    <summary>  Real-time decoding of neural activity is central to neuroscience and
neurotechnology applications, from closed-loop experiments to brain-computer
interfaces, where models are subject to strict latency constraints. Traditional
methods, including simple recurrent neural networks, are fast and lightweight
but often struggle to generalize to unseen data. In contrast, recent
Transformer-based approaches leverage large-scale pretraining for strong
generalization performance, but typically have much larger computational
requirements and are not always suitable for low-resource or real-time
settings. To address these shortcomings, we present POSSM, a novel hybrid
architecture that combines individual spike tokenization via a cross-attention
module with a recurrent state-space model (SSM) backbone to enable (1) fast and
causal online prediction on neural activity and (2) efficient generalization to
new sessions, individuals, and tasks through multi-dataset pretraining. We
evaluate POSSM's decoding performance and inference speed on intracortical
decoding of monkey motor tasks, and show that it extends to clinical
applications, namely handwriting and speech decoding in human subjects.
Notably, we demonstrate that pretraining on monkey motor-cortical recordings
improves decoding performance on the human handwriting task, highlighting the
exciting potential for cross-species transfer. In all of these tasks, we find
that POSSM achieves decoding accuracy comparable to state-of-the-art
Transformers, at a fraction of the inference cost (up to 9x faster on GPU).
These results suggest that hybrid SSMs are a promising approach to bridging the
gap between accuracy, inference speed, and generalization when training neural
decoders for real-time, closed-loop applications.
</summary>
    <author>
      <name>Avery Hee-Woon Ryoo</name>
    </author>
    <author>
      <name>Nanda H. Krishna</name>
    </author>
    <author>
      <name>Ximeng Mao</name>
    </author>
    <author>
      <name>Mehdi Azabou</name>
    </author>
    <author>
      <name>Eva L. Dyer</name>
    </author>
    <author>
      <name>Matthew G. Perich</name>
    </author>
    <author>
      <name>Guillaume Lajoie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05305v1</id>
    <updated>2025-06-05T17:52:30Z</updated>
    <published>2025-06-05T17:52:30Z</published>
    <title>ProRefine: Inference-time Prompt Refinement with Textual Feedback</title>
    <summary>  Agentic workflows, where multiple AI agents collaborate to accomplish complex
tasks like reasoning or planning, are becoming increasingly prevalent. However,
these workflows often suffer from error propagation and sub-optimal
performance, largely due to poorly designed prompts that fail to effectively
guide individual agents. This is a critical problem because it limits the
reliability and scalability of these powerful systems. We introduce ProRefine,
an innovative inference-time prompt optimization method that leverages textual
feedback from large language models (LLMs) to address this challenge. ProRefine
dynamically refines prompts for multi-step reasoning tasks without additional
training or ground truth labels. Evaluated on five benchmark mathematical
reasoning datasets, ProRefine significantly surpasses zero-shot
Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only
boosts accuracy but also allows smaller models to match the performance of
larger ones, highlighting its potential for efficient and scalable AI
deployment, and democratizing access to high-performing AI.
</summary>
    <author>
      <name>Deepak Pandita</name>
    </author>
    <author>
      <name>Tharindu Cyril Weerasooriya</name>
    </author>
    <author>
      <name>Ankit Parag Shah</name>
    </author>
    <author>
      <name>Christopher M. Homan</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <link href="http://arxiv.org/abs/2506.05305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05301v1</id>
    <updated>2025-06-05T17:51:05Z</updated>
    <published>2025-06-05T17:51:05Z</published>
    <title>SeedVR2: One-Step Video Restoration via Diffusion Adversarial
  Post-Training</title>
    <summary>  Recent advances in diffusion-based video restoration (VR) demonstrate
significant improvement in visual quality, yet yield a prohibitive
computational cost during inference. While several distillation-based
approaches have exhibited the potential of one-step image restoration,
extending existing approaches to VR remains challenging and underexplored,
particularly when dealing with high-resolution video in real-world settings. In
this work, we propose a one-step diffusion-based VR model, termed as SeedVR2,
which performs adversarial VR training against real data. To handle the
challenging high-resolution VR within a single step, we introduce several
enhancements to both model architecture and training procedures. Specifically,
an adaptive window attention mechanism is proposed, where the window size is
dynamically adjusted to fit the output resolutions, avoiding window
inconsistency observed under high-resolution VR using window attention with a
predefined window size. To stabilize and improve the adversarial post-training
towards VR, we further verify the effectiveness of a series of losses,
including a proposed feature matching loss without significantly sacrificing
training efficiency. Extensive experiments show that SeedVR2 can achieve
comparable or even better performance compared with existing VR approaches in a
single step.
</summary>
    <author>
      <name>Jianyi Wang</name>
    </author>
    <author>
      <name>Shanchuan Lin</name>
    </author>
    <author>
      <name>Zhijie Lin</name>
    </author>
    <author>
      <name>Yuxi Ren</name>
    </author>
    <author>
      <name>Meng Wei</name>
    </author>
    <author>
      <name>Zongsheng Yue</name>
    </author>
    <author>
      <name>Shangchen Zhou</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Ceyuan Yang</name>
    </author>
    <author>
      <name>Xuefeng Xiao</name>
    </author>
    <author>
      <name>Chen Change Loy</name>
    </author>
    <author>
      <name>Lu Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft Ver. Project page: https://iceclear.github.io/projects/seedvr2/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
