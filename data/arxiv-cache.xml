<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-17T00:53:52Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-16T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">123773</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.13805v1</id>
    <updated>2025-10-15T17:59:25Z</updated>
    <published>2025-10-15T17:59:25Z</published>
    <title>$\texttt{SBi3PCF:}$ Simulation-based inference with the integrated 3PCF</title>
    <summary>  We present $\texttt{SBi3PCF}$, a simulation-based inference (SBI) framework
for analysing a higher-order weak lensing statistic, the integrated 3-point
correlation function (i3PCF). Our approach forward-models the cosmic shear
field using the $\texttt{CosmoGridV1}$ suite of N-body simulations, including a
comprehensive set of systematic effects such as intrinsic alignment, baryonic
feedback, photometric redshift uncertainty, shear calibration bias, and shape
noise. Using this, we have produced a set of DES Y3-like synthetic measurements
for 2-point shear correlation functions $\xi_{\pm}$ (2PCFs) and i3PCFs
$\zeta_{\pm}$ across 6 cosmological and 11 systematic parameters. Having
validated these measurements against theoretical predictions and thoroughly
examined for potential systematic biases, we have found that the impact of
source galaxy clustering and reduced shear on the i3PCF is negligible for
Stage-III surveys. Furthermore, we have tested the Gaussianity assumption for
the likelihood of our data vector and found that while the sampling
distribution of the 2PCF can be well approximated by a Gaussian function, the
likelihood of the combined 2PCF + i3PCF data vector including filter sizes of
$90'$ and larger can deviate from this assumption. Our SBI pipeline employs
masked autoregressive flows to perform neural likelihood estimation and is
validated to give statistically accurate posterior estimates. On mock data, we
find that including the i3PCF yields a substantial $63.8\%$ median improvement
in the figure of merit for $\Omega_m - \sigma_8 - w_0$. These findings are
consistent with previous works on the i3PCF and demonstrate that our SBI
framework can achieve the accuracy and realism needed to analyse the i3PCF in
wide-area weak lensing surveys.
</summary>
    <author>
      <name>David Gebauer</name>
    </author>
    <author>
      <name>Anik Halder</name>
    </author>
    <author>
      <name>Stella Seitz</name>
    </author>
    <author>
      <name>Dhayaa Anbajagane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages + appendix. 21 figures. Comments are welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.13805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13796v1</id>
    <updated>2025-10-15T17:56:15Z</updated>
    <published>2025-10-15T17:56:15Z</published>
    <title>The Mechanistic Emergence of Symbol Grounding in Language Models</title>
    <summary>  Symbol grounding (Harnad, 1990) describes how symbols such as words acquire
their meanings by connecting to real-world sensorimotor experiences. Recent
work has shown preliminary evidence that grounding may emerge in
(vision-)language models trained at scale without using explicit grounding
objectives. Yet, the specific loci of this emergence and the mechanisms that
drive it remain largely unexplored. To address this problem, we introduce a
controlled evaluation framework that systematically traces how symbol grounding
arises within the internal computations through mechanistic and causal
analysis. Our findings show that grounding concentrates in middle-layer
computations and is implemented through the aggregate mechanism, where
attention heads aggregate the environmental ground to support the prediction of
linguistic forms. This phenomenon replicates in multimodal dialogue and across
architectures (Transformers and state-space models), but not in unidirectional
LSTMs. Our results provide behavioral and mechanistic evidence that symbol
grounding can emerge in language models, with practical implications for
predicting and potentially controlling the reliability of generation.
</summary>
    <author>
      <name>Shuyu Wu</name>
    </author>
    <author>
      <name>Ziqiao Ma</name>
    </author>
    <author>
      <name>Xiaoxi Luo</name>
    </author>
    <author>
      <name>Yidong Huang</name>
    </author>
    <author>
      <name>Josue Torres-Fonseca</name>
    </author>
    <author>
      <name>Freda Shi</name>
    </author>
    <author>
      <name>Joyce Chai</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13780v1</id>
    <updated>2025-10-15T17:34:31Z</updated>
    <published>2025-10-15T17:34:31Z</published>
    <title>Macro-Level Correlational Analysis of Mental Disorders: Economy,
  Education, Society, and Technology Development</title>
    <summary>  This paper quantifies the age-stratified global burden of four mental
disorders in 27 regions from 1990 to 2021 using GBD 2021. To put it in detail,
it links the age-standardized years of disability adjustment with 18 world
development indicators across economic, educational, social and information
technology sectors. Then, by means of Pearson correlation, mutual information,
Granger causality and maximum information coefficient and other methods, the
linear, nonlinear and lagged dependency relationships were evaluated. After
research, it was found that there is a very prominent spatio-temporal
heterogeneity among young people aged 20 to 39, and the coupling relationship
is stronger. From the overall situation, education corresponds to a low burden.
Unemployment corresponds to a high burden. Through lag analysis, it can be
known that the influence time of economic and technological factors is
relatively short, while that of educational factors is relatively long. These
results highlight the macro determinants that play a role at different time
scales and also provide population-level references for verifying computational
mental health models and for intervention measures in specific regions and for
specific ages.
</summary>
    <author>
      <name>Yingzhi Tao</name>
    </author>
    <author>
      <name>Chang Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, ICDM workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.13780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.5.4; I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13774v1</id>
    <updated>2025-10-15T17:26:24Z</updated>
    <published>2025-10-15T17:26:24Z</published>
    <title>UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of
  Robust Spatial Representations</title>
    <summary>  Forecasting urban phenomena such as housing prices and public health
indicators requires the effective integration of various geospatial data.
Current methods primarily utilize task-specific models, while recent foundation
models for spatial representations often support only limited modalities and
lack multimodal fusion capabilities. To overcome these challenges, we present
UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal
Fusion (SMF). The framework employs modality-specific encoders to process
different types of inputs, including street view imagery, remote sensing data,
cartographic maps, and points of interest (POIs) data. These multimodal inputs
are integrated via a Transformer-based fusion module that learns unified
representations. An extensive evaluation across 41 tasks in 56 cities worldwide
demonstrates UrbanFusion's strong generalization and predictive performance
compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms
prior foundation models on location-encoding, 2) allows multimodal input during
inference, and 3) generalizes well to regions unseen during training.
UrbanFusion can flexibly utilize any subset of available modalities for a given
location during both pretraining and inference, enabling broad applicability
across diverse data availability scenarios. All source code is available at
https://github.com/DominikM198/UrbanFusion.
</summary>
    <author>
      <name>Dominik J. Mühlematter</name>
    </author>
    <author>
      <name>Lin Che</name>
    </author>
    <author>
      <name>Ye Hong</name>
    </author>
    <author>
      <name>Martin Raubal</name>
    </author>
    <author>
      <name>Nina Wiedemann</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13763v1</id>
    <updated>2025-10-15T17:11:19Z</updated>
    <published>2025-10-15T17:11:19Z</published>
    <title>PriorGuide: Test-Time Prior Adaptation for Simulation-Based Inference</title>
    <summary>  Amortized simulator-based inference offers a powerful framework for tackling
Bayesian inference in computational fields such as engineering or neuroscience,
increasingly leveraging modern generative methods like diffusion models to map
observed data to model parameters or future predictions. These approaches yield
posterior or posterior-predictive samples for new datasets without requiring
further simulator calls after training on simulated parameter-data pairs.
However, their applicability is often limited by the prior distribution(s) used
to generate model parameters during this training phase. To overcome this
constraint, we introduce PriorGuide, a technique specifically designed for
diffusion-based amortized inference methods. PriorGuide leverages a novel
guidance approximation that enables flexible adaptation of the trained
diffusion model to new priors at test time, crucially without costly
retraining. This allows users to readily incorporate updated information or
expert knowledge post-training, enhancing the versatility of pre-trained
inference models.
</summary>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Severi Rissanen</name>
    </author>
    <author>
      <name>Paul E. Chang</name>
    </author>
    <author>
      <name>Nasrulloh Loka</name>
    </author>
    <author>
      <name>Daolang Huang</name>
    </author>
    <author>
      <name>Arno Solin</name>
    </author>
    <author>
      <name>Markus Heinonen</name>
    </author>
    <author>
      <name>Luigi Acerbi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.13763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13760v1</id>
    <updated>2025-10-15T17:10:39Z</updated>
    <published>2025-10-15T17:10:39Z</published>
    <title>Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for
  Medical AI Assistants on the Edge</title>
    <summary>  Vision Transformers (ViTs) have demonstrated strong capabilities in
interpreting complex medical imaging data. However, their significant
computational and memory demands pose challenges for deployment in real-time,
resource-constrained mobile and wearable devices used in clinical environments.
We introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI
assistants that perform structured analysis of medical images directly on the
edge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical
imaging and com- bines a training procedure with multi-query attention,
preserving stability under ternary weights with low-precision activations.
Furthermore, BiTMedViT employs task-aware distillation from a high-capacity
teacher to recover accuracy lost due to extreme quantization. Lastly, we also
present a pipeline that maps the ternarized ViTs to a custom CUDA kernel for
efficient memory bandwidth utilization and latency reduction on the Jetson Orin
Nano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on
MedMNIST across 12 datasets, while reducing model size by 43x, memory traffic
by 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that
of SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a
practical and scientifically grounded route for extreme-precision medical
imaging ViTs deployable on the edge, narrowing the gap between algorithmic
advances and deployable clinical tools.
</summary>
    <author>
      <name>Mikolaj Walczak</name>
    </author>
    <author>
      <name>Uttej Kallakuri</name>
    </author>
    <author>
      <name>Edward Humes</name>
    </author>
    <author>
      <name>Xiaomin Lin</name>
    </author>
    <author>
      <name>Tinoosh Mohsenin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 2025 IEEE/ACM International Conf. on Computer-Aided
  Design (ICCAD) Oct. 26-30 2025, Munich, DE</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.13760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13756v1</id>
    <updated>2025-10-15T17:05:37Z</updated>
    <published>2025-10-15T17:05:37Z</published>
    <title>RECODE: Reasoning Through Code Generation for Visual Question Answering</title>
    <summary>  Multimodal Large Language Models (MLLMs) struggle with precise reasoning for
structured visuals like charts and diagrams, as pixel-based perception lacks a
mechanism for verification. To address this, we propose to leverage derendering
-- the process of reverse-engineering visuals into executable code -- as a new
modality for verifiable visual reasoning. Specifically, we propose RECODE, an
agentic framework that first generates multiple candidate programs to reproduce
the input image. It then uses a critic to select the most faithful
reconstruction and iteratively refines the code. This process not only
transforms an ambiguous perceptual task into a verifiable, symbolic problem,
but also enables precise calculations and logical inferences later on. On
various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,
RECODE significantly outperforms methods that do not leverage code or only use
code for drawing auxiliary lines or cropping. Our work demonstrates that
grounding visual perception in executable code provides a new path toward more
accurate and verifiable multimodal reasoning.
</summary>
    <author>
      <name>Junhong Shen</name>
    </author>
    <author>
      <name>Mu Cai</name>
    </author>
    <author>
      <name>Bo Hu</name>
    </author>
    <author>
      <name>Ameet Talwalkar</name>
    </author>
    <author>
      <name>David A Ross</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <author>
      <name>Alireza Fathi</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13738v1</id>
    <updated>2025-10-15T16:45:59Z</updated>
    <published>2025-10-15T16:45:59Z</published>
    <title>HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based
  Sequential Recommendation</title>
    <summary>  Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems.
</summary>
    <author>
      <name>Jingyi Zhou</name>
    </author>
    <author>
      <name>Cheng Chen</name>
    </author>
    <author>
      <name>Kai Zuo</name>
    </author>
    <author>
      <name>Manjie Xu</name>
    </author>
    <author>
      <name>Zhendong Fu</name>
    </author>
    <author>
      <name>Yibo Chen</name>
    </author>
    <author>
      <name>Xu Tang</name>
    </author>
    <author>
      <name>Yao Hu</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13724v1</id>
    <updated>2025-10-15T16:28:34Z</updated>
    <published>2025-10-15T16:28:34Z</published>
    <title>FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI
  Model Access</title>
    <summary>  We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.
</summary>
    <author>
      <name>Aditya Tanikanti</name>
    </author>
    <author>
      <name>Benoit Côté</name>
    </author>
    <author>
      <name>Yanfei Guo</name>
    </author>
    <author>
      <name>Le Chen</name>
    </author>
    <author>
      <name>Nickolaus Saint</name>
    </author>
    <author>
      <name>Ryan Chard</name>
    </author>
    <author>
      <name>Ken Raffenetti</name>
    </author>
    <author>
      <name>Rajeev Thakur</name>
    </author>
    <author>
      <name>Thomas Uram</name>
    </author>
    <author>
      <name>Ian Foster</name>
    </author>
    <author>
      <name>Michael E. Papka</name>
    </author>
    <author>
      <name>Venkatram Vishwanath</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13714v1</id>
    <updated>2025-10-15T16:13:44Z</updated>
    <published>2025-10-15T16:13:44Z</published>
    <title>Dedelayed: Deleting remote inference delay via on-device correction</title>
    <summary>  Remote inference allows lightweight devices to leverage powerful cloud
models. However, communication network latency makes predictions stale and
unsuitable for real-time tasks. To address this, we introduce Dedelayed, a
delay-corrective method that mitigates arbitrary remote inference delays,
allowing the local device to produce low-latency outputs in real time. Our
method employs a lightweight local model that processes the current frame and
fuses in features that a heavyweight remote model computes from past frames. On
video from the BDD100K driving dataset, Dedelayed improves semantic
segmentation accuracy over the stronger of the local-only and remote-only
baselines across all realistic communication network delays beyond 33 ms.
Without incurring additional delay, it improves accuracy by 6.4 mIoU compared
to fully local inference and 9.8 mIoU compared to remote inference, for a
round-trip delay of 100 ms. The advantage grows under longer delays and
higher-motion scenes, as delay-mitigated split inference sustains accuracy more
effectively, providing clear advantages for real-time tasks that must remain
aligned with the current world state.
</summary>
    <author>
      <name>Dan Jacobellis</name>
    </author>
    <author>
      <name>Mateen Ulhaq</name>
    </author>
    <author>
      <name>Fabien Racapé</name>
    </author>
    <author>
      <name>Hyomin Choi</name>
    </author>
    <author>
      <name>Neeraja J. Yadwadkar</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
