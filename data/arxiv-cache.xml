<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-29T01:08:31Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-28T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117632</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.19459v1</id>
    <updated>2025-07-25T17:43:29Z</updated>
    <published>2025-07-25T17:43:29Z</published>
    <title>Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive
  Initialization</title>
    <summary>  The advent of novel view synthesis techniques such as NeRF and 3D Gaussian
Splatting (3DGS) has enabled learning precise 3D models only from posed
monocular images. Although these methods are attractive, they hold two major
limitations that prevent their use in space applications: they require poses
during training, and have high computational cost at training and inference. To
address these limitations, this work contributes: (1) a Convolutional Neural
Network (CNN) based primitive initializer for 3DGS using monocular images; (2)
a pipeline capable of training with noisy or implicit pose estimates; and (3)
and analysis of initialization variants that reduce the training cost of
precise 3D models. A CNN takes a single image as input and outputs a coarse 3D
model represented as an assembly of primitives, along with the target's pose
relative to the camera. This assembly of primitives is then used to initialize
3DGS, significantly reducing the number of training iterations and input images
needed -- by at least an order of magnitude. For additional flexibility, the
CNN component has multiple variants with different pose estimation techniques.
This work performs a comparison between these variants, evaluating their
effectiveness for downstream 3DGS training under noisy or implicit pose
estimates. The results demonstrate that even with imperfect pose supervision,
the pipeline is able to learn high-fidelity 3D representations, opening the
door for the use of novel view synthesis in space applications.
</summary>
    <author>
      <name>Pol Francesch Huc</name>
    </author>
    <author>
      <name>Emily Bates</name>
    </author>
    <author>
      <name>Simone D'Amico</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19457v1</id>
    <updated>2025-07-25T17:42:32Z</updated>
    <published>2025-07-25T17:42:32Z</published>
    <title>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</title>
    <summary>  Large language models (LLMs) are increasingly adapted to downstream tasks via
reinforcement learning (RL) methods like Group Relative Policy Optimization
(GRPO), which often require thousands of rollouts to learn new tasks. We argue
that the interpretable nature of language can often provide a much richer
learning medium for LLMs, compared with policy gradients derived from sparse,
scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt
optimizer that thoroughly incorporates natural language reflection to learn
high-level rules from trial and error. Given any AI system containing one or
more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool
calls, and tool outputs) and reflects on them in natural language to diagnose
problems, propose and test prompt updates, and combine complementary lessons
from the Pareto frontier of its own attempts. As a result of GEPA's design, it
can often turn even just a few rollouts into a large quality gain. Across four
tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up
to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,
MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an
inference-time search strategy for code optimization.
</summary>
    <author>
      <name>Lakshya A Agrawal</name>
    </author>
    <author>
      <name>Shangyin Tan</name>
    </author>
    <author>
      <name>Dilara Soylu</name>
    </author>
    <author>
      <name>Noah Ziems</name>
    </author>
    <author>
      <name>Rishi Khare</name>
    </author>
    <author>
      <name>Krista Opsahl-Ong</name>
    </author>
    <author>
      <name>Arnav Singhvi</name>
    </author>
    <author>
      <name>Herumb Shandilya</name>
    </author>
    <author>
      <name>Michael J Ryan</name>
    </author>
    <author>
      <name>Meng Jiang</name>
    </author>
    <author>
      <name>Christopher Potts</name>
    </author>
    <author>
      <name>Koushik Sen</name>
    </author>
    <author>
      <name>Alexandros G. Dimakis</name>
    </author>
    <author>
      <name>Ion Stoica</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <author>
      <name>Matei Zaharia</name>
    </author>
    <author>
      <name>Omar Khattab</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.6; I.2.4; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19437v1</id>
    <updated>2025-07-25T17:08:16Z</updated>
    <published>2025-07-25T17:08:16Z</published>
    <title>Observations Meet Actions: Learning Control-Sufficient Representations
  for Robust Policy Generalization</title>
    <summary>  Capturing latent variations ("contexts") is key to deploying
reinforcement-learning (RL) agents beyond their training regime. We recast
context-based RL as a dual inference-control problem and formally characterize
two properties and their hierarchy: observation sufficiency (preserving all
predictive information) and control sufficiency (retaining decision-making
relevant information). Exploiting this dichotomy, we derive a contextual
evidence lower bound(ELBO)-style objective that cleanly separates
representation learning from policy learning and optimizes it with Bottlenecked
Contextual Policy Optimization (BCPO), an algorithm that places a variational
information-bottleneck encoder in front of any off-policy policy learner. On
standard continuous-control benchmarks with shifting physical parameters, BCPO
matches or surpasses other baselines while using fewer samples and retaining
performance far outside the training regime. The framework unifies theory,
diagnostics, and practice for context-based RL.
</summary>
    <author>
      <name>Yuliang Gu</name>
    </author>
    <author>
      <name>Hongpeng Cao</name>
    </author>
    <author>
      <name>Marco Caccamo</name>
    </author>
    <author>
      <name>Naira Hovakimyan</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19432v1</id>
    <updated>2025-07-25T17:02:18Z</updated>
    <published>2025-07-25T17:02:18Z</published>
    <title>Resolving Build Conflicts via Example-Based and Rule-Based Program
  Transformations</title>
    <summary>  Merge conflicts often arise when developers integrate changes from different
software branches. The conflicts can result from overlapping edits in programs
(i.e., textual conflicts) or cause build and test errors (i.e., build and test
conflicts). They degrade software quality and hinder programmer productivity.
While several tools detect build conflicts, few offer meaningful support for
resolving cases like those caused by method removal. To overcome limitations of
existing tools, we introduce BUCOR (Build Conflict Resolver), a new conflict
resolver. BUCOR first detects conflicts by comparing three versions related to
a merging scenario: base b, left l, and right r. To resolve conflicts, it
employs two complementary strategies: example-based transformation (BUCOR-E)
and rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to
handle common, well-understood conflicts. BUCOR-E mines branch versions (l and
r) for exemplar edits applied to fix related build errors. From these examples,
it infers and generalizes program transformation patterns to resolve more
complex conflicts.
  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct
conflict types. BUCOR generated at least one solution for 65 cases and
correctly resolved 43 conflicts. We observed that this hybrid
approach--combining context-aware, example-based learning with structured,
rule-based resolution--can effectively help resolve conflicts. Our research
sheds light on future directions for more intelligent and automated merge
tools.
</summary>
    <author>
      <name>Sheikh Shadab Towqir</name>
    </author>
    <author>
      <name>Fei He</name>
    </author>
    <author>
      <name>Todd Mytkowicz</name>
    </author>
    <author>
      <name>Na Meng</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19427v1</id>
    <updated>2025-07-25T16:53:13Z</updated>
    <published>2025-07-25T16:53:13Z</published>
    <title>Step-3 is Large yet Affordable: Model-system Co-design for
  Cost-effective Decoding</title>
    <summary>  Large language models (LLMs) face low hardware efficiency during decoding,
especially for long-context reasoning tasks. This paper introduces Step-3, a
321B-parameter VLM with hardware-aware model-system co-design optimized for
minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel
Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces
both KV cache size and computation while maintaining high attention
expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed
inference system that decouples attention and Feed-Forward Network (FFN) layers
into specialized subsystems. This co-design achieves unprecedented cost
efficiency: Step-3 significantly reduces theoretical decoding costs compared
with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at
longer context. Step-3 achieves low cost while activating 38B parameters per
token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that
hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are
critical to cost-effectiveness. We perform a head-to-head comparison with
DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs
achieves a decoding throughput of up to 4,039 tokens per second per GPU under
50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324
in the same setup and sets a new Pareto frontier for LLM decoding.
</summary>
    <author>
      <name> StepFun</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Bin Wang</name>
    </author>
    <author>
      <name>Bojun Wang</name>
    </author>
    <author>
      <name>Changyi Wan</name>
    </author>
    <author>
      <name>Guanzhe Huang</name>
    </author>
    <author>
      <name>Hanpeng Hu</name>
    </author>
    <author>
      <name>Haonan Jia</name>
    </author>
    <author>
      <name>Hao Nie</name>
    </author>
    <author>
      <name>Mingliang Li</name>
    </author>
    <author>
      <name>Nuo Chen</name>
    </author>
    <author>
      <name>Siyu Chen</name>
    </author>
    <author>
      <name>Song Yuan</name>
    </author>
    <author>
      <name>Wuxun Xie</name>
    </author>
    <author>
      <name>Xiaoniu Song</name>
    </author>
    <author>
      <name>Xing Chen</name>
    </author>
    <author>
      <name>Xingping Yang</name>
    </author>
    <author>
      <name>Xuelin Zhang</name>
    </author>
    <author>
      <name>Yanbo Yu</name>
    </author>
    <author>
      <name>Yaoyu Wang</name>
    </author>
    <author>
      <name>Yibo Zhu</name>
    </author>
    <author>
      <name>Yimin Jiang</name>
    </author>
    <author>
      <name>Yu Zhou</name>
    </author>
    <author>
      <name>Yuanwei Lu</name>
    </author>
    <author>
      <name>Houyi Li</name>
    </author>
    <author>
      <name>Jingcheng Hu</name>
    </author>
    <author>
      <name>Ka Man Lo</name>
    </author>
    <author>
      <name>Ailin Huang</name>
    </author>
    <author>
      <name>Binxing Jiao</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Boyu Chen</name>
    </author>
    <author>
      <name>Changxin Miao</name>
    </author>
    <author>
      <name>Chang Lou</name>
    </author>
    <author>
      <name>Chen Hu</name>
    </author>
    <author>
      <name>Chen Xu</name>
    </author>
    <author>
      <name>Chenfeng Yu</name>
    </author>
    <author>
      <name>Chengyuan Yao</name>
    </author>
    <author>
      <name>Daokuan Lv</name>
    </author>
    <author>
      <name>Dapeng Shi</name>
    </author>
    <author>
      <name>Deshan Sun</name>
    </author>
    <author>
      <name>Ding Huang</name>
    </author>
    <author>
      <name>Dingyuan Hu</name>
    </author>
    <author>
      <name>Dongqing Pang</name>
    </author>
    <author>
      <name>Enle Liu</name>
    </author>
    <author>
      <name>Fajie Zhang</name>
    </author>
    <author>
      <name>Fanqi Wan</name>
    </author>
    <author>
      <name>Gulin Yan</name>
    </author>
    <author>
      <name>Han Zhang</name>
    </author>
    <author>
      <name>Han Zhou</name>
    </author>
    <author>
      <name>Hanghao Wu</name>
    </author>
    <author>
      <name>Hangyu Guo</name>
    </author>
    <author>
      <name>Hanqi Chen</name>
    </author>
    <author>
      <name>Hanshan Zhang</name>
    </author>
    <author>
      <name>Hao Wu</name>
    </author>
    <author>
      <name>Haocheng Zhang</name>
    </author>
    <author>
      <name>Haolong Yan</name>
    </author>
    <author>
      <name>Haoran Lv</name>
    </author>
    <author>
      <name>Haoran Wei</name>
    </author>
    <author>
      <name>Hebin Zhou</name>
    </author>
    <author>
      <name>Heng Wang</name>
    </author>
    <author>
      <name>Heng Wang</name>
    </author>
    <author>
      <name>Hongxin Li</name>
    </author>
    <author>
      <name>Hongyu Zhou</name>
    </author>
    <author>
      <name>Hongyuan Wang</name>
    </author>
    <author>
      <name>Huiyong Guo</name>
    </author>
    <author>
      <name>Jia Wang</name>
    </author>
    <author>
      <name>Jiahao Gong</name>
    </author>
    <author>
      <name>Jialing Xie</name>
    </author>
    <author>
      <name>Jian Zhou</name>
    </author>
    <author>
      <name>Jianjian Sun</name>
    </author>
    <author>
      <name>Jiaoren Wu</name>
    </author>
    <author>
      <name>Jiaran Zhang</name>
    </author>
    <author>
      <name>Jiayu Liu</name>
    </author>
    <author>
      <name>Jie Cheng</name>
    </author>
    <author>
      <name>Jie Luo</name>
    </author>
    <author>
      <name>Jie Yan</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Jieyi Hou</name>
    </author>
    <author>
      <name>Jinguang Zhang</name>
    </author>
    <author>
      <name>Jinlan Cao</name>
    </author>
    <author>
      <name>Jisheng Yin</name>
    </author>
    <author>
      <name>Junfeng Liu</name>
    </author>
    <author>
      <name>Junhao Huang</name>
    </author>
    <author>
      <name>Junzhe Lin</name>
    </author>
    <author>
      <name>Kaijun Tan</name>
    </author>
    <author>
      <name>Kaixiang Li</name>
    </author>
    <author>
      <name>Kang An</name>
    </author>
    <author>
      <name>Kangheng Lin</name>
    </author>
    <author>
      <name>Kenkun Liu</name>
    </author>
    <author>
      <name>Lei Yang</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <author>
      <name>Liangyu Chen</name>
    </author>
    <author>
      <name>Lieyu Shi</name>
    </author>
    <author>
      <name>Liguo Tan</name>
    </author>
    <author>
      <name>Lin Lin</name>
    </author>
    <author>
      <name>Lin Zhang</name>
    </author>
    <author>
      <name>Lina Chen</name>
    </author>
    <author>
      <name>Liwen Huang</name>
    </author>
    <author>
      <name>Liying Shi</name>
    </author>
    <author>
      <name>Longlong Gu</name>
    </author>
    <author>
      <name>Mei Chen</name>
    </author>
    <author>
      <name>Mengqiang Ren</name>
    </author>
    <author>
      <name>Ming Li</name>
    </author>
    <author>
      <name>Mingzhe Chen</name>
    </author>
    <author>
      <name>Na Wang</name>
    </author>
    <author>
      <name>Nan Wu</name>
    </author>
    <author>
      <name>Qi Han</name>
    </author>
    <author>
      <name>Qian Zhao</name>
    </author>
    <author>
      <name>Qiang Zhang</name>
    </author>
    <author>
      <name>Qianni Liu</name>
    </author>
    <author>
      <name>Qiaohui Chen</name>
    </author>
    <author>
      <name>Qiling Wu</name>
    </author>
    <author>
      <name>Qinglin He</name>
    </author>
    <author>
      <name>Qinyuan Tan</name>
    </author>
    <author>
      <name>Qiufeng Wang</name>
    </author>
    <author>
      <name>Qiuping Wu</name>
    </author>
    <author>
      <name>Qiuyan Liang</name>
    </author>
    <author>
      <name>Quan Sun</name>
    </author>
    <author>
      <name>Rui Li</name>
    </author>
    <author>
      <name>Ruihang Miao</name>
    </author>
    <author>
      <name>Ruosi Wan</name>
    </author>
    <author>
      <name>Ruyan Guo</name>
    </author>
    <author>
      <name>Shangwu Zhong</name>
    </author>
    <author>
      <name>Shaoliang Pang</name>
    </author>
    <author>
      <name>Shengjie Fan</name>
    </author>
    <author>
      <name>Shijie Shang</name>
    </author>
    <author>
      <name>Shilei Jiang</name>
    </author>
    <author>
      <name>Shiliang Yang</name>
    </author>
    <author>
      <name>Shiming Hao</name>
    </author>
    <author>
      <name>Shuli Gao</name>
    </author>
    <author>
      <name>Siming Huang</name>
    </author>
    <author>
      <name>Siqi Liu</name>
    </author>
    <author>
      <name>Tiancheng Cao</name>
    </author>
    <author>
      <name>Tianhao Cheng</name>
    </author>
    <author>
      <name>Tianhao Peng</name>
    </author>
    <author>
      <name>Wang You</name>
    </author>
    <author>
      <name>Wei Ji</name>
    </author>
    <author>
      <name>Wen Sun</name>
    </author>
    <author>
      <name>Wenjin Deng</name>
    </author>
    <author>
      <name>Wenqing He</name>
    </author>
    <author>
      <name>Wenzhen Zheng</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Xiangwen Kong</name>
    </author>
    <author>
      <name>Xianzhen Luo</name>
    </author>
    <author>
      <name>Xiaobo Yang</name>
    </author>
    <author>
      <name>Xiaojia Liu</name>
    </author>
    <author>
      <name>Xiaoxiao Ren</name>
    </author>
    <author>
      <name>Xin Han</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Xin Wu</name>
    </author>
    <author>
      <name>Xu Zhao</name>
    </author>
    <author>
      <name>Yanan Wei</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Yangguang Li</name>
    </author>
    <author>
      <name>Yangshijie Xu</name>
    </author>
    <author>
      <name>Yanming Xu</name>
    </author>
    <author>
      <name>Yaqiang Shi</name>
    </author>
    <author>
      <name>Yeqing Shen</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <author>
      <name>Yifei Yang</name>
    </author>
    <author>
      <name>Yifeng Gong</name>
    </author>
    <author>
      <name>Yihan Chen</name>
    </author>
    <author>
      <name>Yijing Yang</name>
    </author>
    <author>
      <name>Yinmin Zhang</name>
    </author>
    <author>
      <name>Yizhuang Zhou</name>
    </author>
    <author>
      <name>Yuanhao Ding</name>
    </author>
    <author>
      <name>Yuantao Fan</name>
    </author>
    <author>
      <name>Yuanzhen Yang</name>
    </author>
    <author>
      <name>Yuchu Luo</name>
    </author>
    <author>
      <name>Yue Peng</name>
    </author>
    <author>
      <name>Yufan Lu</name>
    </author>
    <author>
      <name>Yuhang Deng</name>
    </author>
    <author>
      <name>Yuhe Yin</name>
    </author>
    <author>
      <name>Yujie Liu</name>
    </author>
    <author>
      <name>Yukun Chen</name>
    </author>
    <author>
      <name>Yuling Zhao</name>
    </author>
    <author>
      <name>Yun Mou</name>
    </author>
    <author>
      <name>Yunlong Li</name>
    </author>
    <author>
      <name>Yunzhou Ju</name>
    </author>
    <author>
      <name>Yusheng Li</name>
    </author>
    <author>
      <name>Yuxiang Yang</name>
    </author>
    <author>
      <name>Yuxiang Zhang</name>
    </author>
    <author>
      <name>Yuyang Chen</name>
    </author>
    <author>
      <name>Zejia Weng</name>
    </author>
    <author>
      <name>Zhe Xie</name>
    </author>
    <author>
      <name>Zheng Ge</name>
    </author>
    <author>
      <name>Zheng Gong</name>
    </author>
    <author>
      <name>Zhenyi Lu</name>
    </author>
    <author>
      <name>Zhewei Huang</name>
    </author>
    <author>
      <name>Zhichao Chang</name>
    </author>
    <author>
      <name>Zhiguo Huang</name>
    </author>
    <author>
      <name>Zhirui Wang</name>
    </author>
    <author>
      <name>Zidong Yang</name>
    </author>
    <author>
      <name>Zili Wang</name>
    </author>
    <author>
      <name>Ziqi Wang</name>
    </author>
    <author>
      <name>Zixin Zhang</name>
    </author>
    <author>
      <name>Binxing Jiao</name>
    </author>
    <author>
      <name>Daxin Jiang</name>
    </author>
    <author>
      <name>Heung-Yeung Shum</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19413v1</id>
    <updated>2025-07-25T16:27:17Z</updated>
    <published>2025-07-25T16:27:17Z</published>
    <title>Riesz representers for the rest of us</title>
    <summary>  The application of semiparametric efficient estimators, particularly those
that leverage machine learning, is rapidly expanding within epidemiology and
causal inference. Much of the recent methodological literature on these
estimators relies heavily on the Riesz representation theorem and Riesz
regression. This paper aims to introduce the Riesz representation theorem to an
applied audience, explaining why and how Riesz regression is becoming widely
used in the semiparametric estimator statistical literature.
</summary>
    <author>
      <name>Nicholas T. Williams</name>
    </author>
    <author>
      <name>Oliver J. Hines</name>
    </author>
    <author>
      <name>Kara E. Rudolph</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19409v1</id>
    <updated>2025-07-25T16:19:47Z</updated>
    <published>2025-07-25T16:19:47Z</published>
    <title>Modality Agnostic Efficient Long Range Encoder</title>
    <summary>  The long-context capability of recent large transformer models can be
surmised to rely on techniques such as attention/model parallelism, as well as
hardware-level optimizations. While these strategies allow input lengths to
scale to millions of tokens, they do not fundamentally mitigate the quadratic
computational and memory complexity of the core attention mechanism. In this
paper, we address the challenge of long-context processing on a single device
using generic implementations by reducing the quadratic memory footprint and
inference cost. Existing approaches to extend the context length for generic
single device implementations -- such as token merging and modified attentions
-- are often modality specific and attain a suboptimal tradeoff between
accuracy and efficiency. To overcome these limitations, we propose MAELRE
(Modality Agnostic Efficient Long Range Encoder), a unified and efficient
transformer architecture designed for long-range encoding across diverse
modalities. MAELRE integrates token merging with attention approximation,
progressively merging tokens at different stages of internal computational
blocks. It employs a lightweight attention approximation when the number of
tokens is large, and switches to standard dot-product attention as the sequence
becomes shorter through successive aggregation. We demonstrate that MAELRE
achieves superior accuracy while reducing computational cost compared to
existing long-context models on classification tasks spanning multiple
modalities, including text, time series, audio, and vision.
</summary>
    <author>
      <name>Toufiq Parag</name>
    </author>
    <author>
      <name>Ahmed Elgammal</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19401v2</id>
    <updated>2025-07-28T04:04:38Z</updated>
    <published>2025-07-25T16:08:22Z</published>
    <title>The gauge theory dual of the bilayer XY model with second order
  Josephson coupling</title>
    <summary>  We formulate a duality transformation for a bilayer XY model where the layers
are coupled by second order Josephson effect, which favors inter-layer phase
difference of either $0$ or $\pi$. The model may represent a bilayer
superconductor or a spin-1 ferromagnetic Bose gas in the easy-plane limit. The
second order Josephson term is mapped to a U(1) gauge field, known to be
trivially confining in two dimensions, and we argue that a Coulomb-gas analysis
is not applicable to the dual theory. Instead, we appeal to the vast knowledge
of gauge theory and infer that the only phase transition out of low-temperature
ordered phase is an Ising transition driven by condensation of $\mathbb{Z}_2$
domain wall loops. The domain wall loops can be seen as a surviving vestige of
single-layer vortex-anti-vortex pair, heavily deformed by the second order
Josephson coupling. A theoretical or computational method that concentrates on
point defects would most likely miss out on these excitations and reach
erroneous results. Our dual theory offers a clear, intuitive picture of how the
second order Josephson coupling induces confinement of vortices and drastically
changes the physics.
</summary>
    <author>
      <name>Pye Ton How</name>
    </author>
    <author>
      <name>Sungkit Yip</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">V2: fix the missing arXiv identifier of ref [19]</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.19401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19391v1</id>
    <updated>2025-07-25T15:50:42Z</updated>
    <published>2025-07-25T15:50:42Z</published>
    <title>Transcript Franking for Encrypted Messaging</title>
    <summary>  Message franking is an indispensable abuse mitigation tool for end-to-end
encrypted (E2EE) messaging platforms. With it, users who receive harmful
content can securely report that content to platform moderators. However, while
real-world deployments of reporting require the disclosure of multiple
messages, existing treatments of message franking only consider the report of a
single message. As a result, there is a gap between the security goals achieved
by constructions and those needed in practice. Our work introduces transcript
franking, a new type of protocol that allows reporting subsets of conversations
such that moderators can cryptographically verify message causality and
contents. We define syntax, semantics, and security for transcript franking in
two-party and group messaging. We then present efficient constructions for
transcript franking and prove their security. Looking toward deployment
considerations, we provide detailed discussion of how real-world messaging
systems can incorporate our protocols.
</summary>
    <author>
      <name>Armin Namavari</name>
    </author>
    <author>
      <name>Thomas Ristenpart</name>
    </author>
    <link href="http://arxiv.org/abs/2507.19391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.19372v1</id>
    <updated>2025-07-25T15:24:56Z</updated>
    <published>2025-07-25T15:24:56Z</published>
    <title>Learning neuro-symbolic convergent term rewriting systems</title>
    <summary>  Building neural systems that can learn to execute symbolic algorithms is a
challenging open problem in artificial intelligence, especially when aiming for
strong generalization and out-of-distribution performance. In this work, we
introduce a general framework for learning convergent term rewriting systems
using a neuro-symbolic architecture inspired by the rewriting algorithm itself.
We present two modular implementations of such architecture: the Neural
Rewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a
result of algorithmic-inspired design and key architectural elements, both
models can generalize to out-of-distribution instances, with FastNRS offering
significant improvements in terms of memory efficiency, training speed, and
inference time. We evaluate both architectures on four tasks involving the
simplification of mathematical formulas and further demonstrate their
versatility in a multi-domain learning scenario, where a single model is
trained to solve multiple types of problems simultaneously. The proposed system
significantly outperforms two strong neural baselines: the Neural Data Router,
a recent transformer variant specifically designed to solve algorithmic
problems, and GPT-4o, one of the most powerful general-purpose large-language
models. Moreover, our system matches or outperforms the latest o1-preview model
from OpenAI that excels in reasoning benchmarks.
</summary>
    <author>
      <name>Flavio Petruzzellis</name>
    </author>
    <author>
      <name>Alberto Testolin</name>
    </author>
    <author>
      <name>Alessandro Sperduti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 31 figures. Submitted for review by Artificial Intelligence
  Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.19372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.19372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
