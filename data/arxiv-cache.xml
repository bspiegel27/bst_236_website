<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-06T00:55:56Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-05T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">111609</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.01406v1</id>
    <updated>2025-05-02T17:35:03Z</updated>
    <published>2025-05-02T17:35:03Z</published>
    <title>VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in
  Video Diffusion Models</title>
    <summary>  The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}
</summary>
    <author>
      <name>Mohammadreza Teymoorianfard</name>
    </author>
    <author>
      <name>Shiqing Ma</name>
    </author>
    <author>
      <name>Amir Houmansadr</name>
    </author>
    <link href="http://arxiv.org/abs/2505.01406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01386v1</id>
    <updated>2025-05-02T16:49:10Z</updated>
    <published>2025-05-02T16:49:10Z</published>
    <title>Carbon Aware Transformers Through Joint Model-Hardware Optimization</title>
    <summary>  The rapid growth of machine learning (ML) systems necessitates a more
comprehensive evaluation of their environmental impact, particularly their
carbon footprint, which comprises operational carbon from training and
inference execution and embodied carbon from hardware manufacturing and its
entire life-cycle. Despite the increasing importance of embodied emissions,
there is a lack of tools and frameworks to holistically quantify and optimize
the total carbon footprint of ML systems. To address this, we propose
CATransformers, a carbon-aware architecture search framework that enables
sustainability-driven co-optimization of ML models and hardware architectures.
By incorporating both operational and embodied carbon metrics into early design
space exploration of domain-specific hardware accelerators, CATransformers
demonstrates that optimizing for carbon yields design choices distinct from
those optimized solely for latency or energy efficiency. We apply our framework
to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models
achieving up to 17% reduction in total carbon emissions while maintaining
accuracy and latency compared to state-of-the-art edge small CLIP baselines.
This work underscores the need for holistic optimization methods to design
high-performance, environmentally sustainable AI systems.
</summary>
    <author>
      <name>Irene Wang</name>
    </author>
    <author>
      <name>Newsha Ardalani</name>
    </author>
    <author>
      <name>Mostafa Elhoushi</name>
    </author>
    <author>
      <name>Daniel Jiang</name>
    </author>
    <author>
      <name>Samuel Hsia</name>
    </author>
    <author>
      <name>Ekin Sumbul</name>
    </author>
    <author>
      <name>Divya Mahajan</name>
    </author>
    <author>
      <name>Carole-Jean Wu</name>
    </author>
    <author>
      <name>Bilge Acun</name>
    </author>
    <link href="http://arxiv.org/abs/2505.01386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01383v1</id>
    <updated>2025-05-02T16:47:05Z</updated>
    <published>2025-05-02T16:47:05Z</published>
    <title>FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft
  Research</title>
    <summary>  We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.
</summary>
    <author>
      <name>Yan Miao</name>
    </author>
    <author>
      <name>Will Shen</name>
    </author>
    <author>
      <name>Hang Cui</name>
    </author>
    <author>
      <name>Sayan Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/2505.01383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01381v1</id>
    <updated>2025-05-02T16:43:37Z</updated>
    <published>2025-05-02T16:43:37Z</published>
    <title>Probing the Cores of Subdwarf B Stars: How do They Compare to Cores in
  Helium Core-Burning Red Giants?</title>
    <summary>  The mixing of material from stellar convective cores into their adjacent
radiative layers has been a matter of long-standing debate. Pulsating subdwarf
B stars offer excellent conditions to advance our understanding of this
problem. In this work we use a model-independent approach to infer information
about the cores of three subdwarf B stars and compare it with similar
inferences from earlier analysis of red giants in the helium core-burning
phase. This is achieved by fitting an analytical description of the
gravity-mode pulsation periods to pulsation data collected by the Kepler
satellite. From the fits we infer the reduced asymptotic period spacings and
the amplitude and position of sharp structural variations associated with
chemical discontinuities in the stellar interiors. Our results indicate the
presence of sharp structural variations with similar properties in all three
stars, located near the edge of the gravity-mode propagation cavity and likely
associated with the C-O/He transition. We find that these structural variations
differ systematically from those of helium core-burning red giant stars, having
larger amplitudes and being located at a larger buoyancy radius. This suggests
that chemical mixing beyond the adiabatically stratified core into the
radiatively stratified layers may be more extensive in subdwarf B stars than in
helium core-burning red giants. Alternatively, the stratification of the mixing
region beyond the adiabatically stratified core may differ significantly
between the two types of stars. The model-independent constraints set on the
structural variations inside these three stars are the first of a kind and will
be key to enhance the modelling of layers adjacent to stellar convective cores
and to test non-canonical stellar evolution channels leading to the formation
of hot subdwarf stars.
</summary>
    <author>
      <name>Margarida S. Cunha</name>
    </author>
    <author>
      <name>Juliana Amaral</name>
    </author>
    <author>
      <name>Sofia Avelino</name>
    </author>
    <author>
      <name>Anselmo Falorca</name>
    </author>
    <author>
      <name>Yuri Damasceno</name>
    </author>
    <author>
      <name>Pedro Avelino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Astronomy and Astrophysics</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01372v1</id>
    <updated>2025-05-02T16:18:40Z</updated>
    <published>2025-05-02T16:18:40Z</published>
    <title>Evaluating Explanations: An Explanatory Virtues Framework for
  Mechanistic Interpretability -- The Strange Science Part I.ii</title>
    <summary>  Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.
</summary>
    <author>
      <name>Kola Ayonrinde</name>
    </author>
    <author>
      <name>Louis Jaburi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages (plus appendices), 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01351v1</id>
    <updated>2025-05-02T15:38:55Z</updated>
    <published>2025-05-02T15:38:55Z</published>
    <title>Closing the Loop: A Systematic Review of Experience-Driven Game
  Adaptation</title>
    <summary>  Adaptive game systems aim to enrich player experiences by dynamically
adjusting game content in response to user data. While extensive research has
addressed content personalization and player experience modeling, the
integration of these components into fully operational adaptive gameplay
systems remains limited. This systematic review, conducted in accordance with
PRISMA guidelines, analyzes 17 empirical studies published between January 2015
and May 2024, identifying and analyzing approaches that implement the complete
experience-driven loop -- including player sensing, modeling, and content
adaptation. Game telemetry remains the most prevalent sensing modality,
although other non-invasive methods suitable for affective modeling -- such as
facial expression analysis (FEA) and peripheral interaction data -- remain
underutilized despite their potential for real-time emotional inference.
Knowledge-based methods, such as rule-based systems and heuristics, dominate
modeling and adaptation due to their interpretability and low resource demands,
whereas machine learning approaches face challenges related to data
availability and transparency. Despite their relevance to immersive and
therapeutic experiences, affective states such as stress and anxiety remain
largely ignored, as systems continue to favor performance over
emotion-sensitive adaptation. These findings highlight a crucial research
direction: advancing emotionally responsive game systems that move beyond
performance optimization by incorporating underutilized sensing modalities --
such as FEA and peripheral interaction -- to enable real-time affect-driven
personalization. Advancing in this direction holds strong potential to increase
immersion, personalize gameplay, and support affect regulation across
entertainment and therapeutic contexts.
</summary>
    <author>
      <name>Phil Lopes</name>
    </author>
    <author>
      <name>Nuno Fachada</name>
    </author>
    <author>
      <name>Maria Fonseca</name>
    </author>
    <link href="http://arxiv.org/abs/2505.01351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01324v1</id>
    <updated>2025-05-02T14:55:34Z</updated>
    <published>2025-05-02T14:55:34Z</published>
    <title>Design-Based Inference under Random Potential Outcomes via Riesz
  Representation</title>
    <summary>  We introduce a general framework for design-based causal inference that
accommodates stochastic potential outcomes, thereby extending the classical
Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation,
each unit's potential outcome is modelled as a function $\tilde{y}_i(z,
\omega)$, where $\omega$ denotes latent randomness external to the treatment
assignment. Building on recent work that connects design-based estimation with
the Riesz representation theorem, we construct causal estimators by embedding
potential outcomes in a Hilbert space and defining treatment effects as linear
functionals. This allows us to derive unbiased and consistent estimators, even
when potential outcomes exhibit random variation. The framework retains the key
advantage of design-based analysis, namely, the use of a known randomisation
scheme for identification, while enabling inference in settings with inherent
stochasticity. We establish large-sample properties under local dependence,
provide a variance estimator compatible with sparse dependency structures, and
illustrate the method through a simulation. Our results unify design-based
reasoning with random-outcome modelling, broadening the applicability of causal
inference in complex experimental environments.
</summary>
    <author>
      <name>Yukai Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 2 figures, 2 Tables, 2 Algorithms. Preprint prepared for
  journal submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20, 62K99, 62D05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01322v1</id>
    <updated>2025-05-02T14:53:56Z</updated>
    <published>2025-05-02T14:53:56Z</published>
    <title>FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian
  Scene without Spatial Priors</title>
    <summary>  Text-driven object insertion in 3D scenes is an emerging task that enables
intuitive scene editing through natural language. However, existing 2D
editing-based methods often rely on spatial priors such as 2D masks or 3D
bounding boxes, and they struggle to ensure consistency of the inserted object.
These limitations hinder flexibility and scalability in real-world
applications. In this paper, we propose FreeInsert, a novel framework that
leverages foundation models including MLLMs, LGMs, and diffusion models to
disentangle object generation from spatial placement. This enables unsupervised
and flexible object insertion in 3D scenes without spatial priors. FreeInsert
starts with an MLLM-based parser that extracts structured semantics, including
object types, spatial relationships, and attachment regions, from user
instructions. These semantics guide both the reconstruction of the inserted
object for 3D consistency and the learning of its degrees of freedom. We
leverage the spatial reasoning capabilities of MLLMs to initialize object pose
and scale. A hierarchical, spatially aware refinement stage further integrates
spatial semantics and MLLM-inferred priors to enhance placement. Finally, the
appearance of the object is improved using the inserted-object image to enhance
visual fidelity. Experimental results demonstrate that FreeInsert achieves
semantically coherent, spatially precise, and visually realistic 3D insertions
without relying on spatial priors, offering a user-friendly and flexible
editing experience.
</summary>
    <author>
      <name>Chenxi Li</name>
    </author>
    <author>
      <name>Weijie Wang</name>
    </author>
    <author>
      <name>Qiang Li</name>
    </author>
    <author>
      <name>Bruno Lepri</name>
    </author>
    <author>
      <name>Nicu Sebe</name>
    </author>
    <author>
      <name>Weizhi Nie</name>
    </author>
    <link href="http://arxiv.org/abs/2505.01322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01279v1</id>
    <updated>2025-05-02T13:55:22Z</updated>
    <published>2025-05-02T13:55:22Z</published>
    <title>MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for
  Multi-Granular Spatiotemporal Traffic Forecasting</title>
    <summary>  Accurate traffic forecasting and swift inference provision are essential for
intelligent transportation systems. However, the present Graph Convolutional
Network (GCN)-based approaches cannot extract and fuse multi-granular
spatiotemporal features across various spatial and temporal scales
sufficiently, proven to yield less accurate forecasts. Besides, additional
feature extraction branches introduced in prior studies critically increased
model complexity and extended inference time, making it challenging to provide
fast inference for traffic forecasting. In this paper, we propose
MultiGran-STGCNFog, an efficient fog distributed inference system with a novel
traffic forecasting model that employs multi-granular spatiotemporal feature
fusion on generated dynamic traffic graphs to fully capture interdependent
traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer
execution order and layer-device scheduling scheme simultaneously, contributes
to considerable inference throughput improvement by leveraging heterogeneous
fog devices in a pipelined manner. Extensive experiments on real-world datasets
demonstrate the superiority of the proposed method over selected baselines.
</summary>
    <author>
      <name>Zhaoyan Wang</name>
    </author>
    <author>
      <name>Xiangchi Song</name>
    </author>
    <author>
      <name>In-Young Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">nine pages and five figures included</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01249v1</id>
    <updated>2025-05-02T13:17:08Z</updated>
    <published>2025-05-02T13:17:08Z</published>
    <title>Fusing Foveal Fixations Using Linear Retinal Transformations and
  Bayesian Experimental Design</title>
    <summary>  Humans (and many vertebrates) face the problem of fusing together multiple
fixations of a scene in order to obtain a representation of the whole, where
each fixation uses a high-resolution fovea and decreasing resolution in the
periphery. In this paper we explicitly represent the retinal transformation of
a fixation as a linear downsampling of a high-resolution latent image of the
scene, exploiting the known geometry. This linear transformation allows us to
carry out exact inference for the latent variables in factor analysis (FA) and
mixtures of FA models of the scene. Further, this allows us to formulate and
solve the choice of "where to look next" as a Bayesian experimental design
problem using the Expected Information Gain criterion. Experiments on the Frey
faces and MNIST datasets demonstrate the effectiveness of our models.
</summary>
    <author>
      <name>Christopher K. I. Williams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
