<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-27T00:52:29Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-26T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">109511</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.19898v1</id>
    <updated>2025-03-25T17:56:31Z</updated>
    <published>2025-03-25T17:56:31Z</published>
    <title>Non-minimally coupled gravity constraints from DESI DR2 data</title>
    <summary>  It has been observed that the hint about phantom crossing in the DESI BAO
observation might point to non-minimally coupled gravity. We analyzed DESI DR2
BAO together with CMB and Type Ia supernova data to constrain non-minimal
coupling of gravity in the effective field theory (EFT) approach. Using a
non-parametric method to infer the EFT functions, we found that with DESI BAO,
DESY5 SN, and Planck CMB data the signal for non-minimal coupling reaches
$\sim3\sigma$. It is found that current data can constrain up to the quadratic
order $(n=2)$ if the EFT function representing non-minimal coupling is Taylor
expanded as a general function of the dark energy fraction $\Omega_{\rm{DE}}$,
i.e. $\Omega^{\text{EFT}}(a)=\sum_{i=0}^{n} c^{\rm{EFT}}_i
\Omega^i_{\text{DE}}(a)$. This calls for a more flexible parametrization of the
EFT functions than commonly used ones in literature.
</summary>
    <author>
      <name>Jiaming Pan</name>
    </author>
    <author>
      <name>Gen Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.19898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19878v1</id>
    <updated>2025-03-25T17:43:08Z</updated>
    <published>2025-03-25T17:43:08Z</published>
    <title>CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation</title>
    <summary>  Large language models (LLMs) have revolutionized natural language processing
(NLP), particularly through Retrieval-Augmented Generation (RAG), which
enhances LLM capabilities by integrating external knowledge. However,
traditional RAG systems face critical limitations, including disrupted
contextual integrity due to text chunking, and over-reliance on semantic
similarity for retrieval. To address these issues, we propose CausalRAG, a
novel framework that incorporates causal graphs into the retrieval process. By
constructing and tracing causal relationships, CausalRAG preserves contextual
continuity and improves retrieval precision, leading to more accurate and
interpretable responses. We evaluate CausalRAG against regular RAG and
graph-based RAG approaches, demonstrating its superiority across several
metrics. Our findings suggest that grounding retrieval in causal reasoning
provides a promising approach to knowledge-intensive tasks.
</summary>
    <author>
      <name>Nengbo Wang</name>
    </author>
    <author>
      <name>Xiaotian Han</name>
    </author>
    <author>
      <name>Jagdip Singh</name>
    </author>
    <author>
      <name>Jing Ma</name>
    </author>
    <author>
      <name>Vipin Chaudhary</name>
    </author>
    <link href="http://arxiv.org/abs/2503.19878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19876v1</id>
    <updated>2025-03-25T17:38:28Z</updated>
    <published>2025-03-25T17:38:28Z</published>
    <title>SLA-Awareness for AI-assisted coding</title>
    <summary>  The integration of AI-assisted coding tools within development environments
drastically reduces development time, and allows developers to focus more on
creative and critical aspects of software engineering through the use of Code
Large Language Models (CodeLLMs). These coding assistants automate repetitive
and time-consuming coding tasks such as code generation, code completion, code
summarization, and code translation. Responsiveness is a crucial requirement of
these coding assistants to maintain real-time interactivity, such that their
use does not impede the developers' workflows. Different coding tasks have
unique characteristics and latency requirements: Time-To-First-Token (TTFT)
latency is essential for code completion tasks, while End-To-End (E2E) latency
is crucial for code translation tasks. Managing these varying requirements
simultaneously while optimizing resource usage poses significant challenges.
Existing work adopts the Model-as-a-Service paradigm for serving individual
CodeLLMs, but cannot effectively manage latency requirements of concurrent
coding tasks and sequences of CodeLLM inference calls, due to a lack of
end-to-end latency awareness. Another challenge is keeping resource utilization
high, when the serving system is deployed on a shared cluster environment. To
address these challenges, we propose Coding Assistant Task Orchestrator (CATO),
a runtime system designed to serve a diverse assortment of coding tasks while
meeting latency requirements and maximizing resource utilization. Our
experiments demonstrate that when all types of coding tasks were served
simultaneously, for TTFT-critical tasks, CATO improves overall Goodput rate and
resource utilization by up to 10% and 41.1%, respectively. P95 E2E latency was
also reduced by 18% for code summarization tasks, and P95 TTFT for code
generation tasks were reduced by 14% compared against state-of-the-art systems.
</summary>
    <author>
      <name>Kishanthan Thangarajah</name>
    </author>
    <author>
      <name>Arthur Leung</name>
    </author>
    <author>
      <name>Boyuan Chen</name>
    </author>
    <author>
      <name>Ahmed E. Hassan</name>
    </author>
    <link href="http://arxiv.org/abs/2503.19876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19819v1</id>
    <updated>2025-03-25T16:30:58Z</updated>
    <published>2025-03-25T16:30:58Z</published>
    <title>Domain-incremental White Blood Cell Classification with Privacy-aware
  Continual Learning</title>
    <summary>  White blood cell (WBC) classification plays a vital role in hematology for
diagnosing various medical conditions. However, it faces significant challenges
due to domain shifts caused by variations in sample sources (e.g., blood or
bone marrow) and differing imaging conditions across hospitals. Traditional
deep learning models often suffer from catastrophic forgetting in such dynamic
environments, while foundation models, though generally robust, experience
performance degradation when the distribution of inference data differs from
that of the training data. To address these challenges, we propose a generative
replay-based Continual Learning (CL) strategy designed to prevent forgetting in
foundation models for WBC classification. Our method employs lightweight
generators to mimic past data with a synthetic latent representation to enable
privacy-preserving replay. To showcase the effectiveness, we carry out
extensive experiments with a total of four datasets with different task
ordering and four backbone models including ResNet50, RetCCL, CTransPath, and
UNI. Experimental results demonstrate that conventional fine-tuning methods
degrade performance on previously learned tasks and struggle with domain
shifts. In contrast, our continual learning strategy effectively mitigates
catastrophic forgetting, preserving model performance across varying domains.
This work presents a practical solution for maintaining reliable WBC
classification in real-world clinical settings, where data distributions
frequently evolve.
</summary>
    <author>
      <name>Pratibha Kumari</name>
    </author>
    <author>
      <name>Afshin Bozorgpour</name>
    </author>
    <author>
      <name>Daniel Reisenb√ºchler</name>
    </author>
    <author>
      <name>Edgar Jost</name>
    </author>
    <author>
      <name>Martina Crysandt</name>
    </author>
    <author>
      <name>Christian Matek</name>
    </author>
    <author>
      <name>Dorit Merhof</name>
    </author>
    <link href="http://arxiv.org/abs/2503.19819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19777v1</id>
    <updated>2025-03-25T15:47:13Z</updated>
    <published>2025-03-25T15:47:13Z</published>
    <title>LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary
  Semantic Segmentation</title>
    <summary>  We propose a training-free method for open-vocabulary semantic segmentation
using Vision-and-Language Models (VLMs). Our approach enhances the initial
per-patch predictions of VLMs through label propagation, which jointly
optimizes predictions by incorporating patch-to-patch relationships. Since VLMs
are primarily optimized for cross-modal alignment and not for intra-modal
similarity, we use a Vision Model (VM) that is observed to better capture these
relationships. We address resolution limitations inherent to patch-based
encoders by applying label propagation at the pixel level as a refinement step,
significantly improving segmentation accuracy near class boundaries. Our
method, called LPOSS+, performs inference over the entire image, avoiding
window-based processing and thereby capturing contextual interactions across
the full image. LPOSS+ achieves state-of-the-art performance among
training-free methods, across a diverse set of datasets. Code:
https://github.com/vladan-stojnic/LPOSS
</summary>
    <author>
      <name>Vladan Stojniƒá</name>
    </author>
    <author>
      <name>Yannis Kalantidis</name>
    </author>
    <author>
      <name>Ji≈ô√≠ Matas</name>
    </author>
    <author>
      <name>Giorgos Tolias</name>
    </author>
    <link href="http://arxiv.org/abs/2503.19777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19755v1</id>
    <updated>2025-03-25T15:18:43Z</updated>
    <published>2025-03-25T15:18:43Z</published>
    <title>ORION: A Holistic End-to-End Autonomous Driving Framework by
  Vision-Language Instructed Action Generation</title>
    <summary>  End-to-end (E2E) autonomous driving methods still struggle to make correct
decisions in interactive closed-loop evaluation due to limited causal reasoning
capability. Current methods attempt to leverage the powerful understanding and
reasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma.
However, the problem is still open that few VLMs for E2E methods perform well
in the closed-loop evaluation due to the gap between the semantic reasoning
space and the purely numerical trajectory output in the action space. To tackle
this issue, we propose ORION, a holistic E2E autonomous driving framework by
vision-language instructed action generation. ORION uniquely combines a
QT-Former to aggregate long-term history context, a Large Language Model (LLM)
for driving scenario reasoning, and a generative planner for precision
trajectory prediction. ORION further aligns the reasoning space and the action
space to implement a unified E2E optimization for both visual
question-answering (VQA) and planning tasks. Our method achieves an impressive
closed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate
(SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art
(SOTA) methods by a large margin of 14.28 DS and 19.61% SR.
</summary>
    <author>
      <name>Haoyu Fu</name>
    </author>
    <author>
      <name>Diankun Zhang</name>
    </author>
    <author>
      <name>Zongchuang Zhao</name>
    </author>
    <author>
      <name>Jianfeng Cui</name>
    </author>
    <author>
      <name>Dingkang Liang</name>
    </author>
    <author>
      <name>Chong Zhang</name>
    </author>
    <author>
      <name>Dingyuan Zhang</name>
    </author>
    <author>
      <name>Hongwei Xie</name>
    </author>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <link href="http://arxiv.org/abs/2503.19755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19748v1</id>
    <updated>2025-03-25T15:11:14Z</updated>
    <published>2025-03-25T15:11:14Z</published>
    <title>No-prior Bayesian inference reIMagined: probabilistic approximations of
  inferential models</title>
    <summary>  When prior information is lacking, the go-to strategy for probabilistic
inference is to combine a "default prior" and the likelihood via Bayes's
theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under
this umbrella. This construction is natural, but the corresponding posterior
distributions generally only offer limited, approximately valid uncertainty
quantification. The present paper takes a reimagined approach offering
posterior distributions with stronger reliability properties. The proposed
construction starts with an inferential model (IM), one that takes the
mathematical form of a data-driven possibility measure and features exactly
valid uncertainty quantification, and then returns a so-called inner
probabilistic approximation thereof. This inner probabilistic approximation
inherits many of the original IM's desirable properties, including credible
sets with exact coverage and asymptotic efficiency. The approximation also
agrees with the familiar Bayes/fiducial solution obtained in applications where
the model has a group transformation structure. A Monte Carlo method for
evaluating the probabilistic approximation is presented, along with numerical
illustrations.
</summary>
    <author>
      <name>Ryan Martin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages + appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.19748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19706v1</id>
    <updated>2025-03-25T14:33:32Z</updated>
    <published>2025-03-25T14:33:32Z</published>
    <title>Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained
  View-invariant Video Representations</title>
    <summary>  View-invariant representation learning from egocentric (first-person, ego)
and exocentric (third-person, exo) videos is a promising approach toward
generalizing video understanding systems across multiple viewpoints. However,
this area has been underexplored due to the substantial differences in
perspective, motion patterns, and context between ego and exo views. In this
paper, we propose a novel masked ego-exo modeling that promotes both causal
temporal dynamics and cross-view alignment, called Bootstrap Your Own Views
(BYOV), for fine-grained view-invariant video representation learning from
unpaired ego-exo videos. We highlight the importance of capturing the
compositional nature of human actions as a basis for robust cross-view
understanding. Specifically, self-view masking and cross-view masking
predictions are designed to learn view-invariant and powerful representations
concurrently. Experimental results demonstrate that our BYOV significantly
surpasses existing approaches with notable gains across all metrics in four
downstream ego-exo video tasks. The code is available at
https://github.com/park-jungin/byov.
</summary>
    <author>
      <name>Jungin Park</name>
    </author>
    <author>
      <name>Jiyoung Lee</name>
    </author>
    <author>
      <name>Kwanghoon Sohn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025 Camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.19706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19697v1</id>
    <updated>2025-03-25T14:25:32Z</updated>
    <published>2025-03-25T14:25:32Z</published>
    <title>Spectral classification of young stars using conditional invertible
  neural networks II. Application to Trumpler 14 in Carina</title>
    <summary>  We introduce an updated version of our deep learning tool that predicts
stellar parameters from the optical spectra of young low-mass stars with
intermediate spectral resolution. We adopt a conditional invertible neural
network (cINN) architecture to infer the posterior distribution of stellar
parameters and train our cINN on two Phoenix stellar atmosphere model libraries
(Settl and Dusty). Compared to the cINNs presented in our first study, the
updated cINN considers the influence of the relative flux error on the
parameter estimation and predicts an additional fourth parameter, veiling. We
test the performance of cINN on synthetic test models to quantify the intrinsic
error of the cINN as a function of relative flux error and on 36 class III
template stars to validate the performance on real spectra. Using our cINN, we
estimate the stellar parameters of young stars in Trumpler 14 (Tr14) in the
Carina Nebula Complex, observed with VLT-MUSE, and compare them with those
derived using the classical template fitting method. We provide Teff, log g,
Av, and veiling values measured by our cINN as well as stellar ages and masses
derived from the HR diagram. Our parameter estimates generally agree well with
those measured by template fitting. However, for K- and G-type stars, the Teff
derived from template fitting is, on average, 2-3 subclasses hotter than the
cINN estimates, while the corresponding veiling values from template fitting
appear to be underestimated compared to the cINN predictions. We obtain an
average age of 0.7(+3.2)(-0.6) Myr for the Tr14 stars. By examining the impact
of veiling on the equivalent width-based classification, we demonstrate that
the main cause of temperature overestimation for K- and G-type stars in the
previous study is that veiling and effective temperature are not considered
simultaneously in their process.
</summary>
    <author>
      <name>Da Eun Kang</name>
    </author>
    <author>
      <name>Dominika Itrich</name>
    </author>
    <author>
      <name>Victor F. Ksoll</name>
    </author>
    <author>
      <name>Leonardo Testi</name>
    </author>
    <author>
      <name>Ralf S. Klessen</name>
    </author>
    <author>
      <name>Sergio Molinari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 24 figures, Accepted for publication by Astronomy &amp;
  Astrophysics on 21 March</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.19697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19666v2</id>
    <updated>2025-03-26T10:39:33Z</updated>
    <published>2025-03-25T13:52:26Z</published>
    <title>Towards Efficient Training of Graph Neural Networks: A Multiscale
  Approach</title>
    <summary>  Graph Neural Networks (GNNs) have emerged as a powerful tool for learning and
inferring from graph-structured data, and are widely used in a variety of
applications, often considering large amounts of data and large graphs.
However, training on such data requires large memory and extensive
computations. In this paper, we introduce a novel framework for efficient
multiscale training of GNNs, designed to integrate information across
multiscale representations of a graph. Our approach leverages a hierarchical
graph representation, taking advantage of coarse graph scales in the training
process, where each coarse scale graph has fewer nodes and edges. Based on this
approach, we propose a suite of GNN training methods: such as coarse-to-fine,
sub-to-full, and multiscale gradient computation. We demonstrate the
effectiveness of our methods on various datasets and learning tasks.
</summary>
    <author>
      <name>Eshed Gal</name>
    </author>
    <author>
      <name>Moshe Eliasof</name>
    </author>
    <author>
      <name>Carola-Bibiane Sch√∂nlieb</name>
    </author>
    <author>
      <name>Eldad Haber</name>
    </author>
    <author>
      <name>Eran Treister</name>
    </author>
    <link href="http://arxiv.org/abs/2503.19666v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19666v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
