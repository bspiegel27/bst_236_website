<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-12T00:57:58Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-11T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">118570</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.06494v1</id>
    <updated>2025-08-08T17:59:52Z</updated>
    <published>2025-08-08T17:59:52Z</published>
    <title>LightSwitch: Multi-view Relighting with Material-guided Diffusion</title>
    <summary>  Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.
</summary>
    <author>
      <name>Yehonathan Litman</name>
    </author>
    <author>
      <name>Fernando De la Torre</name>
    </author>
    <author>
      <name>Shubham Tulsiani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2025, Project page &amp; Code:
  https://yehonathanlitman.github.io/light_switch/</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.06494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06472v1</id>
    <updated>2025-08-08T17:21:26Z</updated>
    <published>2025-08-08T17:21:26Z</published>
    <title>Revisiting the Gas Dynamics of Henize 2-10: Possible Drivers of the
  Starburst</title>
    <summary>  The triggers of starburst episodes are a key component to our understanding
of the baryon cycle in galaxies. Galaxy mergers are a commonly suggested
catalyst for starbursts, but once the galaxies coalesce into a single
kinematically disturbed system, their merger history can be difficult to
assess. This is particularly true for dwarf galaxies, which are expected to
dominate the merger rate at all redshifts due to their large numbers. One such
dwarf galaxy undergoing an enigmatic starburst episode is Henize 2-10, which
appears to be isolated. Possible scenarios that might have caused the starburst
episode include a previous merger or stochastic processes within the galaxy
itself, such as self-regulation via feedback processes. We present new VLA
21-cm observations and unpublished archival CARMA CO data to investigate the
dynamical state and star formation activity in the galaxy. We do not detect an
HI tail consistent with the structure reported by Kobulnicky et al. (1995),
which was suggested as evidence for a merger or interaction, but rather these
new observations indicate an extended HI distribution. We also find that the HI
appears dynamically decoupled from an extended CO feature (inferred to be a
tidal tail in previous work), suggesting large-scale dynamical processes of
some type are affecting the gas in this system. We provide a meta-analysis of
available results to enhance our understanding of what might be triggering the
starburst episode in Henize 2-10, and speculate that the large CO feature could
be falling into the galaxy and potentially trigger starburst activity.
</summary>
    <author>
      <name>Josephine M. Dalsin</name>
    </author>
    <author>
      <name>Allison H. Costa</name>
    </author>
    <author>
      <name>Remy Indebetouw</name>
    </author>
    <author>
      <name>Kelsey E. Johnson</name>
    </author>
    <author>
      <name>Natalie O. Johnson</name>
    </author>
    <author>
      <name>Sabrina Stierwalt</name>
    </author>
    <link href="http://arxiv.org/abs/2508.06472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06447v1</id>
    <updated>2025-08-08T16:42:38Z</updated>
    <published>2025-08-08T16:42:38Z</published>
    <title>SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token
  Pruning</title>
    <summary>  Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.
</summary>
    <author>
      <name>Lingkun Long</name>
    </author>
    <author>
      <name>Rubing Yang</name>
    </author>
    <author>
      <name>Yushi Huang</name>
    </author>
    <author>
      <name>Desheng Hui</name>
    </author>
    <author>
      <name>Ao Zhou</name>
    </author>
    <author>
      <name>Jianlei Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2508.06447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06439v1</id>
    <updated>2025-08-08T16:26:38Z</updated>
    <published>2025-08-08T16:26:38Z</published>
    <title>Schwinger--DeWitt expansion for the heat kernel of nonminimal operators
  in causal theories</title>
    <summary>  We suggest a systematic calculational scheme for heat kernels of covariant
nonminimal operators in causal theories whose characteristic surfaces are null
with respect to a generic metric. The calculational formalism is based on a
pseudodifferential operator calculus which allows one to build a linear
operator map from the heat kernel of the minimal operator to the nonminimal
one. This map is realized as a local expansion in powers of spacetime
curvature, dimensional background fields, and their covariant derivatives with
the coefficients -- the functions of the Synge world function and its
derivatives. Finiteness of these functions, determined by multiple proper time
integrals, is achieved by a special subtraction procedure which is an important
part of the calculational scheme. We illustrate this technique on the examples
of the vector Proca model and the vector field operator with a nondegenerate
principal symbol. We also discuss smoothness properties of heat kernels of
nonminimal operators in connection with the nondegenerate nature of their
operator symbols.
</summary>
    <author>
      <name>Andrei O. Barvinsky</name>
    </author>
    <author>
      <name>Alexey E. Kalugin</name>
    </author>
    <author>
      <name>Władysław Wachowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.06439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06435v1</id>
    <updated>2025-08-08T16:23:24Z</updated>
    <published>2025-08-08T16:23:24Z</published>
    <title>Learning the Topic, Not the Language: How LLMs Classify Online
  Immigration Discourse Across Languages</title>
    <summary>  Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.
</summary>
    <author>
      <name>Andrea Nasuto</name>
    </author>
    <author>
      <name>Stefano Maria Iacus</name>
    </author>
    <author>
      <name>Francisco Rowe</name>
    </author>
    <author>
      <name>Devika Jain</name>
    </author>
    <link href="http://arxiv.org/abs/2508.06435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06414v1</id>
    <updated>2025-08-08T15:58:11Z</updated>
    <published>2025-08-08T15:58:11Z</published>
    <title>What Builds Effective In-Context Examples for Code Generation?</title>
    <summary>  In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.
</summary>
    <author>
      <name>Dongze Li</name>
    </author>
    <author>
      <name>Songqiang Chen</name>
    </author>
    <author>
      <name>Jialun Cao</name>
    </author>
    <author>
      <name>Shing-Chi Cheung</name>
    </author>
    <link href="http://arxiv.org/abs/2508.06414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06411v1</id>
    <updated>2025-08-08T15:56:05Z</updated>
    <published>2025-08-08T15:56:05Z</published>
    <title>Dimensional Characterization and Pathway Modeling for Catastrophic AI
  Risks</title>
    <summary>  Although discourse around the risks of Artificial Intelligence (AI) has
grown, it often lacks a comprehensive, multidimensional framework, and concrete
causal pathways mapping hazard to harm. This paper aims to bridge this gap by
examining six commonly discussed AI catastrophic risks: CBRN, cyber offense,
sudden loss of control, gradual loss of control, environmental risk, and
geopolitical risk. First, we characterize these risks across seven key
dimensions, namely intent, competency, entity, polarity, linearity, reach, and
order. Next, we conduct risk pathway modeling by mapping step-by-step
progressions from the initial hazard to the resulting harms. The dimensional
approach supports systematic risk identification and generalizable mitigation
strategies, while risk pathway models help identify scenario-specific
interventions. Together, these methods offer a more structured and actionable
foundation for managing catastrophic AI risks across the value chain.
</summary>
    <author>
      <name>Ze Shen Chin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages including references, 6 figures. To be presented in
  Technical AI Governance Forum 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.06411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06385v1</id>
    <updated>2025-08-08T15:16:08Z</updated>
    <published>2025-08-08T15:16:08Z</published>
    <title>Bayesian online collective anomaly and change point detection in
  fine-grained time series</title>
    <summary>  Fine-grained time series data are crucial for accurate and timely online
change detection. While both collective anomalies and change points can coexist
in such data, their joint online detection has received limited attention. In
this research, we develop a Bayesian framework capturing time series with
collective anomalies and change points, and introduce a recursive online
inference algorithm to detect the most recent collective anomaly and change
point jointly. For scaling, we further propose an algorithm enhanced with
collective anomaly removal that effectively reduces the time and space
complexity to linear. We demonstrate the effectiveness of our approach via
extensive experiments on simulated data and two real-world applications.
</summary>
    <author>
      <name>Xian Chen</name>
    </author>
    <author>
      <name>Weichi Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2508.06385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06355v1</id>
    <updated>2025-08-08T14:36:59Z</updated>
    <published>2025-08-08T14:36:59Z</published>
    <title>Quantum Algorithm for Estimating Intrinsic Geometry</title>
    <summary>  High-dimensional datasets typically cluster around lower-dimensional
manifolds but are also often marred by severe noise, obscuring the intrinsic
geometry essential for downstream learning tasks. We present a quantum
algorithm for estimating the intrinsic geometry of a point cloud --
specifically its local intrinsic dimension and local scalar curvature. These
quantities are crucial for dimensionality reduction, feature extraction, and
anomaly detection -- tasks that are central to a wide range of data-driven and
data-assisted applications. In this work, we propose a quantum algorithm which
takes a dataset with pairwise geometric distance, output the estimation of
local dimension and curvature at a given point. We demonstrate that this
quantum algorithm achieves an exponential speedup over its classical
counterpart, and, as a corollary, further extend our main technique to
diffusion maps, yielding exponential improvements even over existing quantum
algorithms. Our work marks another step toward efficient quantum applications
in geometrical data analysis, moving beyond topological summaries toward
precise geometric inference and opening a novel, scalable path to
quantum-enhanced manifold learning.
</summary>
    <author>
      <name>Nhat A. Nghiem</name>
    </author>
    <author>
      <name>Tuan K. Do</name>
    </author>
    <author>
      <name>Tzu-Chieh Wei</name>
    </author>
    <author>
      <name>Trung V. Phan</name>
    </author>
    <link href="http://arxiv.org/abs/2508.06355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.06349v1</id>
    <updated>2025-08-08T14:24:04Z</updated>
    <published>2025-08-08T14:24:04Z</published>
    <title>Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional
  Resonance</title>
    <summary>  Emoji reactions are a frequently used feature of messaging platforms. Prior
work mainly interpreted emojis as indicators of emotional resonance or user
sentiment. However, emoji reactions may instead reflect broader social
dynamics. Here, we investigate the communicative function of emoji reactions on
Telegram by analyzing the relationship between the emotional and rhetorical
content of messages and the emoji reactions they receive. We collect and
analyze over 650k Telegram messages that received at least one emoji reaction.
We annotate each message with sentiment, emotion, persuasion strategy, and
speech act labels, and infer the sentiment and emotion of emoji reactions using
both lexicons and large languages. We find a systematic mismatch between
message sentiment and reaction sentiment, with positive reactions dominating
even when the message is neutral or negative. We show that this pattern remains
consistent across rhetorical strategies and emotional tones, suggesting that
emoji reactions may signal a degree of social approval rather than reflecting
emotional resonance. Finally, we shed light on the communicative strategies
that predict greater emoji engagement. These findings have methodological
implications for sentiment analysis, as interpreting emoji reactions as direct
proxies for emotional response may be misleading.
</summary>
    <author>
      <name>Serena Tardelli</name>
    </author>
    <author>
      <name>Lorenzo Alvisi</name>
    </author>
    <author>
      <name>Lorenzo Cima</name>
    </author>
    <author>
      <name>Stefano Cresci</name>
    </author>
    <author>
      <name>Maurizio Tesconi</name>
    </author>
    <link href="http://arxiv.org/abs/2508.06349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.06349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
