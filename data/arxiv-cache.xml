<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-14T00:51:19Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-13T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">108691</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.09579v1</id>
    <updated>2025-03-12T17:50:42Z</updated>
    <published>2025-03-12T17:50:42Z</published>
    <title>Cost-Optimal Grouped-Query Attention for Long-Context LLMs</title>
    <summary>  Building effective and efficient Transformer-based large language models
(LLMs) has recently become a research focus, requiring maximizing model
language capabilities and minimizing training and deployment costs. Existing
efforts have primarily described complex relationships among model performance,
parameter size, and data size, as well as searched for the optimal compute
allocation to train LLMs. However, they overlook the impacts of context length
and attention head configuration (the number of query and key-value heads in
grouped-query attention) on training and inference. In this paper, we
systematically compare models with different parameter sizes, context lengths,
and attention head configurations in terms of model performance, computational
cost, and memory cost. Then, we extend the existing scaling methods, which are
based solely on parameter size and training compute, to guide the construction
of cost-optimal LLMs during both training and inference. Our quantitative
scaling studies show that, when processing sufficiently long sequences, a
larger model with fewer attention heads can achieve a lower loss while
incurring lower computational and memory costs. Our findings provide valuable
insights for developing practical LLMs, especially in long-context processing
scenarios. We will publicly release our code and data.
</summary>
    <author>
      <name>Yingfa Chen</name>
    </author>
    <author>
      <name>Yutong Wu</name>
    </author>
    <author>
      <name>Xu Han</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09573v1</id>
    <updated>2025-03-12T17:43:40Z</updated>
    <published>2025-03-12T17:43:40Z</published>
    <title>Block Diffusion: Interpolating Between Autoregressive and Diffusion
  Language Models</title>
    <summary>  Diffusion language models offer unique benefits over autoregressive models
due to their potential for parallelized generation and controllability, yet
they lag in likelihood modeling and are limited to fixed-length generation. In
this work, we introduce a class of block diffusion language models that
interpolate between discrete denoising diffusion and autoregressive models.
Block diffusion overcomes key limitations of both approaches by supporting
flexible-length generation and improving inference efficiency with KV caching
and parallel token sampling. We propose a recipe for building effective block
diffusion models that includes an efficient training algorithm, estimators of
gradient variance, and data-driven noise schedules to minimize the variance.
Block diffusion sets a new state-of-the-art performance among diffusion models
on language modeling benchmarks and enables generation of arbitrary-length
sequences. We provide the code, along with the model weights and blog post on
the project page: https://m-arriola.com/bd3lms/
</summary>
    <author>
      <name>Marianne Arriola</name>
    </author>
    <author>
      <name>Aaron Gokaslan</name>
    </author>
    <author>
      <name>Justin T Chiu</name>
    </author>
    <author>
      <name>Zhihan Yang</name>
    </author>
    <author>
      <name>Zhixuan Qi</name>
    </author>
    <author>
      <name>Jiaqi Han</name>
    </author>
    <author>
      <name>Subham Sekhar Sahoo</name>
    </author>
    <author>
      <name>Volodymyr Kuleshov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025 Oral. We provide the code at
  https://github.com/kuleshov-group/bd3lms</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09568v1</id>
    <updated>2025-03-12T17:35:26Z</updated>
    <published>2025-03-12T17:35:26Z</published>
    <title>Biological Multi-Layer and Single Cell Network-Based Multiomics Models -
  a Review</title>
    <summary>  Recent advances in single cell sequencing and multi-omics techniques have
significantly improved our understanding of biological phenomena and our
capacity to model them. Despite combined capture of data modalities showing
similar progress, notably single cell transcriptomics and proteomics,
simultaneous multi-omics level probing still remains challenging. As an
alternative to combined capture of biological data, in this review, we explore
current and upcoming methods for post-hoc network inference and integration
with an emphasis on single cell transcriptomics and proteomics. By examining
various approaches, from probabilistic models to graph-based algorithms, we
outline the challenges and potential strategies for effectively combining
biological data types while simultaneously highlighting the importance of model
validation. With this review, we aim to inform readers of the breadth of tools
currently available for the purpose-specific generation of heterogeneous
multi-layer networks.
</summary>
    <author>
      <name>Marcello Barylli</name>
    </author>
    <author>
      <name>Joyaditya Saha</name>
    </author>
    <author>
      <name>Tineke E. Buffart</name>
    </author>
    <author>
      <name>Jan Koster</name>
    </author>
    <author>
      <name>Kristiaan J. Lenos</name>
    </author>
    <author>
      <name>Louis Vermeulen</name>
    </author>
    <author>
      <name>Vivek M. Sheraton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09566v1</id>
    <updated>2025-03-12T17:33:22Z</updated>
    <published>2025-03-12T17:33:22Z</published>
    <title>TPDiff: Temporal Pyramid Video Diffusion Model</title>
    <summary>  The development of video diffusion models unveils a significant challenge:
the substantial computational demands. To mitigate this challenge, we note that
the reverse process of diffusion exhibits an inherent entropy-reducing nature.
Given the inter-frame redundancy in video modality, maintaining full frame
rates in high-entropy stages is unnecessary. Based on this insight, we propose
TPDiff, a unified framework to enhance training and inference efficiency. By
dividing diffusion into several stages, our framework progressively increases
frame rate along the diffusion process with only the last stage operating on
full frame rate, thereby optimizing computational efficiency. To train the
multi-stage diffusion model, we introduce a dedicated training framework:
stage-wise diffusion. By solving the partitioned probability flow ordinary
differential equations (ODE) of diffusion under aligned data and noise, our
training strategy is applicable to various diffusion forms and further enhances
training efficiency. Comprehensive experimental evaluations validate the
generality of our method, demonstrating 50% reduction in training cost and 1.5x
improvement in inference efficiency.
</summary>
    <author>
      <name>Lingmin Ran</name>
    </author>
    <author>
      <name>Mike Zheng Shou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://showlab.github.io/TPDiff/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09527v1</id>
    <updated>2025-03-12T16:42:26Z</updated>
    <published>2025-03-12T16:42:26Z</published>
    <title>CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in
  3D Action Role-Playing Games</title>
    <summary>  Recent advances in Vision-Language-Action models (VLAs) have expanded the
capabilities of embodied intelligence. However, significant challenges remain
in real-time decision-making in complex 3D environments, which demand
second-level responses, high-resolution perception, and tactical reasoning
under dynamic conditions. To advance the field, we introduce CombatVLA, an
efficient VLA model optimized for combat tasks in 3D action role-playing
games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action
pairs collected by an action tracker, where the data is formatted as
action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates
into an action execution framework, allowing efficient inference through our
truncated AoT strategy. Experimental results demonstrate that CombatVLA not
only outperforms all existing models on the combat understanding benchmark but
also achieves a 50-fold acceleration in game combat. Moreover, it has a higher
task success rate than human players. We will open-source all resources,
including the action tracker, dataset, benchmark, model weights, training code,
and the implementation of the framework at https://combatvla.github.io/.
</summary>
    <author>
      <name>Peng Chen</name>
    </author>
    <author>
      <name>Pi Bu</name>
    </author>
    <author>
      <name>Yingyao Wang</name>
    </author>
    <author>
      <name>Xinyi Wang</name>
    </author>
    <author>
      <name>Ziming Wang</name>
    </author>
    <author>
      <name>Jie Guo</name>
    </author>
    <author>
      <name>Yingxiu Zhao</name>
    </author>
    <author>
      <name>Qi Zhu</name>
    </author>
    <author>
      <name>Jun Song</name>
    </author>
    <author>
      <name>Siran Yang</name>
    </author>
    <author>
      <name>Jiamang Wang</name>
    </author>
    <author>
      <name>Bo Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2503.09527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09516v1</id>
    <updated>2025-03-12T16:26:39Z</updated>
    <published>2025-03-12T16:26:39Z</published>
    <title>Search-R1: Training LLMs to Reason and Leverage Search Engines with
  Reinforcement Learning</title>
    <summary>  Efficiently acquiring external knowledge and up-to-date information is
essential for effective reasoning and text generation in large language models
(LLMs). Retrieval augmentation and tool-use training approaches where a search
engine is treated as a tool lack complex multi-turn retrieval flexibility or
require large-scale supervised data. Prompting advanced LLMs with reasoning
capabilities during inference to use search engines is not optimal, since the
LLM does not learn how to optimally interact with the search engine. This paper
introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM
learns -- solely through reinforcement learning (RL) -- to autonomously
generate (multiple) search queries during step-by-step reasoning with real-time
retrieval. Search-R1 optimizes LLM rollouts with multi-turn search
interactions, leveraging retrieved token masking for stable RL training and a
simple outcome-based reward function. Experiments on seven question-answering
datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%
(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further
provides empirical insights into RL optimization methods, LLM choices, and
response length dynamics in retrieval-augmented reasoning. The code and model
checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.
</summary>
    <author>
      <name>Bowen Jin</name>
    </author>
    <author>
      <name>Hansi Zeng</name>
    </author>
    <author>
      <name>Zhenrui Yue</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Hamed Zamani</name>
    </author>
    <author>
      <name>Jiawei Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09514v1</id>
    <updated>2025-03-12T16:25:18Z</updated>
    <published>2025-03-12T16:25:18Z</published>
    <title>CM-Diff: A Single Generative Network for Bidirectional Cross-Modality
  Translation Diffusion Model Between Infrared and Visible Images</title>
    <summary>  The image translation method represents a crucial approach for mitigating
information deficiencies in the infrared and visible modalities, while also
facilitating the enhancement of modality-specific datasets. However, existing
methods for infrared and visible image translation either achieve
unidirectional modality translation or rely on cycle consistency for
bidirectional modality translation, which may result in suboptimal performance.
In this work, we present the cross-modality translation diffusion model
(CM-Diff) for simultaneously modeling data distributions in both the infrared
and visible modalities. We address this challenge by combining translation
direction labels for guidance during training with cross-modality feature
control. Specifically, we view the establishment of the mapping relationship
between the two modalities as the process of learning data distributions and
understanding modality differences, achieved through a novel Bidirectional
Diffusion Training (BDT) strategy. Additionally, we propose a Statistical
Constraint Inference (SCI) strategy to ensure the generated image closely
adheres to the data distribution of the target modality. Experimental results
demonstrate the superiority of our CM-Diff over state-of-the-art methods,
highlighting its potential for generating dual-modality datasets.
</summary>
    <author>
      <name>Bin Hu</name>
    </author>
    <author>
      <name>Chenqiang Gao</name>
    </author>
    <author>
      <name>Shurui Liu</name>
    </author>
    <author>
      <name>Junjie Guo</name>
    </author>
    <author>
      <name>Fang Chen</name>
    </author>
    <author>
      <name>Fangcen Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.09514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09513v1</id>
    <updated>2025-03-12T16:23:14Z</updated>
    <published>2025-03-12T16:23:14Z</published>
    <title>RESTRAIN: Reinforcement Learning-Based Secure Framework for
  Trigger-Action IoT Environment</title>
    <summary>  Internet of Things (IoT) platforms with trigger-action capability allow event
conditions to trigger actions in IoT devices autonomously by creating a chain
of interactions. Adversaries exploit this chain of interactions to maliciously
inject fake event conditions into IoT hubs, triggering unauthorized actions on
target IoT devices to implement remote injection attacks. Existing defense
mechanisms focus mainly on the verification of event transactions using
physical event fingerprints to enforce the security policies to block unsafe
event transactions. These approaches are designed to provide offline defense
against injection attacks. The state-of-the-art online defense mechanisms offer
real-time defense, but extensive reliability on the inference of attack impacts
on the IoT network limits the generalization capability of these approaches. In
this paper, we propose a platform-independent multi-agent online defense
system, namely RESTRAIN, to counter remote injection attacks at runtime.
RESTRAIN allows the defense agent to profile attack actions at runtime and
leverages reinforcement learning to optimize a defense policy that complies
with the security requirements of the IoT network. The experimental results
show that the defense agent effectively takes real-time defense actions against
complex and dynamic remote injection attacks and maximizes the security gain
with minimal computational overhead.
</summary>
    <author>
      <name>Md Morshed Alam</name>
    </author>
    <author>
      <name>Lokesh Chandra Das</name>
    </author>
    <author>
      <name>Sandip Roy</name>
    </author>
    <author>
      <name>Sachin Shetty</name>
    </author>
    <author>
      <name>Weichao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2503.09513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09504v1</id>
    <updated>2025-03-12T16:13:50Z</updated>
    <published>2025-03-12T16:13:50Z</published>
    <title>Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework</title>
    <summary>  The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL).
However, its complex architecture and advantages over dense models in image
classification remain unclear. In previous studies, MoE performance has often
been affected by noise and outliers in the input space. Some approaches
incorporate input clustering for training MoE models, but most clustering
algorithms lack access to labeled data, limiting their effectiveness. This
paper introduces the Double-stage Feature-level Clustering and
Pseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists
of input feature extraction, feature-level clustering, and a computationally
efficient pseudo-labeling strategy. This approach reduces the impact of noise
and outliers while leveraging a small subset of labeled data to label a large
portion of unlabeled inputs. We propose a conditional end-to-end joint training
method that improves expert specialization by training the MoE model on
well-labeled, clustered inputs. Unlike traditional MoE and dense models, the
DFCP-MoE framework effectively captures input space diversity, leading to
competitive inference results. We validate our approach on three benchmark
datasets for multi-class classification tasks.
</summary>
    <author>
      <name>Bakary Badjie</name>
    </author>
    <author>
      <name>José Cecílio</name>
    </author>
    <author>
      <name>António Casimiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, 1 Figure, and 3 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09499v1</id>
    <updated>2025-03-12T16:03:03Z</updated>
    <published>2025-03-12T16:03:03Z</published>
    <title>MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging
  Questions</title>
    <summary>  Large vision-language models (VLMs) face challenges in achieving robust,
transferable reasoning abilities due to reliance on labor-intensive manual
instruction datasets or computationally expensive self-supervised methods. To
address these issues, we introduce MindGYM, a framework that enhances VLMs
through synthetic self-challenging questions, consisting of three stages: (1)
Seed Single-Hop Question Synthesis, generating cognitive questions across
textual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based
queries) spanning eight semantic areas like ethical analysis; (2) Challenging
Multi-Hop Question Synthesis, combining seed questions via diverse principles
like bridging, visual-textual alignment, to create multi-step problems
demanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a
structured pipeline that progressively trains the model from scaffolded
reasoning to standalone inference. By leveraging the model's self-synthesis
capability, MindGYM achieves high data efficiency (e.g., +16% gains on
MathVision-Mini with only 400 samples), computational efficiency (reducing both
training and inference costs), and robust generalization across tasks.
Extensive evaluations on seven benchmarks demonstrate superior performance over
strong baselines, with notable improvements (+15.77% win rates) in reasoning
depth and breadth validated via GPT-based scoring. MindGYM underscores the
viability of self-challenging for refining VLM capabilities while minimizing
human intervention and resource demands. Code and data are released to advance
multimodal reasoning research.
</summary>
    <author>
      <name>Zhe Xu</name>
    </author>
    <author>
      <name>Daoyuan Chen</name>
    </author>
    <author>
      <name>Zhenqing Ling</name>
    </author>
    <author>
      <name>Yaliang Li</name>
    </author>
    <author>
      <name>Ying Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
