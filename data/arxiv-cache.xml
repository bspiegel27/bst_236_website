<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-02-04T01:11:43Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-02-04T01:11:43Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>132728</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2602.02491v1</id>
    <title>New explanations and inference for least angle regression</title>
    <updated>2026-02-02T18:59:39Z</updated>
    <link href="https://arxiv.org/abs/2602.02491v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02491v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Efron et al. (2004) introduced least angle regression (LAR) as an algorithm for linear predictions, intended as an alternative to forward selection with connections to penalized regression. However, LAR has remained somewhat of a "black box," where some basic behavioral properties of LAR output are not well understood, including an appropriate termination point for the algorithm. We provide a novel framework for inference with LAR, which also allows LAR to be understood from new perspectives with several newly developed mathematical properties. The LAR algorithm at a data level can viewed as estimating a population counterpart "path" that organizes a response mean along regressor variables which are ordered according to a decreasing series of population "correlation" parameters; such parameters are shown to have meaningful interpretations for explaining variable contributions whereby zero correlations denote unimportant variables. In the output of LAR, estimates of all non-zero population correlations turn out to have independent normal distributions for use in inference, while estimates of zero-valued population correlations have a certain non-normal joint distribution. These properties help to provide a formal rule for stopping the LAR algorithm. While the standard bootstrap for regression can fail for LAR, a modified bootstrap provides a practical and formally justified tool for interpreting the entrance of variables and quantifying uncertainty in estimation. The LAR inference method is studied through simulation and illustrated with data examples.</summary>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:59:39Z</published>
    <arxiv:comment>50 pages, 9 figures</arxiv:comment>
    <arxiv:primary_category term="math.ST"/>
    <author>
      <name>Karl B. Gregory</name>
    </author>
    <author>
      <name>Daniel J. Nordman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02485v1</id>
    <title>DeepDive: Tracing the early quenching pathways of massive quiescent galaxies at $z&gt;3$ from their star-formation histories and chemical abundances</title>
    <updated>2026-02-02T18:58:07Z</updated>
    <link href="https://arxiv.org/abs/2602.02485v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02485v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We investigate the chemical abundances and star-formation histories (SFH) of ten massive ($\mathrm{log}_{10} (M_{\star}/\mathrm{M}_{\odot})&gt;10.5$) quiescent galaxies at $3&lt;z&lt;4$ using deep, medium-resolution spectroscopic data obtained as part of the \textit{JWST DeepDive} Cycle 2 GO program. Our \textit{DeepDive} sample demonstrates early formation and quenching times inferred from spectro-photometric fitting, with most galaxies having formed 50\% of their stellar mass by $z \sim 5$, and quenching by $z \sim 4$, showing good agreement across the various SFH parameterizations explored in this work. Though they differ slightly between SFH parameterizations, the inferred formation timescales for the {\it DeepDive} sample span both rapid ($\lesssim$ 100 Myr) and more extended ($\gtrsim$ 200 Myr) episodes, corresponding to star formation occurring over a few to several dynamical times given their compact sizes and high densities at $z\sim3-4$. On average, massive quiescent galaxies at $3&lt;z&lt;4$ are $α$-enhanced ($\langle [α/\mathrm{Fe}]\rangle$= $0.22^{+0.22}_{-0.17}$), although there is strong diversity ($\sim0.3$ dex in scatter) among individual [$α$/Fe] values. Our results for $α$-enhancement are consistent with lower-redshift studies, implying weak evolution in [$α$/Fe] from $z \sim 4$ to $z\sim 1$. The SFH timescales associated with the low [$α$/Fe] measurements suggest longer formation timescales, potentially pointing to earlier enrichment by Type Ia supernovae, or metals preferentially being removed via outflows driven either by powerful early active galactic nuclei or supernovae. Overall, this work represents the first, statistically representative combined study of the star-formation histories and chemical abundances of massive quiescent galaxies at $z&gt;3$.</summary>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:58:07Z</published>
    <arxiv:comment>22 pages, 10 figures (11 inc. appendix figure), submitted to ApJ. Comments welcome</arxiv:comment>
    <arxiv:primary_category term="astro-ph.GA"/>
    <author>
      <name>Massissilia L. Hamadouche</name>
    </author>
    <author>
      <name>Katherine E. Whitaker</name>
    </author>
    <author>
      <name>Francesco Valentino</name>
    </author>
    <author>
      <name>Jacqueline Antwi-Danso</name>
    </author>
    <author>
      <name>Kei Ito</name>
    </author>
    <author>
      <name>Aliza Beverage</name>
    </author>
    <author>
      <name>Pengpei Zhu</name>
    </author>
    <author>
      <name>Gabriel Brammer</name>
    </author>
    <author>
      <name>Vasily Kokorev</name>
    </author>
    <author>
      <name>Gabriella de Lucia</name>
    </author>
    <author>
      <name>William M. Baker</name>
    </author>
    <author>
      <name>Marion Farcy</name>
    </author>
    <author>
      <name>Anna Gallazzi</name>
    </author>
    <author>
      <name>Steven Gillman</name>
    </author>
    <author>
      <name>Rashmi Gottumukkala</name>
    </author>
    <author>
      <name>Michaela Hirschmann</name>
    </author>
    <author>
      <name>Christian Kragh Jespersen</name>
    </author>
    <author>
      <name>Takumi Kakimoto</name>
    </author>
    <author>
      <name>Minju M. Lee</name>
    </author>
    <author>
      <name>Masato Onodera</name>
    </author>
    <author>
      <name>Rhythm Shimakawa</name>
    </author>
    <author>
      <name>Masayuki Tanaka</name>
    </author>
    <author>
      <name>John R. Weaver</name>
    </author>
    <author>
      <name>Po-Feng Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02482v1</id>
    <title>Expanding the Capabilities of Reinforcement Learning via Text Feedback</title>
    <updated>2026-02-02T18:56:56Z</updated>
    <link href="https://arxiv.org/abs/2602.02482v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02482v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:56:56Z</published>
    <arxiv:comment>43 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yuda Song</name>
    </author>
    <author>
      <name>Lili Chen</name>
    </author>
    <author>
      <name>Fahim Tajwar</name>
    </author>
    <author>
      <name>Remi Munos</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <author>
      <name>J. Andrew Bagnell</name>
    </author>
    <author>
      <name>Aarti Singh</name>
    </author>
    <author>
      <name>Andrea Zanette</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02477v1</id>
    <title>Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability</title>
    <updated>2026-02-02T18:54:54Z</updated>
    <link href="https://arxiv.org/abs/2602.02477v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02477v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:54:54Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Xiao Liang</name>
    </author>
    <author>
      <name>Zhong-Zhi Li</name>
    </author>
    <author>
      <name>Zhenghao Lin</name>
    </author>
    <author>
      <name>Eric Hancheng Jiang</name>
    </author>
    <author>
      <name>Hengyuan Zhang</name>
    </author>
    <author>
      <name>Yelong Shen</name>
    </author>
    <author>
      <name>Kai-Wei Chang</name>
    </author>
    <author>
      <name>Ying Nian Wu</name>
    </author>
    <author>
      <name>Yeyun Gong</name>
    </author>
    <author>
      <name>Weizhu Chen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02470v1</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <updated>2026-02-02T18:50:57Z</updated>
    <link href="https://arxiv.org/abs/2602.02470v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02470v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:50:57Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Xutao Ma</name>
    </author>
    <author>
      <name>Yixiao Huang</name>
    </author>
    <author>
      <name>Hanlin Zhu</name>
    </author>
    <author>
      <name>Somayeh Sojoudi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02467v1</id>
    <title>Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models</title>
    <updated>2026-02-02T18:49:39Z</updated>
    <link href="https://arxiv.org/abs/2602.02467v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02467v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:49:39Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Noam Steinmetz Yalon</name>
    </author>
    <author>
      <name>Ariel Goldstein</name>
    </author>
    <author>
      <name>Liad Mudrik</name>
    </author>
    <author>
      <name>Mor Geva</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02462v1</id>
    <title>Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models</title>
    <updated>2026-02-02T18:48:44Z</updated>
    <link href="https://arxiv.org/abs/2602.02462v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02462v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:48:44Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Gabriele Maraia</name>
    </author>
    <author>
      <name>Marco Valentino</name>
    </author>
    <author>
      <name>Fabio Massimo Zanzotto</name>
    </author>
    <author>
      <name>Leonardo Ranaldi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02459v1</id>
    <title>TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments</title>
    <updated>2026-02-02T18:47:49Z</updated>
    <link href="https://arxiv.org/abs/2602.02459v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02459v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:47:49Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Zhiyu Huang</name>
    </author>
    <author>
      <name>Yun Zhang</name>
    </author>
    <author>
      <name>Johnson Liu</name>
    </author>
    <author>
      <name>Rui Song</name>
    </author>
    <author>
      <name>Chen Tang</name>
    </author>
    <author>
      <name>Jiaqi Ma</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02456v1</id>
    <title>Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning</title>
    <updated>2026-02-02T18:47:02Z</updated>
    <link href="https://arxiv.org/abs/2602.02456v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02456v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:47:02Z</published>
    <arxiv:comment>ICRA 2026, 8 pages</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Albert Gassol Puigjaner</name>
    </author>
    <author>
      <name>Angelos Zacharia</name>
    </author>
    <author>
      <name>Kostas Alexis</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.02453v1</id>
    <title>Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling</title>
    <updated>2026-02-02T18:43:57Z</updated>
    <link href="https://arxiv.org/abs/2602.02453v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.02453v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-02T18:43:57Z</published>
    <arxiv:comment>Working paper</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Andong Chen</name>
    </author>
    <author>
      <name>Wenxin Zhu</name>
    </author>
    <author>
      <name>Qiuyu Ding</name>
    </author>
    <author>
      <name>Yuchen Song</name>
    </author>
    <author>
      <name>Muyun Yang</name>
    </author>
    <author>
      <name>Tiejun Zhao</name>
    </author>
  </entry>
</feed>
