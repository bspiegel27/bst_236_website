<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-29T00:57:20Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-28T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">113446</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.21505v1</id>
    <updated>2025-05-27T17:59:52Z</updated>
    <published>2025-05-27T17:59:52Z</published>
    <title>How does Alignment Enhance LLMs' Multilingual Capabilities? A Language
  Neurons Perspective</title>
    <summary>  Multilingual Alignment is an effective and representative paradigm to enhance
LLMs' multilingual capabilities, which transfers the capabilities from the
high-resource languages to the low-resource languages. Meanwhile, some
researches on language-specific neurons reveal that there are language-specific
neurons that are selectively activated in LLMs when processing different
languages. This provides a new perspective to analyze and understand LLMs'
mechanisms more specifically in multilingual scenarios. In this work, we
propose a new finer-grained neuron identification algorithm, which detects
language neurons~(including language-specific neurons and language-related
neurons) and language-agnostic neurons. Furthermore, based on the
distributional characteristics of different types of neurons, we divide the
LLMs' internal process for multilingual inference into four parts: (1)
multilingual understanding, (2) shared semantic space reasoning, (3)
multilingual output space transformation, and (4) vocabulary space outputting.
Additionally, we systematically analyze the models before and after alignment
with a focus on different types of neurons. We also analyze the phenomenon of
''Spontaneous Multilingual Alignment''. Overall, our work conducts a
comprehensive investigation based on different types of neurons, providing
empirical results and valuable insights for better understanding multilingual
alignment and multilingual capabilities of LLMs.
</summary>
    <author>
      <name>Shimao Zhang</name>
    </author>
    <author>
      <name>Zhejian Lai</name>
    </author>
    <author>
      <name>Xiang Liu</name>
    </author>
    <author>
      <name>Shuaijie She</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <author>
      <name>Yeyun Gong</name>
    </author>
    <author>
      <name>Shujian Huang</name>
    </author>
    <author>
      <name>Jiajun Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21499v1</id>
    <updated>2025-05-27T17:59:05Z</updated>
    <published>2025-05-27T17:59:05Z</published>
    <title>AdInject: Real-World Black-Box Attacks on Web Agents via Advertising
  Delivery</title>
    <summary>  Vision-Language Model (VLM) based Web Agents represent a significant step
towards automating complex tasks by simulating human-like interaction with
websites. However, their deployment in uncontrolled web environments introduces
significant security vulnerabilities. Existing research on adversarial
environmental injection attacks often relies on unrealistic assumptions, such
as direct HTML manipulation, knowledge of user intent, or access to agent model
parameters, limiting their practical applicability. In this paper, we propose
AdInject, a novel and real-world black-box attack method that leverages the
internet advertising delivery to inject malicious content into the Web Agent's
environment. AdInject operates under a significantly more realistic threat
model than prior work, assuming a black-box agent, static malicious content
constraints, and no specific knowledge of user intent. AdInject includes
strategies for designing malicious ad content aimed at misleading agents into
clicking, and a VLM-based ad content optimization technique that infers
potential user intents from the target website's context and integrates these
intents into the ad content to make it appear more relevant or critical to the
agent's task, thus enhancing attack effectiveness. Experimental evaluations
demonstrate the effectiveness of AdInject, attack success rates exceeding 60%
in most scenarios and approaching 100% in certain cases. This strongly
demonstrates that prevalent advertising delivery constitutes a potent and
real-world vector for environment injection attacks against Web Agents. This
work highlights a critical vulnerability in Web Agent security arising from
real-world environment manipulation channels, underscoring the urgent need for
developing robust defense mechanisms against such threats. Our code is
available at https://github.com/NicerWang/AdInject.
</summary>
    <author>
      <name>Haowei Wang</name>
    </author>
    <author>
      <name>Junjie Wang</name>
    </author>
    <author>
      <name>Xiaojun Jia</name>
    </author>
    <author>
      <name>Rupeng Zhang</name>
    </author>
    <author>
      <name>Mingyang Li</name>
    </author>
    <author>
      <name>Zhe Liu</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Qing Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21495v1</id>
    <updated>2025-05-27T17:58:00Z</updated>
    <published>2025-05-27T17:58:00Z</published>
    <title>CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an
  open-source device for Multimodal robot Perception</title>
    <summary>  Robust robot manipulation in unstructured environments often requires
understanding object properties that extend beyond geometry, such as material
or compliance-properties that can be challenging to infer using vision alone.
Multimodal haptic sensing provides a promising avenue for inferring such
properties, yet progress has been constrained by the lack of large, diverse,
and realistic haptic datasets. In this work, we introduce the CLAMP device, a
low-cost (&lt;\$200) sensorized reacher-grabber designed to collect large-scale,
in-the-wild multimodal haptic data from non-expert users in everyday settings.
We deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP
dataset, the largest open-source multimodal haptic dataset to date, comprising
12.3 million datapoints across 5357 household objects. Using this dataset, we
train a haptic encoder that can infer material and compliance object properties
from multimodal haptic data. We leverage this encoder to create the CLAMP
model, a visuo-haptic perception model for material recognition that
generalizes to novel objects and three robot embodiments with minimal
finetuning. We also demonstrate the effectiveness of our model in three
real-world robot manipulation tasks: sorting recyclable and non-recyclable
waste, retrieving objects from a cluttered bag, and distinguishing overripe
from ripe bananas. Our results show that large-scale, in-the-wild haptic data
collection can unlock new capabilities for generalizable robot manipulation.
Website: https://emprise.cs.cornell.edu/clamp/
</summary>
    <author>
      <name>Pranav N. Thakkar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Anjelica</arxiv:affiliation>
    </author>
    <author>
      <name>Shubhangi Sinha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Anjelica</arxiv:affiliation>
    </author>
    <author>
      <name>Karan Baijal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Anjelica</arxiv:affiliation>
    </author>
    <author>
      <name> Yuhan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Anjelica</arxiv:affiliation>
    </author>
    <author>
      <name> Bian</name>
    </author>
    <author>
      <name>Leah Lackey</name>
    </author>
    <author>
      <name>Ben Dodson</name>
    </author>
    <author>
      <name>Heisen Kong</name>
    </author>
    <author>
      <name>Jueun Kwon</name>
    </author>
    <author>
      <name>Amber Li</name>
    </author>
    <author>
      <name>Yifei Hu</name>
    </author>
    <author>
      <name>Alexios Rekoutis</name>
    </author>
    <author>
      <name>Tom Silver</name>
    </author>
    <author>
      <name>Tapomayukh Bhattacharjee</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21492v1</id>
    <updated>2025-05-27T17:56:11Z</updated>
    <published>2025-05-27T17:56:11Z</published>
    <title>Dust removal timescale in galaxies across cosmic time</title>
    <summary>  Understanding the evolution of dust in galaxies is crucial because it affects
the dynamics and cooling of gas, star formation, and chemical evolution. Recent
work on dust removal in galaxies indicates timescales of gigayears, with old
stellar populations and AGNs as the primary drivers of this process. However,
most statistically significant studies are focused on low redshifts $z &lt; 0.4$.
Here, we determine the dust removal timescale in galaxies over a wide range of
redshifts, up to $z \sim 5$. We use publicly available catalogue data of
infrared-selected galaxies, observed by \textit{Herschel}. Using the inferred
dust masses, stellar masses, and stellar ages, we calculate the dust removal
timescale in a sample of more than 120,000 galaxies. We find that, with
increasing redshift, the dust removal timescale decreases from 1.8 Gyr at
redshift $z \sim 0.05$ to less than 500\,Myr at $z &gt; 3$. Galaxies at higher
redshifts undergo more efficient dust removal than galaxies at lower redshift,
likely driven by AGN activity, supernova shocks, and astration. These findings
indicate that dust removal evolves over cosmic time, reflecting the changing
mechanisms regulating dust content of galaxies as the Universe evolves.
</summary>
    <author>
      <name>Aleksandra Leśniewska</name>
    </author>
    <author>
      <name>Jens Hjorth</name>
    </author>
    <author>
      <name>Christa Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to A&amp;A, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.21492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21482v1</id>
    <updated>2025-05-27T17:52:42Z</updated>
    <published>2025-05-27T17:52:42Z</published>
    <title>Tissue-specific predictive performance: A unified estimation and
  inference framework for multi-category screening tests</title>
    <summary>  Multi-Cancer Early Detection (MCED) testing with tissue localization aims to
detect and identify multiple cancer types from a single blood sample. Such
tests have the potential to aid clinical decisions and significantly improve
health outcomes. Despite this promise, MCED testing has not yet achieved
regulatory approval, reimbursement or broad clinical adoption. One major reason
for this shortcoming is uncertainty about test performance resulting from the
reporting of clinically obtuse metrics. Traditionally, MCED tests report
aggregate measures of test performance, disregarding cancer type, that obscure
biological variability and underlying differences in the test's behavior,
limiting insight into true effectiveness. Clinically informative evaluation of
an MCED test's performance requires metrics that are specific to cancer types.
In the context of a case-control sampling design, this paper derives analytical
methods that estimate cancer-specific intrinsic accuracy, tissue localization
readout-specific predictive value and the marginal test classification
distribution, each with corresponding confidence interval formulae. A
simulation study is presented that evaluates performance of the proposed
methodology and provides guidance for implementation. An application to a
published MCED test dataset is given. These statistical approaches allow for
estimation and inference for the pointed metric of an MCED test that allow its
evaluation to support a potential role in early cancer detection. This
framework enables more precise clinical decision-making, supports optimized
trial designs across classical, digital, AI-driven, and hybrid stratified
diagnostic screening platforms, and facilitates informed healthcare decisions
by clinicians, policymakers, regulators, scientists, and patients.
</summary>
    <author>
      <name>A. Gregory DiRienzo</name>
    </author>
    <author>
      <name>Elie Massaad</name>
    </author>
    <author>
      <name>Hutan Ashrafian</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21479v1</id>
    <updated>2025-05-27T17:51:18Z</updated>
    <published>2025-05-27T17:51:18Z</published>
    <title>Are Language Models Consequentialist or Deontological Moral Reasoners?</title>
    <summary>  As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .
</summary>
    <author>
      <name>Keenan Samway</name>
    </author>
    <author>
      <name>Max Kleiman-Weiner</name>
    </author>
    <author>
      <name>David Guzman Piedrahita</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <author>
      <name>Zhijing Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21472v1</id>
    <updated>2025-05-27T17:45:21Z</updated>
    <published>2025-05-27T17:45:21Z</published>
    <title>Mitigating Hallucination in Large Vision-Language Models via Adaptive
  Attention Calibration</title>
    <summary>  Large vision-language models (LVLMs) achieve impressive performance on
multimodal tasks but often suffer from hallucination, and confidently describe
objects or attributes not present in the image. Current inference-time
interventions, while training-free, struggle to maintain accuracy in open-ended
and long-form generation scenarios. We introduce the Confidence-Aware Attention
Calibration (CAAC) framework to address this challenge by targeting two key
biases: spatial perception bias, which distributes attention disproportionately
across image tokens, and modality bias, which shifts focus from visual to
textual inputs over time. CAAC employs a two-step approach: Visual-Token
Calibration (VTC) to balance attention across visual tokens, and Adaptive
Attention Re-Scaling (AAR) to reinforce visual grounding based on the model's
confidence. This confidence-driven adjustment ensures consistent visual
alignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks
demonstrate that CAAC outperforms baselines, particularly in long-form
generations, effectively reducing hallucination.
</summary>
    <author>
      <name>Mehrdad Fazli</name>
    </author>
    <author>
      <name>Bowen Wei</name>
    </author>
    <author>
      <name>Ziwei Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21473v1</id>
    <updated>2025-05-27T17:45:21Z</updated>
    <published>2025-05-27T17:45:21Z</published>
    <title>DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via
  Next-Detail Prediction</title>
    <summary>  This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image
generation method that models images through a novel next-detail prediction
strategy. By learning a resolution-aware token sequence supervised with
progressively degraded images, DetailFlow enables the generation process to
start from the global structure and incrementally refine details. This
coarse-to-fine 1D token sequence aligns well with the autoregressive inference
mechanism, providing a more natural and efficient way for the AR model to
generate complex visual content. Our compact 1D AR model achieves high-quality
image synthesis with significantly fewer tokens than previous approaches, i.e.
VAR/VQGAN. We further propose a parallel inference mechanism with
self-correction that accelerates generation speed by approximately 8x while
reducing accumulation sampling error inherent in teacher-forcing supervision.
On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128
tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require
680 tokens in their AR models. Moreover, due to the significantly reduced token
count and parallel inference mechanism, our method runs nearly 2x faster
inference speed compared to VAR and FlexVAR. Extensive experimental results
demonstrate DetailFlow's superior generation quality and efficiency compared to
existing state-of-the-art methods.
</summary>
    <author>
      <name>Yiheng Liu</name>
    </author>
    <author>
      <name>Liao Qu</name>
    </author>
    <author>
      <name>Huichao Zhang</name>
    </author>
    <author>
      <name>Xu Wang</name>
    </author>
    <author>
      <name>Yi Jiang</name>
    </author>
    <author>
      <name>Yiming Gao</name>
    </author>
    <author>
      <name>Hu Ye</name>
    </author>
    <author>
      <name>Xian Li</name>
    </author>
    <author>
      <name>Shuai Wang</name>
    </author>
    <author>
      <name>Daniel K. Du</name>
    </author>
    <author>
      <name>Shu Cheng</name>
    </author>
    <author>
      <name>Zehuan Yuan</name>
    </author>
    <author>
      <name>Xinglong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21471v1</id>
    <updated>2025-05-27T17:45:04Z</updated>
    <published>2025-05-27T17:45:04Z</published>
    <title>Scaling External Knowledge Input Beyond Context Windows of LLMs via
  Multi-Agent Collaboration</title>
    <summary>  With the rapid advancement of post-training techniques for reasoning and
information seeking, large language models (LLMs) can incorporate a large
quantity of retrieved knowledge to solve complex tasks. However, the limited
context window of LLMs obstructs scaling the amount of external knowledge
input, prohibiting further improvement, especially for tasks requiring
significant amount of external knowledge. Existing context window extension
methods inevitably cause information loss. LLM-based multi-agent methods emerge
as a new paradigm to handle massive input in a distributional manner, where we
identify two core bottlenecks in existing knowledge synchronization and
reasoning processes. In this work, we develop a multi-agent framework,
$\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability
in inference-time knowledge integration without longer-context training.
Benchmarked with our enhanced multi-hop question answering test,
$\textbf{$\boldsymbol{\infty}$Bench+}$, and other public test sets including
long survey generation, ExtAgents significantly enhances the performance over
existing non-training methods with the same amount of external knowledge input,
regardless of whether it falls $\textit{within or exceeds the context window}$.
Moreover, the method maintains high efficiency due to high parallelism. Further
study in the coordination of LLM agents on increasing external knowledge input
could benefit real-world applications.
</summary>
    <author>
      <name>Zijun Liu</name>
    </author>
    <author>
      <name>Zhennan Wan</name>
    </author>
    <author>
      <name>Peng Li</name>
    </author>
    <author>
      <name>Ming Yan</name>
    </author>
    <author>
      <name>Ji Zhang</name>
    </author>
    <author>
      <name>Fei Huang</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 9 figures. Code and data are available at
  https://github.com/THUNLP-MT/ExtAgents</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.21471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21469v1</id>
    <updated>2025-05-27T17:42:22Z</updated>
    <published>2025-05-27T17:42:22Z</published>
    <title>PropMolFlow: Property-guided Molecule Generation with Geometry-Complete
  Flow Matching</title>
    <summary>  Molecule generation is advancing rapidly in chemical discovery and drug
design. Flow matching methods have recently set the state of the art (SOTA) in
unconditional molecule generation, surpassing score-based diffusion models.
However, diffusion models still lead in property-guided generation. In this
work, we introduce PropMolFlow, a novel approach for property-guided molecule
generation based on geometry-complete SE(3)-equivariant flow matching.
Integrating five different property embedding methods with a Gaussian expansion
of scalar properties, PropMolFlow outperforms previous SOTA diffusion models in
conditional molecule generation across various properties while preserving the
stability and validity of the generated molecules, consistent with its
unconditional counterpart. Additionally, it enables faster inference with
significantly fewer time steps compared to baseline models. We highlight the
importance of validating the properties of generated molecules through DFT
calculations performed at the same level of theory as the training data.
Specifically, our analysis identifies properties that require DFT validation
and others where a pretrained SE(3) geometric vector perceptron regressors
provide sufficiently accurate predictions on generated molecules. Furthermore,
we introduce a new property metric designed to assess the model's ability to
propose molecules with underrepresented property values, assessing its capacity
for out-of-distribution generalization. Our findings reveal shortcomings in
existing structural metrics, which mistakenly validate open-shell molecules or
molecules with invalid valence-charge configurations, underscoring the need for
improved evaluation frameworks. Overall, this work paves the way for developing
targeted property-guided generation methods, enhancing the design of molecular
generative models for diverse applications.
</summary>
    <author>
      <name>Cheng Zeng</name>
    </author>
    <author>
      <name>Jirui Jin</name>
    </author>
    <author>
      <name>George Karypis</name>
    </author>
    <author>
      <name>Mark Transtrum</name>
    </author>
    <author>
      <name>Ellad B. Tadmor</name>
    </author>
    <author>
      <name>Richard G. Hennig</name>
    </author>
    <author>
      <name>Adrian Roitberg</name>
    </author>
    <author>
      <name>Stefano Martiniani</name>
    </author>
    <author>
      <name>Mingjie Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
