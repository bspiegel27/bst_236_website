<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-28T00:57:11Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">113304</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.20157v1</id>
    <updated>2025-05-26T15:57:35Z</updated>
    <published>2025-05-26T15:57:35Z</published>
    <title>Gaussian Process Methods for Covariate-Based Intensity Estimation</title>
    <summary>  We study nonparametric Bayesian inference for the intensity function of a
covariate-driven point process. We extend recent results from the literature,
showing that a wide class of Gaussian priors, combined with flexible link
functions, achieve minimax optimal posterior contraction rates. Our result
includes widespread prior choices such as the popular Mat\'ern processes, with
the standard exponential (and sigmoid) link, and implies that the resulting
methodologically attractive procedures optimally solve the statistical problem
at hand, in the increasing domain asymptotics and under the common assumption
in spatial statistics that the covariates are stationary and ergodic.
</summary>
    <author>
      <name>Patric Dolmeta</name>
    </author>
    <author>
      <name>Matteo Giordano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, to appear in New Trends in Functional Statistics and Related
  Fields (IWFOS 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20156v1</id>
    <updated>2025-05-26T15:57:27Z</updated>
    <published>2025-05-26T15:57:27Z</published>
    <title>HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for
  Multiple Characters</title>
    <summary>  Recent years have witnessed significant progress in audio-driven human
animation. However, critical challenges remain in (i) generating highly dynamic
videos while preserving character consistency, (ii) achieving precise emotion
alignment between characters and audio, and (iii) enabling multi-character
audio-driven animation. To address these challenges, we propose
HunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model
capable of simultaneously generating dynamic, emotion-controllable, and
multi-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces
three key innovations: (i) A character image injection module is designed to
replace the conventional addition-based character conditioning scheme,
eliminating the inherent condition mismatch between training and inference.
This ensures the dynamic motion and strong character consistency; (ii) An Audio
Emotion Module (AEM) is introduced to extract and transfer the emotional cues
from an emotion reference image to the target generated video, enabling
fine-grained and accurate emotion style control; (iii) A Face-Aware Audio
Adapter (FAA) is proposed to isolate the audio-driven character with
latent-level face mask, enabling independent audio injection via
cross-attention for multi-character scenarios. These innovations empower
HunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets
and a newly proposed wild dataset, generating realistic avatars in dynamic,
immersive scenarios.
</summary>
    <author>
      <name>Yi Chen</name>
    </author>
    <author>
      <name>Sen Liang</name>
    </author>
    <author>
      <name>Zixiang Zhou</name>
    </author>
    <author>
      <name>Ziyao Huang</name>
    </author>
    <author>
      <name>Yifeng Ma</name>
    </author>
    <author>
      <name>Junshu Tang</name>
    </author>
    <author>
      <name>Qin Lin</name>
    </author>
    <author>
      <name>Yuan Zhou</name>
    </author>
    <author>
      <name>Qinglin Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.20156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20155v1</id>
    <updated>2025-05-26T15:57:08Z</updated>
    <published>2025-05-26T15:57:08Z</published>
    <title>Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs</title>
    <summary>  Large Language Models (LLMs) deliver state-of-the-art capabilities across
numerous tasks, but their immense size and inference costs pose significant
computational challenges for practical deployment. While structured pruning
offers a promising avenue for model compression, existing methods often
struggle with the detrimental effects of aggressive, simultaneous width and
depth reductions, leading to substantial performance degradation. This paper
argues that a critical, often overlooked, aspect in making such aggressive
joint pruning viable is the strategic re-initialization and adjustment of
remaining weights to improve the model post-pruning training accuracies. We
introduce Pangu Light, a framework for LLM acceleration centered around
structured pruning coupled with novel weight re-initialization techniques
designed to address this ``missing piece''. Our framework systematically
targets multiple axes, including model width, depth, attention heads, and
RMSNorm, with its effectiveness rooted in novel re-initialization methods like
Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)
that mitigate performance drops by providing the network a better training
starting point. Further enhancing efficiency, Pangu Light incorporates
specialized optimizations such as absorbing Post-RMSNorm computations and
tailors its strategies to Ascend NPU characteristics. The Pangu Light models
consistently exhibit a superior accuracy-efficiency trade-off, outperforming
prominent baseline pruning methods like Nemotron and established LLMs like
Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average
score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and
2225 tokens/s.
</summary>
    <author>
      <name>Hanting Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Jiarui Qin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Jialong Guo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Tao Yuan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Yichun Yin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Huiling Zhen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Yasheng Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Jinpeng Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Xiaojun Meng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Meng Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Rongju Ruan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Zheyuan Bai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Yehui Tang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Can Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Xinghao Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Fisher Yu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Ruiming Tang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <author>
      <name>Yunhe Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">and Other Contributors</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2505.20155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20153v1</id>
    <updated>2025-05-26T15:55:33Z</updated>
    <published>2025-05-26T15:55:33Z</published>
    <title>Simple, Efficient Entropy Estimation using Harmonic Numbers</title>
    <summary>  The estimation of entropy, a fundamental measure of uncertainty, is central
to diverse data applications. For discrete random variables, however, efficient
entropy estimation presents challenges, particularly when the cardinality of
the support set is large relative to the available sample size. This is
because, without other assumptions, there may be insufficient data to
adequately characterize a probability mass function. Further complications stem
from the dependence among transformations of empirical frequencies within the
sample.
  This paper demonstrates that a simple entropy estimator based on the harmonic
number function achieves asymptotic efficiency for discrete random variables
with tail probabilities satisfying $p_j =o(j^{-2})$ as $j\rightarrow\infty$.
This result renders statistical inference newly feasible for a broad class of
distributions. Further, the proposed estimator has superior mean squared error
bounds compared to the classical plug-in estimator, while retaining its
computational simplicity, offering practical and theoretical advantages.
</summary>
    <author>
      <name>Octavio César Mesner</name>
    </author>
    <link href="http://arxiv.org/abs/2505.20153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20151v1</id>
    <updated>2025-05-26T15:54:19Z</updated>
    <published>2025-05-26T15:54:19Z</published>
    <title>The evolving categories multinomial distribution: introduction with
  applications to movement ecology and vote transfer</title>
    <summary>  We introduce the evolving categories multinomial (ECM) distribution for
multivariate count data taken over time. This distribution models the counts of
individuals following iid stochastic dynamics among categories, with the number
and identity of the categories also evolving over time. We specify the one-time
and two-times marginal distributions of the counts and the first and second
order moments. When the total number of individuals is unknown, placing a
Poisson prior on it yields a new distribution (ECM-Poisson), whose main
properties we also describe. Since likelihoods are intractable or impractical,
we propose two estimating functions for parameter estimation: a Gaussian
pseudo-likelihood and a pairwise composite likelihood. We show two application
scenarios: the inference of movement parameters of animals moving continuously
in space-time with irregular survey regions, and the inference of vote transfer
in two-rounds elections. We give three illustrations: a simulation study with
Ornstein-Uhlenbeck moving individuals, paying special attention to the
autocorrelation parameter; the inference of movement and behavior parameters of
lesser prairie-chickens; and the estimation of vote transfer in the 2021
Chilean presidential election.
</summary>
    <author>
      <name>Ricardo Carrizo Vergara</name>
    </author>
    <author>
      <name>Marc Kéry</name>
    </author>
    <author>
      <name>Trevor Hefley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Main body: 18 pages. Appendices and references: 14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H05, 62M10, 62P12, 62P25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20147v1</id>
    <updated>2025-05-26T15:46:53Z</updated>
    <published>2025-05-26T15:46:53Z</published>
    <title>FUDOKI: Discrete Flow-based Unified Understanding and Generation via
  Kinetic-Optimal Velocities</title>
    <summary>  The rapid progress of large language models (LLMs) has catalyzed the
emergence of multimodal large language models (MLLMs) that unify visual
understanding and image generation within a single framework. However, most
existing MLLMs rely on autoregressive (AR) architectures, which impose inherent
limitations on future development, such as the raster-scan order in image
generation and restricted reasoning abilities in causal context modeling. In
this work, we challenge the dominance of AR-based approaches by introducing
FUDOKI, a unified multimodal model purely based on discrete flow matching, as
an alternative to conventional AR paradigms. By leveraging metric-induced
probability paths with kinetic optimal velocities, our framework goes beyond
the previous masking-based corruption process, enabling iterative refinement
with self-correction capability and richer bidirectional context integration
during generation. To mitigate the high cost of training from scratch, we
initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to
the discrete flow matching paradigm. Experimental results show that FUDOKI
achieves performance comparable to state-of-the-art AR-based MLLMs across both
visual understanding and image generation tasks, highlighting its potential as
a foundation for next-generation unified multimodal models. Furthermore, we
show that applying test-time scaling techniques to FUDOKI yields significant
performance gains, further underscoring its promise for future enhancement
through reinforcement learning.
</summary>
    <author>
      <name>Jin Wang</name>
    </author>
    <author>
      <name>Yao Lai</name>
    </author>
    <author>
      <name>Aoxue Li</name>
    </author>
    <author>
      <name>Shifeng Zhang</name>
    </author>
    <author>
      <name>Jiacheng Sun</name>
    </author>
    <author>
      <name>Ning Kang</name>
    </author>
    <author>
      <name>Chengyue Wu</name>
    </author>
    <author>
      <name>Zhenguo Li</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20142v1</id>
    <updated>2025-05-26T15:44:26Z</updated>
    <published>2025-05-26T15:44:26Z</published>
    <title>Model Stitching by Functional Latent Alignment</title>
    <summary>  Evaluating functional similarity involves quantifying the degree to which
independently trained neural networks learn functionally similar
representations. Reliably inferring the functional similarity of these networks
remains an open problem with far-reaching implications for AI. Model stitching
has emerged as a promising paradigm, where an optimal affine transformation
aligns two models to solve a task, with the stitched model serving as a proxy
for functional similarity. In this work, we draw inspiration from the knowledge
distillation literature and propose Functional Latent Alignment (FuLA) as a
novel optimality condition for model stitching. We revisit previously explored
functional similarity testbeds and introduce a new one, based on which FuLA
emerges as an overall more reliable method of functional similarity.
Specifically, our experiments in (a) adversarial training, (b) shortcut
training and, (c) cross-layer stitching, reveal that FuLA is less prone to
artifacts tied to training on task cues while achieving non-trivial alignments
that are missed by stitch-level matching.
</summary>
    <author>
      <name>Ioannis Athanasiadis</name>
    </author>
    <author>
      <name>Anmar Karmush</name>
    </author>
    <author>
      <name>Michael Felsberg</name>
    </author>
    <link href="http://arxiv.org/abs/2505.20142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20136v1</id>
    <updated>2025-05-26T15:39:11Z</updated>
    <published>2025-05-26T15:39:11Z</published>
    <title>Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge
  Proofs</title>
    <summary>  As Artificial Intelligence (AI) systems, particularly those based on machine
learning (ML), become integral to high-stakes applications, their probabilistic
and opaque nature poses significant challenges to traditional verification and
validation methods. These challenges are exacerbated in regulated sectors
requiring tamper-proof, auditable evidence, as highlighted by apposite legal
frameworks, e.g., the EU AI Act. Conversely, Zero-Knowledge Proofs (ZKPs) offer
a cryptographic solution that enables provers to demonstrate, through verified
computations, adherence to set requirements without revealing sensitive model
details or data. Through a systematic survey of ZKP protocols, we identify five
key properties (non-interactivity, transparent setup, standard representations,
succinctness, and post-quantum security) critical for their application in AI
validation and verification pipelines. Subsequently, we perform a follow-up
systematic survey analyzing ZKP-enhanced ML applications across an adaptation
of the Team Data Science Process (TDSP) model (Data &amp; Preprocessing, Training &amp;
Offline Metrics, Inference, and Online Metrics), detailing verification
objectives, ML models, and adopted protocols. Our findings indicate that
current research on ZKP-Enhanced ML primarily focuses on inference
verification, while the data preprocessing and training stages remain
underexplored. Most notably, our analysis identifies a significant convergence
within the research domain toward the development of a unified Zero-Knowledge
Machine Learning Operations (ZKMLOps) framework. This emerging framework
leverages ZKPs to provide robust cryptographic guarantees of correctness,
integrity, and privacy, thereby promoting enhanced accountability,
transparency, and compliance with Trustworthy AI principles.
</summary>
    <author>
      <name>Filippo Scaramuzza</name>
    </author>
    <author>
      <name>Giovanni Quattrocchi</name>
    </author>
    <author>
      <name>Damian A. Tamburri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20130v1</id>
    <updated>2025-05-26T15:29:01Z</updated>
    <published>2025-05-26T15:29:01Z</published>
    <title>Balancing Interference and Correlation in Spatial Experimental Designs:
  A Causal Graph Cut Approach</title>
    <summary>  This paper focuses on the design of spatial experiments to optimize the
amount of information derived from the experimental data and enhance the
accuracy of the resulting causal effect estimator. We propose a surrogate
function for the mean squared error (MSE) of the estimator, which facilitates
the use of classical graph cut algorithms to learn the optimal design. Our
proposal offers three key advances: (1) it accommodates moderate to large
spatial interference effects; (2) it adapts to different spatial covariance
functions; (3) it is computationally efficient. Theoretical results and
numerical experiments based on synthetic environments and a dispatch simulator
that models a city-scale ridesharing market, further validate the effectiveness
of our design. A python implementation of our method is available at
https://github.com/Mamba413/CausalGraphCut.
</summary>
    <author>
      <name>Zhu Jin</name>
    </author>
    <author>
      <name>Li Jingyi</name>
    </author>
    <author>
      <name>Zhou Hongyi</name>
    </author>
    <author>
      <name>Lin Yinan</name>
    </author>
    <author>
      <name>Lin Zhenhua</name>
    </author>
    <author>
      <name>Shi Chengchun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICML2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20127v1</id>
    <updated>2025-05-26T15:26:07Z</updated>
    <published>2025-05-26T15:26:07Z</published>
    <title>Agentic AI Process Observability: Discovering Behavioral Variability</title>
    <summary>  AI agents that leverage Large Language Models (LLMs) are increasingly
becoming core building blocks of modern software systems. A wide range of
frameworks is now available to support the specification of such applications.
These frameworks enable the definition of agent setups using natural language
prompting, which specifies the roles, goals, and tools assigned to the various
agents involved. Within such setups, agent behavior is non-deterministic for
any given input, highlighting the critical need for robust debugging and
observability tools. In this work, we explore the use of process and causal
discovery applied to agent execution trajectories as a means of enhancing
developer observability. This approach aids in monitoring and understanding the
emergent variability in agent behavior. Additionally, we complement this with
LLM-based static analysis techniques to distinguish between intended and
unintended behavioral variability. We argue that such instrumentation is
essential for giving developers greater control over evolving specifications
and for identifying aspects of functionality that may require more precise and
explicit definitions.
</summary>
    <author>
      <name>Fabiana Fournier</name>
    </author>
    <author>
      <name>Lior Limonad</name>
    </author>
    <author>
      <name>Yuval David</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
