<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-05T00:57:21Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-04T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">114132</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.03149v1</id>
    <updated>2025-06-03T17:59:47Z</updated>
    <published>2025-06-03T17:59:47Z</published>
    <title>Causal Estimation of Tokenisation Bias</title>
    <summary>  Modern language models are typically trained over subword sequences, but
ultimately define probabilities over character-strings. Ideally, the choice of
the tokeniser -- which maps character-strings to subwords -- should not affect
the probability assigned to the underlying character-string; in practice, it
does. We define this mismatch as tokenisation bias. In this work, we quantify
one particular type of tokenisation bias: the effect of including or not a
subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the
probability a trained model assigns to the corresponding characters (i.e.,
\textit{``hello''}). Estimating this effect is challenging because each model
is trained with only one tokeniser. We address this by framing tokenisation
bias as a causal effect and estimating it using the regression discontinuity
design. Specifically, we exploit the fact that tokenisation algorithms rank
subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an
arbitrary cutoff point. As such, we can estimate a causal effect by comparing
similar subwords around this cutoff. Experimentally, we find that tokenisation
consistently affects models' outputs across scales, vocabularies, and
tokenisers. Notably, a subword's presence in a small model's vocabulary may
increase its characters' probability by up to 17 times, highlighting
tokenisation as a key design choice in language modelling.
</summary>
    <author>
      <name>Pietro Lesci</name>
    </author>
    <author>
      <name>Clara Meister</name>
    </author>
    <author>
      <name>Thomas Hofmann</name>
    </author>
    <author>
      <name>Andreas Vlachos</name>
    </author>
    <author>
      <name>Tiago Pimentel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ACL 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.03149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03147v2</id>
    <updated>2025-06-04T14:45:58Z</updated>
    <published>2025-06-03T17:59:33Z</published>
    <title>UniWorld: High-Resolution Semantic Encoders for Unified Visual
  Understanding and Generation</title>
    <summary>  Although existing unified models achieve strong performance in
vision-language understanding and text-to-image generation, they remain limited
in addressing image perception and manipulation -- capabilities increasingly
demanded in practical applications. Recently, OpenAI introduced the powerful
GPT-4o-Image model, which showcases advanced capabilities in comprehensive
image perception and manipulation, sparking widespread interest. Through
carefully designed experiments, we observe that GPT-4o-Image likely relies on
semantic encoders rather than VAEs for feature extraction, despite VAEs being
commonly regarded as crucial for image manipulation tasks. Inspired by this
insight, we propose UniWorld, a unified generative framework built upon
semantic features extracted from powerful multimodal large language models and
contrastive semantic encoders. Using only 2.7M training data, UniWorld achieves
impressive performance across diverse tasks, including image understanding,
generation, manipulation, and perception. We fully open-source the UniWorld
framework, including model weights, training and evaluation scripts, and
datasets to promote reproducibility and further research.
</summary>
    <author>
      <name>Bin Lin</name>
    </author>
    <author>
      <name>Zongjian Li</name>
    </author>
    <author>
      <name>Xinhua Cheng</name>
    </author>
    <author>
      <name>Yuwei Niu</name>
    </author>
    <author>
      <name>Yang Ye</name>
    </author>
    <author>
      <name>Xianyi He</name>
    </author>
    <author>
      <name>Shenghai Yuan</name>
    </author>
    <author>
      <name>Wangbo Yu</name>
    </author>
    <author>
      <name>Shaodong Wang</name>
    </author>
    <author>
      <name>Yunyang Ge</name>
    </author>
    <author>
      <name>Yatian Pang</name>
    </author>
    <author>
      <name>Li Yuan</name>
    </author>
    <link href="http://arxiv.org/abs/2506.03147v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03147v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03136v1</id>
    <updated>2025-06-03T17:58:42Z</updated>
    <published>2025-06-03T17:58:42Z</published>
    <title>Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning</title>
    <summary>  We propose CURE, a novel reinforcement learning framework with a dedicated
reward design that co-evolves coding and unit test generation capabilities
based on their interaction outcomes, without any ground-truth code as
supervision. This approach enables flexible and scalable training and allows
the unit tester to learn directly from the coder's mistakes. Our derived
ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and
Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,
outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They
naturally extend to downstream tasks such as test-time scaling and agentic
coding-achieving a 8.1% improvement over the base model. For the long-CoT
model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while
achieving 64.8% inference efficiency in unit test generation. Notably, we also
find that our model can serve as an effective reward model for reinforcement
learning on base models. Project: https://github.com/Gen-Verse/CURE
</summary>
    <author>
      <name>Yinjie Wang</name>
    </author>
    <author>
      <name>Ling Yang</name>
    </author>
    <author>
      <name>Ye Tian</name>
    </author>
    <author>
      <name>Ke Shen</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project: https://github.com/Gen-Verse/CURE</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.03136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03111v1</id>
    <updated>2025-06-03T17:40:39Z</updated>
    <published>2025-06-03T17:40:39Z</published>
    <title>Rectified Flows for Fast Multiscale Fluid Flow Modeling</title>
    <summary>  The statistical modeling of fluid flows is very challenging due to their
multiscale dynamics and extreme sensitivity to initial conditions. While
recently proposed conditional diffusion models achieve high fidelity, they
typically require hundreds of stochastic sampling steps at inference. We
introduce a rectified flow framework that learns a time-dependent velocity
field, transporting input to output distributions along nearly straight
trajectories. By casting sampling as solving an ordinary differential equation
(ODE) along this straighter flow field, our method makes each integration step
much more effective, using as few as eight steps versus (more than) 128 steps
in standard score-based diffusion, without sacrificing predictive fidelity.
Experiments on challenging multiscale flow benchmarks show that rectified flows
recover the same posterior distributions as diffusion models, preserve
fine-scale features that MSE-trained baselines miss, and deliver
high-resolution samples in a fraction of inference time.
</summary>
    <author>
      <name>Victor Armegioiu</name>
    </author>
    <author>
      <name>Yannick Ramic</name>
    </author>
    <author>
      <name>Siddhartha Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/2506.03111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03104v1</id>
    <updated>2025-06-03T17:37:15Z</updated>
    <published>2025-06-03T17:37:15Z</published>
    <title>Two-Phase Treatment with Noncompliance: Identifying the Cumulative
  Average Treatment Effect via Multisite Instrumental Variables</title>
    <summary>  In evaluating a multi-phase intervention, the cumulative average treatment
effect (ATE) is often the causal estimand of key interest. Yet some individuals
who do not respond well to the Phase-I treatment may subsequently display
noncompliant behaviors. However, noncompliance tends to be constrained by the
stochastic availability of slots under the alternative treatment condition in
Phase II, which makes the notion of the "complier average treatment effect"
problematic. Moreover, the Phase-I treatment is expected to affect an
individual's potential outcomes through additional pathways that violate the
exclusion restriction. Extending an instrumental variable (IV) strategy for
multisite trials, we clarify conditions for identifying the cumulative ATE of a
two-phase treatment by employing the random assignment of the Phase-I treatment
as the IV. Our strategy relaxes the exclusion restriction and the sequential
ignorability in their conventional forms. We evaluate the performance of the
new strategy through simulations. Reanalyzing data from the Tennessee class
size study in which students and teachers were assigned at random to either a
small or a regular class in kindergarten (Phase I) yet noncompliance occurred
in Grade 1 (Phase II), we estimate the cumulative ATE of receiving two years of
instruction in a small class versus a regular class.
</summary>
    <author>
      <name>Guanglei Hong</name>
    </author>
    <author>
      <name>Xu Qin</name>
    </author>
    <author>
      <name>Zhengyan Xu</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages; 1 figure; 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.03104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03099v1</id>
    <updated>2025-06-03T17:29:28Z</updated>
    <published>2025-06-03T17:29:28Z</published>
    <title>TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via
  Autoregressive Diffusion Models</title>
    <summary>  In this paper, we present TalkingMachines -- an efficient framework that
transforms pretrained video generation models into real-time, audio-driven
character animators. TalkingMachines enables natural conversational experiences
by integrating an audio large language model (LLM) with our video generation
foundation model. Our primary contributions include: (1) We adapt a pretrained
SOTA image-to-video DiT into an audio-driven avatar generation model of 18
billion parameters; (2) We enable infinite video streaming without error
accumulation through asymmetric knowledge distillation from a bidirectional
teacher model into a sparse causal, autoregressive student model; (3) We design
a high-throughput, low-latency inference pipeline incorporating several key
engineering optimizations such as: (a) disaggregation of the DiT and VAE
decoder across separate devices, (b) efficient overlap of inter-device
communication and computation using CUDA streams, (c) elimination of redundant
recomputations to maximize frame-generation throughput. Please see demo videos
here - https://aaxwaz.github.io/TalkingMachines/
</summary>
    <author>
      <name>Chetwin Low</name>
    </author>
    <author>
      <name>Weimin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.03099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03095v1</id>
    <updated>2025-06-03T17:27:04Z</updated>
    <published>2025-06-03T17:27:04Z</published>
    <title>DPO Learning with LLMs-Judge Signal for Computer Use Agents</title>
    <summary>  Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.
</summary>
    <author>
      <name>Man Luo</name>
    </author>
    <author>
      <name>David Cobbley</name>
    </author>
    <author>
      <name>Xin Su</name>
    </author>
    <author>
      <name>Shachar Rosenman</name>
    </author>
    <author>
      <name>Vasudev Lal</name>
    </author>
    <author>
      <name>Shao-Yen Tseng</name>
    </author>
    <author>
      <name>Phillip Howard</name>
    </author>
    <link href="http://arxiv.org/abs/2506.03095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03093v1</id>
    <updated>2025-06-03T17:24:55Z</updated>
    <published>2025-06-03T17:24:55Z</published>
    <title>From Flat to Hierarchical: Extracting Sparse Representations with
  Matching Pursuit</title>
    <summary>  Motivated by the hypothesis that neural network representations encode
abstract, interpretable features as linearly accessible, approximately
orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in
interpretability. However, recent work has demonstrated phenomenology of model
representations that lies outside the scope of this hypothesis, showing
signatures of hierarchical, nonlinear, and multi-dimensional features. This
raises the question: do SAEs represent features that possess structure at odds
with their motivating hypothesis? If not, does avoiding this mismatch help
identify said features and gain further insights into neural network
representations? To answer these questions, we take a construction-based
approach and re-contextualize the popular matching pursuits (MP) algorithm from
sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a
sequence of residual-guided steps, allowing it to capture hierarchical and
nonlinearly accessible features. Comparing this architecture with existing SAEs
on a mixture of synthetic and natural data settings, we show: (i) hierarchical
concepts induce conditionally orthogonal features, which existing SAEs are
unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE
recovers highly meaningful features, helping us unravel shared structure in the
seemingly dichotomous representation spaces of different modalities in a
vision-language model, hence demonstrating the assumption that useful features
are solely linearly accessible is insufficient. We also show that the
sequential encoder principle of MP-SAE affords an additional benefit of
adaptive sparsity at inference time, which may be of independent interest.
Overall, we argue our results provide credence to the idea that
interpretability should begin with the phenomenology of representations, with
methods emerging from assumptions that fit it.
</summary>
    <author>
      <name>Valérie Costa</name>
    </author>
    <author>
      <name>Thomas Fel</name>
    </author>
    <author>
      <name>Ekdeep Singh Lubana</name>
    </author>
    <author>
      <name>Bahareh Tolooshams</name>
    </author>
    <author>
      <name>Demba Ba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.03093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03092v1</id>
    <updated>2025-06-03T17:24:08Z</updated>
    <published>2025-06-03T17:24:08Z</published>
    <title>Thin coronal jets and plasmoid-mediated reconnection: Insights from
  Solar Orbiter observations and Bifrost simulations</title>
    <summary>  Coronal jets are ubiquitous, collimated million-degree ejections that
contribute to the energy and mass supply of the upper solar atmosphere and the
solar wind. Solar Orbiter provides an unprecedented opportunity to observe
fine-scale jets from a unique vantage point close to the Sun. We aim to (1)
uncover thin jets originating from Coronal Bright Points (CBPs), revealing
previously unresolved contributions to coronal upflows; and (2) improve our
understanding of plasmoid-mediated reconnection and its observable signatures.
We analyze eleven datasets from the High Resolution Imager 174 \r{A} of the
Extreme Ultraviolet Imager (HRIEUV) onboard Solar Orbiter, focusing on narrow
jets from CBPs and signatures of magnetic reconnection within current sheets
and outflow regions. To support the observations, we compare with CBP
simulations performed with the Bifrost code. We have identified thin coronal
jets originating from CBPs with widths ranging from 253 km to 706 km: scales
that could not be resolved with previous EUV imaging instruments. Remarkably,
these jets are 30-85% brighter than their surroundings and can extend up to 22
Mm while maintaining their narrow form. In one of the datasets, we directly
identify plasmoid-mediated reconnection through the development within the
current sheet of a small-scale plasmoid that reaches a size of 332 km and
propagates at 40 km/s. In another dataset, we infer plasmoid signatures through
the intermittent boomerang-like pattern that appears in the outflow region.
Both direct and indirect plasmoid-mediated reconnection signatures are
supported by comparisons with the synthetic HRIEUV emission from the
simulations.
</summary>
    <author>
      <name>D. Nóbrega-Siverio</name>
    </author>
    <author>
      <name>R. Joshi</name>
    </author>
    <author>
      <name>E. Sola-Viladesau</name>
    </author>
    <author>
      <name>D. Berghmans</name>
    </author>
    <author>
      <name>D. Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to A&amp;A, 15 pages, 9 figures, movies available upon request</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.03092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.03084v1</id>
    <updated>2025-06-03T17:05:06Z</updated>
    <published>2025-06-03T17:05:06Z</published>
    <title>InterMamba: Efficient Human-Human Interaction Generation with Adaptive
  Spatio-Temporal Mamba</title>
    <summary>  Human-human interaction generation has garnered significant attention in
motion synthesis due to its vital role in understanding humans as social
beings. However, existing methods typically rely on transformer-based
architectures, which often face challenges related to scalability and
efficiency. To address these issues, we propose a novel, efficient human-human
interaction generation method based on the Mamba framework, designed to meet
the demands of effectively capturing long-sequence dependencies while providing
real-time feedback. Specifically, we introduce an adaptive spatio-temporal
Mamba framework that utilizes two parallel SSM branches with an adaptive
mechanism to integrate the spatial and temporal features of motion sequences.
To further enhance the model's ability to capture dependencies within
individual motion sequences and the interactions between different individual
sequences, we develop two key modules: the self-adaptive spatio-temporal Mamba
module and the cross-adaptive spatio-temporal Mamba module, enabling efficient
feature learning. Extensive experiments demonstrate that our method achieves
state-of-the-art results on two interaction datasets with remarkable quality
and efficiency. Compared to the baseline method InterGen, our approach not only
improves accuracy but also requires a minimal parameter size of just 66M ,only
36% of InterGen's, while achieving an average inference speed of 0.57 seconds,
which is 46% of InterGen's execution time.
</summary>
    <author>
      <name>Zizhao Wu</name>
    </author>
    <author>
      <name>Yingying Sun</name>
    </author>
    <author>
      <name>Yiming Chen</name>
    </author>
    <author>
      <name>Xiaoling Gu</name>
    </author>
    <author>
      <name>Ruyu Liu</name>
    </author>
    <author>
      <name>Jiazhou Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2506.03084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.03084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
