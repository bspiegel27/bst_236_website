<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-25T00:59:03Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-24T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">115621</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.18906v1</id>
    <updated>2025-06-23T17:59:59Z</updated>
    <published>2025-06-23T17:59:59Z</published>
    <title>State updates and useful qubits in relativistic quantum information</title>
    <summary>  We address the longstanding challenge of consistently updating quantum states
after selective measurements in a relativistic spacetime. Standard updates
along the future lightcones preserve causality but break correlations between
causally disconnected parties, whereas updates along the past lightcone either
imply retrocausality or do not respect the causal propagation of information.
We introduce a minimal extension of multipartite states to encode
subsystem-specific contextual information. This "polyperspective" formalism
ensures causally consistent covariant state updates, preserves multipartite
correlations, and respects conservation laws.
</summary>
    <author>
      <name>José Polo-Gómez</name>
    </author>
    <author>
      <name>T. Rick Perche</name>
    </author>
    <author>
      <name>Eduardo Martín-Martínez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures. RevTeX 4.2</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.18906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18887v1</id>
    <updated>2025-06-23T17:56:34Z</updated>
    <published>2025-06-23T17:56:34Z</published>
    <title>Steering Conceptual Bias via Transformer Latent-Subspace Activation</title>
    <summary>  This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.
</summary>
    <author>
      <name>Vansh Sharma</name>
    </author>
    <author>
      <name>Venkat Raman</name>
    </author>
    <link href="http://arxiv.org/abs/2506.18887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.6; I.2.1; D.3.3; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18881v1</id>
    <updated>2025-06-23T17:52:16Z</updated>
    <published>2025-06-23T17:52:16Z</published>
    <title>Let Your Video Listen to Your Music!</title>
    <summary>  Aligning the rhythm of visual motion in a video with a given music track is a
practical need in multimedia production, yet remains an underexplored task in
autonomous video editing. Effective alignment between motion and musical beats
enhances viewer engagement and visual appeal, particularly in music videos,
promotional content, and cinematic editing. Existing methods typically depend
on labor-intensive manual cutting, speed adjustments, or heuristic-based
editing techniques to achieve synchronization. While some generative models
handle joint video and music generation, they often entangle the two
modalities, limiting flexibility in aligning video to music beats while
preserving the full visual content. In this paper, we propose a novel and
efficient framework, termed MVAA (Music-Video Auto-Alignment), that
automatically edits video to align with the rhythm of a given music track while
preserving the original visual content. To enhance flexibility, we modularize
the task into a two-step process in our MVAA: aligning motion keyframes with
audio beats, followed by rhythm-aware video inpainting. Specifically, we first
insert keyframes at timestamps aligned with musical beats, then use a
frame-conditioned diffusion model to generate coherent intermediate frames,
preserving the original video's semantic content. Since comprehensive test-time
training can be time-consuming, we adopt a two-stage strategy: pretraining the
inpainting module on a small video set to learn general motion priors, followed
by rapid inference-time fine-tuning for video-specific adaptation. This hybrid
approach enables adaptation within 10 minutes with one epoch on a single NVIDIA
4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show
that our approach can achieve high-quality beat alignment and visual
smoothness.
</summary>
    <author>
      <name>Xinyu Zhang</name>
    </author>
    <author>
      <name>Dong Gong</name>
    </author>
    <author>
      <name>Zicheng Duan</name>
    </author>
    <author>
      <name>Anton van den Hengel</name>
    </author>
    <author>
      <name>Lingqiao Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">project page: https://zhangxinyu-xyz.github.io/MVAA/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.18881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18879v1</id>
    <updated>2025-06-23T17:50:11Z</updated>
    <published>2025-06-23T17:50:11Z</published>
    <title>CommVQ: Commutative Vector Quantization for KV Cache Compression</title>
    <summary>  Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (KV) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive quantization with a
lightweight encoder and codebook to compress the KV cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive quantization and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%
with 2-bit quantization, while outperforming state-of-the-art KV cache
quantization methods. Notably, it enables 1-bit KV cache quantization with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.
</summary>
    <author>
      <name>Junyan Li</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Muhammad Yusuf Hassan</name>
    </author>
    <author>
      <name>Talha Chafekar</name>
    </author>
    <author>
      <name>Tianle Cai</name>
    </author>
    <author>
      <name>Zhile Ren</name>
    </author>
    <author>
      <name>Pengsheng Guo</name>
    </author>
    <author>
      <name>Foroozan Karimzadeh</name>
    </author>
    <author>
      <name>Colorado Reed</name>
    </author>
    <author>
      <name>Chong Wang</name>
    </author>
    <author>
      <name>Chuang Gan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2025 poster</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.18879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18876v1</id>
    <updated>2025-06-23T17:48:23Z</updated>
    <published>2025-06-23T17:48:23Z</published>
    <title>A field-level reaction for screened modified gravity</title>
    <summary>  We present a field-level reaction framework to emulate the nonlinear effects
of screened modified gravity on the cosmic web. This approach is designed to
enable field-level inference with data from Stage IV cosmological surveys.
Building on the reaction method, which models the nonlinear matter power
spectrum in modified gravity as corrections to a "pseudo" $\Lambda$CDM
cosmology, we extend the method to full field-level predictions by applying it
to the output of $N$-body simulations, including both positions and velocities.
We focus on modifications to gravity that are scale-independent at the linear
level, allowing us to isolate and emulate nonlinear deviations, particularly
screening effects. Our neural network predicts the field-level correction
("reaction") to a pseudo$\Lambda$CDM simulation whose linear clustering matches
that of the target. The emulator achieves sub-percent accuracy across a broad
range of summary statistics, including 0.4\% agreement in the matter power
spectrum at scales $k &lt; 1$ Mpc$/h$, and 2\% accuracy in redshift-space
distortion multipoles at $k &lt; 0.3$ Mpc$/h$. We also validate the emulator
against $N$-body simulations with increased force resolution and time steps,
confirming the robustness of its performance. These results demonstrate that
our framework is a practical and reliable tool for incorporating screened
modified gravity models into field-level cosmological inference, enabling
stringent tests of extra fundamental forces at cosmological scales.
</summary>
    <author>
      <name>Daniela Saadeh</name>
    </author>
    <author>
      <name>Kazuya Koyama</name>
    </author>
    <author>
      <name>Xan Morice-Atkinson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 figures, to be submitted to MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.18876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18870v1</id>
    <updated>2025-06-23T17:38:48Z</updated>
    <published>2025-06-23T17:38:48Z</published>
    <title>Amplifying Machine Learning Attacks Through Strategic Compositions</title>
    <summary>  Machine learning (ML) models are proving to be vulnerable to a variety of
attacks that allow the adversary to learn sensitive information, cause
mispredictions, and more. While these attacks have been extensively studied,
current research predominantly focuses on analyzing each attack type
individually. In practice, however, adversaries may employ multiple attack
strategies simultaneously rather than relying on a single approach. This
prompts a crucial yet underexplored question: When the adversary has multiple
attacks at their disposal, are they able to mount or amplify the effect of one
attack with another? In this paper, we take the first step in studying the
strategic interactions among different attacks, which we define as attack
compositions. Specifically, we focus on four well-studied attacks during the
model's inference phase: adversarial examples, attribute inference, membership
inference, and property inference. To facilitate the study of their
interactions, we propose a taxonomy based on three stages of the attack
pipeline: preparation, execution, and evaluation. Using this taxonomy, we
identify four effective attack compositions, such as property inference
assisting attribute inference at its preparation level and adversarial examples
assisting property inference at its execution level. We conduct extensive
experiments on the attack compositions using three ML model architectures and
three benchmark image datasets. Empirical results demonstrate the effectiveness
of these four attack compositions. We implement and release a modular reusable
toolkit, COAT. Arguably, our work serves as a call for researchers and
practitioners to consider advanced adversarial settings involving multiple
attack strategies, aiming to strengthen the security and robustness of AI
systems.
</summary>
    <author>
      <name>Yugeng Liu</name>
    </author>
    <author>
      <name>Zheng Li</name>
    </author>
    <author>
      <name>Hai Huang</name>
    </author>
    <author>
      <name>Michael Backes</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.18870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18863v1</id>
    <updated>2025-06-23T17:26:40Z</updated>
    <published>2025-06-23T17:26:40Z</published>
    <title>Variational Bayesian Channel Estimation and Data Detection for Cell-Free
  Massive MIMO with Low-Resolution Quantized Fronthaul Links</title>
    <summary>  We study the joint channel estimation and data detection (JED) problem in a
cell-free massive multiple-input multiple-output (CF-mMIMO) network, where
access points (APs) communicate with a central processing unit (CPU) over
fronthaul links. However, the bandwidth of these links is limited, and thus,
presents challenges to the applicability of CF-mMIMO, especially with an
ever-increasing number of users. To address this, we propose a method based on
variational Bayesian (VB) inference for performing the JED process, where the
APs forward low-resolution quantized versions of the signals to the CPU. We
consider two approaches: \emph{quantization-and-estimation} (Q-E) and
\emph{estimation-and-quantization} (E-Q). In the Q-E approach, each AP uses a
low-bit quantizer to quantize the signal before forwarding it to the CPU, while
in the E-Q approach, each AP first performs local channel estimation and then
sends a low-bit quantized version of the estimated channel to the CPU. We
evaluate the performance of our VB-based approach under perfect fronthaul link
(PFL) with unquantized received signals, Q-E, and E-Q in terms of symbol error
rate (SER), normalized mean square error (NMSE) of the channel estimation,
computational complexity, and fronthaul signaling overhead. We also compare
these results with those of the linear minimum mean squared error (LMMSE)
method under the PFL scenario. Our numerical results show that both the VB(Q-E)
and VB(E-Q) approaches achieve superior performance compared to LMMSE(PFL),
benefiting from the nonlinear modeling inherent in VB. Furthermore, the VB(Q-E)
method outperforms VB(E-Q) due to errors in the local channel estimation
process at the APs within the VB(E-Q) approach.
</summary>
    <author>
      <name>Sajjad Nassirpour</name>
    </author>
    <author>
      <name>Toan-Van Nguyen</name>
    </author>
    <author>
      <name>Hien Q. Ngo</name>
    </author>
    <author>
      <name>Le-Nam Tran</name>
    </author>
    <author>
      <name>Tharmalingam Ratnarajah</name>
    </author>
    <author>
      <name>Duy H. N. Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures, accepted for journal publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.18863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18852v1</id>
    <updated>2025-06-23T17:13:30Z</updated>
    <published>2025-06-23T17:13:30Z</published>
    <title>Mechanistic Interpretability Needs Philosophy</title>
    <summary>  Mechanistic interpretability (MI) aims to explain how neural networks work by
uncovering their underlying causal mechanisms. As the field grows in influence,
it is increasingly important to examine not just models themselves, but the
assumptions, concepts and explanatory strategies implicit in MI research. We
argue that mechanistic interpretability needs philosophy: not as an
afterthought, but as an ongoing partner in clarifying its concepts, refining
its methods, and assessing the epistemic and ethical stakes of interpreting AI
systems. Taking three open problems from the MI literature as examples, this
position paper illustrates the value philosophy can add to MI research, and
outlines a path toward deeper interdisciplinary dialogue.
</summary>
    <author>
      <name>Iwan Williams</name>
    </author>
    <author>
      <name>Ninell Oldenburg</name>
    </author>
    <author>
      <name>Ruchira Dhar</name>
    </author>
    <author>
      <name>Joshua Hatherley</name>
    </author>
    <author>
      <name>Constanza Fierro</name>
    </author>
    <author>
      <name>Nina Rajcic</name>
    </author>
    <author>
      <name>Sandrine R. Schiller</name>
    </author>
    <author>
      <name>Filippos Stamatiou</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <link href="http://arxiv.org/abs/2506.18852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18846v1</id>
    <updated>2025-06-23T17:07:04Z</updated>
    <published>2025-06-23T17:07:04Z</published>
    <title>Bayesian decomposition using Besov priors</title>
    <summary>  In many inverse problems, the unknown is composed of multiple components with
different regularities, for example, in imaging problems, where the unknown can
have both rough and smooth features. We investigate linear Bayesian inverse
problems, where the unknown consists of two components: one smooth and one
piecewise constant. We model the unknown as a sum of two components and assign
individual priors on each component to impose the assumed behavior. We propose
and compare two prior models: (i) a combination of a Haar wavelet-based Besov
prior and a smoothing Besov prior, and (ii) a hierarchical Gaussian prior on
the gradient coupled with a smoothing Besov prior. To achieve a balanced
reconstruction, we place hyperpriors on the prior parameters and jointly infer
both the components and the hyperparameters. We propose Gibbs sampling schemes
for posterior inference in both prior models. We demonstrate the capabilities
of our approach on 1D and 2D deconvolution problems, where the unknown consists
of smooth parts with jumps. The numerical results indicate that our methods
improve the reconstruction quality compared to single-prior approaches and that
the prior parameters can be successfully estimated to yield a balanced
decomposition.
</summary>
    <author>
      <name>Andreas Horst</name>
    </author>
    <author>
      <name>Babak Maboudi Afkham</name>
    </author>
    <author>
      <name>Yiqiu Dong</name>
    </author>
    <author>
      <name>Jakob Lemvig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 13 figures, this is a preprint of an article submitted to
  the IOP journal on inverse problems</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.18846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; G.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.18842v1</id>
    <updated>2025-06-23T17:00:34Z</updated>
    <published>2025-06-23T17:00:34Z</published>
    <title>LIGHTHOUSE: Fast and precise distance to shoreline calculations from
  anywhere on earth</title>
    <summary>  We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.
</summary>
    <author>
      <name>Patrick Beukema</name>
    </author>
    <author>
      <name>Henry Herzog</name>
    </author>
    <author>
      <name>Yawen Zhang</name>
    </author>
    <author>
      <name>Hunter Pitelka</name>
    </author>
    <author>
      <name>Favyen Bastani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures, 1 table, ICML 2025 ML4RS</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.18842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.18842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
