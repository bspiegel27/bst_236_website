<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-23T00:56:13Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-22T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">109239</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.16428v1</id>
    <updated>2025-03-20T17:59:58Z</updated>
    <published>2025-03-20T17:59:58Z</published>
    <title>XAttention: Block Sparse Attention with Antidiagonal Scoring</title>
    <summary>  Long-Context Transformer Models (LCTMs) are vital for real-world applications
but suffer high computational costs due to attention's quadratic complexity.
Block-sparse attention mitigates this by focusing computation on critical
regions, yet existing methods struggle with balancing accuracy and efficiency
due to costly block importance measurements. In this paper, we introduce
XAttention, a plug-and-play framework that dramatically accelerates
long-context inference in Transformers models using sparse attention.
XAttention's key innovation is the insight that the sum of antidiagonal values
(i.e., from the lower-left to upper-right) in the attention matrix provides a
powerful proxy for block importance. This allows for precise identification and
pruning of non-essential blocks, resulting in high sparsity and dramatically
accelerated inference. Across comprehensive evaluations on demanding
long-context benchmarks-including RULER and LongBench for language, VideoMME
for video understanding, and VBench for video generation. XAttention achieves
accuracy comparable to full attention while delivering substantial
computational gains. We demonstrate up to 13.5x acceleration in attention
computation. These results underscore XAttention's ability to unlock the
practical potential of block sparse attention, paving the way for scalable and
efficient deployment of LCTMs in real-world applications. Code is available at
https://github.com/mit-han-lab/x-attention.
</summary>
    <author>
      <name>Ruyi Xu</name>
    </author>
    <author>
      <name>Guangxuan Xiao</name>
    </author>
    <author>
      <name>Haofeng Huang</name>
    </author>
    <author>
      <name>Junxian Guo</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16419v1</id>
    <updated>2025-03-20T17:59:38Z</updated>
    <published>2025-03-20T17:59:38Z</published>
    <title>Stop Overthinking: A Survey on Efficient Reasoning for Large Language
  Models</title>
    <summary>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as
OpenAI o1 and DeepSeek-R1, have further improved performance in System-2
reasoning domains like mathematics and programming by harnessing supervised
fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the
Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences
improve performance, they also introduce significant computational overhead due
to verbose and redundant outputs, known as the "overthinking phenomenon". In
this paper, we provide the first structured survey to systematically
investigate and explore the current progress toward achieving efficient
reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we
categorize existing works into several key directions: (1) model-based
efficient reasoning, which considers optimizing full-length reasoning models
into more concise reasoning models or directly training efficient reasoning
models; (2) reasoning output-based efficient reasoning, which aims to
dynamically reduce reasoning steps and length during inference; (3) input
prompts-based efficient reasoning, which seeks to enhance reasoning efficiency
based on input prompt properties such as difficulty or length control.
Additionally, we introduce the use of efficient data for training reasoning
models, explore the reasoning capabilities of small language models, and
discuss evaluation methods and benchmarking.
</summary>
    <author>
      <name>Yang Sui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Yu-Neng Chuang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Guanchu Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Jiamu Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Tianyi Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Jiayi Yuan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Hongyi Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Wen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name> Shaochen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name> Zhong</name>
    </author>
    <author>
      <name>Hanjie Chen</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Website:
  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16413v1</id>
    <updated>2025-03-20T17:59:12Z</updated>
    <published>2025-03-20T17:59:12Z</published>
    <title>M3: 3D-Spatial MultiModal Memory</title>
    <summary>  We present 3D Spatial MultiModal Memory (M3), a multimodal memory system
designed to retain information about medium-sized static scenes through video
sources for visual perception. By integrating 3D Gaussian Splatting techniques
with foundation models, M3 builds a multimodal memory capable of rendering
feature representations across granularities, encompassing a wide range of
knowledge. In our exploration, we identify two key challenges in previous works
on feature splatting: (1) computational constraints in storing high-dimensional
features for each Gaussian primitive, and (2) misalignment or information loss
between distilled features and foundation model features. To address these
challenges, we propose M3 with key components of principal scene components and
Gaussian memory attention, enabling efficient training and inference. To
validate M3, we conduct comprehensive quantitative evaluations of feature
similarity and downstream tasks, as well as qualitative visualizations to
highlight the pixel trace of Gaussian memory attention. Our approach
encompasses a diverse range of foundation models, including vision-language
models (VLMs), perception models, and large multimodal and language models
(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy
M3's feature field in indoor scenes on a quadruped robot. Notably, we claim
that M3 is the first work to address the core compression challenges in 3D
feature distillation.
</summary>
    <author>
      <name>Xueyan Zou</name>
    </author>
    <author>
      <name>Yuchen Song</name>
    </author>
    <author>
      <name>Ri-Zhao Qiu</name>
    </author>
    <author>
      <name>Xuanbin Peng</name>
    </author>
    <author>
      <name>Jianglong Ye</name>
    </author>
    <author>
      <name>Sifei Liu</name>
    </author>
    <author>
      <name>Xiaolong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR2025 homepage: https://m3-spatial-memory.github.io code:
  https://github.com/MaureenZOU/m3-spatial</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16409v1</id>
    <updated>2025-03-20T17:58:57Z</updated>
    <published>2025-03-20T17:58:57Z</published>
    <title>A foundational derivation of quantum weak values and time-dependent
  density functional theory</title>
    <summary>  The equations of time-dependent density functional theory are derived, via
the expression for the quantum weak value, from ring polymer quantum theory
using a symmetry between time and imaginary time. The imaginary time path
integral formalism of Feynman, in which inverse temperature is seen to be a
Wick rotation of time, allows one to write the equilibrium partition function
of a quantum system in a form isomorphic with the path integral expression for
the dynamics. Therefore the self-consistent field theory equations which are
solutions to the equilibrium partition function are Wick rotated back into a
set of dynamic equations, which are shown to give the formula for the quantum
weak value. As a special case, this in turn reduces to the equations of
time-dependent density functional theory. This first-principles derivation does
not use the theorems of density functional theory, which are instead applied to
guarantee equivalence with standard quantum mechanics. An expression for
finite-temperature dynamics is also derived using the limits of the equilibrium
equations and the ground state dynamics. This finite temperature expression
shows that a ring polymer model for quantum particles holds for time-dependent
systems as well as static situations. Issues arising in time-dependent density
functional theory, such as causality, initial state dependence, and
$v$-representability, are discussed in the context of the ring polymer
derivation. Connections with a variety of quantum phenomena is reviewed, and a
possible link with de Broglie-Bohm theory is mentioned.
</summary>
    <author>
      <name>Russell B. Thompson</name>
    </author>
    <author>
      <name>Zarin Tasneem</name>
    </author>
    <author>
      <name>Yves Caudano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16400v1</id>
    <updated>2025-03-20T17:54:37Z</updated>
    <published>2025-03-20T17:54:37Z</published>
    <title>ScalingNoise: Scaling Inference-Time Search for Generating Infinite
  Videos</title>
    <summary>  Video diffusion models (VDMs) facilitate the generation of high-quality
videos, with current research predominantly concentrated on scaling efforts
during training through improvements in data quality, computational resources,
and model complexity. However, inference-time scaling has received less
attention, with most approaches restricting models to a single generation
attempt. Recent studies have uncovered the existence of "golden noises" that
can enhance video quality during generation. Building on this, we find that
guiding the scaling inference-time search of VDMs to identify better noise
candidates not only evaluates the quality of the frames generated in the
current step but also preserves the high-level object features by referencing
the anchor frame from previous multi-chunks, thereby delivering long-term
value. Our analysis reveals that diffusion models inherently possess flexible
adjustments of computation by varying denoising steps, and even a one-step
denoising approach, when guided by a reward signal, yields significant
long-term benefits. Based on the observation, we proposeScalingNoise, a
plug-and-play inference-time search strategy that identifies golden initial
noises for the diffusion sampling process to improve global content consistency
and visual diversity. Specifically, we perform one-step denoising to convert
initial noises into a clip and subsequently evaluate its long-term value,
leveraging a reward model anchored by previously generated content. Moreover,
to preserve diversity, we sample candidates from a tilted noise distribution
that up-weights promising noises. In this way, ScalingNoise significantly
reduces noise-induced errors, ensuring more coherent and spatiotemporally
consistent video generation. Extensive experiments on benchmark datasets
demonstrate that the proposed ScalingNoise effectively improves long video
generation.
</summary>
    <author>
      <name>Haolin Yang</name>
    </author>
    <author>
      <name>Feilong Tang</name>
    </author>
    <author>
      <name>Ming Hu</name>
    </author>
    <author>
      <name>Yulong Li</name>
    </author>
    <author>
      <name>Junjie Guo</name>
    </author>
    <author>
      <name>Yexin Liu</name>
    </author>
    <author>
      <name>Zelin Peng</name>
    </author>
    <author>
      <name>Junjun He</name>
    </author>
    <author>
      <name>Zongyuan Ge</name>
    </author>
    <author>
      <name>Imran Razzak</name>
    </author>
    <link href="http://arxiv.org/abs/2503.16400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16397v1</id>
    <updated>2025-03-20T17:54:02Z</updated>
    <published>2025-03-20T17:54:02Z</published>
    <title>Scale-wise Distillation of Diffusion Models</title>
    <summary>  We present SwD, a scale-wise distillation framework for diffusion models
(DMs), which effectively employs next-scale prediction ideas for
diffusion-based few-step generators. In more detail, SwD is inspired by the
recent insights relating diffusion processes to the implicit spectral
autoregression. We suppose that DMs can initiate generation at lower data
resolutions and gradually upscale the samples at each denoising step without
loss in performance while significantly reducing computational costs. SwD
naturally integrates this idea into existing diffusion distillation methods
based on distribution matching. Also, we enrich the family of distribution
matching approaches by introducing a novel patch loss enforcing finer-grained
similarity to the target distribution. When applied to state-of-the-art
text-to-image diffusion models, SwD approaches the inference times of two full
resolution steps and significantly outperforms the counterparts under the same
computation budget, as evidenced by automated metrics and human preference
studies.
</summary>
    <author>
      <name>Nikita Starodubcev</name>
    </author>
    <author>
      <name>Denis Kuznedelev</name>
    </author>
    <author>
      <name>Artem Babenko</name>
    </author>
    <author>
      <name>Dmitry Baranchuk</name>
    </author>
    <link href="http://arxiv.org/abs/2503.16397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16379v1</id>
    <updated>2025-03-20T17:42:11Z</updated>
    <published>2025-03-20T17:42:11Z</published>
    <title>The impact of baryons on the sparsity of simulated galaxy clusters from
  The Three Hundred Project</title>
    <summary>  Measurements of the sparsity of galaxy clusters can be used to probe the
cosmological information encoded in the host dark matter halo profile, and
infer constraints on the cosmological model parameters. Key to the success of
these analyses is the control of potential sources of systematic uncertainty.
As an example, the presence of baryons can alter the cluster sparsity with
respect to predictions from N-body simulations. Similarly, a radial dependent
mass bias, as in the case of masses inferred under the hydrostatic equilibrium
(HE) hypothesis, can affect sparsity estimates. We examine the imprint of
baryonic processes on the sparsity statistics. Then, we investigate the
relation between cluster sparsities and gas mass fraction. Finally, we perform
a study of the impact of HE mass bias on sparsity measurements and the
implication on cosmological parameter inference analyses. We use catalogues of
simulated galaxy clusters from The Three Hundred project and run a comparative
analysis of the sparsity of clusters from N-body/hydro simulations implementing
different feedback model scenarios. Sparsities which probe the mass profile
across a large radial range are affected by the presence of baryons in a way
that is particularly sensitive to astrophysical feedback, whereas those probing
exclusively external cluster regions are less affected. In the former case, we
find the sparsities to be moderately correlated with measurements of the gas
fraction in the inner cluster regions. We infer constraints on $S_8$ using
synthetic average sparsity measurements generated to evaluate the impact of
baryons, selection effects and HE bias. In the case of multiple sparsities
these lead to highly bias results. Hence, we calibrate linear bias models that
enable us to correct for these effects and recover unbiased constraints that
are significantly tighter than those inferred from single sparsity analyses.
</summary>
    <author>
      <name>P. S. Corasaniti</name>
    </author>
    <author>
      <name>T. R. G. Richardson</name>
    </author>
    <author>
      <name>S. Ettori</name>
    </author>
    <author>
      <name>M. De Petris</name>
    </author>
    <author>
      <name>E. Rasia</name>
    </author>
    <author>
      <name>W. Cui</name>
    </author>
    <author>
      <name>G. Yepes</name>
    </author>
    <author>
      <name>G. Gianfagna</name>
    </author>
    <author>
      <name>A. M. C. Le Bun</name>
    </author>
    <author>
      <name>Y. Rasera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 29 figures, accepted in A&amp;A</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16356v1</id>
    <updated>2025-03-20T17:14:34Z</updated>
    <published>2025-03-20T17:14:34Z</published>
    <title>CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</title>
    <summary>  Knowledge Editing (KE) enables the modification of outdated or incorrect
information in large language models (LLMs). While existing KE methods can
update isolated facts, they struggle to generalize these updates to multi-hop
reasoning tasks that depend on the modified knowledge. Through an analysis of
reasoning circuits -- the neural pathways LLMs use for knowledge-based
inference, we observe that current layer-localized KE approaches, such as MEMIT
and WISE, which edit only single or a few model layers, struggle to effectively
incorporate updated information into these reasoning pathways. To address this
limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method
that enables more effective integration of updated knowledge in LLMs. CaKE
leverages strategically curated data, guided by our circuits-based analysis,
that enforces the model to utilize the modified knowledge, stimulating the
model to develop appropriate reasoning circuits for newly integrated knowledge.
Experimental results show that CaKE enables more accurate and consistent use of
updated knowledge across related reasoning tasks, leading to an average of 20%
improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to
existing KE methods. We release the code and data in
https://github.com/zjunlp/CaKE.
</summary>
    <author>
      <name>Yunzhi Yao</name>
    </author>
    <author>
      <name>Jizhan Fang</name>
    </author>
    <author>
      <name>Jia-Chen Gu</name>
    </author>
    <author>
      <name>Ningyu Zhang</name>
    </author>
    <author>
      <name>Shumin Deng</name>
    </author>
    <author>
      <name>Huajun Chen</name>
    </author>
    <author>
      <name>Nanyun Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16355v1</id>
    <updated>2025-03-20T17:13:57Z</updated>
    <published>2025-03-20T17:13:57Z</published>
    <title>DEMNUni: the Sunyaev-Zel'dovich effect in the presence of massive
  neutrinos and dynamical dark energy</title>
    <summary>  In recent years, the study of secondary anisotropies in the Cosmic Microwave
Background has become a fundamental instrument to test our understanding of
Cosmology and Astrophysics. Using a set of lightcones produced with the ``Dark
Energy and Massive Neutrino Universe'' $N$-body simulations we study how
different dark energy models and neutrino masses impact the properties of the
Sunyaev-Zel'dovich (SZ) effects, focusing on the signal arising from galaxy
clusters and groups. We analyse the distribution of values, Compton-$y$
parameter for the thermal SZ effect and $\Delta T/T$ for the kinematic SZ
effect, and study their angular power spectra. We find that the distribution of
logarithmic Compton parameter can be fitted with a skewed Gaussian, with a mean
that, at fixed dark energy model, decreases linearly with an approximate slope
of $10 f_\nu$. Regarding the power spectrum of the thermal SZ effect, we find
that an increase in $\sum {m_\nu}$ is observed as a power-law scaling with
respect to $\sigma_8^{\mathrm{cb}}$, with exponents ranging from 7.2 to 8.2. We
also find that four cosmological models, one with $\sum {m_\nu} = 0.16$ eV and
three with $\sum {m_\nu} = 0.32$ eV, fit equally well the Planck data for the
Compton-$y$. For all the \texttt{DEMNUni} models we forecast the cumulative
signal-to-noise for thermal SZ observations with the LAT instrument of Simons
Observatory; furthermore, we compute a tailored $\chi_\mathrm{SNR}^2$ estimator
to infer if they can be distinguished from the reference $\Lambda$CDM. We also
provide estimates for the power spectrum of the cluster component of the
kinematic SZ effect, in all the different cosmological scenarios.
</summary>
    <author>
      <name>Davide Luchina</name>
    </author>
    <author>
      <name>Mauro Roncarelli</name>
    </author>
    <author>
      <name>Matteo Calabrese</name>
    </author>
    <author>
      <name>Giulio Fabbian</name>
    </author>
    <author>
      <name>Carmelita Carbone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 12 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16354v1</id>
    <updated>2025-03-20T17:13:51Z</updated>
    <published>2025-03-20T17:13:51Z</published>
    <title>Hypercyclicity of Weighted shifts on weighted Bergman and Dirichlet
  spaces</title>
    <summary>  Let $B_w$ and $F_w$ denote, respectively, the weighted backward and forward
shift operators defined on the weighted Bergman space $A^p_{\phi}$, or the
weighted Dirichlet space ${D}^p_{\phi}$ of the unit disc, where the weight
function $\phi(z)$ is mostly radial. We first obtain sufficient conditions for
$B_w$ and $F_w$ to be continuous on these spaces. For radial weights, we derive
norm estimates for coefficient functionals on $A^p_{\phi}$ and $D^p_{\phi}$,
and using those estimates we infer when the weighted shifts or their adjoints
are hypercyclic. We also deal with a non-radial case.
</summary>
    <author>
      <name>Bibhash Kumar Das</name>
    </author>
    <author>
      <name>Aneesh Mundayadan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="47A16, 46E22, 32K05, 47B32, 47B37, 37A99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
