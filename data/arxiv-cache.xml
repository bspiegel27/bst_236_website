<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-09T00:53:05Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">123035</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.06220v1</id>
    <updated>2025-10-07T17:59:58Z</updated>
    <published>2025-10-07T17:59:58Z</published>
    <title>Studying the gravitational-wave population without looking that FAR out</title>
    <summary>  From catalogs of gravitational-wave transients, the population-level
properties of their sources and the formation channels of merging compact
binaries can be constrained. However, astrophysical conclusions can be biased
by misspecification or misestimation of the population likelihood. Despite
detection thresholds on the false-alarm rate (FAR) or signal-to-noise ratio
(SNR), the current catalog is likely contaminated by noise transients. Further,
computing the population likelihood becomes less accurate as the catalog grows.
Current methods to address these challenges often scale poorly with the number
of events and potentially become infeasible for future catalogs. Here, we
evaluate a simple remedy: increasing the significance threshold for including
events in population analyses. To determine the efficacy of this approach, we
analyze simulated catalogs of up to 1600 gravitational-wave signals from
black-hole mergers using full Bayesian parameter estimation with current
detector sensitivities. We show that the growth in statistical uncertainty
about the black-hole population, as we analyze fewer events but with higher
SNR, depends on the source parameters of interest. When the SNR threshold is
raised from 11 to 15 -- reducing our catalog size by two--thirds -- we find
that statistical uncertainties on the mass distribution only grow by a few 10%
and constraints on the spin distribution are essentially unchanged; meanwhile,
uncertainties on the high-redshift cosmic merger rate more than double.
Simultaneously, numerical uncertainty in the estimate of the population
likelihood more than halves, allowing us to ensure unbiased inference without
additional computational expense. Our results demonstrate that focusing on
higher-significance events is an effective way to facilitate robust
astrophysical inference with growing gravitational-wave catalogs.
</summary>
    <author>
      <name>Noah E. Wolfe</name>
    </author>
    <author>
      <name>Matthew Mould</name>
    </author>
    <author>
      <name>Jack Heinzel</name>
    </author>
    <author>
      <name>Salvatore Vitale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures; to be submitted to Physical Review D. Comments
  welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.06220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06217v1</id>
    <updated>2025-10-07T17:59:41Z</updated>
    <published>2025-10-07T17:59:41Z</published>
    <title>TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular
  Reasoning</title>
    <summary>  Process Reward Models (PRMs) have recently emerged as a powerful framework
for enhancing the reasoning capabilities of large reasoning models (LRMs),
particularly in the context of test-time scaling (TTS). However, their
potential for supervising LRMs on tabular reasoning domains remains
underexplored. Through detailed empirical analyses, we identify that existing
PRMs, though widely adopted for supervising text-only reasoning steps, struggle
with table-specific operations such as sub-table retrieval and schema
interaction, leading to critical performance bottlenecks. To address this
limitation, we propose TaTToo, a novel table-grounded PRM framework that (i)
reasons explicitly over tabular reasoning steps and (ii) integrates tool-based
verification to provide precise reward supervision. Concretely, we first design
a scalable data curation pipeline that constructs over 60k high-quality
step-level annotations by integrating table verification rationales with
tool-based executions. Building on the collected data, we train TaTToo with a
dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use
reasoning patterns, followed by reinforcement learning with tool-grounded
reward shaping to align our model with table-based verification. We provide a
comprehensive evaluation of the policy improvement induced by our newly
designed PRM. Across 5 challenging tabular reasoning benchmarks covering
numerical reasoning, fact-checking, and data analysis, TaTToo improves
downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines
such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong
generalizability across diverse TTS strategies.
</summary>
    <author>
      <name>Jiaru Zou</name>
    </author>
    <author>
      <name>Soumya Roy</name>
    </author>
    <author>
      <name>Vinay Kumar Verma</name>
    </author>
    <author>
      <name>Ziyi Wang</name>
    </author>
    <author>
      <name>David Wipf</name>
    </author>
    <author>
      <name>Pan Lu</name>
    </author>
    <author>
      <name>Sumit Negi</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <author>
      <name>Jingrui He</name>
    </author>
    <link href="http://arxiv.org/abs/2510.06217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06215v1</id>
    <updated>2025-10-07T17:59:15Z</updated>
    <published>2025-10-07T17:59:15Z</published>
    <title>Fine-grained Defocus Blur Control for Generative Image Models</title>
    <summary>  Current text-to-image diffusion models excel at generating diverse,
high-quality images, yet they struggle to incorporate fine-grained camera
metadata such as precise aperture settings. In this work, we introduce a novel
text-to-image diffusion framework that leverages camera metadata, or EXIF data,
which is often embedded in image files, with an emphasis on generating
controllable lens blur. Our method mimics the physical image formation process
by first generating an all-in-focus image, estimating its monocular depth,
predicting a plausible focus distance with a novel focus distance transformer,
and then forming a defocused image with an existing differentiable lens blur
model. Gradients flow backwards through this whole process, allowing us to
learn without explicit supervision to generate defocus effects based on content
elements and the provided EXIF data. At inference time, this enables precise
interactive user control over defocus effects while preserving scene contents,
which is not achievable with existing diffusion models. Experimental results
demonstrate that our model enables superior fine-grained control without
altering the depicted scene.
</summary>
    <author>
      <name>Ayush Shrivastava</name>
    </author>
    <author>
      <name>Connelly Barnes</name>
    </author>
    <author>
      <name>Xuaner Zhang</name>
    </author>
    <author>
      <name>Lingzhi Zhang</name>
    </author>
    <author>
      <name>Andrew Owens</name>
    </author>
    <author>
      <name>Sohrab Amirghodsi</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project link: https://www.ayshrv.com/defocus-blur-gen</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.06215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06202v1</id>
    <updated>2025-10-07T17:54:14Z</updated>
    <published>2025-10-07T17:54:14Z</published>
    <title>Mapping surface height dynamics to subsurface flow physics in
  free-surface turbulent flow using a shallow recurrent decoder</title>
    <summary>  Near-surface turbulent flows beneath a free surface are reconstructed from
sparse measurements of the surface height variation, by a novel neural network
algorithm known as the SHallow REcurrent Decoder (SHRED). The reconstruction of
turbulent flow fields from limited, partial, or indirect measurements remains a
grand challenge in science and engineering. The central goal in such
applications is to leverage easy-to-measure proxy variables in order to
estimate quantities which have not been, and perhaps cannot in practice be,
measured. Specifically, in the application considered here, the aim is to use a
sparse number of surface height point measurements of a flow field, or drone
video footage of surface features, in order to infer the turbulent flow field
beneath the surface. SHRED is a deep learning architecture that learns a
delay-coordinate embedding from a few surface height (point) sensors and maps
it, via a shallow decoder trained in a compressed basis, to full subsurface
fields, enabling fast, robust training from minimal data. We demonstrate the
SHRED sensing architecture on both fully resolved DNS data and PIV laboratory
data from a turbulent water tank. SHRED is capable of robustly mapping surface
height fluctuations to full-state flow fields up to about two integral length
scales deep, with as few as three surface measurements.
</summary>
    <author>
      <name>Kristoffer S. Moen</name>
    </author>
    <author>
      <name>Jørgen R. Aarnes</name>
    </author>
    <author>
      <name>Simen Å. Ellingsen</name>
    </author>
    <author>
      <name>J. Nathan Kutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.06202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06195v1</id>
    <updated>2025-10-07T17:52:08Z</updated>
    <published>2025-10-07T17:52:08Z</published>
    <title>Latent Speech-Text Transformer</title>
    <summary>  Auto-regressive speech-text models are typically pre-trained on a large
number of interleaved sequences of text tokens and raw speech encoded as speech
tokens using vector quantization. These models have demonstrated
state-of-the-art performance in speech-to-speech understanding and generation
benchmarks, together with promising scaling laws, primarily enabled by the
representational alignment between text and speech. Nevertheless, they suffer
from shortcomings, partly owing to the disproportionately longer sequences of
speech tokens in contrast to textual tokens. This results in a large compute
imbalance between modalities during pre-training as well as during inference,
and a potential hindrance to effectively aligning speech and text, ultimately
translating to several orders of magnitude slower scaling laws. We introduce
the Latent Speech-Text Transformer (LST), which makes pre-training speech-text
models more data-efficient by dynamically and inexpensively aggregating speech
tokens into latent speech patches. These patches serve as higher-level units
that can either align with corresponding textual units to aid capability
transfer or even encapsulate common speech sequences like silences to be more
compute-efficient. We show that LST outperforms vanilla approaches on
speech-to-speech as well as text-to-text benchmarks in both data- and
compute-controlled settings, the former indicating more effective
representational alignment and the latter indicating steeper scaling laws for
speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute
gain in speech accuracy under compute-controlled training and 5.3% under
data-controlled training, while also improving text performance. We will
release our models, code, and the evaluation data to facilitate further
research.
</summary>
    <author>
      <name>Yen-Ju Lu</name>
    </author>
    <author>
      <name>Yashesh Gaur</name>
    </author>
    <author>
      <name>Wei Zhou</name>
    </author>
    <author>
      <name>Benjamin Muller</name>
    </author>
    <author>
      <name>Jesus Villalba</name>
    </author>
    <author>
      <name>Najim Dehak</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <author>
      <name>Gargi Ghosh</name>
    </author>
    <author>
      <name>Mike Lewis</name>
    </author>
    <author>
      <name>Srinivasan Iyer</name>
    </author>
    <author>
      <name>Duc Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.06195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06191v1</id>
    <updated>2025-10-07T17:50:21Z</updated>
    <published>2025-10-07T17:50:21Z</published>
    <title>Rapid calibration of atrial electrophysiology models using Gaussian
  process emulators in the ensemble Kalman filter</title>
    <summary>  Atrial fibrillation (AF) is a common cardiac arrhythmia characterised by
disordered electrical activity in the atria. The standard treatment is catheter
ablation, which is invasive and irreversible. Recent advances in computational
electrophysiology offer the potential for patient-specific models, often
referred to as digital twins, that can be used to guide clinical decisions. To
be of practical value, we must be able to rapidly calibrate physics-based
models using routine clinical measurements. We pose this calibration task as a
static inverse problem, where the goal is to infer tissue-level
electrophysiological parameters from the available observations. To make this
tractable, we replace the expensive forward model with Gaussian process
emulators (GPEs), and propose a novel adaptation of the ensemble Kalman filter
(EnKF) for static non-linear inverse problems. The approach yields parameter
samples that can be interpreted as coming from the best Gaussian approximation
of the posterior distribution. We compare our results with those obtained using
Markov chain Monte Carlo (MCMC) sampling and demonstrate the potential of the
approach to enable near-real-time patient-specific calibration, a key step
towards predicting outcomes of AF treatment within clinical timescales. The
approach is readily applicable to a wide range of static inverse problems in
science and engineering.
</summary>
    <author>
      <name>Mariya Mamajiwala</name>
    </author>
    <author>
      <name>Cesare Corrado</name>
    </author>
    <author>
      <name>Chris Lanyon</name>
    </author>
    <author>
      <name>Steven A. Niederer</name>
    </author>
    <author>
      <name>Richard D. Wilkinson</name>
    </author>
    <author>
      <name>Richard H. Clayton</name>
    </author>
    <link href="http://arxiv.org/abs/2510.06191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06189v2</id>
    <updated>2025-10-08T01:21:49Z</updated>
    <published>2025-10-07T17:49:24Z</published>
    <title>Barbarians at the Gate: How AI is Upending Systems Research</title>
    <summary>  Artificial Intelligence (AI) is starting to transform the research process as
we know it by automating the discovery of new solutions. Given a task, the
typical AI-driven approach is (i) to generate a set of diverse solutions, and
then (ii) to verify these solutions and select one that solves the problem.
Crucially, this approach assumes the existence of a reliable verifier, i.e.,
one that can accurately determine whether a solution solves the given problem.
We argue that systems research, long focused on designing and evaluating new
performance-oriented algorithms, is particularly well-suited for AI-driven
solution discovery. This is because system performance problems naturally admit
reliable verifiers: solutions are typically implemented in real systems or
simulators, and verification reduces to running these software artifacts
against predefined workloads and measuring performance. We term this approach
as AI-Driven Research for Systems (ADRS), which iteratively generates,
evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS
instance, we present case studies across diverse domains, including load
balancing for multi-region cloud scheduling, Mixture-of-Experts inference,
LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS
discovers algorithms that outperform state-of-the-art human designs (e.g.,
achieving up to 5.0x runtime improvements or 50% cost reductions). We distill
best practices for guiding algorithm evolution, from prompt design to evaluator
construction, for existing frameworks. We then discuss the broader implications
for the systems community: as AI assumes a central role in algorithm design, we
argue that human researchers will increasingly focus on problem formulation and
strategic guidance. Our results highlight both the disruptive potential and the
urgent need to adapt systems research practices in the age of AI.
</summary>
    <author>
      <name>Audrey Cheng</name>
    </author>
    <author>
      <name>Shu Liu</name>
    </author>
    <author>
      <name>Melissa Pan</name>
    </author>
    <author>
      <name>Zhifei Li</name>
    </author>
    <author>
      <name>Bowen Wang</name>
    </author>
    <author>
      <name>Alex Krentsel</name>
    </author>
    <author>
      <name>Tian Xia</name>
    </author>
    <author>
      <name>Mert Cemri</name>
    </author>
    <author>
      <name>Jongseok Park</name>
    </author>
    <author>
      <name>Shuo Yang</name>
    </author>
    <author>
      <name>Jeff Chen</name>
    </author>
    <author>
      <name>Lakshya Agrawal</name>
    </author>
    <author>
      <name>Aditya Desai</name>
    </author>
    <author>
      <name>Jiarong Xing</name>
    </author>
    <author>
      <name>Koushik Sen</name>
    </author>
    <author>
      <name>Matei Zaharia</name>
    </author>
    <author>
      <name>Ion Stoica</name>
    </author>
    <link href="http://arxiv.org/abs/2510.06189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06182v1</id>
    <updated>2025-10-07T17:44:30Z</updated>
    <published>2025-10-07T17:44:30Z</published>
    <title>Mixing Mechanisms: How Language Models Retrieve Bound Entities
  In-Context</title>
    <summary>  A key component of in-context reasoning is the ability of language models
(LMs) to bind entities for later retrieval. For example, an LM might represent
"Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann"
when asked "Who loves pie?" Prior research on short lists of bound entities
found strong evidence that LMs implement such retrieval via a positional
mechanism, where "Ann" is retrieved based on its position in context. In this
work, we find that this mechanism generalizes poorly to more complex settings;
as the number of bound entities in context increases, the positional mechanism
becomes noisy and unreliable in middle positions. To compensate for this, we
find that LMs supplement the positional mechanism with a lexical mechanism
(retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism
(retrieving "Ann" through a direct pointer). Through extensive experiments on
nine models and ten binding tasks, we uncover a consistent pattern in how LMs
mix these mechanisms to drive model behavior. We leverage these insights to
develop a causal model combining all three mechanisms that estimates next token
distributions with 95% agreement. Finally, we show that our model generalizes
to substantially longer inputs of open-ended text interleaved with entity
groups, further demonstrating the robustness of our findings in more natural
settings. Overall, our study establishes a more complete picture of how LMs
bind and retrieve entities in-context.
</summary>
    <author>
      <name>Yoav Gur-Arieh</name>
    </author>
    <author>
      <name>Mor Geva</name>
    </author>
    <author>
      <name>Atticus Geiger</name>
    </author>
    <link href="http://arxiv.org/abs/2510.06182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06175v1</id>
    <updated>2025-10-07T17:35:28Z</updated>
    <published>2025-10-07T17:35:28Z</published>
    <title>VecInfer: Efficient LLM Inference with Low-Bit KV Cache via
  Outlier-Suppressed Vector Quantization</title>
    <summary>  The Key-Value (KV) cache introduces substantial memory overhead during large
language model (LLM) inference. Although existing vector quantization (VQ)
methods reduce KV cache usage and provide flexible representational capacity
across bit-widths, they suffer severe performance degradation at ultra-low
bit-widths due to key cache outliers that hinder effective codebook
utilization. To address this challenge, we propose VecInfer, a novel VQ method
for aggressive KV cache compression while enabling efficient inference. By
applying smooth and Hadamard transformations, VecInfer suppresses outliers in
the key cache, enabling the codebook to comprehensively cover the original data
distribution and thereby reducing quantization difficulty. To facilitate
efficient deployment, we design an optimized CUDA kernel that fuses computation
with dequantization to minimize memory access overhead. Extensive evaluations
demonstrate that VecInfer consistently outperforms existing quantization
baselines across both long-context understanding and mathematical reasoning
tasks. With only 2-bit quantization, VecInfer achieves performance comparable
to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in
large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in
single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.
</summary>
    <author>
      <name>Dingyu Yao</name>
    </author>
    <author>
      <name>Chenxu Yang</name>
    </author>
    <author>
      <name>Zhengyang Tong</name>
    </author>
    <author>
      <name>Zheng Lin</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Jian Luan</name>
    </author>
    <author>
      <name>Weiping Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.06175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06133v1</id>
    <updated>2025-10-07T17:08:33Z</updated>
    <published>2025-10-07T17:08:33Z</published>
    <title>CreditDecoding: Accelerating Parallel Decoding in Diffusion Large
  Language Models with Trace Credits</title>
    <summary>  Diffusion large language models (dLLMs) generate text through iterative
denoising steps, achieving parallel decoding by denoising only high-confidence
positions at each step. However, existing approaches often repetitively remask
tokens due to initially low confidence scores, leading to redundant iterations
and limiting overall acceleration. Through the analysis of dLLM decoding
traces, we observe that the model often determines the final prediction for a
token several steps before the decoding step. To leverage this historical
information and avoid redundant steps, we introduce the concept of Trace
Credit, which quantifies each token's convergence potential by accumulating
historical logits. Furthermore, we propose CreditDecoding, a training-free
parallel decoding algorithm that accelerates the confidence convergence of
correct but underconfident tokens by fusing current logits with Trace Credit.
This process significantly reduces redundant iterations and enhances decoding
robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup
and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times
speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.
Importantly, CreditDecoding scales effectively to long sequences and is
orthogonal to mainstream inference optimizations, making it a readily
integrable and versatile solution.
</summary>
    <author>
      <name>Kangyu Wang</name>
    </author>
    <author>
      <name>Zhiyun Jiang</name>
    </author>
    <author>
      <name>Haibo Feng</name>
    </author>
    <author>
      <name>Weijia Zhao</name>
    </author>
    <author>
      <name>Lin Liu</name>
    </author>
    <author>
      <name>Jianguo Li</name>
    </author>
    <author>
      <name>Zhenzhong Lan</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,8 figures,4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.06133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
