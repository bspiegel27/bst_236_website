<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-13T00:51:56Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-12T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">108626</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.08686v1</id>
    <updated>2025-03-11T17:59:46Z</updated>
    <published>2025-03-11T17:59:46Z</published>
    <title>OmniMamba: Efficient and Unified Multimodal Understanding and Generation
  via State Space Models</title>
    <summary>  Recent advancements in unified multimodal understanding and visual generation
(or multimodal generation) models have been hindered by their quadratic
computational complexity and dependence on large-scale training data. We
present OmniMamba, the first linear-architecture-based multimodal generation
model that generates both text and images through a unified next-token
prediction paradigm. The model fully leverages Mamba-2's high computational and
memory efficiency, extending its capabilities from text generation to
multimodal generation. To address the data inefficiency of existing unified
models, we propose two key innovations: (1) decoupled vocabularies to guide
modality-specific generation, and (2) task-specific LoRA for
parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage
training strategy to mitigate data imbalance between two tasks. Equipped with
these techniques, OmniMamba achieves competitive performance with JanusFlow
while surpassing Show-o across benchmarks, despite being trained on merely 2M
image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba
stands out with outstanding inference efficiency, achieving up to a 119.2 times
speedup and 63% GPU memory reduction for long-sequence generation compared to
Transformer-based counterparts. Code and models are released at
https://github.com/hustvl/OmniMamba
</summary>
    <author>
      <name>Jialv Zou</name>
    </author>
    <author>
      <name>Bencheng Liao</name>
    </author>
    <author>
      <name>Qian Zhang</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08685v1</id>
    <updated>2025-03-11T17:59:41Z</updated>
    <published>2025-03-11T17:59:41Z</published>
    <title>"Principal Components" Enable A New Language of Images</title>
    <summary>  We introduce a novel visual tokenization framework that embeds a provable
PCA-like structure into the latent token space. While existing visual
tokenizers primarily optimize for reconstruction fidelity, they often neglect
the structural properties of the latent space -- a critical factor for both
interpretability and downstream tasks. Our method generates a 1D causal token
sequence for images, where each successive token contributes non-overlapping
information with mathematically guaranteed decreasing explained variance,
analogous to principal component analysis. This structural constraint ensures
the tokenizer extracts the most salient visual features first, with each
subsequent token adding diminishing yet complementary information.
Additionally, we identified and resolved a semantic-spectrum coupling effect
that causes the unwanted entanglement of high-level semantic content and
low-level spectral details in the tokens by leveraging a diffusion decoder.
Experiments demonstrate that our approach achieves state-of-the-art
reconstruction performance and enables better interpretability to align with
the human vision system. Moreover, auto-regressive models trained on our token
sequences achieve performance comparable to current state-of-the-art methods
while requiring fewer tokens for training and inference.
</summary>
    <author>
      <name>Xin Wen</name>
    </author>
    <author>
      <name>Bingchen Zhao</name>
    </author>
    <author>
      <name>Ismail Elezi</name>
    </author>
    <author>
      <name>Jiankang Deng</name>
    </author>
    <author>
      <name>Xiaojuan Qi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally, project page:
  https://visual-gen.github.io/semanticist/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08684v1</id>
    <updated>2025-03-11T17:59:00Z</updated>
    <published>2025-03-11T17:59:00Z</published>
    <title>Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents</title>
    <summary>  Previous studies have found that PLM-based retrieval models exhibit a
preference for LLM-generated content, assigning higher relevance scores to
these documents even when their semantic quality is comparable to human-written
ones. This phenomenon, known as source bias, threatens the sustainable
development of the information access ecosystem. However, the underlying causes
of source bias remain unexplored. In this paper, we explain the process of
information retrieval with a causal graph and discover that PLM-based
retrievers learn perplexity features for relevance estimation, causing source
bias by ranking the documents with low perplexity higher. Theoretical analysis
further reveals that the phenomenon stems from the positive correlation between
the gradients of the loss functions in language modeling task and retrieval
task. Based on the analysis, a causal-inspired inference-time debiasing method
is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses
the bias effect of the perplexity and then separates the bias effect from the
overall estimated relevance score. Experimental results across three domains
demonstrate the superior debiasing effectiveness of CDC, emphasizing the
validity of our proposed explanatory framework. Source codes are available at
https://github.com/WhyDwelledOnAi/Perplexity-Trap.
</summary>
    <author>
      <name>Haoyu Wang</name>
    </author>
    <author>
      <name>Sunhao Dai</name>
    </author>
    <author>
      <name>Haiyuan Zhao</name>
    </author>
    <author>
      <name>Liang Pang</name>
    </author>
    <author>
      <name>Xiao Zhang</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Zhenhua Dong</name>
    </author>
    <author>
      <name>Jun Xu</name>
    </author>
    <author>
      <name>Ji-Rong Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08683v1</id>
    <updated>2025-03-11T17:58:42Z</updated>
    <published>2025-03-11T17:58:42Z</published>
    <title>CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous
  Driving</title>
    <summary>  Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise
for improving safety by addressing the perception and prediction uncertainties
inherent in single-agent systems. However, traditional cooperative methods are
constrained by rigid collaboration protocols and limited generalization to
unseen interactive scenarios. While LLM-based approaches offer generalized
reasoning capabilities, their challenges in spatial planning and unstable
inference latency hinder their direct application in cooperative driving. To
address these limitations, we propose CoLMDriver, the first full-pipeline
LLM-based cooperative driving system, enabling effective language-based
negotiation and real-time driving control. CoLMDriver features a parallel
driving pipeline with two key components: (i) an LLM-based negotiation module
under an actor-critic paradigm, which continuously refines cooperation policies
through feedback from previous decisions of all vehicles; and (ii) an
intention-guided waypoint generator, which translates negotiation outcomes into
executable waypoints. Additionally, we introduce InterDrive, a CARLA-based
simulation benchmark comprising 10 challenging interactive driving scenarios
for evaluating V2V cooperation. Experimental results demonstrate that
CoLMDriver significantly outperforms existing approaches, achieving an 11%
higher success rate across diverse highly interactive V2V driving scenarios.
Code will be released on https://github.com/cxliu0314/CoLMDriver.
</summary>
    <author>
      <name>Changxing Liu</name>
    </author>
    <author>
      <name>Genjia Liu</name>
    </author>
    <author>
      <name>Zijun Wang</name>
    </author>
    <author>
      <name>Jinchang Yang</name>
    </author>
    <author>
      <name>Siheng Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08678v1</id>
    <updated>2025-03-11T17:56:03Z</updated>
    <published>2025-03-11T17:56:03Z</published>
    <title>GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D
  Garment Reconstruction and Editing</title>
    <summary>  We introduce GarmentCrafter, a new approach that enables non-professional
users to create and modify 3D garments from a single-view image. While recent
advances in image generation have facilitated 2D garment design, creating and
editing 3D garments remains challenging for non-professional users. Existing
methods for single-view 3D reconstruction often rely on pre-trained generative
models to synthesize novel views conditioning on the reference image and camera
pose, yet they lack cross-view consistency, failing to capture the internal
relationships across different views. In this paper, we tackle this challenge
through progressive depth prediction and image warping to approximate novel
views. Subsequently, we train a multi-view diffusion model to complete occluded
and unknown clothing regions, informed by the evolving camera pose. By jointly
inferring RGB and depth, GarmentCrafter enforces inter-view coherence and
reconstructs precise geometries and fine details. Extensive experiments
demonstrate that our method achieves superior visual fidelity and inter-view
coherence compared to state-of-the-art single-view 3D garment reconstruction
methods.
</summary>
    <author>
      <name>Yuanhao Wang</name>
    </author>
    <author>
      <name>Cheng Zhang</name>
    </author>
    <author>
      <name>Gonçalo Frazão</name>
    </author>
    <author>
      <name>Jinlong Yang</name>
    </author>
    <author>
      <name>Alexandru-Eugen Ichim</name>
    </author>
    <author>
      <name>Thabo Beeler</name>
    </author>
    <author>
      <name>Fernando De la Torre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://humansensinglab.github.io/garment-crafter/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08665v1</id>
    <updated>2025-03-11T17:51:07Z</updated>
    <published>2025-03-11T17:51:07Z</published>
    <title>REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder</title>
    <summary>  We present a novel perspective on learning video embedders for generative
modeling: rather than requiring an exact reproduction of an input video, an
effective embedder should focus on synthesizing visually plausible
reconstructions. This relaxed criterion enables substantial improvements in
compression ratios without compromising the quality of downstream generative
models. Specifically, we propose replacing the conventional encoder-decoder
video embedder with an encoder-generator framework that employs a diffusion
transformer (DiT) to synthesize missing details from a compact latent space.
Therein, we develop a dedicated latent conditioning module to condition the DiT
decoder on the encoded video latent embedding. Our experiments demonstrate that
our approach enables superior encoding-decoding performance compared to
state-of-the-art methods, particularly as the compression ratio increases. To
demonstrate the efficacy of our approach, we report results from our video
embedders achieving a temporal compression ratio of up to 32x (8x higher than
leading video embedders) and validate the robustness of this ultra-compact
latent space for text-to-video generation, providing a significant efficiency
boost in latent diffusion model training and inference.
</summary>
    <author>
      <name>Yitian Zhang</name>
    </author>
    <author>
      <name>Long Mai</name>
    </author>
    <author>
      <name>Aniruddha Mahapatra</name>
    </author>
    <author>
      <name>David Bourgin</name>
    </author>
    <author>
      <name>Yicong Hong</name>
    </author>
    <author>
      <name>Jonah Casebeer</name>
    </author>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Yun Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08655v1</id>
    <updated>2025-03-11T17:46:46Z</updated>
    <published>2025-03-11T17:46:46Z</published>
    <title>On a new robust method of inference for general time series models</title>
    <summary>  In this article, we propose a novel logistic quasi-maximum likelihood
estimation (LQMLE) for general parametric time series models. Compared to the
classical Gaussian QMLE and existing robust estimations, it enjoys many
distinctive advantages, such as robustness in respect of distributional
misspecification and heavy-tailedness of the innovation, more resiliency to
outliers, smoothness and strict concavity of the log logistic quasi-likelihood
function, and boundedness of the influence function among others. Under some
mild conditions, we establish the strong consistency and asymptotic normality
of the LQMLE. Moreover, we propose a new and vital parameter identifiability
condition to ensure desirable asymptotics of the LQMLE. Further, based on the
LQMLE, we consider the Wald test and the Lagrange multiplier test for the
unknown parameters, and derive the limiting distributions of the corresponding
test statistics. The applicability of our methodology is demonstrated by
several time series models, including DAR, GARCH, ARMA-GARCH, DTARMACH, and
EXPAR. Numerical simulation studies are carried out to assess the finite-sample
performance of our methodology, and an empirical example is analyzed to
illustrate its usefulness.
</summary>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Xinghao Qiao</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <author>
      <name>Howell Tong</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08653v1</id>
    <updated>2025-03-11T17:44:05Z</updated>
    <published>2025-03-11T17:44:05Z</published>
    <title>Leveraging national forest inventory data to estimate forest carbon
  density status and trends for small areas</title>
    <summary>  National forest inventory (NFI) data are often costly to collect, which
inhibits efforts to estimate parameters of interest for small spatial,
temporal, or biophysical domains. Traditionally, design-based estimators are
used to estimate status of forest parameters of interest, but are unreliable
for small areas where data are sparse. Additionally, design-based estimates
constructed directly from the survey data are often unavailable when sample
sizes are especially small. Traditional model-based small area estimation
approaches, such as the Fay-Herriot (FH) model, rely on these direct estimates
for inference; hence, missing direct estimates preclude the use of such
approaches. Here, we detail a Bayesian spatio-temporal small area estimation
model that efficiently leverages sparse NFI data to estimate status and trends
for forest parameters. The proposed model bypasses the use of direct estimates
and instead uses plot-level NFI measurements along with auxiliary data
including remotely sensed tree canopy cover. We produce forest carbon estimates
from the United States NFI over 14 years across the contiguous US (CONUS) and
conduct a simulation study to assess our proposed model's accuracy, precision,
and bias, compared to that of a design-based estimator. The proposed model
provides improved precision and accuracy over traditional estimation methods,
and provides useful insights into county-level forest carbon dynamics across
the CONUS.
</summary>
    <author>
      <name>Elliot S. Shannon</name>
    </author>
    <author>
      <name>Andrew O. Finley</name>
    </author>
    <author>
      <name>Paul B. May</name>
    </author>
    <author>
      <name>Grant M. Domke</name>
    </author>
    <author>
      <name>Hans-Erik Andersen</name>
    </author>
    <author>
      <name>George C. Gaines III</name>
    </author>
    <author>
      <name>Arne Nothdurft</name>
    </author>
    <author>
      <name>Sudipto Banerjee</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08643v1</id>
    <updated>2025-03-11T17:36:11Z</updated>
    <published>2025-03-11T17:36:11Z</published>
    <title>Rethinking Diffusion Model in High Dimension</title>
    <summary>  Curse of Dimensionality is an unavoidable challenge in statistical
probability models, yet diffusion models seem to overcome this limitation,
achieving impressive results in high-dimensional data generation. Diffusion
models assume that they can learn the statistical properties of the underlying
probability distribution, enabling sampling from this distribution to generate
realistic samples. But is this really how they work? To address this question,
this paper conducts a detailed analysis of the objective function and inference
methods of diffusion models, leading to several important conclusions that help
answer the above question: 1) In high-dimensional sparse scenarios, the target
of the objective function fitting degrades from a weighted sum of multiple
samples to a single sample. 2) The mainstream inference methods can all be
represented within a simple unified framework, without requiring statistical
concepts such as Markov chains and SDEs. 3) Guided by this simple framework,
more efficient inference methods can be discovered.
</summary>
    <author>
      <name>Zhenxin Zheng</name>
    </author>
    <author>
      <name>Zhenjie Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08640v1</id>
    <updated>2025-03-11T17:30:58Z</updated>
    <published>2025-03-11T17:30:58Z</published>
    <title>Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse
  Attention</title>
    <summary>  Many-shot in-context learning has recently shown promise as an alternative to
finetuning, with the major advantage that the same model can be served for
multiple tasks. However, this shifts the computational burden from
training-time to inference-time, making deployment of many-shot ICL challenging
to justify in-practice. This cost is further increased if a custom
demonstration set is retrieved for each inference example. We present Dynamic
Block-Sparse Attention, a training-free framework for retrieval-based many-shot
in-context learning. By combining carefully designed block-sparse attention and
retrieval of cached groups of demonstrations, we achieve comparable per-example
latency to finetuning while maintaining on average &gt;95% of the best method's
accuracy across strong ICL and finetuning baselines. We hope that this will
further enable the deployment of many-shot ICL at scale.
</summary>
    <author>
      <name>Emily Xiao</name>
    </author>
    <author>
      <name>Chin-Jou Li</name>
    </author>
    <author>
      <name>Yilin Zhang</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <author>
      <name>Amanda Bertsch</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
