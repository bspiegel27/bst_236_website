<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-20T00:57:36Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-19T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">112369</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.11497v1</id>
    <updated>2025-05-16T17:59:40Z</updated>
    <published>2025-05-16T17:59:40Z</published>
    <title>QVGen: Pushing the Limit of Quantized Video Generative Models</title>
    <summary>  Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,
their substantial computational and memory demands pose serious challenges to
real-world deployment, even on high-end GPUs. As a commonly adopted solution,
quantization has proven notable success in reducing cost for image DMs, while
its direct application to video DMs remains ineffective. In this paper, we
present QVGen, a novel quantization-aware training (QAT) framework tailored for
high-performance and inference-efficient video DMs under extremely low-bit
quantization (e.g., 4-bit or below). We begin with a theoretical analysis
demonstrating that reducing the gradient norm is essential to facilitate
convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to
mitigate large quantization errors, leading to significantly enhanced
convergence. To eliminate the inference overhead of $\Phi$, we propose a
rank-decay strategy that progressively eliminates $\Phi$. Specifically, we
repeatedly employ singular value decomposition (SVD) and a proposed rank-based
regularization $\mathbf{\gamma}$ to identify and decay low-contributing
components. This strategy retains performance while zeroing out inference
overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,
with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the
first to reach full-precision comparable quality under 4-bit settings.
Moreover, it significantly outperforms existing methods. For instance, our
3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and
$+8.43$ in Scene Consistency on VBench.
</summary>
    <author>
      <name>Yushi Huang</name>
    </author>
    <author>
      <name>Ruihao Gong</name>
    </author>
    <author>
      <name>Jing Liu</name>
    </author>
    <author>
      <name>Yifu Ding</name>
    </author>
    <author>
      <name>Chengtao Lv</name>
    </author>
    <author>
      <name>Haotong Qin</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Our code will be released upon acceptance</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.11497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11484v1</id>
    <updated>2025-05-16T17:47:50Z</updated>
    <published>2025-05-16T17:47:50Z</published>
    <title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
    <summary>  Test-Time Scaling (TTS) refers to approaches that improve reasoning
performance by allocating extra computation during inference, without altering
the model's parameters. While existing TTS methods operate in a discrete token
space by generating more intermediate steps, recent studies in Coconut and
SoftCoT have demonstrated that thinking in the continuous latent space can
further enhance the reasoning performance. Such latent thoughts encode
informative thinking without the information loss associated with
autoregressive token generation, sparking increased interest in
continuous-space reasoning. Unlike discrete decoding, where repeated sampling
enables exploring diverse reasoning paths, latent representations in continuous
space are fixed for a given input, which limits diverse exploration, as all
decoded paths originate from the same latent thought. To overcome this
limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling
paradigm by enabling diverse exploration of thinking paths. Specifically, we
perturb latent thoughts via multiple specialized initial tokens and apply
contrastive learning to promote diversity among soft thought representations.
Experiments across five reasoning benchmarks and two distinct LLM architectures
demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms
SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility
with conventional scaling techniques such as self-consistency. Source code is
available at https://github.com/xuyige/SoftCoT.
</summary>
    <author>
      <name>Yige Xu</name>
    </author>
    <author>
      <name>Xu Guo</name>
    </author>
    <author>
      <name>Zhiwei Zeng</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.11484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11483v1</id>
    <updated>2025-05-16T17:47:15Z</updated>
    <published>2025-05-16T17:47:15Z</published>
    <title>msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural
  Networks for TinyML</title>
    <summary>  AI spans from large language models to tiny models running on
microcontrollers (MCUs). Extremely memory-efficient model architectures are
decisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,
inference latency must remain small to fit real-time constraints. An approach
to tackle this is patch-based fusion, which aims to optimize data flows across
neural network layers. In this paper, we introduce msf-CNN, a novel technique
that efficiently finds optimal fusion settings for convolutional neural
networks (CNNs) by walking through the fusion solution space represented as a
directed acyclic graph. Compared to previous work on CNN fusion for MCUs,
msf-CNN identifies a wider set of solutions. We published an implementation of
msf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We
show that msf-CNN can achieve inference using 50% less RAM compared to the
prior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers
additional flexibility for system designers.
</summary>
    <author>
      <name>Zhaolan Huang</name>
    </author>
    <author>
      <name>Emmanuel Baccelli</name>
    </author>
    <link href="http://arxiv.org/abs/2505.11483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11470v1</id>
    <updated>2025-05-16T17:25:40Z</updated>
    <published>2025-05-16T17:25:40Z</published>
    <title>No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies</title>
    <summary>  We introduce two reference-free metrics for quality evaluation of taxonomies.
The first metric evaluates robustness by calculating the correlation between
semantic and taxonomic similarity, covering a type of error not handled by
existing metrics. The second uses Natural Language Inference to assess logical
adequacy. Both metrics are tested on five taxonomies and are shown to correlate
well with F1 against gold-standard taxonomies.
</summary>
    <author>
      <name>Pascal Wullschleger</name>
    </author>
    <author>
      <name>Majid Zarharan</name>
    </author>
    <author>
      <name>Donnacha Daly</name>
    </author>
    <author>
      <name>Marc Pouly</name>
    </author>
    <author>
      <name>Jennifer Foster</name>
    </author>
    <link href="http://arxiv.org/abs/2505.11470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11460v1</id>
    <updated>2025-05-16T17:14:31Z</updated>
    <published>2025-05-16T17:14:31Z</published>
    <title>Mechanistic inference of stochastic gene expression from structured
  single-cell data</title>
    <summary>  Single-cell gene expression measurements encode variability spanning
molecular noise, cellular heterogeneity, and technical artifacts. Mechanistic
models provide a principled framework to disentangle these sources and extract
insight, but inferring underlying dynamics from standard sequencing count data
faces fundamental limitations. Structured datasets with temporal, spatial, or
multimodal features offer constraints that help resolve these ambiguities, but
demand more complex models and advanced inference strategies, including machine
learning techniques with associated tradeoffs. This review highlights recent
progress in the judicious integration of structured single-cell data,
stochastic model development, and innovative inference strategies to extract
gene-level insights. These approaches lay the foundation for mechanistic
understanding of regulatory networks and multicellular systems.
</summary>
    <author>
      <name>Christopher E. Miles</name>
    </author>
    <link href="http://arxiv.org/abs/2505.11460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11444v1</id>
    <updated>2025-05-16T17:00:52Z</updated>
    <published>2025-05-16T17:00:52Z</published>
    <title>A Generative Framework for Causal Estimation via Importance-Weighted
  Diffusion Distillation</title>
    <summary>  Estimating individualized treatment effects from observational data is a
central challenge in causal inference, largely due to covariate imbalance and
confounding bias from non-randomized treatment assignment. While inverse
probability weighting (IPW) is a well-established solution to this problem, its
integration into modern deep learning frameworks remains limited. In this work,
we propose Importance-Weighted Diffusion Distillation (IWDD), a novel
generative framework that combines the pretraining of diffusion models with
importance-weighted score distillation to enable accurate and fast causal
estimation-including potential outcome prediction and treatment effect
estimation. We demonstrate how IPW can be naturally incorporated into the
distillation of pretrained diffusion models, and further introduce a
randomization-based adjustment that eliminates the need to compute IPW
explicitly-thereby simplifying computation and, more importantly, provably
reducing the variance of gradient estimates. Empirical results show that IWDD
achieves state-of-the-art out-of-sample prediction performance, with the
highest win rates compared to other baselines, significantly improving causal
estimation and supporting the development of individualized treatment
strategies. We will release our PyTorch code for reproducibility and future
research.
</summary>
    <author>
      <name>Xinran Song</name>
    </author>
    <author>
      <name>Tianyu Chen</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2505.11444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11438v1</id>
    <updated>2025-05-16T16:57:42Z</updated>
    <published>2025-05-16T16:57:42Z</published>
    <title>Inferring correlated distributions: boosted top jets</title>
    <summary>  Improving the understanding of signal and background distributions in
signal-region is a valuable key to enhance any analysis in collider physics.
This is usually a difficult task because -- among others -- signal and
backgrounds are hard to discriminate in signal-region, simulations may reach a
limit of reliability if they need to model non-perturbative QCD, and
distributions are multi-dimensional and many times may be correlated within
each class. Bayesian density estimation is a technique that leverages prior
knowledge and data correlations to effectively extract information from data in
signal-region. In this work we extend previous works on data-driven mixture
models for meaningful unsupervised signal extraction in collider physics to
incorporate correlations between features. Using a standard dataset of top and
QCD jets, we show how simulators, despite having an expected bias, can be used
to inject sufficient inductive nuance into an inference model in terms of
priors to then be corrected by data and estimate the true correlated
distributions between features within each class. We compare the model with and
without correlations to show how the signal extraction is sensitive to their
inclusion and we quantify the improvement due to the inclusion of correlations
using both supervised and unsupervised metrics.
</summary>
    <author>
      <name>Ezequiel Alvarez</name>
    </author>
    <author>
      <name>Manuel Szewc</name>
    </author>
    <author>
      <name>Alejandro Szynkman</name>
    </author>
    <author>
      <name>Santiago Tanco</name>
    </author>
    <author>
      <name>Tatiana Tarutina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 12 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.11438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11417v1</id>
    <updated>2025-05-16T16:29:21Z</updated>
    <published>2025-05-16T16:29:21Z</published>
    <title>EdgeWisePersona: A Dataset for On-Device User Profiling from Natural
  Language Interactions</title>
    <summary>  This paper introduces a novel dataset and evaluation benchmark designed to
assess and improve small language models deployable on edge devices, with a
focus on user profiling from multi-session natural language interactions in
smart home environments. At the core of the dataset are structured user
profiles, each defined by a set of routines - context-triggered, repeatable
patterns of behavior that govern how users interact with their home systems.
Using these profiles as input, a large language model (LLM) generates
corresponding interaction sessions that simulate realistic, diverse, and
context-aware dialogues between users and their devices.
  The primary task supported by this dataset is profile reconstruction:
inferring user routines and preferences solely from interactions history. To
assess how well current models can perform this task under realistic
conditions, we benchmarked several state-of-the-art compact language models and
compared their performance against large foundation models. Our results show
that while small models demonstrate some capability in reconstructing profiles,
they still fall significantly short of large models in accurately capturing
user behavior. This performance gap poses a major challenge - particularly
because on-device processing offers critical advantages, such as preserving
user privacy, minimizing latency, and enabling personalized experiences without
reliance on the cloud. By providing a realistic, structured testbed for
developing and evaluating behavioral modeling under these constraints, our
dataset represents a key step toward enabling intelligent, privacy-respecting
AI systems that learn and adapt directly on user-owned devices.
</summary>
    <author>
      <name>Patryk Bartkowiak</name>
    </author>
    <author>
      <name>Michal Podstawski</name>
    </author>
    <link href="http://arxiv.org/abs/2505.11417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11416v1</id>
    <updated>2025-05-16T16:29:19Z</updated>
    <published>2025-05-16T16:29:19Z</published>
    <title>MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron
  Selection</title>
    <summary>  Modern neural networks often activate all neurons for every input, leading to
unnecessary computation and inefficiency. We introduce Matrix-Interpolated
Dropout Layer (MID-L), a novel module that dynamically selects and activates
only the most informative neurons by interpolating between two transformation
paths via a learned, input-dependent gating vector. Unlike conventional dropout
or static sparsity methods, MID-L employs a differentiable Top-k masking
strategy, enabling per-input adaptive computation while maintaining end-to-end
differentiability. MID-L is model-agnostic and integrates seamlessly into
existing architectures. Extensive experiments on six benchmarks, including
MNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves
up to average 55\% reduction in active neurons, 1.7$\times$ FLOPs savings, and
maintains or exceeds baseline accuracy. We further validate the informativeness
and selectivity of the learned neurons via Sliced Mutual Information (SMI) and
observe improved robustness under overfitting and noisy data conditions.
Additionally, MID-L demonstrates favorable inference latency and memory usage
profiles, making it suitable for both research exploration and deployment on
compute-constrained systems. These results position MID-L as a general-purpose,
plug-and-play dynamic computation layer, bridging the gap between dropout
regularization and efficient inference.
</summary>
    <author>
      <name>Pouya Shaeri</name>
    </author>
    <author>
      <name>Ariane Middel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted in a Computer Science Conference, currently in Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.11416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11409v1</id>
    <updated>2025-05-16T16:17:22Z</updated>
    <published>2025-05-16T16:17:22Z</published>
    <title>Visual Planning: Let's Think Only with Images</title>
    <summary>  Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.
</summary>
    <author>
      <name>Yi Xu</name>
    </author>
    <author>
      <name>Chengzu Li</name>
    </author>
    <author>
      <name>Han Zhou</name>
    </author>
    <author>
      <name>Xingchen Wan</name>
    </author>
    <author>
      <name>Caiqi Zhang</name>
    </author>
    <author>
      <name>Anna Korhonen</name>
    </author>
    <author>
      <name>Ivan VuliÄ‡</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables
  including references and appendices)</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.11409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
