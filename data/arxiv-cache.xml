<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-25T01:01:28Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-24T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117485</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.17728v1</id>
    <updated>2025-07-23T17:43:07Z</updated>
    <published>2025-07-23T17:43:07Z</published>
    <title>Megrez2 Technical Report</title>
    <summary>  We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.
</summary>
    <author>
      <name>Boxun Li</name>
    </author>
    <author>
      <name>Yadong Li</name>
    </author>
    <author>
      <name>Zhiyuan Li</name>
    </author>
    <author>
      <name>Congyi Liu</name>
    </author>
    <author>
      <name>Weilin Liu</name>
    </author>
    <author>
      <name>Guowei Niu</name>
    </author>
    <author>
      <name>Zheyue Tan</name>
    </author>
    <author>
      <name>Haiyang Xu</name>
    </author>
    <author>
      <name>Zhuyu Yao</name>
    </author>
    <author>
      <name>Tao Yuan</name>
    </author>
    <author>
      <name>Dong Zhou</name>
    </author>
    <author>
      <name>Yueqing Zhuang</name>
    </author>
    <author>
      <name>Bo Zhao</name>
    </author>
    <author>
      <name>Guohao Dai</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2507.17728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17700v1</id>
    <updated>2025-07-23T17:08:53Z</updated>
    <published>2025-07-23T17:08:53Z</published>
    <title>From Atoms to Dynamics: Learning the Committor Without Collective
  Variables</title>
    <summary>  This Brief Communication introduces a graph-neural-network architecture built
on geometric vector perceptrons to predict the committor function directly from
atomic coordinates, bypassing the need for hand-crafted collective variables
(CVs). The method offers atom-level interpretability, pinpointing the key
atomic players in complex transitions without relying on prior assumptions.
Applied across diverse molecular systems, the method accurately infers the
committor function and highlights the importance of each heavy atom in the
transition mechanism. It also yields precise estimates of the rate constants
for the underlying processes. The proposed approach opens new avenues for
understanding and modeling complex dynamics, by enabling CV-free learning and
automated identification of physically meaningful reaction coordinates of
complex molecular processes.
</summary>
    <author>
      <name>Sergio Contreras Arredondo</name>
    </author>
    <author>
      <name>Chenyu Tang</name>
    </author>
    <author>
      <name>Radu A. Talmazan</name>
    </author>
    <author>
      <name>Alberto Meg√≠as</name>
    </author>
    <author>
      <name>Cheng Giuseppe Chen</name>
    </author>
    <author>
      <name>Christophe Chipot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages (including supplementary information with 13 pages), 15
  figures (5 figures in the main text and 10 figures in the supplementary
  information)</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.17700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17697v1</id>
    <updated>2025-07-23T17:03:33Z</updated>
    <published>2025-07-23T17:03:33Z</published>
    <title>Frequentist Asymptotics of Variational Laplace</title>
    <summary>  Variational inference is a general framework to obtain approximations to the
posterior distribution in a Bayesian context. In essence, variational inference
entails an optimization over a given family of probability distributions to
choose the member of this family best approximating the posterior. Variational
Laplace, an iterative update scheme motivated by this objective, is widely used
in different contexts in the cognitive neuroscience community. However, until
now, the theoretical properties of this scheme have not been systematically
investigated. Here, we study variational Laplace in the light of frequentist
asymptotic statistics. Asymptotical frequentist theory enables one to judge the
quality of point estimates by their limit behaviour. We apply this framework to
find that point estimates generated by variational Laplace enjoy the desirable
properties of asymptotic consistency and efficiency in two toy examples.
Furthermore, we derive conditions that are sufficient to establish these
properties in a general setting. Besides of point estimates, we also study the
frequentist convergence of distributions in the sense of total variation
distance, which may be useful to relate variational Laplace both to recent
findings regarding variational inference as well as to classical frequentist
considerations on the Bayesian posterior. Finally, to illustrate the validity
of our theoretical considerations, we conduct simulation experiments in our
study examples.
</summary>
    <author>
      <name>Janis Keck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 3 figures, originally submitted as a master's thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.17697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17687v1</id>
    <updated>2025-07-23T16:51:23Z</updated>
    <published>2025-07-23T16:51:23Z</published>
    <title>Towards Effective Open-set Graph Class-incremental Learning</title>
    <summary>  Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)
to adapt to evolving graph analytical tasks by incrementally learning new class
knowledge while retaining knowledge of old classes. Existing GCIL methods
primarily focus on a closed-set assumption, where all test samples are presumed
to belong to previously known classes. Such an assumption restricts their
applicability in real-world scenarios, where unknown classes naturally emerge
during inference, and are absent during training. In this paper, we explore a
more challenging open-set graph class-incremental learning scenario with two
intertwined challenges: catastrophic forgetting of old classes, which impairs
the detection of unknown classes, and inadequate open-set recognition, which
destabilizes the retention of learned knowledge. To address the above problems,
a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding
generation to effectively mitigate catastrophic forgetting and enable robust
detection of unknown classes. To be specific, a prototypical conditional
variational autoencoder is designed to synthesize node embeddings for old
classes, enabling knowledge replay without storing raw graph data. To handle
unknown classes, we employ a mixing-based strategy to generate
out-of-distribution (OOD) samples from pseudo in-distribution and current node
embeddings. A novel prototypical hypersphere classification loss is further
proposed, which anchors in-distribution embeddings to their respective class
prototypes, while repelling OOD embeddings away. Instead of assigning all
unknown samples into one cluster, our proposed objective function explicitly
models them as outliers through prototype-aware rejection regions, ensuring a
robust open-set recognition. Extensive experiments on five benchmarks
demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN
methods.
</summary>
    <author>
      <name>Jiazhen Chen</name>
    </author>
    <author>
      <name>Zheng Ma</name>
    </author>
    <author>
      <name>Sichao Fu</name>
    </author>
    <author>
      <name>Mingbin Feng</name>
    </author>
    <author>
      <name>Tony S. Wirjanto</name>
    </author>
    <author>
      <name>Weihua Ou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by 33rd ACM International Conference on Multimedia (MM 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.17687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17686v1</id>
    <updated>2025-07-23T16:51:09Z</updated>
    <published>2025-07-23T16:51:09Z</published>
    <title>Debiased maximum-likelihood estimators for hazard ratios under
  machine-learning adjustment</title>
    <summary>  Previous studies have shown that hazard ratios between treatment groups
estimated with the Cox model are uninterpretable because the indefinite
baseline hazard of the model fails to identify temporal change in the risk set
composition due to treatment assignment and unobserved factors among multiple,
contradictory scenarios. To alleviate this problem, especially in studies based
on observational data with uncontrolled dynamic treatment and real-time
measurement of many covariates, we propose abandoning the baseline hazard and
using machine learning to explicitly model the change in the risk set with or
without latent variables. For this framework, we clarify the context in which
hazard ratios can be causally interpreted, and then develop a method based on
Neyman orthogonality to compute debiased maximum-likelihood estimators of
hazard ratios. Computing the constructed estimators is more efficient than
computing those based on weighted regression with marginal structural Cox
models. Numerical simulations confirm that the proposed method identifies the
ground truth with minimal bias. These results lay the foundation for developing
a useful, alternative method for causal inference with uncontrolled,
observational data in modern epidemiology.
</summary>
    <author>
      <name>Takashi Hayakawa</name>
    </author>
    <author>
      <name>Satoshi Asai</name>
    </author>
    <link href="http://arxiv.org/abs/2507.17686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17659v1</id>
    <updated>2025-07-23T16:24:57Z</updated>
    <published>2025-07-23T16:24:57Z</published>
    <title>See the Forest and the Trees: A Synergistic Reasoning Framework for
  Knowledge-Based Visual Question Answering</title>
    <summary>  Multimodal Large Language Models (MLLMs) have pushed the frontiers of
Knowledge-Based Visual Question Answering (KBVQA), yet their reasoning is
fundamentally bottlenecked by a reliance on uni-dimensional evidence. This
"seeing only the trees, but not the forest" approach prevents robust,
multi-faceted understanding. Inspired by the principle of seeing both the
forest and trees, we propose Synergos-VQA, a novel synergistic reasoning
framework. At its core, Synergos-VQA concurrently generates and fuses three
complementary evidence streams at inference time: (1) Holistic Evidence to
perceive the entire scene (the "forest"), (2) Structural Evidence from a
prototype-driven module to identify key objects (the "trees"), and (3) Causal
Evidence from a counterfactual probe to ensure the reasoning is robustly
grounded. By synergistically fusing this multi-faceted evidence, our framework
achieves a more comprehensive and reliable reasoning process. Extensive
experiments show that Synergos-VQA decisively establishes a new
state-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.
Furthermore, our approach demonstrates strong plug-and-play capabilities,
significantly boosting various open-source MLLMs and proving that superior
methodological design can outperform sheer model scale.
</summary>
    <author>
      <name>Junjie Wang</name>
    </author>
    <author>
      <name>Yunhan Tang</name>
    </author>
    <author>
      <name>Yijie Wang</name>
    </author>
    <author>
      <name>Zhihao Yuan</name>
    </author>
    <author>
      <name>Huan Wang</name>
    </author>
    <author>
      <name>Yangfan He</name>
    </author>
    <author>
      <name>Bin Li</name>
    </author>
    <link href="http://arxiv.org/abs/2507.17659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17618v1</id>
    <updated>2025-07-23T15:49:03Z</updated>
    <published>2025-07-23T15:49:03Z</published>
    <title>A Hybrid Early-Exit Algorithm for Large Language Models Based on Space
  Alignment Decoding (SPADE)</title>
    <summary>  Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.
</summary>
    <author>
      <name>Bowen Zheng</name>
    </author>
    <author>
      <name>Ming Ma</name>
    </author>
    <author>
      <name>Zhongqiao Lin</name>
    </author>
    <author>
      <name>Tianming Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2507.17618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17617v1</id>
    <updated>2025-07-23T15:48:16Z</updated>
    <published>2025-07-23T15:48:16Z</published>
    <title>Reusing Attention for One-stage Lane Topology Understanding</title>
    <summary>  Understanding lane toplogy relationships accurately is critical for safe
autonomous driving. However, existing two-stage methods suffer from
inefficiencies due to error propagations and increased computational overheads.
To address these challenges, we propose a one-stage architecture that
simultaneously predicts traffic elements, lane centerlines and topology
relationship, improving both the accuracy and inference speed of lane topology
understanding for autonomous driving. Our key innovation lies in reusing
intermediate attention resources within distinct transformer decoders. This
approach effectively leverages the inherent relational knowledge within the
element detection module to enable the modeling of topology relationships among
traffic elements and lanes without requiring additional computationally
expensive graph networks. Furthermore, we are the first to demonstrate that
knowledge can be distilled from models that utilize standard definition (SD)
maps to those operates without using SD maps, enabling superior performance
even in the absence of SD maps. Extensive experiments on the OpenLane-V2
dataset show that our approach outperforms baseline methods in both accuracy
and efficiency, achieving superior results in lane detection, traffic element
identification, and topology reasoning. Our code is available at
https://github.com/Yang-Li-2000/one-stage.git.
</summary>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Zongzheng Zhang</name>
    </author>
    <author>
      <name>Xuchong Qiu</name>
    </author>
    <author>
      <name>Xinrun Li</name>
    </author>
    <author>
      <name>Ziming Liu</name>
    </author>
    <author>
      <name>Leichen Wang</name>
    </author>
    <author>
      <name>Ruikai Li</name>
    </author>
    <author>
      <name>Zhenxin Zhu</name>
    </author>
    <author>
      <name>Huan-ang Gao</name>
    </author>
    <author>
      <name>Xiaojian Lin</name>
    </author>
    <author>
      <name>Zhiyong Cui</name>
    </author>
    <author>
      <name>Hang Zhao</name>
    </author>
    <author>
      <name>Hao Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IROS 2025, Project Page:
  https://github.com/Yang-Li-2000/one-stage.git</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.17617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17600v1</id>
    <updated>2025-07-23T15:30:59Z</updated>
    <published>2025-07-23T15:30:59Z</published>
    <title>Nonparametric inference for nonstationary spatial point processes</title>
    <summary>  Point pattern data often exhibit features such as abrupt changes, hotspots
and spatially varying dependence in local intensity. Under a Poisson process
framework, these correspond to discontinuities and nonstationarity in the
underlying intensity function -- features that are difficult to capture with
standard modeling approaches. This paper proposes a spatial Cox process model
in which nonstationarity is induced through a random partition of the spatial
domain, with conditionally independent Gaussian process priors specified across
the resulting regions. This construction allows for heterogeneous spatial
behavior, including sharp transitions in intensity. To ensure exact inference,
a discretization-free MCMC algorithm is developed to target the
infinite-dimensional posterior distribution without approximation. The random
partition framework also reduces the computational burden typically associated
with Gaussian process models. Spatial covariates can be incorporated to account
for structured variation in intensity. The proposed methodology is evaluated
through synthetic examples and real-world applications, demonstrating its
ability to flexibly capture complex spatial structures. The paper concludes
with a discussion of potential extensions and directions for future work.
</summary>
    <author>
      <name>Izabel Nolau</name>
    </author>
    <author>
      <name>Fl√°vio B. Gon√ßalves</name>
    </author>
    <author>
      <name>Dani Gamerman</name>
    </author>
    <link href="http://arxiv.org/abs/2507.17600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.17596v2</id>
    <updated>2025-07-24T11:04:42Z</updated>
    <published>2025-07-23T15:28:23Z</published>
    <title>PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving</title>
    <summary>  While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.
</summary>
    <author>
      <name>Maciej K. Wozniak</name>
    </author>
    <author>
      <name>Lianhang Liu</name>
    </author>
    <author>
      <name>Yixi Cai</name>
    </author>
    <author>
      <name>Patric Jensfelt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.17596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.17596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
