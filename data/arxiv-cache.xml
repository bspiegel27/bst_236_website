<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-08T00:56:01Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-07T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">120314</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.04439v1</id>
    <updated>2025-09-04T17:54:19Z</updated>
    <published>2025-09-04T17:54:19Z</published>
    <title>ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</title>
    <summary>  While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.
</summary>
    <author>
      <name>Matthew Ho</name>
    </author>
    <author>
      <name>Chen Si</name>
    </author>
    <author>
      <name>Zhaoxiang Feng</name>
    </author>
    <author>
      <name>Fangxu Yu</name>
    </author>
    <author>
      <name>Zhijian Liu</name>
    </author>
    <author>
      <name>Zhiting Hu</name>
    </author>
    <author>
      <name>Lianhui Qin</name>
    </author>
    <link href="http://arxiv.org/abs/2509.04439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04437v1</id>
    <updated>2025-09-04T17:53:45Z</updated>
    <published>2025-09-04T17:53:45Z</published>
    <title>From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray
  Collimators via Hough Transform</title>
    <summary>  Collimation in X-ray imaging restricts exposure to the region-of-interest
(ROI) and minimizes the radiation dose applied to the patient. The detection of
collimator shadows is an essential image-based preprocessing step in digital
radiography posing a challenge when edges get obscured by scattered X-ray
radiation. Regardless, the prior knowledge that collimation forms
polygonal-shaped shadows is evident. For this reason, we introduce a deep
learning-based segmentation that is inherently constrained to its geometry. We
achieve this by incorporating a differentiable Hough transform-based network to
detect the collimation borders and enhance its capability to extract the
information about the ROI center. During inference, we combine the information
of both tasks to enable the generation of refined, line-constrained
segmentation masks. We demonstrate robust reconstruction of collimated regions
achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real
Xray images. While this application involves at most four shadow borders, our
method is not fundamentally limited by a specific number of edges.
</summary>
    <author>
      <name>Benjamin El-Zein</name>
    </author>
    <author>
      <name>Dominik Eckert</name>
    </author>
    <author>
      <name>Andreas Fieselmann</name>
    </author>
    <author>
      <name>Christopher Syben</name>
    </author>
    <author>
      <name>Ludwig Ritschl</name>
    </author>
    <author>
      <name>Steffen Kappler</name>
    </author>
    <author>
      <name>Sebastian Stober</name>
    </author>
    <link href="http://arxiv.org/abs/2509.04437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04415v1</id>
    <updated>2025-09-04T17:37:35Z</updated>
    <published>2025-09-04T17:37:35Z</published>
    <title>Interpretable Clustering with Adaptive Heterogeneous Causal Structure
  Learning in Mixed Observational Data</title>
    <summary>  Understanding causal heterogeneity is essential for scientific discovery in
domains such as biology and medicine. However, existing methods lack causal
awareness, with insufficient modeling of heterogeneity, confounding, and
observational constraints, leading to poor interpretability and difficulty
distinguishing true causal heterogeneity from spurious associations. We propose
an unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering
with Adaptive Heterogeneous Causal Structure Learning), that jointly infers
latent clusters and their associated causal structures from mixed-type
observational data without requiring temporal ordering, environment labels,
interventions or other prior knowledge. HCL relaxes the homogeneity and
sufficiency assumptions by introducing an equivalent representation that
encodes both structural heterogeneity and confounding. It further develops a
bi-directional iterative strategy to alternately refine causal clustering and
structure learning, along with a self-supervised regularization that balance
cross-cluster universality and specificity. Together, these components enable
convergence toward interpretable, heterogeneous causal patterns. Theoretically,
we show identifiability of heterogeneous causal structures under mild
conditions. Empirically, HCL achieves superior performance in both clustering
and structure learning tasks, and recovers biologically meaningful mechanisms
in real-world single-cell perturbation data, demonstrating its utility for
discovering interpretable, mechanism-level causal heterogeneity.
</summary>
    <author>
      <name>Wenrui Li</name>
    </author>
    <author>
      <name>Qinghao Zhang</name>
    </author>
    <author>
      <name>Xiaowo Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2509.04415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04406v1</id>
    <updated>2025-09-04T17:24:31Z</updated>
    <published>2025-09-04T17:24:31Z</published>
    <title>Few-step Flow for 3D Generation via Marginal-Data Transport Distillation</title>
    <summary>  Flow-based 3D generation models typically require dozens of sampling steps
during inference. Though few-step distillation methods, particularly
Consistency Models (CMs), have achieved substantial advancements in
accelerating 2D diffusion models, they remain under-explored for more complex
3D generation tasks. In this study, we propose a novel framework, MDT-dist, for
few-step 3D flow distillation. Our approach is built upon a primary objective:
distilling the pretrained model to learn the Marginal-Data Transport. Directly
learning this objective needs to integrate the velocity fields, while this
integral is intractable to be implemented. Therefore, we propose two
optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),
to equivalently convert the optimization target from the transport level to the
velocity and the distribution level respectively. Velocity Matching (VM) learns
to stably match the velocity fields between the student and the teacher, but
inevitably provides biased gradient estimates. Velocity Distillation (VD)
further enhances the optimization process by leveraging the learned velocity
fields to perform probability density distillation. When evaluated on the
pioneer 3D generation framework TRELLIS, our method reduces sampling steps of
each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s
(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high
visual and geometric fidelity. Extensive experiments demonstrate that our
method significantly outperforms existing CM distillation methods, and enables
TRELLIS to achieve superior performance in few-step 3D generation.
</summary>
    <author>
      <name>Zanwei Zhou</name>
    </author>
    <author>
      <name>Taoran Yi</name>
    </author>
    <author>
      <name>Jiemin Fang</name>
    </author>
    <author>
      <name>Chen Yang</name>
    </author>
    <author>
      <name>Lingxi Xie</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
    <author>
      <name>Wei Shen</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://github.com/Zanue/MDT-dist</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.04406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04405v1</id>
    <updated>2025-09-04T17:18:04Z</updated>
    <published>2025-09-04T17:18:04Z</published>
    <title>Generation of Lognormal Synthetic Lyman-$α$ Forest Spectra for
  $P_{1D}$ Analysis</title>
    <summary>  The one-dimensional flux power spectrum (P1D) of the Lyman-$\alpha$ forest
probes small-scale structure in the intergalactic medium (IGM) and is therefore
sensitive to a variety of cosmological and astrophysical parameters. These
include the amplitude and shape of the matter power spectrum, the thermal
history of the IGM, the sum of neutrino masses, and potential small-scale
fluctuations due to the nature of dark matter. However, P1D is also highly
sensitive to observational and instrumental systematics, making accurate
synthetic spectra essential for validating analyses and quantifying these
effects, especially in high-volume surveys like the Dark Energy Spectroscopic
Instrument (DESI). We present an efficient lognormal mock framework for
generating one-dimensional Lyman-$\alpha$ forest spectra tailored for P1D
analysis. Our method captures the redshift evolution of the mean transmitted
flux and the scale-dependent shape and amplitude of the one-dimensional flux
power spectrum by tuning Gaussian field correlations and transformation
parameters. Across the DESI Early Data Release (EDR) redshift range ($2.0 \leq
z \leq 3.8$), and a wide range of scales ($10^{-4}$ s km$^{-1} \leq k \leq 1.0$
s km$^{-1}$), our mocks recover the mean flux evolution with redshift to
sub-percent accuracy, and the P1D at the percent level. Additionally, we
discuss potential extensions of this framework, such as the incorporation of
astrophysical contaminants, continuum uncertainties, and instrumental effects.
Such improvements would expand its utility in ongoing and upcoming surveys and
enable a broader range of validation efforts and systematics studies for P1D
inference and precision cosmology.
</summary>
    <author>
      <name>Meagan Herbold</name>
    </author>
    <author>
      <name>Naim Göksel Karaçaylı</name>
    </author>
    <author>
      <name>Paul Martini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures, 1 table, prepared for submission to JCAP</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.04405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04399v1</id>
    <updated>2025-09-04T17:10:15Z</updated>
    <published>2025-09-04T17:10:15Z</published>
    <title>Leveraging Equivariances and Symmetries in the Control Barrier Function
  Synthesis</title>
    <summary>  The synthesis of Control Barrier Functions (CBFs) often involves demanding
computations or a meticulous construction. However, structural properties of
the system dynamics and constraints have the potential to mitigate these
challenges. In this paper, we explore how equivariances in the dynamics,
loosely speaking a form of symmetry, can be leveraged in the CBF synthesis.
Although CBFs are generally not inherently symmetric, we show how equivariances
in the dynamics and symmetries in the constraints induce symmetries in CBFs
derived through reachability analysis. This insight allows us to infer their
CBF values across the entire domain from their values on a subset, leading to
significant computational savings. Interestingly, equivariances can be even
leveraged to the CBF synthesis for non-symmetric constraints. Specifically, we
show how a partially known CBF can be leveraged together with equivariances to
construct a CBF for various new constraints. Throughout the paper, we provide
examples illustrating the theoretical findings. Furthermore, a numerical study
investigates the computational gains from invoking equivariances into the CBF
synthesis.
</summary>
    <author>
      <name>Adrian Wiltz</name>
    </author>
    <author>
      <name>Dimos V. Dimarogonas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.04399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04398v1</id>
    <updated>2025-09-04T17:10:01Z</updated>
    <published>2025-09-04T17:10:01Z</published>
    <title>IPA: An Information-Preserving Input Projection Framework for Efficient
  Foundation Model Adaptation</title>
    <summary>  Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce
adaptation cost by injecting low-rank updates into pretrained weights. However,
LoRA's down-projection is randomly initialized and data-agnostic, discarding
potentially useful information. Prior analyses show that this projection
changes little during training, while the up-projection carries most of the
adaptation, making the random input compression a performance bottleneck. We
propose IPA, a feature-aware projection framework that explicitly preserves
information in the reduced hidden space. In the linear case, we instantiate IPA
with algorithms approximating top principal components, enabling efficient
projector pretraining with negligible inference overhead. Across language and
vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on
average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on
VTAB-1k, while matching full LoRA performance with roughly half the trainable
parameters when the projection is frozen.
</summary>
    <author>
      <name>Yuan Yin</name>
    </author>
    <author>
      <name>Shashanka Venkataramanan</name>
    </author>
    <author>
      <name>Tuan-Hung Vu</name>
    </author>
    <author>
      <name>Andrei Bursuc</name>
    </author>
    <author>
      <name>Matthieu Cord</name>
    </author>
    <link href="http://arxiv.org/abs/2509.04398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04377v1</id>
    <updated>2025-09-04T16:40:01Z</updated>
    <published>2025-09-04T16:40:01Z</published>
    <title>PagedEviction: Structured Block-wise KV Cache Pruning for Efficient
  Large Language Model Inference</title>
    <summary>  KV caching significantly improves the efficiency of Large Language Model
(LLM) inference by storing attention states from previously processed tokens,
enabling faster generation of subsequent tokens. However, as sequence length
increases, the KV cache quickly becomes a major memory bottleneck. To address
this, we propose PagedEviction, a novel fine-grained, structured KV cache
pruning strategy that enhances the memory efficiency of vLLM's PagedAttention.
Unlike existing approaches that rely on attention-based token importance or
evict tokens across different vLLM pages, PagedEviction introduces an efficient
block-wise eviction algorithm tailored for paged memory layouts. Our method
integrates seamlessly with PagedAttention without requiring any modifications
to its CUDA attention kernels. We evaluate PagedEviction across
Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models
on the LongBench benchmark suite, demonstrating improved memory usage with
better accuracy than baselines on long context tasks.
</summary>
    <author>
      <name>Krishna Teja Chitty-Venkata</name>
    </author>
    <author>
      <name>Jie Ye</name>
    </author>
    <author>
      <name>Xian-He Sun</name>
    </author>
    <author>
      <name>Anthony Kougkas</name>
    </author>
    <author>
      <name>Murali Emani</name>
    </author>
    <author>
      <name>Venkatram Vishwanath</name>
    </author>
    <author>
      <name>Bogdan Nicolae</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.04377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04356v1</id>
    <updated>2025-09-04T16:18:04Z</updated>
    <published>2025-09-04T16:18:04Z</published>
    <title>SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic
  Avatars</title>
    <summary>  We present SRWToolkit, an open-source Wizard of Oz toolkit designed to
facilitate the rapid prototyping of social robotic avatars powered by local
large language models (LLMs). Our web-based toolkit enables multimodal
interaction through text input, button-activated speech, and wake-word command.
The toolkit offers real-time configuration of avatar appearance, behavior,
language, and voice via an intuitive control panel. In contrast to prior works
that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and
ensures on-device functionality through local LLM inference. In our small-scale
user study ($n=11$), participants created and interacted with diverse robotic
roles (hospital receptionist, mathematics teacher, and driving assistant),
which demonstrated positive outcomes in the toolkit's usability, trust, and
user experience. The toolkit enables rapid and efficient development of robot
characters customized to researchers' needs, supporting scalable research in
human-robot interaction.
</summary>
    <author>
      <name>Atikkhan Faridkhan Nilgar</name>
    </author>
    <author>
      <name>Kristof Van Laerhoven</name>
    </author>
    <author>
      <name>Ayub Kinoti</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2025 International Conference on Social Robotics (ICSR)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2509.04356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.04348v1</id>
    <updated>2025-09-04T16:05:54Z</updated>
    <published>2025-09-04T16:05:54Z</published>
    <title>GWTC-4.0: Constraints on the Cosmic Expansion Rate and Modified
  Gravitational-wave Propagation</title>
    <summary>  We analyze data from 142 of the 218 gravitational-wave (GW) sources in the
fourth LIGO-Virgo-KAGRA Collaboration (LVK) Gravitational-Wave Transient
Catalog (GWTC-4.0) to estimate the Hubble constant $H_0$ jointly with the
population properties of merging compact binaries. We measure the luminosity
distance and redshifted masses of GW sources directly; in contrast, we infer GW
source redshifts statistically through i) location of features in the compact
object mass spectrum and merger rate evolution, and ii) identifying potential
host galaxies in the GW localization volume. Probing the relationship between
source luminosity distances and redshifts obtained in this way yields
constraints on cosmological parameters. We also constrain parameterized
deviations from general relativity which affect GW propagation, specifically
those modifying the dependence of a GW signal on the source luminosity
distance. Assuming our fiducial model for the source-frame mass distribution
and using GW candidates detected up to the end of the fourth observing run
(O4a), together with the GLADE+ all-sky galaxy catalog, we estimate $H_0 =
76.6^{+13.0}_{-9.5} (76.6^{+25.2}_{-14.0})$ km s$^{-1}$ Mpc$^{-1}$. This value
is reported as a median with 68.3% (90%) symmetric credible interval, and
includes combination with the $H_0$ measurement from GW170817 and its
electromagnetic counterpart. Using a parametrization of modified GW propagation
in terms of the magnitude parameter $\Xi_0$, we estimate $\Xi_0 =
1.2^{+0.8}_{-0.4} (1.2^{+2.4}_{-0.5})$, where $\Xi_0 = 1$ recovers the behavior
of general relativity.
</summary>
    <author>
      <name> The LIGO Scientific Collaboration</name>
    </author>
    <author>
      <name> the Virgo Collaboration</name>
    </author>
    <author>
      <name> the KAGRA Collaboration</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">As part of the Astrophysical Journal Letters Focus Issue on the
  Gravitational Wave Transient Catalog</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.04348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.04348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
