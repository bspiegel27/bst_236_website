<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-31T01:02:45Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-30T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117835</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.22058v1</id>
    <updated>2025-07-29T17:59:04Z</updated>
    <published>2025-07-29T17:59:04Z</published>
    <title>X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image
  Generative Models Great Again</title>
    <summary>  Numerous efforts have been made to extend the ``next token prediction''
paradigm to visual contents, aiming to create a unified approach for both image
generation and understanding. Nevertheless, attempts to generate images through
autoregressive modeling with discrete tokens have been plagued by issues such
as low visual fidelity, distorted outputs, and failure to adhere to complex
instructions when rendering intricate details. These shortcomings are likely
attributed to cumulative errors during autoregressive inference or information
loss incurred during the discretization process. Probably due to this
challenge, recent research has increasingly shifted toward jointly training
image generation with diffusion objectives and language generation with
autoregressive objectives, moving away from unified modeling approaches. In
this work, we demonstrate that reinforcement learning can effectively mitigate
artifacts and largely enhance the generation quality of a discrete
autoregressive modeling method, thereby enabling seamless integration of image
and language generation. Our framework comprises a semantic image tokenizer, a
unified autoregressive model for both language and images, and an offline
diffusion decoder for image generation, termed X-Omni. X-Omni achieves
state-of-the-art performance in image generation tasks using a 7B language
model, producing images with high aesthetic quality while exhibiting strong
capabilities in following instructions and rendering long texts.
</summary>
    <author>
      <name>Zigang Geng</name>
    </author>
    <author>
      <name>Yibing Wang</name>
    </author>
    <author>
      <name>Yeyao Ma</name>
    </author>
    <author>
      <name>Chen Li</name>
    </author>
    <author>
      <name>Yongming Rao</name>
    </author>
    <author>
      <name>Shuyang Gu</name>
    </author>
    <author>
      <name>Zhao Zhong</name>
    </author>
    <author>
      <name>Qinglin Lu</name>
    </author>
    <author>
      <name>Han Hu</name>
    </author>
    <author>
      <name>Xiaosong Zhang</name>
    </author>
    <author>
      <name> Linus</name>
    </author>
    <author>
      <name>Di Wang</name>
    </author>
    <author>
      <name>Jie Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22053v1</id>
    <updated>2025-07-29T17:56:38Z</updated>
    <published>2025-07-29T17:56:38Z</published>
    <title>Foundation Models for Demand Forecasting via Dual-Strategy Ensembling</title>
    <summary>  Accurate demand forecasting is critical for supply chain optimization, yet
remains difficult in practice due to hierarchical complexity, domain shifts,
and evolving external factors. While recent foundation models offer strong
potential for time series forecasting, they often suffer from architectural
rigidity and limited robustness under distributional change. In this paper, we
propose a unified ensemble framework that enhances the performance of
foundation models for sales forecasting in real-world supply chains. Our method
combines two complementary strategies: (1) Hierarchical Ensemble (HE), which
partitions training and inference by semantic levels (e.g., store, category,
department) to capture localized patterns; and (2) Architectural Ensemble (AE),
which integrates predictions from diverse model backbones to mitigate bias and
improve stability. We conduct extensive experiments on the M5 benchmark and
three external sales datasets, covering both in-domain and zero-shot
forecasting. Results show that our approach consistently outperforms strong
baselines, improves accuracy across hierarchical levels, and provides a simple
yet effective mechanism for boosting generalization in complex forecasting
environments.
</summary>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Defu Cao</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22038v1</id>
    <updated>2025-07-29T17:40:23Z</updated>
    <published>2025-07-29T17:40:23Z</published>
    <title>Sample Complexity of Branch-length Estimation by Maximum Likelihood</title>
    <summary>  We consider the branch-length estimation problem on a bifurcating tree: a
character evolves along the edges of a binary tree according to a two-state
symmetric Markov process, and we seek to recover the edge transition
probabilities from repeated observations at the leaves. This problem arises in
phylogenetics, and is related to latent tree graphical model inference. In
general, the log-likelihood function is non-concave and may admit many critical
points. Nevertheless, simple coordinate maximization has been known to perform
well in practice, defying the complexity of the likelihood landscape. In this
work, we provide the first theoretical guarantee as to why this might be the
case. We show that deep inside the Kesten-Stigum reconstruction regime,
provided with polynomially many $m$ samples (assuming the tree is balanced),
there exists a universal parameter regime (independent of the size of the tree)
where the log-likelihood function is strongly concave and smooth with high
probability. On this high-probability likelihood landscape event, we show that
the standard coordinate maximization algorithm converges exponentially fast to
the maximum likelihood estimator, which is within $O(1/\sqrt{m})$ from the true
parameter, provided a sufficiently close initial point.
</summary>
    <author>
      <name>David Clancy Jr.</name>
    </author>
    <author>
      <name>Hanbaek Lyu</name>
    </author>
    <author>
      <name>Sebastien Roch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2507.22038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22025v2</id>
    <updated>2025-07-30T12:17:53Z</updated>
    <published>2025-07-29T17:22:07Z</published>
    <title>UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and
  Precise Inference-Time Grounding</title>
    <summary>  The emergence of Multimodal Large Language Models (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing GUI agents at both the training and inference stages. For training,
we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:
1) a Continuous Reward function to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a Cropping-based Resampling strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
Decomposed Grounding with Selection, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks ScreenSpot-Pro and
ScreenSpot-v2. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on ScreenSpot-Pro.
</summary>
    <author>
      <name>Shuquan Lian</name>
    </author>
    <author>
      <name>Yuhang Wu</name>
    </author>
    <author>
      <name>Jia Ma</name>
    </author>
    <author>
      <name>Zihan Song</name>
    </author>
    <author>
      <name>Bingqi Chen</name>
    </author>
    <author>
      <name>Xiawu Zheng</name>
    </author>
    <author>
      <name>Hui Li</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22025v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22025v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22004v2</id>
    <updated>2025-07-30T11:55:45Z</updated>
    <published>2025-07-29T16:53:44Z</published>
    <title>Horseshoe Forests for High-Dimensional Causal Survival Analysis</title>
    <summary>  We develop a Bayesian tree ensemble model to estimate heterogeneous treatment
effects in censored survival data with high-dimensional covariates. Instead of
imposing sparsity through the tree structure, we place a horseshoe prior
directly on the step heights to achieve adaptive global-local shrinkage. This
strategy allows flexible regularisation and reduces noise. We develop a
reversible jump Gibbs sampler to accommodate the non-conjugate horseshoe prior
within the tree ensemble framework. We show through extensive simulations that
the method accurately estimates treatment effects in high-dimensional covariate
spaces, at various sparsity levels, and under non-linear treatment effect
functions. We further illustrate the practical utility of the proposed approach
by a re-analysis of pancreatic ductal adenocarcinoma (PDAC) survival data from
The Cancer Genome Atlas.
</summary>
    <author>
      <name>Tijn Jacobs</name>
    </author>
    <author>
      <name>Wessel N. van Wieringen</name>
    </author>
    <author>
      <name>Stéphanie L. van der Pas</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21998v1</id>
    <updated>2025-07-29T16:46:32Z</updated>
    <published>2025-07-29T16:46:32Z</published>
    <title>Misspecifications in structural equation modeling: The choice of latent
  variables, causal-formative constructs or composites</title>
    <summary>  Empirical research in many social disciplines involves constructs that are
not directly observable, such as behaviors. To model them, constructs must be
operationalized using their relations with indicators. Structural equation
modeling (SEM) is the primary approach for this purpose. In SEM, three types of
constructs are distinguished: latent variables, causal-formative constructs,
and composites. To estimate the parameters of the different models, various
estimators have been developed. Many Monte Carlo studies have examined the
estimation performances of different estimators for the construct types. One
aspect evaluated is the consequences of construct misspecification - when the
true construct type differs from the modeling choice - on parameter estimates
and model fit. For example, parameter bias in models that misspecify latent
variables as composites is often attributed to the chosen estimator, although
model parameters depend on different estimators, making it impossible to
examine the factors individually. This article aims to disentangle the issues
of construct misspecification and parameter estimation by a comprehensive Monte
Carlo study of all combinations between true and assumed construct types. To
focus on misspecification, we used the same estimator for all models, namely
the maximum likelihood (ML) estimator. To generalize beyond ML, we replicated
the simulation using another estimator. We aim to examine the role of construct
misspecification, not estimator choice, on the estimation performance and show
that misspecification leads indeed to biased path coefficient estimates.
Further, we evaluate whether fit measures can distinguish models with correct
from those with misspecified constructs. We find that none of the criteria
considered is suited for this. These findings stress the importance of
thoughtful construct specification and the need for further research.
</summary>
    <author>
      <name>Jonas Bauer</name>
    </author>
    <author>
      <name>Axel Mayer</name>
    </author>
    <author>
      <name>Christiane Fuchs</name>
    </author>
    <author>
      <name>Tamara Schamberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 13 figures, supplementary material and code available
  online</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.21998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21974v1</id>
    <updated>2025-07-29T16:21:42Z</updated>
    <published>2025-07-29T16:21:42Z</published>
    <title>Reasoning Language Models for Root Cause Analysis in 5G Wireless
  Networks</title>
    <summary>  Root Cause Analysis (RCA) in mobile networks remains a challenging task due
to the need for interpretability, domain expertise, and causal reasoning. In
this work, we propose a lightweight framework that leverages Large Language
Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of
annotated troubleshooting problems designed to benchmark RCA capabilities. Our
evaluation reveals that existing open-source reasoning LLMs struggle with these
problems, underscoring the need for domain-specific adaptation. To address this
issue, we propose a two-stage training methodology that combines supervised
fine-tuning with reinforcement learning to improve the accuracy and reasoning
quality of LLMs. The proposed approach fine-tunes a series of RCA models to
integrate domain knowledge and generate structured, multi-step diagnostic
explanations, improving both interpretability and effectiveness. Extensive
experiments across multiple LLM sizes show significant performance gains over
state-of-the-art reasoning and non-reasoning models, including strong
generalization to randomized test variants. These results demonstrate the
promise of domain-adapted, reasoning-enhanced LLMs for practical and
explainable RCA in network operation and management.
</summary>
    <author>
      <name>Mohamed Sana</name>
    </author>
    <author>
      <name>Nicola Piovesan</name>
    </author>
    <author>
      <name>Antonio De Domenico</name>
    </author>
    <author>
      <name>Yibin Kang</name>
    </author>
    <author>
      <name>Haozhe Zhang</name>
    </author>
    <author>
      <name>Merouane Debbah</name>
    </author>
    <author>
      <name>Fadhel Ayed</name>
    </author>
    <link href="http://arxiv.org/abs/2507.21974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21969v1</id>
    <updated>2025-07-29T16:16:33Z</updated>
    <published>2025-07-29T16:16:33Z</published>
    <title>Towards Cognitive Synergy in LLM-Based Multi-Agent Systems: Integrating
  Theory of Mind and Critical Evaluation</title>
    <summary>  Recently, the field of Multi-Agent Systems (MAS) has gained popularity as
researchers are trying to develop artificial intelligence capable of efficient
collective reasoning. Agents based on Large Language Models (LLMs) perform well
in isolated tasks, yet struggle with higher-order cognition required for
adaptive collaboration. Human teams achieve synergy not only through knowledge
sharing, but also through recursive reasoning, structured critique, and the
ability to infer others' mental states. Current artificial systems lack these
essential mechanisms, limiting their ability to engage in sophisticated
collective reasoning. This work explores cognitive processes that enable
effective collaboration, focusing on adaptive theory of mind (ToM) and
systematic critical evaluation. We investigate three key questions. First, how
does the ability to model others' perspectives enhance coordination and reduce
redundant reasoning? Second, to what extent does structured critique improve
reasoning quality by identifying logical gaps and mitigating biases? Third, the
interplay of these mechanisms can lead to emergent cognitive synergy, where the
collective intelligence of the system exceeds the sum of its parts. Through an
empirical case study on complex decision making, we show that the integration
of these cognitive mechanisms leads to more coherent, adaptive, and rigorous
agent interactions. This article contributes to the field of cognitive science
and AI research by presenting a structured framework that emulates human-like
collaborative reasoning MAS. It highlights the significance of dynamic ToM and
critical evaluation in advancing multi-agent systems' ability to tackle
complex, real-world challenges.
</summary>
    <author>
      <name>Adam Kostka</name>
    </author>
    <author>
      <name>Jarosław A. Chudziak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CogSci 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.21969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21961v1</id>
    <updated>2025-07-29T16:10:52Z</updated>
    <published>2025-07-29T16:10:52Z</published>
    <title>Following the Committor Flow: A Data-Driven Discovery of Transition
  Pathways</title>
    <summary>  The discovery of transition pathways to unravel distinct reaction mechanisms
and, in general, rare events that occur in molecular systems is still a
challenge. Recent advances have focused on analyzing the transition path
ensemble using the committor probability, widely regarded as the most
informative one-dimensional reaction coordinate. Consistency between transition
pathways and the committor function is essential for accurate mechanistic
insight. In this work, we propose an iterative framework to infer the committor
and, subsequently, to identify the most relevant transition pathways. Starting
from an initial guess for the transition path, we generate biased sampling from
which we train a neural network to approximate the committor probability. From
this learned committor, we extract dominant transition channels as discretized
strings lying on isocommittor surfaces. These pathways are then used to enhance
sampling and iteratively refine both the committor and the transition paths
until convergence. The resulting committor enables accurate estimation of the
reaction rate constant. We demonstrate the effectiveness of our approach on
benchmark systems, including a two-dimensional model potential, peptide
conformational transitions, and a Diels--Alder reaction.
</summary>
    <author>
      <name>Cheng Giuseppe Chen</name>
    </author>
    <author>
      <name>Chenyu Tang</name>
    </author>
    <author>
      <name>Alberto Megías</name>
    </author>
    <author>
      <name>Radu A. Talmazan</name>
    </author>
    <author>
      <name>Sergio Contreras Arredondo</name>
    </author>
    <author>
      <name>Benoît Roux</name>
    </author>
    <author>
      <name>Christophe Chipot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages (including supplemental material with 10 pages), 9 figures
  (4 figures in the main text and 5 figures in the supplemental material)</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.21961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21950v1</id>
    <updated>2025-07-29T16:02:21Z</updated>
    <published>2025-07-29T16:02:21Z</published>
    <title>Regional Price Dynamics and Market Integration in the U.S. Beef
  Industry: An Econometric Analysis</title>
    <summary>  The United States, a leading global producer and consumer of beef, continues
to face substantial challenges in achieving price harmonization across its
regional markets. This paper evaluates the validity of the Law of One Price
(LOP) in the U.S. beef industry and investigates causal relationships among
regional price dynamics. Through a series of econometric tests, we establish
that regional price series are integrated of order one, displaying
non-stationarity in levels and stationarity in first differences. The analysis
reveals partial LOP compliance in the Northeast and West, while full
convergence remains elusive at the national level. Although no region
demonstrates persistent price leadership, Southern prices appear particularly
sensitive to exogenous shocks. These findings reflect asymmetrical integration
across U.S. beef markets and suggest the presence of structural frictions that
hinder complete market unification.
</summary>
    <author>
      <name>Leonardo Manríquez-Méndez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 2 figures, 13 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.21950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.21950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
