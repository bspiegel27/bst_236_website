<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-18T01:01:03Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-17T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117070</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.12464v1</id>
    <updated>2025-07-16T17:59:32Z</updated>
    <published>2025-07-16T17:59:32Z</published>
    <title>CytoSAE: Interpretable Cell Embeddings for Hematology</title>
    <summary>  Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.
</summary>
    <author>
      <name>Muhammed Furkan Dasdelen</name>
    </author>
    <author>
      <name>Hyesu Lim</name>
    </author>
    <author>
      <name>Michele Buck</name>
    </author>
    <author>
      <name>Katharina S. Götze</name>
    </author>
    <author>
      <name>Carsten Marr</name>
    </author>
    <author>
      <name>Steffen Schneider</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.12464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12457v2</id>
    <updated>2025-07-17T06:38:08Z</updated>
    <published>2025-07-16T17:56:44Z</published>
    <title>Does $K$-fold CV based penalty perform variable selection or does it
  lead to $n^{1/2}$-consistency in Lasso?</title>
    <summary>  Least absolute shrinkage and selection operator or Lasso, introduced by
Tibshirani (1996), is one of the widely used regularization methods in
regression. It is observed that the properties of Lasso vary wildly depending
on the choice of the penalty parameter. The recent results of Lahiri (2021)
suggest that, depending on the nature of the penalty parameter, Lasso can
either be variable selection consistent or be $n^{1/2}-$consistent. However,
practitioners generally implement Lasso by choosing the penalty parameter in a
data-dependent way, the most popular being the $K$-fold cross-validation. In
this paper, we explore the variable selection consistency and
$n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold
cross-validation with $K$ being fixed. We consider the fixed-dimensional
heteroscedastic linear regression model and show that Lasso with $K$-fold
cross-validation based penalty is $n^{1/2}-$consistent, but not variable
selection consistent. We also establish the $n^{1/2}-$consistency of the
$K$-fold cross-validation based penalty as an intermediate result.
Additionally, as a consequence of $n^{1/2}-$consistency, we establish the
validity of Bootstrap to approximate the distribution of the Lasso estimator
based on $K-$fold cross-validation. We validate the Bootstrap approximation in
finite samples based on a moderate simulation study. Thus, our results
essentially justify the use of $K$-fold cross-validation in practice to draw
inferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression.
</summary>
    <author>
      <name>Mayukh Choudhury</name>
    </author>
    <author>
      <name>Debraj Das</name>
    </author>
    <link href="http://arxiv.org/abs/2507.12457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12443v1</id>
    <updated>2025-07-16T17:29:15Z</updated>
    <published>2025-07-16T17:29:15Z</published>
    <title>LLM-Based Config Synthesis requires Disambiguation</title>
    <summary>  Beyond hallucinations, another problem in program synthesis using LLMs is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for LLM-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently overlap in header space, making the relative
priority of actions impossible for the LLM to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of overlaps,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an LLM augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by LLMs, but their integration is ambiguous and
can lead to different global behaviors.
</summary>
    <author>
      <name>Rajdeep Mondal</name>
    </author>
    <author>
      <name>Nikolaj Bjorner</name>
    </author>
    <author>
      <name>Todd Millstein</name>
    </author>
    <author>
      <name>Alan Tang</name>
    </author>
    <author>
      <name>George Varghese</name>
    </author>
    <link href="http://arxiv.org/abs/2507.12443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12442v1</id>
    <updated>2025-07-16T17:28:40Z</updated>
    <published>2025-07-16T17:28:40Z</published>
    <title>Characterizing State Space Model (SSM) and SSM-Transformer Hybrid
  Language Model Performance with Long Context Length</title>
    <summary>  The demand for machine intelligence capable of processing continuous,
long-context inputs on local devices is growing rapidly. However, the quadratic
complexity and memory requirements of traditional Transformer architectures
make them inefficient and often unusable for these tasks. This has spurred a
paradigm shift towards new architectures like State Space Models (SSMs) and
hybrids, which promise near-linear scaling. While most current research focuses
on the accuracy and theoretical throughput of these models, a systematic
performance characterization on practical consumer hardware is critically
needed to guide system-level optimization and unlock new applications.
  To address this gap, we present a comprehensive, comparative benchmarking of
carefully selected Transformer, SSM, and hybrid models specifically for
long-context inference on consumer and embedded GPUs. Our analysis reveals that
SSMs are not only viable but superior for this domain, capable of processing
sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than
comparable Transformers. While Transformers may be up to 1.8x faster at short
sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x
faster at very long contexts (~57K tokens). Our operator-level analysis reveals
that custom, hardware-aware SSM kernels dominate the inference runtime,
accounting for over 55% of latency on edge platforms, identifying them as a
primary target for future hardware acceleration. We also provide detailed,
device-specific characterization results to guide system co-design for the
edge. To foster further research, we will open-source our characterization
framework.
</summary>
    <author>
      <name>Saptarshi Mitra</name>
    </author>
    <author>
      <name>Rachid Karami</name>
    </author>
    <author>
      <name>Haocheng Xu</name>
    </author>
    <author>
      <name>Sitao Huang</name>
    </author>
    <author>
      <name>Hyoukjun Kwon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.12442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12435v1</id>
    <updated>2025-07-16T17:24:06Z</updated>
    <published>2025-07-16T17:24:06Z</published>
    <title>Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal
  Inference in Neural Networks</title>
    <summary>  Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.
</summary>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>David Mccoy</name>
    </author>
    <author>
      <name>Nolan Gunter</name>
    </author>
    <author>
      <name>Kaitlyn Lee</name>
    </author>
    <author>
      <name>Alejandro Schuler</name>
    </author>
    <author>
      <name>Mark van der Laan</name>
    </author>
    <link href="http://arxiv.org/abs/2507.12435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12424v1</id>
    <updated>2025-07-16T17:11:48Z</updated>
    <published>2025-07-16T17:11:48Z</published>
    <title>Hierarchical Temporal Point Process Modeling of Aggressive Behavior
  Onset in Psychiatric Inpatient Youth with Autism for Branching Factor
  Estimation</title>
    <summary>  Aggressive behavior in autistic inpatient youth often arises in temporally
clustered bursts complicating efforts to distinguish external triggers from
internal escalation. The sample population branching factor-the expected number
of new onsets triggered by a given event-is a key summary of self-excitation in
behavior dynamics. Prior pooled models overestimate this quantity by ignoring
patient-specific variability. We addressed this using a hierarchical Hawkes
process with an exponential kernel and edge-effect correction allowing partial
pooling across patients. This approach reduces bias from high-frequency
individuals and stabilizes estimates for those with sparse data. Bayesian
inference was performed using the No U-Turn Sampler with model evaluation via
convergence diagnostics, power-scaling sensitivity analysis, and multiple
Goodness-of-Fit (GOF) metrics: PSIS-LOO the Lewis test with Durbin's
modification and residual analysis based on the Random Time Change Theorem
(RTCT). The hierarchical model yielded a significantly lower and more precise
branching factor estimate mean (0.742 +- 0.026) than the pooled model (0.899 +-
0.015) and narrower intervals than the unpooled model (0.717 +- 0.139). This
led to a threefold smaller cascade of events per onset under the hierarchical
model. Sensitivity analyses confirmed robustness to prior and likelihood
perturbations while the unpooled model showed instability for sparse
individuals. GOF measures consistently favored or on par to the hierarchical
model. Hierarchical Hawkes modeling with edge-effect correction provides robust
estimation of branching dynamics by capturing both within- and between-patient
variability. This enables clearer separation of endogenous from exogenous
events supports linkage to physiological signals and enhances early warning
systems individualized treatment and resource allocation in inpatient care.
</summary>
    <author>
      <name>Michael Potter</name>
    </author>
    <author>
      <name>Michael Everett</name>
    </author>
    <author>
      <name>Deniz Erdogmus</name>
    </author>
    <author>
      <name>Yuna Watanabe</name>
    </author>
    <author>
      <name>Tales Imbiriba</name>
    </author>
    <author>
      <name>Matthew S. Goodwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to BMC Bioinformatics</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.12424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12412v1</id>
    <updated>2025-07-16T17:00:41Z</updated>
    <published>2025-07-16T17:00:41Z</published>
    <title>NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal
  Data</title>
    <summary>  In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.
</summary>
    <author>
      <name>Dzung Dinh</name>
    </author>
    <author>
      <name>Boqi Chen</name>
    </author>
    <author>
      <name>Marc Niethammer</name>
    </author>
    <author>
      <name>Junier Oliva</name>
    </author>
    <link href="http://arxiv.org/abs/2507.12412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12399v1</id>
    <updated>2025-07-16T16:44:29Z</updated>
    <published>2025-07-16T16:44:29Z</published>
    <title>ROC-n-reroll: How verifier imperfection affects test-time scaling</title>
    <summary>  Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.
</summary>
    <author>
      <name>Florian E. Dorner</name>
    </author>
    <author>
      <name>Yatong Chen</name>
    </author>
    <author>
      <name>André F. Cruz</name>
    </author>
    <author>
      <name>Fanny Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.12399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12395v1</id>
    <updated>2025-07-16T16:41:17Z</updated>
    <published>2025-07-16T16:41:17Z</published>
    <title>Invariant non-equilibrium dynamics of transcriptional regulation
  optimize information flow</title>
    <summary>  Eukaryotic gene regulation is based on stochastic yet controlled promoter
switching, during which genes transition between transcriptionally active and
inactive states. Despite the molecular complexity of this process, recent
studies reveal a surprising invariance of the "switching correlation time"
($T_C$), which characterizes promoter activity fluctuations, across gene
expression levels in diverse genes and organisms. A biophysically plausible
explanation for this invariance remains missing. Here, we show that this
invariance imposes stringent constraints on minimal yet plausible models of
transcriptional regulation, requiring at least four system states and
non-equilibrium dynamics that break detailed balance. Using Bayesian inference
on Drosophila gap gene expression data, we demonstrate that such models (i)
accurately reproduce the observed $T_C$-invariance; (ii) remain robust to
parameter perturbations; and (iii) maximize information transmission from
transcription factor concentration to gene expression. These findings suggest
that eukaryotic gene regulation has evolved to balance precision with reaction
rate and energy dissipation constraints, favoring non-equilibrium architectures
for optimal information transmission.
</summary>
    <author>
      <name>Benjamin Zoller</name>
    </author>
    <author>
      <name>Alexis Bénichou</name>
    </author>
    <author>
      <name>Thomas Gregor</name>
    </author>
    <author>
      <name>Gašper Tkačik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, 10 SI figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.12395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12385v1</id>
    <updated>2025-07-16T16:31:40Z</updated>
    <published>2025-07-16T16:31:40Z</published>
    <title>Convergence of drift-diffusion PDEs arising as Wasserstein gradient
  flows of convex functions</title>
    <summary>  We study the quantitative convergence of drift-diffusion PDEs that arise as
Wasserstein gradient flows of linearly convex functions over the space of
probability measures on ${\mathbb R}^d$. In this setting, the objective is in
general not displacement convex, so it is not clear a priori whether global
convergence even holds. Still, our analysis reveals that diffusion {allows} a
favorable interaction between Wasserstein geometry and linear convexity,
leading to a general quantitative convergence theory, analogous to that of
gradient flows in convex settings in the Euclidean space.
  Specifically, we prove that if the objective is convex and suitably coercive,
the suboptimality gap decreases at a rate $O(1/t)$. This improves to a rate
faster than any polynomial -- or even exponential in compact settings -- when
the objective is strongly convex relative to the entropy.
  Our results extend the range of mean-field Langevin dynamics that enjoy
quantitative convergence guarantees, and enable new applications to
optimization over the space of probability measures. To illustrate this, we
show quantitative convergence results for the minimization of
entropy-regularized nonconvex problems, we propose and study an
\emph{approximate Fisher Information} regularization covered by our setting,
and we apply our results to an estimator for trajectory inference which
involves the minimization of the relative entropy with respect to the Wiener
measure in path space.
</summary>
    <author>
      <name>Lénaïc Chizat</name>
    </author>
    <author>
      <name>Maria Colombo</name>
    </author>
    <author>
      <name>Xavier Fernández-Real</name>
    </author>
    <link href="http://arxiv.org/abs/2507.12385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="49Q22, 35Q84, 35B40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
