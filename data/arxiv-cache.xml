<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-16T00:54:09Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-15T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">123678</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.12796v1</id>
    <updated>2025-10-14T17:59:47Z</updated>
    <published>2025-10-14T17:59:47Z</published>
    <title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</title>
    <summary>  Scaling Vision-Language-Action (VLA) models on large-scale data offers a
promising path to achieving a more generalized driving intelligence. However,
VLA models are limited by a ``supervision deficit'': the vast model capacity is
supervised by sparse, low-dimensional actions, leaving much of their
representational power underutilized. To remedy this, we propose
\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to
predict future images. This task generates a dense, self-supervised signal that
compels the model to learn the underlying dynamics of the driving environment.
We showcase the paradigm's versatility by instantiating it for two dominant VLA
archetypes: an autoregressive world model for VLAs that use discrete visual
tokens, and a diffusion world model for those operating on continuous visual
features. Building on the rich representations learned from world modeling, we
introduce a lightweight action expert to address the inference latency for
real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a
680x larger in-house dataset demonstrate that DriveVLA-W0 significantly
outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling
law, showing that performance gains accelerate as the training dataset size
increases.
</summary>
    <author>
      <name>Yingyan Li</name>
    </author>
    <author>
      <name>Shuyao Shang</name>
    </author>
    <author>
      <name>Weisong Liu</name>
    </author>
    <author>
      <name>Bing Zhan</name>
    </author>
    <author>
      <name>Haochen Wang</name>
    </author>
    <author>
      <name>Yuqi Wang</name>
    </author>
    <author>
      <name>Yuntao Chen</name>
    </author>
    <author>
      <name>Xiaoman Wang</name>
    </author>
    <author>
      <name>Yasong An</name>
    </author>
    <author>
      <name>Chufeng Tang</name>
    </author>
    <author>
      <name>Lu Hou</name>
    </author>
    <author>
      <name>Lue Fan</name>
    </author>
    <author>
      <name>Zhaoxiang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.12796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12793v1</id>
    <updated>2025-10-14T17:58:10Z</updated>
    <published>2025-10-14T17:58:10Z</published>
    <title>ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</title>
    <summary>  Existing Multimodal Large Language Models (MLLMs) suffer from increased
inference costs due to the additional vision tokens introduced by image inputs.
In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens. The key idea behind our
method is to employ multiple MLP connectors, each with a different image
compression ratio, to downsample the vision tokens based on the semantic
complexity of the image. During training, we minimize the KL divergence between
the responses conditioned on different MLP connectors. At inference time, we
introduce an image router, termed Visual Resolution Router (ViR), that
automatically selects the appropriate compression rate for each image patch.
Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.
Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model's perception, reasoning,
and OCR capabilities. We hope this work will contribute to the development of
more efficient MLLMs. The code and models will be released to facilitate future
research.
</summary>
    <author>
      <name>Long Cui</name>
    </author>
    <author>
      <name>Weiyun Wang</name>
    </author>
    <author>
      <name>Jie Shao</name>
    </author>
    <author>
      <name>Zichen Wen</name>
    </author>
    <author>
      <name>Gen Luo</name>
    </author>
    <author>
      <name>Linfeng Zhang</name>
    </author>
    <author>
      <name>Yanting Zhang</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Wenhai Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.12793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12789v1</id>
    <updated>2025-10-14T17:57:56Z</updated>
    <published>2025-10-14T17:57:56Z</published>
    <title>UniFusion: Vision-Language Model as Unified Encoder in Image Generation</title>
    <summary>  Although recent advances in visual generation have been remarkable, most
existing architectures still depend on distinct encoders for images and text.
This separation constrains diffusion models' ability to perform cross-modal
reasoning and knowledge transfer. Prior attempts to bridge this gap often use
the last layer information from VLM, employ multiple visual encoders, or train
large unified models jointly for text and image generation, which demands
substantial computational resources and large-scale data, limiting its
accessibility.We present UniFusion, a diffusion-based generative model
conditioned on a frozen large vision-language model (VLM) that serves as a
unified multimodal encoder. At the core of UniFusion is the Layerwise Attention
Pooling (LAP) mechanism that extracts both high level semantics and low level
details from text and visual tokens of a frozen VLM to condition a diffusion
generative model. We demonstrate that LAP outperforms other shallow fusion
architectures on text-image alignment for generation and faithful transfer of
visual information from VLM to the diffusion model which is key for editing. We
propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),
which conditions a diffusion transformer (DiT) only on the text tokens
generated by the VLM during in-model prompt rewriting. VERIFI combines the
alignment of the conditioning distribution with the VLM's reasoning
capabilities for increased capabilities and flexibility at inference. In
addition, finetuning on editing task not only improves text-image alignment for
generation, indicative of cross-modality knowledge transfer, but also exhibits
tremendous generalization capabilities. Our model when trained on single image
editing, zero-shot generalizes to multiple image references further motivating
the unified encoder design of UniFusion.
</summary>
    <author>
      <name>Kevin Li</name>
    </author>
    <author>
      <name>Manuel Brack</name>
    </author>
    <author>
      <name>Sudeep Katakol</name>
    </author>
    <author>
      <name>Hareesh Ravi</name>
    </author>
    <author>
      <name>Ajinkya Kale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page at https://thekevinli.github.io/unifusion/</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12776v1</id>
    <updated>2025-10-14T17:51:48Z</updated>
    <published>2025-10-14T17:51:48Z</published>
    <title>A Quantum Generative Framework for Modeling Single-Cell Transcriptomes
  with Gene-Gene and Cell-Cell Interactions</title>
    <summary>  Single-cell RNA sequencing (scRNA-seq) data simulation is limited by
classical methods that rely on linear correlations, failing to capture the
intrinsic, nonlinear dependencies and the simultaneous gene-gene and cell-cell
interactions. We introduce qSimCells, a novel hybrid quantum-classical
simulator that leverages quantum entanglement to model single-cell
transcriptomes. The core innovation is a quantum kernel that uses a
parameterized quantum circuit with CNOT gates to encode complex, nonlinear gene
regulatory network (GRN) and cell-cell communication topologies with explicit
directionality (causality). The synthetic data exhibits non-classical
dependencies that challenge standard analysis. We demonstrated that classical
correlation methods (Pearson and Spearman) failed to reconstruct the complete
programmed quantum causal paths, instead reporting spurious statistical
artifacts driven by high base-gene expression probabilities. Applying
CellChat2.0 to the simulated cell-cell communication validated the true
mechanistic links by showing a robust, relative increase in communication
probability (up to 75-fold) only when the quantum entanglement was active. This
work confirms that the quantum kernel is essential for creating high-fidelity
ground truth data, highlighting the need for advanced inference techniques to
capture the complex, non-classical dependencies inherent in gene regulation.
</summary>
    <author>
      <name>Selim Romero</name>
    </author>
    <author>
      <name>Vignesh Kumar</name>
    </author>
    <author>
      <name>Robert S. Chapkin</name>
    </author>
    <author>
      <name>James J. Cai</name>
    </author>
    <link href="http://arxiv.org/abs/2510.12776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12773v1</id>
    <updated>2025-10-14T17:51:26Z</updated>
    <published>2025-10-14T17:51:26Z</published>
    <title>Dr.LLM: Dynamic Layer Routing in LLMs</title>
    <summary>  Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.
</summary>
    <author>
      <name>Ahmed Heakl</name>
    </author>
    <author>
      <name>Martin Gubri</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Sangdoo Yun</name>
    </author>
    <author>
      <name>Seong Joon Oh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, Under submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12764v1</id>
    <updated>2025-10-14T17:45:17Z</updated>
    <published>2025-10-14T17:45:17Z</published>
    <title>AnyUp: Universal Feature Upsampling</title>
    <summary>  We introduce AnyUp, a method for feature upsampling that can be applied to
any vision feature at any resolution, without encoder-specific training.
Existing learning-based upsamplers for features like DINO or CLIP need to be
re-trained for every feature extractor and thus do not generalize to different
feature types at inference time. In this work, we propose an inference-time
feature-agnostic upsampling architecture to alleviate this limitation and
improve upsampling quality. In our experiments, AnyUp sets a new state of the
art for upsampled features, generalizes to different feature types, and
preserves feature semantics while being efficient and easy to apply to a wide
range of downstream tasks.
</summary>
    <author>
      <name>Thomas Wimmer</name>
    </author>
    <author>
      <name>Prune Truong</name>
    </author>
    <author>
      <name>Marie-Julie Rakotosaona</name>
    </author>
    <author>
      <name>Michael Oechsle</name>
    </author>
    <author>
      <name>Federico Tombari</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <author>
      <name>Jan Eric Lenssen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Website: https://wimmerth.github.io/anyup/</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12739v1</id>
    <updated>2025-10-14T17:17:14Z</updated>
    <published>2025-10-14T17:17:14Z</published>
    <title>CoNet-Rx: Collaborative Neural Networks for OFDM Receivers</title>
    <summary>  Deep learning (DL) based methods for orthogonal frequency division
multiplexing (OFDM) radio receivers demonstrated higher signal detection
performance compared to the traditional receivers. However, the existing
DL-based models, usually adapted from computer vision, aren't well suited for
wireless communications. These models require high computational resources and
memory, and have significant inference delays, limiting their use in
resource-constrained settings. Additionally, reducing network size to ease
resource demands often leads to notable performance degradation. This paper
introduces collaborative networks (CoNet), a novel neural network (NN)
architecture designed for OFDM receivers. CoNet uses multiple small ResNet or
CNN subnetworks to simultaneously process signal features from different
perspectives like capturing channel correlations and interference patterns.
These subnetworks fuse their outputs through interaction operations (e.g.,
element-wise multiplication), significantly enhancing detection performance.
Simulation results show CoNet significantly outperforms traditional
architectures like residual networks (ResNets) in bit error rate (BER) and
reduces inference delay when both nets have the same size and the same
computational complexity.
</summary>
    <author>
      <name>Mohanad Obeed</name>
    </author>
    <author>
      <name>Ming Jian</name>
    </author>
    <link href="http://arxiv.org/abs/2510.12739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12734v1</id>
    <updated>2025-10-14T17:12:28Z</updated>
    <published>2025-10-14T17:12:28Z</published>
    <title>Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with
  Unobserved Confounding and the Rashomon Effect</title>
    <summary>  Variable importance (VI) methods are often used for hypothesis generation,
feature selection, and scientific validation. In the standard VI pipeline, an
analyst estimates VI for a single predictive model with only the observed
features. However, the importance of a feature depends heavily on which other
variables are included in the model, and essential variables are often omitted
from observational datasets. Moreover, the VI estimated for one model is often
not the same as the VI estimated for another equally-good model - a phenomenon
known as the Rashomon Effect. We address these gaps by introducing
UNobservables and Inference for Variable importancE using Rashomon SEts
(UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models
in a dataset - to produce bounds on the true VI even with missing features. We
theoretically guarantee the robustness of our approach, show strong performance
on semi-synthetic simulations, and demonstrate its utility in a credit risk
task.
</summary>
    <author>
      <name>Jon Donnelly</name>
    </author>
    <author>
      <name>Srikar Katta</name>
    </author>
    <author>
      <name>Emanuele Borgonovo</name>
    </author>
    <author>
      <name>Cynthia Rudin</name>
    </author>
    <link href="http://arxiv.org/abs/2510.12734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12724v1</id>
    <updated>2025-10-14T17:06:00Z</updated>
    <published>2025-10-14T17:06:00Z</published>
    <title>T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial
  Transformation for Cross-Embodiment Dexterous Grasping</title>
    <summary>  Dexterous grasping remains a central challenge in robotics due to the
complexity of its high-dimensional state and action space. We introduce T(R,O)
Grasp, a diffusion-based framework that efficiently generates accurate and
diverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,
a unified representation that models spatial transformations between robotic
hands and objects while encoding their geometric properties. A graph diffusion
model, coupled with an efficient inverse kinematics solver, supports both
unconditioned and conditioned grasp synthesis. Extensive experiments on a
diverse set of dexterous hands show that T(R,O) Grasp achieves average success
rate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per
second on an NVIDIA A100 40GB GPU, substantially outperforming existing
baselines. In addition, our approach is robust and generalizable across
embodiments while significantly reducing memory consumption. More importantly,
the high inference speed enables closed-loop dexterous manipulation,
underscoring the potential of T(R,O) Grasp to scale into a foundation model for
dexterous grasping.
</summary>
    <author>
      <name>Xin Fei</name>
    </author>
    <author>
      <name>Zhixuan Xu</name>
    </author>
    <author>
      <name>Huaicong Fang</name>
    </author>
    <author>
      <name>Tianrui Zhang</name>
    </author>
    <author>
      <name>Lin Shao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12721v1</id>
    <updated>2025-10-14T17:00:13Z</updated>
    <published>2025-10-14T17:00:13Z</published>
    <title>CARVQ: Corrective Adaptor with Group Residual Vector Quantization for
  LLM Embedding Compression</title>
    <summary>  Large Language Models (LLMs) typically rely on a large number of parameters
for token embedding, leading to substantial storage requirements and memory
footprints. In particular, LLMs deployed on edge devices are memory-bound, and
reducing the memory footprint by compressing the embedding layer not only frees
up the memory bandwidth but also speeds up inference. To address this, we
introduce CARVQ, a post-training novel Corrective Adaptor combined with group
Residual Vector Quantization. CARVQ relies on the composition of both linear
and non-linear maps and mimics the original model embedding to compress to
approximately 1.6 bits without requiring specialized hardware to support
lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,
LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B
and Phi-4, evaluating on common generative, discriminative, math and reasoning
tasks. We show that in most cases, CARVQ can achieve lower average
bitwidth-per-parameter while maintaining reasonable perplexity and accuracy
compared to scalar quantization. Our contributions include a novel compression
technique that is compatible with state-of-the-art transformer quantization
methods and can be seamlessly integrated into any hardware supporting 4-bit
memory to reduce the model's memory footprint in memory-constrained devices.
This work demonstrates a crucial step toward the efficient deployment of LLMs
on edge devices.
</summary>
    <author>
      <name>Dayin Gou</name>
    </author>
    <author>
      <name>Sanghyun Byun</name>
    </author>
    <author>
      <name>Nilesh Malpeddi</name>
    </author>
    <author>
      <name>Gabrielle De Micheli</name>
    </author>
    <author>
      <name>Prathamesh Vaste</name>
    </author>
    <author>
      <name>Jacob Song</name>
    </author>
    <author>
      <name>Woo Seong Chung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EMNLP Findings 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
