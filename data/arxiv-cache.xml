<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-14T00:55:55Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-13T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">114917</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.10981v1</id>
    <updated>2025-06-12T17:59:56Z</updated>
    <published>2025-06-12T17:59:56Z</published>
    <title>SceneCompleter: Dense 3D Scene Completion for Generative Novel View
  Synthesis</title>
    <summary>  Generative models have gained significant attention in novel view synthesis
(NVS) by alleviating the reliance on dense multi-view captures. However,
existing methods typically fall into a conventional paradigm, where generative
models first complete missing areas in 2D, followed by 3D recovery techniques
to reconstruct the scene, which often results in overly smooth surfaces and
distorted geometry, as generative models struggle to infer 3D structure solely
from RGB data. In this paper, we propose SceneCompleter, a novel framework that
achieves 3D-consistent generative novel view synthesis through dense 3D scene
completion. SceneCompleter achieves both visual coherence and 3D-consistent
generative scene completion through two key components: (1) a
geometry-appearance dual-stream diffusion model that jointly synthesizes novel
views in RGBD space; (2) a scene embedder that encodes a more holistic scene
understanding from the reference image. By effectively fusing structural and
textural information, our method demonstrates superior coherence and
plausibility in generative novel view synthesis across diverse datasets.
Project Page: https://chen-wl20.github.io/SceneCompleter
</summary>
    <author>
      <name>Weiliang Chen</name>
    </author>
    <author>
      <name>Jiayi Bi</name>
    </author>
    <author>
      <name>Yuanhui Huang</name>
    </author>
    <author>
      <name>Wenzhao Zheng</name>
    </author>
    <author>
      <name>Yueqi Duan</name>
    </author>
    <link href="http://arxiv.org/abs/2506.10981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10967v1</id>
    <updated>2025-06-12T17:59:09Z</updated>
    <published>2025-06-12T17:59:09Z</published>
    <title>Beyond Attention or Similarity: Maximizing Conditional Diversity for
  Token Pruning in MLLMs</title>
    <summary>  In multimodal large language models (MLLMs), the length of input visual
tokens is often significantly greater than that of their textual counterparts,
leading to a high inference cost. Many works aim to address this issue by
removing redundant visual tokens. However, current approaches either rely on
attention-based pruning, which retains numerous duplicate tokens, or use
similarity-based pruning, overlooking the instruction relevance, consequently
causing suboptimal performance. In this paper, we go beyond attention or
similarity by proposing a novel visual token pruning method named CDPruner,
which maximizes the conditional diversity of retained tokens. We first define
the conditional similarity between visual tokens conditioned on the
instruction, and then reformulate the token pruning problem with determinantal
point process (DPP) to maximize the conditional diversity of the selected
subset. The proposed CDPruner is training-free and model-agnostic, allowing
easy application to various MLLMs. Extensive experiments across diverse MLLMs
show that CDPruner establishes new state-of-the-art on various vision-language
benchmarks. By maximizing conditional diversity through DPP, the selected
subset better represents the input images while closely adhering to user
instructions, thereby preserving strong performance even with high reduction
ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\% and CUDA latency
by 78\%, while maintaining 94\% of the original accuracy. Our code is available
at https://github.com/Theia-4869/CDPruner.
</summary>
    <author>
      <name>Qizhe Zhang</name>
    </author>
    <author>
      <name>Mengzhen Liu</name>
    </author>
    <author>
      <name>Lichen Li</name>
    </author>
    <author>
      <name>Ming Lu</name>
    </author>
    <author>
      <name>Yuan Zhang</name>
    </author>
    <author>
      <name>Junwen Pan</name>
    </author>
    <author>
      <name>Qi She</name>
    </author>
    <author>
      <name>Shanghang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 5 figures, code: https://github.com/Theia-4869/CDPruner,
  project page: https://theia-4869.github.io/CDPruner</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.10967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10965v1</id>
    <updated>2025-06-12T17:59:02Z</updated>
    <published>2025-06-12T17:59:02Z</published>
    <title>Apparent inconsistency between Streda formula and Hall conductivity in
  reentrant integer quantum anomalous Hall effect in twisted MoTe$_2$</title>
    <summary>  Recent experiments in twisted bilayer MoTe$_2$ (tMoTe$_2$) have uncovered a
rich landscape of correlated phases. In this work, we investigate the reentrant
integer quantum anomalous Hall (RIQAH) states reported in F. Xu, et. al.,
arXiv:2504.06972 which displays a notable mismatch between the Hall
conductivity measured via transport and that inferred from the Streda formula.
We argue that this discrepancy can be explained if the RIQAH state is a quantum
Hall bubble phase, analogous to similar well-established phenomena in 2D GaAs
quantum wells. While this explains the RIQAH state at a filling $\nu = -0.63$,
the other RIQAH state at $\nu = -0.7$ has a smaller slope necessitating a
different interpretation. We propose that this discrepancy arises due to a
nearby resistive peak masking the true slope. Furthermore, we identify this
resistive peak as a signature of a magnetic phase transition near $\nu =
-0.75$, possibly driven by a Van Hove singularity. The anomalous Hall response
and Landau fan evolution across this transition suggest a change in Fermi
surface topology and a phase with zero Chern number, potentially corresponding
to an inter-valley coherent state. These observations offer new insights into
the nature of the RIQAH states and raise the possibility that the nearby
superconducting phase may have an inter-valley coherent normal state.
</summary>
    <author>
      <name>Yi Huang</name>
    </author>
    <author>
      <name>Seth Musser</name>
    </author>
    <author>
      <name>Jihang Zhu</name>
    </author>
    <author>
      <name>Yang-Zhi Chou</name>
    </author>
    <author>
      <name>Sankar Das Sarma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.10965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10962v1</id>
    <updated>2025-06-12T17:57:44Z</updated>
    <published>2025-06-12T17:57:44Z</published>
    <title>SpectralAR: Spectral Autoregressive Visual Generation</title>
    <summary>  Autoregressive visual generation has garnered increasing attention due to its
scalability and compatibility with other modalities compared with diffusion
models. Most existing methods construct visual sequences as spatial patches for
autoregressive generation. However, image patches are inherently parallel,
contradicting the causal nature of autoregressive modeling. To address this, we
propose a Spectral AutoRegressive (SpectralAR) visual generation framework,
which realizes causality for visual sequences from the spectral perspective.
Specifically, we first transform an image into ordered spectral tokens with
Nested Spectral Tokenization, representing lower to higher frequency
components. We then perform autoregressive generation in a coarse-to-fine
manner with the sequences of spectral tokens. By considering different levels
of detail in images, our SpectralAR achieves both sequence causality and token
efficiency without bells and whistles. We conduct extensive experiments on
ImageNet-1K for image reconstruction and autoregressive generation, and
SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project
page: https://huang-yh.github.io/spectralar/.
</summary>
    <author>
      <name>Yuanhui Huang</name>
    </author>
    <author>
      <name>Weiliang Chen</name>
    </author>
    <author>
      <name>Wenzhao Zheng</name>
    </author>
    <author>
      <name>Yueqi Duan</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Jiwen Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://huang-yh.github.io/spectralar/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.10962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10948v1</id>
    <updated>2025-06-12T17:50:05Z</updated>
    <published>2025-06-12T17:50:05Z</published>
    <title>Execution Guided Line-by-Line Code Generation</title>
    <summary>  We present a novel approach to neural code generation that incorporates
real-time execution signals into the language model generation process. While
large language models (LLMs) have demonstrated impressive code generation
capabilities, they typically do not utilize execution feedback during
inference, a critical signal that human programmers regularly leverage. Our
method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically
incorporates execution signals as the model generates code, providing
line-by-line feedback that guides the generation process toward executable
solutions. EG-CFG employs a multi-stage process: first, we conduct beam search
to sample candidate program completions for each line; second, we extract
execution signals by executing these candidates against test cases; and
finally, we incorporate these signals into the prompt during generation. By
maintaining consistent signals across tokens within the same line and
refreshing signals at line boundaries, our approach provides coherent guidance
while preserving syntactic structure. Moreover, the method naturally supports
native parallelism at the task level in which multiple agents operate in
parallel, exploring diverse reasoning paths and collectively generating a broad
set of candidate solutions. Our experiments across diverse coding tasks
demonstrate that EG-CFG significantly improves code generation performance
compared to standard approaches, achieving state-of-the-art results across
various levels of complexity, from foundational problems to challenging
competitive programming tasks. Our code is available at:
https://github.com/boazlavon/eg_cfg
</summary>
    <author>
      <name>Boaz Lavon</name>
    </author>
    <author>
      <name>Shahar Katz</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2506.10948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10941v1</id>
    <updated>2025-06-12T17:46:54Z</updated>
    <published>2025-06-12T17:46:54Z</published>
    <title>VINCIE: Unlocking In-context Image Editing from Video</title>
    <summary>  In-context image editing aims to modify images based on a contextual sequence
comprising text and previously generated images. Existing methods typically
depend on task-specific pipelines and expert models (e.g., segmentation and
inpainting) to curate training data. In this work, we explore whether an
in-context image editing model can be learned directly from videos. We
introduce a scalable approach to annotate videos as interleaved multimodal
sequences. To effectively learn from this data, we design a block-causal
diffusion transformer trained on three proxy tasks: next-image prediction,
current segmentation prediction, and next-segmentation prediction.
Additionally, we propose a novel multi-turn image editing benchmark to advance
research in this area. Extensive experiments demonstrate that our model
exhibits strong in-context image editing capabilities and achieves
state-of-the-art results on two multi-turn image editing benchmarks. Despite
being trained exclusively on videos, our model also shows promising abilities
in multi-concept composition, story generation, and chain-of-editing
applications.
</summary>
    <author>
      <name>Leigang Qu</name>
    </author>
    <author>
      <name>Feng Cheng</name>
    </author>
    <author>
      <name>Ziyan Yang</name>
    </author>
    <author>
      <name>Qi Zhao</name>
    </author>
    <author>
      <name>Shanchuan Lin</name>
    </author>
    <author>
      <name>Yichun Shi</name>
    </author>
    <author>
      <name>Yicong Li</name>
    </author>
    <author>
      <name>Wenjie Wang</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <author>
      <name>Lu Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://vincie2025.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.10941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10922v1</id>
    <updated>2025-06-12T17:34:38Z</updated>
    <published>2025-06-12T17:34:38Z</published>
    <title>Robustly Improving LLM Fairness in Realistic Settings via
  Interpretability</title>
    <summary>  Large language models (LLMs) are increasingly deployed in high-stakes hiring
applications, making decisions that directly impact people's careers and
livelihoods. While prior studies suggest simple anti-bias prompts can eliminate
demographic biases in controlled evaluations, we find these mitigations fail
when realistic contextual details are introduced. We address these failures
through internal bias mitigation: by identifying and neutralizing sensitive
attribute directions within model activations, we achieve robust bias reduction
across all tested scenarios. Across leading commercial (GPT-4o, Claude 4
Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,
Mistral-24B), we find that adding realistic context such as company names,
culture descriptions from public careers pages, and selective hiring
constraints (e.g.,``only accept candidates in the top 10\%") induces
significant racial and gender biases (up to 12\% differences in interview
rates). When these biases emerge, they consistently favor Black over White
candidates and female over male candidates across all tested models and
scenarios. Moreover, models can infer demographics and become biased from
subtle cues like college affiliations, with these biases remaining invisible
even when inspecting the model's chain-of-thought reasoning. To address these
limitations, our internal bias mitigation identifies race and gender-correlated
directions and applies affine concept editing at inference time. Despite using
directions from a simple synthetic dataset, the intervention generalizes
robustly, consistently reducing bias to very low levels (typically under 1\%,
always below 2.5\%) while largely maintaining model performance. Our findings
suggest that practitioners deploying LLMs for hiring should adopt more
realistic evaluation methodologies and consider internal mitigation strategies
for equitable outcomes.
</summary>
    <author>
      <name>Adam Karvonen</name>
    </author>
    <author>
      <name>Samuel Marks</name>
    </author>
    <link href="http://arxiv.org/abs/2506.10922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10920v1</id>
    <updated>2025-06-12T17:33:29Z</updated>
    <published>2025-06-12T17:33:29Z</published>
    <title>Decomposing MLP Activations into Interpretable Features via
  Semi-Nonnegative Matrix Factorization</title>
    <summary>  A central goal for mechanistic interpretability has been to identify the
right units of analysis in large language models (LLMs) that causally explain
their outputs. While early work focused on individual neurons, evidence that
neurons often encode multiple concepts has motivated a shift toward analyzing
directions in activation space. A key question is how to find directions that
capture interpretable features in an unsupervised manner. Current methods rely
on dictionary learning with sparse autoencoders (SAEs), commonly trained over
residual stream activations to learn directions from scratch. However, SAEs
often struggle in causal evaluations and lack intrinsic interpretability, as
their learning is not explicitly tied to the computations of the model. Here,
we tackle these limitations by directly decomposing MLP activations with
semi-nonnegative matrix factorization (SNMF), such that the learned features
are (a) sparse linear combinations of co-activated neurons, and (b) mapped to
their activating inputs, making them directly interpretable. Experiments on
Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs
and a strong supervised baseline (difference-in-means) on causal steering,
while aligning with human-interpretable concepts. Further analysis reveals that
specific neuron combinations are reused across semantically-related features,
exposing a hierarchical structure in the MLP's activation space. Together,
these results position SNMF as a simple and effective tool for identifying
interpretable features and dissecting concept representations in LLMs.
</summary>
    <author>
      <name>Or Shafran</name>
    </author>
    <author>
      <name>Atticus Geiger</name>
    </author>
    <author>
      <name>Mor Geva</name>
    </author>
    <link href="http://arxiv.org/abs/2506.10920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10918v1</id>
    <updated>2025-06-12T17:32:02Z</updated>
    <published>2025-06-12T17:32:02Z</published>
    <title>Sequential-Parallel Duality in Prefix Scannable Models</title>
    <summary>  Modern neural sequence models are designed to meet the dual mandate of
parallelizable training and fast sequential inference. Recent developments have
given rise to various models, such as Gated Linear Attention (GLA) and Mamba,
that achieve such ``sequential-parallel duality.'' This raises a natural
question: can we characterize the full class of neural sequence models that
support near-constant-time parallel evaluation and linear-time, constant-space
sequential inference? We begin by describing a broad class of such models --
state space models -- as those whose state updates can be computed using the
classic parallel prefix scan algorithm with a custom associative aggregation
operator. We then define a more general class, Prefix-Scannable Models (PSMs),
by relaxing the state aggregation operator to allow arbitrary (potentially
non-associative) functions such as softmax attention. This generalization
unifies many existing architectures, including element-wise RNNs (e.g., Mamba)
and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new
models with softmax-like operators that achieve O(1) amortized compute per
token and log(N) memory for sequence length N. We empirically evaluate such
models on illustrative small-scale language modeling and canonical synthetic
tasks, including state tracking and associative recall. Empirically, we find
that PSMs retain the expressivity of transformer-based architectures while
matching the inference efficiency of state space models -- in some cases
exhibiting better length generalization than either.
</summary>
    <author>
      <name>Morris Yau</name>
    </author>
    <author>
      <name>Sharut Gupta</name>
    </author>
    <author>
      <name>Valerie Engelmayer</name>
    </author>
    <author>
      <name>Kazuki Irie</name>
    </author>
    <author>
      <name>Stefanie Jegelka</name>
    </author>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <link href="http://arxiv.org/abs/2506.10918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10914v1</id>
    <updated>2025-06-12T17:29:29Z</updated>
    <published>2025-06-12T17:29:29Z</published>
    <title>Foundation Models for Causal Inference via Prior-Data Fitted Networks</title>
    <summary>  Prior-data fitted networks (PFNs) have recently been proposed as a promising
way to train tabular foundation models. PFNs are transformers that are
pre-trained on synthetic data generated from a prespecified prior distribution
and that enable Bayesian inference through in-context learning. In this paper,
we introduce CausalFM, a comprehensive framework for training PFN-based
foundation models in various causal inference settings. First, we formalize the
construction of Bayesian priors for causal inference based on structural causal
models (SCMs) in a principled way and derive necessary criteria for the
validity of such priors. Building on this, we propose a novel family of prior
distributions using causality-inspired Bayesian neural networks that enable
CausalFM to perform Bayesian causal inference in various settings, including
back-door, front-door, and instrumental variable adjustment. Finally, we
instantiate CausalFM and explicitly train a foundation model for estimating
conditional average treatment effects (CATEs) using back-door adjustment. We
show that CausalFM performs competitively for CATE estimation using various
synthetic and semi-synthetic benchmarks. In sum, our framework can be used as a
general recipe to train foundation models for various causal inference
settings. In contrast to the current state-of-the-art in causal inference,
CausalFM offers a novel paradigm with the potential to fundamentally change how
practitioners perform causal inference in medicine, economics, and other
disciplines.
</summary>
    <author>
      <name>Yuchen Ma</name>
    </author>
    <author>
      <name>Dennis Frauen</name>
    </author>
    <author>
      <name>Emil Javurek</name>
    </author>
    <author>
      <name>Stefan Feuerriegel</name>
    </author>
    <link href="http://arxiv.org/abs/2506.10914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
