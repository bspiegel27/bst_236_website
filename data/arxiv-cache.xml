<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-08T00:52:31Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-07T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">122909</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.05094v1</id>
    <updated>2025-10-06T17:57:59Z</updated>
    <published>2025-10-06T17:57:59Z</published>
    <title>VChain: Chain-of-Visual-Thought for Reasoning in Video Generation</title>
    <summary>  Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.
</summary>
    <author>
      <name>Ziqi Huang</name>
    </author>
    <author>
      <name>Ning Yu</name>
    </author>
    <author>
      <name>Gordon Chen</name>
    </author>
    <author>
      <name>Haonan Qiu</name>
    </author>
    <author>
      <name>Paul Debevec</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://eyeline-labs.github.io/VChain Code:
  https://github.com/Eyeline-Labs/VChain</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.05094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05091v1</id>
    <updated>2025-10-06T17:56:55Z</updated>
    <published>2025-10-06T17:56:55Z</published>
    <title>Factuality Matters: When Image Generation and Editing Meet Structured
  Visuals</title>
    <summary>  While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&amp;A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.
</summary>
    <author>
      <name>Le Zhuo</name>
    </author>
    <author>
      <name>Songhao Han</name>
    </author>
    <author>
      <name>Yuandong Pu</name>
    </author>
    <author>
      <name>Boxiang Qiu</name>
    </author>
    <author>
      <name>Sayak Paul</name>
    </author>
    <author>
      <name>Yue Liao</name>
    </author>
    <author>
      <name>Yihao Liu</name>
    </author>
    <author>
      <name>Jie Shao</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Si Liu</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://structvisuals.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.05091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05085v1</id>
    <updated>2025-10-06T17:53:05Z</updated>
    <published>2025-10-06T17:53:05Z</published>
    <title>WOW: WAIC-Optimized Gating of Mixture Priors for External Data Borrowing</title>
    <summary>  The integration of external data using Bayesian mixture priors has become a
powerful approach in clinical trials, offering significant potential to improve
trial efficiency. Despite their strengths in analytical tractability and
practical flexibility, existing methods such as the robust
meta-analytic-predictive (rMAP) and self-adapting mixture (SAM) often presume
borrowing without rigorously assessing whether, how, or when integration is
appropriate. When external and concurrent data are discordant, excessive
borrowing can bias estimates and lead to misleading conclusions. To address
this, we introduce WOW, a Kullback-Leibler-based gating strategy guided by the
widely applicable information criterion (WAIC). WOW conducts a preliminary
compatibility assessment between external and concurrent trial data and gates
the level of borrowing accordingly. The approach is prior-agnostic and can be
seamlessly integrated with any mixture prior method, whether using fixed or
adaptive weighting schemes, after the WOW step. Simulation studies demonstrate
that incorporating the WOW strategy before Bayesian mixture prior borrowing
methods effectively mitigates excessive borrowing and improves estimation
accuracy. By providing robust and reliable inference, WOW strengthens the
performance of mixture-prior methods and supports better decision-making in
clinical trials.
</summary>
    <author>
      <name>Shouhao Zhou</name>
    </author>
    <author>
      <name>Qiuxin Gao</name>
    </author>
    <author>
      <name>Chenqi Fu</name>
    </author>
    <author>
      <name>Yanxun Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2510.05085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05059v1</id>
    <updated>2025-10-06T17:37:35Z</updated>
    <published>2025-10-06T17:37:35Z</published>
    <title>Staircase Streaming for Low-Latency Multi-Agent Inference</title>
    <summary>  Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.
</summary>
    <author>
      <name>Junlin Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Zach</arxiv:affiliation>
    </author>
    <author>
      <name>Jue Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Zach</arxiv:affiliation>
    </author>
    <author>
      <name> Zhen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Zach</arxiv:affiliation>
    </author>
    <author>
      <name> Xu</name>
    </author>
    <author>
      <name>Ben Athiwaratkun</name>
    </author>
    <author>
      <name>Bhuwan Dhingra</name>
    </author>
    <author>
      <name>Ce Zhang</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <link href="http://arxiv.org/abs/2510.05059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05057v1</id>
    <updated>2025-10-06T17:37:24Z</updated>
    <published>2025-10-06T17:37:24Z</published>
    <title>StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact
  State Representation</title>
    <summary>  A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.
</summary>
    <author>
      <name>Mingyu Liu</name>
    </author>
    <author>
      <name>Jiuhe Shu</name>
    </author>
    <author>
      <name>Hui Chen</name>
    </author>
    <author>
      <name>Zeju Li</name>
    </author>
    <author>
      <name>Canyu Zhao</name>
    </author>
    <author>
      <name>Jiange Yang</name>
    </author>
    <author>
      <name>Shenyuan Gao</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2510.05057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05040v1</id>
    <updated>2025-10-06T17:16:41Z</updated>
    <published>2025-10-06T17:16:41Z</published>
    <title>Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive
  Experts</title>
    <summary>  Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.
</summary>
    <author>
      <name>Jihoon Lee</name>
    </author>
    <author>
      <name>Hoyeon Moon</name>
    </author>
    <author>
      <name>Kevin Zhai</name>
    </author>
    <author>
      <name>Arun Kumar Chithanar</name>
    </author>
    <author>
      <name>Anit Kumar Sahu</name>
    </author>
    <author>
      <name>Soummya Kar</name>
    </author>
    <author>
      <name>Chul Lee</name>
    </author>
    <author>
      <name>Souradip Chakraborty</name>
    </author>
    <author>
      <name>Amrit Singh Bedi</name>
    </author>
    <link href="http://arxiv.org/abs/2510.05040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05037v1</id>
    <updated>2025-10-06T17:12:03Z</updated>
    <published>2025-10-06T17:12:03Z</published>
    <title>On the sensitivity of different galaxy properties to warm dark matter</title>
    <summary>  We study the impact of warm dark matter (WDM) particle mass on galaxy
properties using 1,024 state-of-the-art cosmological hydrodynamical simulations
from the DREAMS project. We begin by using a Multilayer Perceptron (MLP)
coupled with a normalizing flow to explore global statistical descriptors of
galaxy populations, such as the mean, standard deviation, and histograms of 14
galaxy properties. We find that subhalo gas mass is the most informative
feature for constraining the WDM mass, achieving a determination coefficient of
R^2 = 0.9. We employ symbolic regression to extract simple, interpretable
relations with the WDM particle mass. Finally, we adopt a more localized
approach by selecting individual dark matter halos and using a Graph Neural
Network (GNN) with a normalizing flow to infer the WDM mass, incorporating
subhalo properties as node features and global simulation statistics as
graph-level features. The GNN approach yields only a residual improvement over
MLP models based solely on global features, indicating that most of the
predictive power resides in the global descriptors, with only marginal gains
from halo-level information.
</summary>
    <author>
      <name>Bel√©n Costanza</name>
    </author>
    <author>
      <name>Bonny Y. Wang</name>
    </author>
    <author>
      <name>Francisco Villaescusa-Navarro</name>
    </author>
    <author>
      <name>Alex M. Garcia</name>
    </author>
    <author>
      <name>Jonah C. Rose</name>
    </author>
    <author>
      <name>Mark Vogelsberger</name>
    </author>
    <author>
      <name>Paul Torrey</name>
    </author>
    <author>
      <name>Arya Farahi</name>
    </author>
    <author>
      <name>Xuejian Shen</name>
    </author>
    <author>
      <name>Ilem Leisher</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3847/1538-4357/ae0e6c</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3847/1538-4357/ae0e6c" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in The Astrophysical Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.05037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05034v1</id>
    <updated>2025-10-06T17:10:44Z</updated>
    <published>2025-10-06T17:10:44Z</published>
    <title>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large
  Multimodal Models</title>
    <summary>  Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training
</summary>
    <author>
      <name>Yunlong Tang</name>
    </author>
    <author>
      <name>Jing Bi</name>
    </author>
    <author>
      <name>Pinxin Liu</name>
    </author>
    <author>
      <name>Zhenyu Pan</name>
    </author>
    <author>
      <name>Zhangyun Tan</name>
    </author>
    <author>
      <name>Qianxiang Shen</name>
    </author>
    <author>
      <name>Jiani Liu</name>
    </author>
    <author>
      <name>Hang Hua</name>
    </author>
    <author>
      <name>Junjia Guo</name>
    </author>
    <author>
      <name>Yunzhong Xiao</name>
    </author>
    <author>
      <name>Chao Huang</name>
    </author>
    <author>
      <name>Zhiyuan Wang</name>
    </author>
    <author>
      <name>Susan Liang</name>
    </author>
    <author>
      <name>Xinyi Liu</name>
    </author>
    <author>
      <name>Yizhi Song</name>
    </author>
    <author>
      <name>Yuhe Nie</name>
    </author>
    <author>
      <name>Jia-Xing Zhong</name>
    </author>
    <author>
      <name>Bozheng Li</name>
    </author>
    <author>
      <name>Daiqing Qi</name>
    </author>
    <author>
      <name>Ziyun Zeng</name>
    </author>
    <author>
      <name>Ali Vosoughi</name>
    </author>
    <author>
      <name>Luchuan Song</name>
    </author>
    <author>
      <name>Zeliang Zhang</name>
    </author>
    <author>
      <name>Daiki Shimada</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <author>
      <name>Chenliang Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 1st version</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.05034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05033v1</id>
    <updated>2025-10-06T17:09:30Z</updated>
    <published>2025-10-06T17:09:30Z</published>
    <title>Causal Abstractions, Categorically Unified</title>
    <summary>  We present a categorical framework for relating causal models that represent
the same system at different levels of abstraction. We define a causal
abstraction as natural transformations between appropriate Markov functors,
which concisely consolidate desirable properties a causal abstraction should
exhibit. Our approach unifies and generalizes previously considered causal
abstractions, and we obtain categorical proofs and generalizations of existing
results on causal abstractions. Using string diagrammatical tools, we can
explicitly describe the graphs that serve as consistent abstractions of a
low-level graph under interventions. We discuss how methods from mechanistic
interpretability, such as circuit analysis and sparse autoencoders, fit within
our categorical framework. We also show how applying do-calculus on a
high-level graphical abstraction of an acyclic-directed mixed graph (ADMG),
when unobserved confounders are present, gives valid results on the low-level
graph, thus generalizing an earlier statement by Anand et al. (2023). We argue
that our framework is more suitable for modeling causal abstractions compared
to existing categorical frameworks. Finally, we discuss how notions such as
$\tau$-consistency and constructive $\tau$-abstractions can be recovered with
our framework.
</summary>
    <author>
      <name>Markus Englberger</name>
    </author>
    <author>
      <name>Devendra Singh Dhami</name>
    </author>
    <link href="http://arxiv.org/abs/2510.05033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05029v1</id>
    <updated>2025-10-06T17:06:28Z</updated>
    <published>2025-10-06T17:06:28Z</published>
    <title>Inferring the spins of merging black holes in the presence of
  data-quality issues</title>
    <summary>  Gravitational waves from black hole binary mergers carry information about
the component spins, but inference is sensitive to analysis assumptions, which
may be broken by terrestrial noise transients known as glitches. Using a
variety of simulated glitches and gravitational wave signals, we study the
conditions under which glitches can bias spin measurements. We confirm the
theoretical expectation that inference and subtraction of glitches invariably
leaves behind residual power due to statistical uncertainty, no matter the
strength (signal-to-noise ratio; SNR) of the original glitch. Next we show that
low-SNR glitches - including those below the threshold for flagging
data-quality issues - can still significantly bias spin inference. Such biases
occur for a range of glitch morphologies, even in cases where glitches and
signals are not precisely aligned in phase. Furthermore, we find that residuals
of glitch subtraction can result in biases as well. Our results suggest that
joint inference of the glitch and gravitational wave parameters, with
appropriate models and priors, is required to address these uncertainties
inherent in glitch mitigation via subtraction.
</summary>
    <author>
      <name>Rhiannon Udall</name>
    </author>
    <author>
      <name>Sophie Bini</name>
    </author>
    <author>
      <name>Katerina Chatziioannou</name>
    </author>
    <author>
      <name>Derek Davis</name>
    </author>
    <author>
      <name>Sophie Hourihane</name>
    </author>
    <author>
      <name>Yannick Lecoeuche</name>
    </author>
    <author>
      <name>Jess McIver</name>
    </author>
    <author>
      <name>Simona Miller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 14 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.05029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
