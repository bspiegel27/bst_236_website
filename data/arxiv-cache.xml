<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-14T00:59:26Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-13T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">118803</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.09136v1</id>
    <updated>2025-08-12T17:59:46Z</updated>
    <published>2025-08-12T17:59:46Z</published>
    <title>Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices</title>
    <summary>  There is a growing demand for deploying large generative AI models on mobile
devices. For recent popular video generative models, however, the Variational
AutoEncoder (VAE) represents one of the major computational bottlenecks. Both
large parameter sizes and mismatched kernels cause out-of-memory errors or
extremely slow inference on mobile devices. To address this, we propose a
low-cost solution that efficiently transfers widely used video VAEs to mobile
devices. (1) We analyze redundancy in existing VAE architectures and get
empirical design insights. By integrating 3D depthwise separable convolutions
into our model, we significantly reduce the number of parameters. (2) We
observe that the upsampling techniques in mainstream video VAEs are poorly
suited to mobile hardware and form the main bottleneck. In response, we propose
a decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building
upon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)
We propose an efficient VAE decoder training method. Since only the decoder is
used during deployment, we distill it to Turbo-VAED instead of retraining the
full VAE, enabling fast mobile adaptation with minimal performance loss. To our
knowledge, our method enables real-time 720p video VAE decoding on mobile
devices for the first time. This approach is widely applicable to most video
VAEs. When integrated into four representative models, with training cost as
low as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on
GPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of
the original reconstruction quality. Compared to mobile-optimized VAEs,
Turbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on
the iPhone 16 Pro. The code and models will soon be available at
https://github.com/hustvl/Turbo-VAED.
</summary>
    <author>
      <name>Ya Zou</name>
    </author>
    <author>
      <name>Jingfeng Yao</name>
    </author>
    <author>
      <name>Siyuan Yu</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2508.09136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09135v1</id>
    <updated>2025-08-12T17:59:32Z</updated>
    <published>2025-08-12T17:59:32Z</published>
    <title>Efficient Statistical Estimation for Sequential Adaptive Experiments
  with Implications for Adaptive Designs</title>
    <summary>  Adaptive experimental designs have gained popularity in clinical trials and
online experiments. Unlike traditional, fixed experimental designs, adaptive
designs can dynamically adjust treatment randomization probabilities and other
design features in response to data accumulated sequentially during the
experiment. These adaptations are useful to achieve diverse objectives,
including reducing uncertainty in the estimation of causal estimands or
increasing participants' chances of receiving better treatments during the
experiment. At the end of the experiment, it is often desirable to answer
causal questions from the observed data. However, the adaptive nature of such
experiments and the resulting dependence among observations pose significant
challenges to providing valid statistical inference and efficient estimation of
causal estimands. Building upon the Targeted Maximum Likelihood Estimator
(TMLE) framework tailored for adaptive designs (van der Laan, 2008), we
introduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a
variety of causal estimands from adaptive experiment data. We establish
asymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed
positivity and design stabilization assumptions for adaptive experiments.
Motivated by efficiency results, we further propose a novel adaptive design
aimed at minimizing the variance of estimators based on data generated under
that design. Using the average treatment effect as a representative example,
simulation studies show that ADL-TMLE demonstrates superior variance-reduction
performance across different types of adaptive experiments, and that the
proposed adaptive design attains lower variance than the standard
efficiency-oriented adaptive design. Finally, we generalize this estimation and
design framework to broader settings with longitudinal structures.
</summary>
    <author>
      <name>Wenxin Zhang</name>
    </author>
    <author>
      <name>Mark van der Laan</name>
    </author>
    <link href="http://arxiv.org/abs/2508.09135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09126v1</id>
    <updated>2025-08-12T17:55:08Z</updated>
    <published>2025-08-12T17:55:08Z</published>
    <title>Neutone SDK: An Open Source Framework for Neural Audio Processing</title>
    <summary>  Neural audio processing has unlocked novel methods of sound transformation
and synthesis, yet integrating deep learning models into digital audio
workstations (DAWs) remains challenging due to real-time / neural network
inference constraints and the complexities of plugin development. In this
paper, we introduce the Neutone SDK: an open source framework that streamlines
the deployment of PyTorch-based neural audio models for both real-time and
offline applications. By encapsulating common challenges such as variable
buffer sizes, sample rate conversion, delay compensation, and control parameter
handling within a unified, model-agnostic interface, our framework enables
seamless interoperability between neural models and host plugins while allowing
users to work entirely in Python. We provide a technical overview of the
interfaces needed to accomplish this, as well as the corresponding SDK
implementations. We also demonstrate the SDK's versatility across applications
such as audio effect emulation, timbre transfer, and sample generation, as well
as its adoption by researchers, educators, companies, and artists alike. The
Neutone SDK is available at https://github.com/Neutone/neutone_sdk
</summary>
    <author>
      <name>Christopher Mitcheltree</name>
    </author>
    <author>
      <name>Bogdan Teleaga</name>
    </author>
    <author>
      <name>Andrew Fyfe</name>
    </author>
    <author>
      <name>Naotake Masuda</name>
    </author>
    <author>
      <name>Matthias Sch√§fer</name>
    </author>
    <author>
      <name>Alfie Bradic</name>
    </author>
    <author>
      <name>Nao Tokui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AES International Conference on Artificial Intelligence
  and Machine Learning for Audio 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.09126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09105v2</id>
    <updated>2025-08-13T11:05:22Z</updated>
    <published>2025-08-12T17:32:24Z</published>
    <title>SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG
  Controlling</title>
    <summary>  Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented
Generation (MRAG) significantly improve the knowledge coverage and contextual
understanding of Large Language Models (LLMs) by introducing external knowledge
sources. However, retrieval and multimodal fusion obscure content provenance,
rendering existing membership inference methods unable to reliably attribute
generated outputs to pre-training, external retrieval, or user input, thus
undermining privacy leakage accountability
  To address these challenges, we propose the first Source-aware Membership
Audit (SMA) that enables fine-grained source attribution of generated content
in a semi-black-box setting with retrieval control capabilities. To address the
environmental constraints of semi-black-box auditing, we further design an
attribution estimation mechanism based on zero-order optimization, which
robustly approximates the true influence of input tokens on the output through
large-scale perturbation sampling and ridge regression modeling. In addition,
SMA introduces a cross-modal attribution technique that projects image inputs
into textual descriptions via MLLMs, enabling token-level attribution in the
text modality, which for the first time facilitates membership inference on
image retrieval traces in MRAG systems. This work shifts the focus of
membership inference from 'whether the data has been memorized' to 'where the
content is sourced from', offering a novel perspective for auditing data
provenance in complex generative systems.
</summary>
    <author>
      <name>Shixuan Sun</name>
    </author>
    <author>
      <name>Siyuan Liang</name>
    </author>
    <author>
      <name>Ruoyu Chen</name>
    </author>
    <author>
      <name>Jianjie Huang</name>
    </author>
    <author>
      <name>Jingzhi Li</name>
    </author>
    <author>
      <name>Xiaochun Cao</name>
    </author>
    <link href="http://arxiv.org/abs/2508.09105v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09105v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09100v1</id>
    <updated>2025-08-12T17:26:48Z</updated>
    <published>2025-08-12T17:26:48Z</published>
    <title>Towards Universal Neural Inference</title>
    <summary>  Real-world data often appears in diverse, disjoint forms -- with varying
schemas, inconsistent semantics, and no fixed feature ordering -- making it
challenging to build general-purpose models that can leverage information
across datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant
Reasoning Engine, a Universal Neural Inference model for semantic reasoning and
prediction over heterogeneous structured data. ASPIRE combines a
permutation-invariant, set-based Transformer with a semantic grounding module
that incorporates natural language descriptions, dataset metadata, and
in-context examples to learn cross-dataset feature dependencies. This
architecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and
support examples, align semantics across disjoint tables, and make predictions
for any specified target. Once trained, ASPIRE generalizes to new inference
tasks without additional tuning. In addition to delivering strong results
across diverse benchmarks, ASPIRE naturally supports cost-aware active feature
acquisition in an open-world setting, selecting informative features under
test-time budget constraints for an arbitrary unseen dataset. These
capabilities position ASPIRE as a step toward truly universal, semantics-aware
inference over structured data.
</summary>
    <author>
      <name>Shreyas Bhat Brahmavar</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Junier Oliva</name>
    </author>
    <link href="http://arxiv.org/abs/2508.09100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09097v1</id>
    <updated>2025-08-12T17:24:56Z</updated>
    <published>2025-08-12T17:24:56Z</published>
    <title>Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs</title>
    <summary>  We introduce Chi-Geometry - a library that generates graph data for testing
and benchmarking GNNs' ability to predict chirality. Chi-Geometry generates
synthetic graph samples with (i) user-specified geometric and topological
traits to isolate certain types of samples and (ii) randomized node positions
and species to minimize extraneous correlations. Each generated graph contains
exactly one chiral center labeled either R or S, while all other nodes are
labeled N/A (non-chiral). The generated samples are then combined into a
cohesive dataset that can be used to assess a GNN's ability to predict
chirality as a node classification task. Chi-Geometry allows more interpretable
and less confounding benchmarking of GNNs for prediction of chirality in the
graph samples which can guide the design of new GNN architectures with improved
predictive performance. We illustrate Chi-Geometry's efficacy by using it to
generate synthetic datasets for benchmarking various state-of-the-art (SOTA)
GNN architectures. The conclusions of these benchmarking results guided our
design of two new GNN architectures. The first GNN architecture established
all-to-all connections in the graph to accurately predict chirality across all
challenging configurations where previously tested SOTA models failed, but at a
computational cost (both for training and inference) that grows quadratically
with the number of graph nodes. The second GNN architecture avoids all-to-all
connections by introducing a virtual node in the original graph structure of
the data, which restores the linear scaling of training and inference
computational cost with respect to the number of nodes in the graph, while
still ensuring competitive accuracy in detecting chirality with respect to SOTA
GNN architectures.
</summary>
    <author>
      <name>Rylie Weaver</name>
    </author>
    <author>
      <name>Massamiliano Lupo Pasini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages total: 9 pages main text, 4 pages references, 8 pages
  appendices. 4 figures and 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.09097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09096v1</id>
    <updated>2025-08-12T17:22:29Z</updated>
    <published>2025-08-12T17:22:29Z</published>
    <title>Link Prediction for Event Logs in the Process Industry</title>
    <summary>  Knowledge management (KM) is vital in the process industry for optimizing
operations, ensuring safety, and enabling continuous improvement through
effective use of operational data and past insights. A key challenge in this
domain is the fragmented nature of event logs in shift books, where related
records, e.g., entries documenting issues related to equipment or processes and
the corresponding solutions, may remain disconnected. This fragmentation
hinders the recommendation of previous solutions to the users. To address this
problem, we investigate record linking (RL) as link prediction, commonly
studied in graph-based machine learning, by framing it as a cross-document
coreference resolution (CDCR) task enhanced with natural language inference
(NLI) and semantic text similarity (STS) by shifting it into the causal
inference (CI). We adapt CDCR, traditionally applied in the news domain, into
an RL model to operate at the passage level, similar to NLI and STS, while
accommodating the process industry's specific text formats, which contain
unstructured text and structured record attributes. Our RL model outperformed
the best versions of NLI- and STS-driven baselines by 28% (11.43 points) and
27% (11.21 points), respectively. Our work demonstrates how domain adaptation
of the state-of-the-art CDCR models, enhanced with reasoning capabilities, can
be effectively tailored to the process industry, improving data quality and
connectivity in shift logs.
</summary>
    <author>
      <name>Anastasia Zhukova</name>
    </author>
    <author>
      <name>Thomas Walton</name>
    </author>
    <author>
      <name>Christian E. Matt</name>
    </author>
    <author>
      <name>Bela Gipp</name>
    </author>
    <link href="http://arxiv.org/abs/2508.09096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09090v2</id>
    <updated>2025-08-13T01:51:32Z</updated>
    <published>2025-08-12T17:16:37Z</published>
    <title>SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via
  Codebooks for recommender system</title>
    <summary>  Modeling multi-interests has arisen as a core problem in real-world RS.
Current multi-interest retrieval methods pose three major challenges: 1)
Interests, typically extracted from predefined external knowledge, are
invariant. Failed to dynamically evolve with users' real-time consumption
preferences. 2) Online inference typically employs an over-exploited strategy,
mainly matching users' existing interests, lacking proactive exploration and
discovery of novel and long-tail interests. To address these challenges, we
propose a novel retrieval framework named SPARC(Soft Probabilistic Adaptive
Retrieval Model via Codebooks). Our contribution is two folds. First, the
framework utilizes Residual Quantized Variational Autoencoder (RQ-VAE) to
construct a discretized interest space. It achieves joint training of the
RQ-VAE with the industrial large scale recommendation model, mining
behavior-aware interests that can perceive user feedback and evolve
dynamically. Secondly, a probabilistic interest module that predicts the
probability distribution over the entire dynamic and discrete interest space.
This facilitates an efficient "soft-search" strategy during online inference,
revolutionizing the retrieval paradigm from "passive matching" to "proactive
exploration" and thereby effectively promoting interest discovery. Online A/B
tests on an industrial platform with tens of millions daily active users, have
achieved substantial gains in business metrics: +0.9% increase in user view
duration, +0.4% increase in user page views (PV), and a +22.7% improvement in
PV500(new content reaching 500 PVs in 24 hours). Offline evaluations are
conducted on open-source Amazon Product datasets. Metrics, such as Recall@K and
Normalized Discounted Cumulative Gain@K(NDCG@K), also showed consistent
improvement. Both online and offline experiments validate the efficacy and
practical value of the proposed method.
</summary>
    <author>
      <name>Jialiang Shi</name>
    </author>
    <author>
      <name>Yaguang Dou</name>
    </author>
    <author>
      <name>Tian Qi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.09090v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09090v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09087v1</id>
    <updated>2025-08-12T17:07:58Z</updated>
    <published>2025-08-12T17:07:58Z</published>
    <title>Addressing Bias in VLMs for Glaucoma Detection Without Protected
  Attribute Supervision</title>
    <summary>  Vision-Language Models (VLMs) have achieved remarkable success on multimodal
tasks such as image-text retrieval and zero-shot classification, yet they can
exhibit demographic biases even when explicit protected attributes are absent
during training. In this work, we focus on automated glaucoma screening from
retinal fundus images, a critical application given that glaucoma is a leading
cause of irreversible blindness and disproportionately affects underserved
populations. Building on a reweighting-based contrastive learning framework, we
introduce an attribute-agnostic debiasing method that (i) infers proxy
subgroups via unsupervised clustering of image-image embeddings, (ii) computes
gradient-similarity weights between the CLIP-style multimodal loss and a
SimCLR-style image-pair contrastive loss, and (iii) applies these weights in a
joint, top-$k$ weighted objective to upweight underperforming clusters. This
label-free approach adaptively targets the hardest examples, thereby reducing
subgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma
subset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES
AUC), and Groupwise AUC to demonstrate equitable performance across inferred
demographic subgroups.
</summary>
    <author>
      <name>Ahsan Habib Akash</name>
    </author>
    <author>
      <name>Greg Murray</name>
    </author>
    <author>
      <name>Annahita Amireskandari</name>
    </author>
    <author>
      <name>Joel Palko</name>
    </author>
    <author>
      <name>Carol Laxson</name>
    </author>
    <author>
      <name>Binod Bhattarai</name>
    </author>
    <author>
      <name>Prashnna Gyawali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd Workshop in Data Engineering in Medical Imaging (DEMI),
  MICCAI-2025 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.09087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.09072v1</id>
    <updated>2025-08-12T16:47:48Z</updated>
    <published>2025-08-12T16:47:48Z</published>
    <title>READER: Retrieval-Assisted Drafter for Efficient LLM Inference</title>
    <summary>  Large Language Models (LLMs) generate tokens autoregressively, with each
token depending on the preceding context. This sequential nature makes the
inference process inherently difficult to accelerate, posing a significant
challenge for efficient deployment. In recent years, various methods have been
proposed to address this issue, with the most effective approaches often
involving the training of additional draft models. In this paper, we introduce
READER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel
lossless speculative decoding method that enhances model-based approaches by
leveraging self-repetitions in the text. Our algorithm expands the speculative
decoding tree using tokens obtained through statistical search. This work
focuses on large batch sizes (&gt;= 8), an underexplored yet important area for
industrial applications. We also analyze the key-value (KV) cache size during
speculative decoding and propose an optimization to improve performance for
large batches. As a result, READER outperforms existing speculative decoding
methods. Notably, READER requires no additional training and can reuse
pre-trained speculator models, increasing the speedup by over 40\%. Our method
demonstrates particularly strong performance on search-based tasks, such as
retrieval-augmented generation, where we achieve more than 10x speedup.
</summary>
    <author>
      <name>Maxim Divilkovskiy</name>
    </author>
    <author>
      <name>Vitaly Malygin</name>
    </author>
    <author>
      <name>Sergey Zlobin</name>
    </author>
    <author>
      <name>Sultan Isali</name>
    </author>
    <author>
      <name>Vasily Kalugin</name>
    </author>
    <author>
      <name>Stanislav Ilyushin</name>
    </author>
    <author>
      <name>Nuriza Aitassova</name>
    </author>
    <author>
      <name>Yi Fei</name>
    </author>
    <author>
      <name>Zeng Weidi</name>
    </author>
    <link href="http://arxiv.org/abs/2508.09072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.09072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
