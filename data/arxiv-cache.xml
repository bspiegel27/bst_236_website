<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-21T01:04:30Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-21T01:04:30Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>129047</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.16922v1</id>
    <title>Next-Embedding Prediction Makes Strong Vision Learners</title>
    <updated>2025-12-18T18:59:58Z</updated>
    <link href="https://arxiv.org/abs/2512.16922v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16922v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:59:58Z</published>
    <arxiv:comment>Project Page: https://sihanxu.me/nepa</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Sihan Xu</name>
    </author>
    <author>
      <name>Ziqiao Ma</name>
    </author>
    <author>
      <name>Wenhao Chai</name>
    </author>
    <author>
      <name>Xuweiyi Chen</name>
    </author>
    <author>
      <name>Weiyang Jin</name>
    </author>
    <author>
      <name>Joyce Chai</name>
    </author>
    <author>
      <name>Saining Xie</name>
    </author>
    <author>
      <name>Stella X. Yu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16919v1</id>
    <title>DVGT: Driving Visual Geometry Transformer</title>
    <updated>2025-12-18T18:59:57Z</updated>
    <link href="https://arxiv.org/abs/2512.16919v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16919v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:59:57Z</published>
    <arxiv:comment>Code is available at https://github.com/wzzheng/DVGT</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Sicheng Zuo</name>
    </author>
    <author>
      <name>Zixun Xie</name>
    </author>
    <author>
      <name>Wenzhao Zheng</name>
    </author>
    <author>
      <name>Shaoqing Xu</name>
    </author>
    <author>
      <name>Fang Li</name>
    </author>
    <author>
      <name>Shengyin Jiang</name>
    </author>
    <author>
      <name>Long Chen</name>
    </author>
    <author>
      <name>Zhi-Xin Yang</name>
    </author>
    <author>
      <name>Jiwen Lu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16918v1</id>
    <title>AdaTooler-V: Adaptive Tool-Use for Images and Videos</title>
    <updated>2025-12-18T18:59:55Z</updated>
    <link href="https://arxiv.org/abs/2512.16918v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16918v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:59:55Z</published>
    <arxiv:comment>Project page: https://github.com/CYWang735/AdaTooler-V</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Chaoyang Wang</name>
    </author>
    <author>
      <name>Kaituo Feng</name>
    </author>
    <author>
      <name>Dongyang Chen</name>
    </author>
    <author>
      <name>Zhongyu Wang</name>
    </author>
    <author>
      <name>Zhixun Li</name>
    </author>
    <author>
      <name>Sicheng Gao</name>
    </author>
    <author>
      <name>Meng Meng</name>
    </author>
    <author>
      <name>Xu Zhou</name>
    </author>
    <author>
      <name>Manyuan Zhang</name>
    </author>
    <author>
      <name>Yuzhang Shang</name>
    </author>
    <author>
      <name>Xiangyu Yue</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16916v1</id>
    <title>Discovering gravitational waveform distortions from lensing: a deep dive into GW231123</title>
    <updated>2025-12-18T18:59:53Z</updated>
    <link href="https://arxiv.org/abs/2512.16916v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16916v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Gravitational waves (GWs) are unique messengers as they travel through the Universe without alteration except for gravitational lensing. Their long wavelengths make them susceptible to diffraction by cosmic structures, providing an unprecedented opportunity to map dark matter substructures. Identifying lensed events requires the analysis of thousands to millions of simulated events to reach high statistical significances. This is computationally prohibitive with standard GW parameter estimation methods. We build on top of state-of-the-art neural posterior algorithms to accelerate the lensed inference from CPU days to minutes with DINGO-lensing. We showcase its capabilities by reanalyzing GW231123, the most promising lensed candidate so far, and find that its statistical significance cannot exceed 4$σ$. We observe that 8% of GW231123-like nonlensed simulations favor lensing, which could be explained by the self-similarity of short-duration signals. Still, 58% of GW231123-like lensed simulations have larger support for lensing, showing that higher detection statistics are possible. Although GW231123 exposes the challenges of claiming the first GW lensing detection, our deep-learning methods have demonstrated to be powerful enough to enable the upcoming discovery of lensed GWs.</summary>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:59:53Z</published>
    <arxiv:comment>7 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="astro-ph.CO"/>
    <author>
      <name>Juno C. L. Chan</name>
    </author>
    <author>
      <name>Jose María Ezquiaga</name>
    </author>
    <author>
      <name>Rico K. L. Lo</name>
    </author>
    <author>
      <name>Joey Bowman</name>
    </author>
    <author>
      <name>Lorena Magaña Zertuche</name>
    </author>
    <author>
      <name>Luka Vujeva</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16910v1</id>
    <title>SFTok: Bridging the Performance Gap in Discrete Tokenizers</title>
    <updated>2025-12-18T18:59:04Z</updated>
    <link href="https://arxiv.org/abs/2512.16910v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16910v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:59:04Z</published>
    <arxiv:comment>Under review. Code is available at https://github.com/Neur-IO/SFTok</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Qihang Rao</name>
    </author>
    <author>
      <name>Borui Zhang</name>
    </author>
    <author>
      <name>Wenzhao Zheng</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Jiwen Lu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16902v1</id>
    <title>In-Context Algebra</title>
    <updated>2025-12-18T18:56:50Z</updated>
    <link href="https://arxiv.org/abs/2512.16902v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16902v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:56:50Z</published>
    <arxiv:comment>28 pages, 18 figures. Code and data at https://algebra.baulab.info</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Eric Todd</name>
    </author>
    <author>
      <name>Jannik Brinkmann</name>
    </author>
    <author>
      <name>Rohit Gandikota</name>
    </author>
    <author>
      <name>David Bau</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16900v1</id>
    <title>FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</title>
    <updated>2025-12-18T18:56:05Z</updated>
    <link href="https://arxiv.org/abs/2512.16900v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16900v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:56:05Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shuyuan Tu</name>
    </author>
    <author>
      <name>Yueming Pan</name>
    </author>
    <author>
      <name>Yinming Huang</name>
    </author>
    <author>
      <name>Xintong Han</name>
    </author>
    <author>
      <name>Zhen Xing</name>
    </author>
    <author>
      <name>Qi Dai</name>
    </author>
    <author>
      <name>Kai Qiu</name>
    </author>
    <author>
      <name>Chong Luo</name>
    </author>
    <author>
      <name>Zuxuan Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16893v1</id>
    <title>Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</title>
    <updated>2025-12-18T18:53:28Z</updated>
    <link href="https://arxiv.org/abs/2512.16893v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16893v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:53:28Z</published>
    <arxiv:comment>Project website is https://research.nvidia.com/labs/amri/projects/instant4d</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Kaiwen Jiang</name>
    </author>
    <author>
      <name>Xueting Li</name>
    </author>
    <author>
      <name>Seonwook Park</name>
    </author>
    <author>
      <name>Ravi Ramamoorthi</name>
    </author>
    <author>
      <name>Shalini De Mello</name>
    </author>
    <author>
      <name>Koki Nagano</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16891v1</id>
    <title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
    <updated>2025-12-18T18:52:18Z</updated>
    <link href="https://arxiv.org/abs/2512.16891v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16891v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:52:18Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haichao Zhang</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Lichen Wang</name>
    </author>
    <author>
      <name>Yunzhe Li</name>
    </author>
    <author>
      <name>Daiwei Chen</name>
    </author>
    <author>
      <name>Yunpeng Xu</name>
    </author>
    <author>
      <name>Yun Fu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.16874v1</id>
    <title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
    <updated>2025-12-18T18:42:19Z</updated>
    <link href="https://arxiv.org/abs/2512.16874v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.16874v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-18T18:42:19Z</published>
    <arxiv:comment>Code and model available at https://github.com/facebookresearch/videoseal</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tomáš Souček</name>
    </author>
    <author>
      <name>Pierre Fernandez</name>
    </author>
    <author>
      <name>Hady Elsahar</name>
    </author>
    <author>
      <name>Sylvestre-Alvise Rebuffi</name>
    </author>
    <author>
      <name>Valeriu Lacatusu</name>
    </author>
    <author>
      <name>Tuan Tran</name>
    </author>
    <author>
      <name>Tom Sander</name>
    </author>
    <author>
      <name>Alexandre Mourachko</name>
    </author>
  </entry>
</feed>
