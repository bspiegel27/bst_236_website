<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-11-20T00:55:41Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-20T00:55:41Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>126532</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2511.14744v1</id>
    <title>Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge</title>
    <updated>2025-11-18T18:43:42Z</updated>
    <link href="https://arxiv.org/abs/2511.14744v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14744v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's "ImageNet moment" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T18:43:42Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Antonia Ebner</name>
    </author>
    <author>
      <name>Christoph Bartmann</name>
    </author>
    <author>
      <name>Sonja Topf</name>
    </author>
    <author>
      <name>Sohvi Luukkonen</name>
    </author>
    <author>
      <name>Johannes Schimunek</name>
    </author>
    <author>
      <name>Günter Klambauer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14700v1</id>
    <title>Nonparametric Uniform Inference in Binary Classification and Policy Values</title>
    <updated>2025-11-18T17:38:59Z</updated>
    <link href="https://arxiv.org/abs/2511.14700v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14700v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We develop methods for nonparametric uniform inference in cost-sensitive binary classification, a framework that encompasses maximum score estimation, predicting utility maximizing actions, and policy learning. These problems are well known for slow convergence rates and non-standard limiting behavior, even under point identified parametric frameworks. In nonparametric settings, they may further suffer from failures of identification. To address these challenges, we introduce a strictly convex surrogate loss that point-identifies a representative nonparametric policy function. We then estimate this surrogate policy to conduct inference on both the optimal classification policy and the optimal policy value. This approach enables Gaussian inference, substantially simplifying empirical implementation relative to working directly with the original classification problem. In particular, we establish root-$n$ asymptotic normality for the optimal policy value and derive a Gaussian approximation for the optimal classification policy at the standard nonparametric rate. Extensive simulation studies corroborate the theoretical findings. We apply our method to the National JTPA Study to conduct inference on the optimal treatment assignment policy and its associated welfare.</summary>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T17:38:59Z</published>
    <arxiv:primary_category term="econ.EM"/>
    <author>
      <name>Nan Liu</name>
    </author>
    <author>
      <name>Yanbo Liu</name>
    </author>
    <author>
      <name>Yuya Sasaki</name>
    </author>
    <author>
      <name>Yuanyuan Wan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14694v1</id>
    <title>Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models</title>
    <updated>2025-11-18T17:29:39Z</updated>
    <link href="https://arxiv.org/abs/2511.14694v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14694v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental "grammar" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.</summary>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T17:29:39Z</published>
    <arxiv:primary_category term="q-bio.GN"/>
    <author>
      <name>Rui Zhu</name>
    </author>
    <author>
      <name>Xiaopu Zhou</name>
    </author>
    <author>
      <name>Haixu Tang</name>
    </author>
    <author>
      <name>Stephen W. Scherer</name>
    </author>
    <author>
      <name>Lucila Ohno-Machado</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14691v1</id>
    <title>Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</title>
    <updated>2025-11-18T17:28:29Z</updated>
    <link href="https://arxiv.org/abs/2511.14691v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14691v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T17:28:29Z</published>
    <arxiv:comment>21 Pages, 5 Figures, 3 Table</arxiv:comment>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Kallol Mondal</name>
      <arxiv:affiliation>Department of Electronics and Communication Engineering, National Institute of Technology Allahabad, Prayagraj</arxiv:affiliation>
      <arxiv:affiliation>Centre for Nanotechnology, Indian Institute of Technology Roorkee</arxiv:affiliation>
    </author>
    <author>
      <name>Ankush Kumar</name>
      <arxiv:affiliation>Centre for Nanotechnology, Indian Institute of Technology Roorkee</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14679v1</id>
    <title>Compensating random transition-detection blackouts in Markov networks</title>
    <updated>2025-11-18T17:20:30Z</updated>
    <link href="https://arxiv.org/abs/2511.14679v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14679v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In Markov networks, measurement blackouts with unknown frequency compromise observations such that thermodynamic quantities can no longer be inferred reliably. In particular, the observed currents neither discern equilibrium from non-equilibrium nor can they be used in extant estimators of entropy production. Our strategy to eliminate these effects is based on formally attributing the blackouts to a second channel connecting states. The unknown frequency of blackouts and the true underlying transition rates can be determined from the short-time limit of observed waiting-time distributions. A post-modification of observed trajectory data yields a virtual effective dynamics from which the lower bound on entropy production based on thermodynamic uncertainty relations can be recovered fully. Moreover, the post-processed data can be used in waiting-time based estimators. Crucially, our strategy does neither require the blackouts to occur homogeneously nor symmetrically under time-reversal.</summary>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T17:20:30Z</published>
    <arxiv:primary_category term="cond-mat.stat-mech"/>
    <author>
      <name>Alexander M. Maier</name>
    </author>
    <author>
      <name>Benjamin Häsler</name>
    </author>
    <author>
      <name>Udo Seifert</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14662v1</id>
    <title>Bias in, Bias out: Annotation Bias in Multilingual Large Language Models</title>
    <updated>2025-11-18T17:02:12Z</updated>
    <link href="https://arxiv.org/abs/2511.14662v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14662v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T17:02:12Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Xia Cui</name>
    </author>
    <author>
      <name>Ziyi Huang</name>
    </author>
    <author>
      <name>Naeemeh Adel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14650v1</id>
    <title>AutoTool: Efficient Tool Selection for Large Language Model Agents</title>
    <updated>2025-11-18T16:41:48Z</updated>
    <link href="https://arxiv.org/abs/2511.14650v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14650v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T16:41:48Z</published>
    <arxiv:comment>Accepted by AAAI 2026, 18 pages, 11 figures, Code: https://github.com/jiajingyyyyyy/AutoTool</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Jingyi Jia</name>
    </author>
    <author>
      <name>Qinbin Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14642v1</id>
    <title>Graded strength of comparative illusions is explained by Bayesian inference</title>
    <updated>2025-11-18T16:33:19Z</updated>
    <link href="https://arxiv.org/abs/2511.14642v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14642v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T16:33:19Z</published>
    <arxiv:comment>49 pages, 7 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yuhan Zhang</name>
    </author>
    <author>
      <name>Erxiao Wang</name>
    </author>
    <author>
      <name>Cory Shain</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14613v1</id>
    <title>3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology</title>
    <updated>2025-11-18T16:08:24Z</updated>
    <link href="https://arxiv.org/abs/2511.14613v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14613v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&amp;E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&amp;E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T16:08:24Z</published>
    <arxiv:comment>11 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Mohammad Vali Sanian</name>
    </author>
    <author>
      <name>Arshia Hemmat</name>
    </author>
    <author>
      <name>Amirhossein Vahidi</name>
    </author>
    <author>
      <name>Jonas Maaskola</name>
    </author>
    <author>
      <name>Jimmy Tsz Hang Lee</name>
    </author>
    <author>
      <name>Stanislaw Makarchuk</name>
    </author>
    <author>
      <name>Yeliz Demirci</name>
    </author>
    <author>
      <name>Nana-Jane Chipampe</name>
    </author>
    <author>
      <name>Omer Bayraktar</name>
    </author>
    <author>
      <name>Lassi Paavolainen</name>
    </author>
    <author>
      <name>Mohammad Lotfollahi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.14600v1</id>
    <title>A Controllable Perceptual Feature Generative Model for Melody Harmonization via Conditional Variational Autoencoder</title>
    <updated>2025-11-18T15:43:05Z</updated>
    <link href="https://arxiv.org/abs/2511.14600v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.14600v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While Large Language Models (LLMs) make symbolic music generation increasingly accessible, producing music with distinctive composition and rich expressiveness remains a significant challenge. Many studies have introduced emotion models to guide the generative process. However, these approaches still fall short of delivering novelty and creativity. In the field of Music Information Retrieval (MIR), auditory perception is recognized as a key dimension of musical experience, offering insights into both compositional intent and emotional patterns. To this end, we propose a neural network named CPFG-Net, along with a transformation algorithm that maps perceptual feature values to chord representations, enabling melody harmonization. The system can controllably predict sequences of perceptual features and tonal structures from given melodies, and subsequently generate harmonically coherent chord progressions. Our network is trained on our newly constructed perceptual feature dataset BCPT-220K, derived from classical music. Experimental results show state-of-the-art perceptual feature prediction capability of our model as well as demonstrate our musical expressiveness and creativity in chord inference. This work offers a novel perspective on melody harmonization and contributes to broader music generation tasks. Our symbolic-based model can be easily extended to audio-based models.</summary>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-18T15:43:05Z</published>
    <arxiv:comment>13 pages, 8 figures, 2 url links</arxiv:comment>
    <arxiv:primary_category term="cs.SD"/>
    <author>
      <name>Dengyun Huang</name>
    </author>
    <author>
      <name>Yonghua Zhu</name>
    </author>
  </entry>
</feed>
