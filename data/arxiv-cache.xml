<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-02-20T01:13:28Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-02-20T01:13:29Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>134477</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2602.16708v1</id>
    <title>Policy Compiler for Secure Agentic Systems</title>
    <updated>2026-02-18T18:57:12Z</updated>
    <link href="https://arxiv.org/abs/2602.16708v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16708v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.
  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.
  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T18:57:12Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Nils Palumbo</name>
    </author>
    <author>
      <name>Sarthak Choudhary</name>
    </author>
    <author>
      <name>Jihye Choi</name>
    </author>
    <author>
      <name>Prasad Chalasani</name>
    </author>
    <author>
      <name>Mihai Christodorescu</name>
    </author>
    <author>
      <name>Somesh Jha</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16702v1</id>
    <title>Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning</title>
    <updated>2026-02-18T18:49:56Z</updated>
    <link href="https://arxiv.org/abs/2602.16702v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16702v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T18:49:56Z</published>
    <arxiv:comment>preprint 10 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Mingjia Shi</name>
    </author>
    <author>
      <name>Yinhan He</name>
    </author>
    <author>
      <name>Yaochen Zhu</name>
    </author>
    <author>
      <name>Jundong Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16698v1</id>
    <title>Causality is Key for Interpretability Claims to Generalise</title>
    <updated>2026-02-18T18:45:04Z</updated>
    <link href="https://arxiv.org/abs/2602.16698v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16698v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T18:45:04Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shruti Joshi</name>
    </author>
    <author>
      <name>Aaron Mueller</name>
    </author>
    <author>
      <name>David Klindt</name>
    </author>
    <author>
      <name>Wieland Brendel</name>
    </author>
    <author>
      <name>Patrik Reizinger</name>
    </author>
    <author>
      <name>Dhanya Sridhar</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16690v1</id>
    <title>Synthetic-Powered Multiple Testing with FDR Control</title>
    <updated>2026-02-18T18:36:24Z</updated>
    <link href="https://arxiv.org/abs/2602.16690v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16690v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T18:36:24Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Yonghoon Lee</name>
    </author>
    <author>
      <name>Meshi Bashari</name>
    </author>
    <author>
      <name>Edgar Dobriban</name>
    </author>
    <author>
      <name>Yaniv Romano</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16682v1</id>
    <title>Learning Situated Awareness in the Real World</title>
    <updated>2026-02-18T18:22:52Z</updated>
    <link href="https://arxiv.org/abs/2602.16682v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16682v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T18:22:52Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Chuhan Li</name>
    </author>
    <author>
      <name>Ruilin Han</name>
    </author>
    <author>
      <name>Joy Hsu</name>
    </author>
    <author>
      <name>Yongyuan Liang</name>
    </author>
    <author>
      <name>Rajiv Dhawan</name>
    </author>
    <author>
      <name>Jiajun Wu</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <author>
      <name>Xin Eric Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16665v1</id>
    <title>Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning</title>
    <updated>2026-02-18T18:05:19Z</updated>
    <link href="https://arxiv.org/abs/2602.16665v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16665v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p&gt;2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges.</summary>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T18:05:19Z</published>
    <arxiv:comment>13 pages, 8 figures</arxiv:comment>
    <arxiv:primary_category term="cond-mat.dis-nn"/>
    <author>
      <name>Li Zeng</name>
    </author>
    <author>
      <name>Mutian Shen</name>
    </author>
    <author>
      <name>Tianle Pu</name>
    </author>
    <author>
      <name>Zohar Nussinov</name>
    </author>
    <author>
      <name>Qing Feng</name>
    </author>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Zhong Liu</name>
    </author>
    <author>
      <name>Changjun Fan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16651v1</id>
    <title>Interpreting the HI 21-cm cosmology maps through Largest Cluster Statistics III: Impact of the lightcone effect</title>
    <updated>2026-02-18T17:47:25Z</updated>
    <link href="https://arxiv.org/abs/2602.16651v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16651v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The redshifted 21-cm signal emitted by neutral Hydrogen (HI) is a promising probe to understand the evolution of the topology of ionized regions during the Epoch of Reionization (EoR). The topology of ionized regions allows us to infer the nature and properties of ionizing sources, i.e., early galaxies and AGNs. Traditional Fourier statistics, such as the power spectrum, help us quantify the strength of fluctuations in this field at different length scales but do not preserve its phase information. Analyzing the 21-cm brightness temperature field in the image domain retains its non-Gaussian characteristics and morphological information. One such approach is to track the coalescence of multiple ionized regions to form one contiguous ionized region spanning the universe. This is referred to as percolation, and its onset is quantified by a sharp rise in the value of the Largest Cluster Statistic (LCS) approaching unity. In this work, we carry out a percolation analysis of 21-cm brightness temperature fields by studying the redshift evolution of the LCS along a lightcone to distinguish between several simulated reionization scenarios. We have extended previous results on reionization model comparison from the analysis of coeval 21-cm maps to understand how the lightcone effect biases the observed percolation behavior and affects the distinguishability of the source models. We estimate the LCS of subvolumes of different sizes in the 21-cm lightcone maps and study their redshift evolution for different reionization scenarios using a moving volume approach. We find that the percolation transition inferred from a lightcone approaches that from the coeval box as we increase the bandwidth of the moving volume in all but one reionization scenario.</summary>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T17:47:25Z</published>
    <arxiv:comment>23 pages, 5 figures, 1 table. To be submitted to JCAP. Comments and suggestions are welcome</arxiv:comment>
    <arxiv:primary_category term="astro-ph.CO"/>
    <author>
      <name>Hemanth Potluri</name>
    </author>
    <author>
      <name>Manas Mohit Dosibhatla</name>
    </author>
    <author>
      <name>Leon Noble</name>
    </author>
    <author>
      <name>Chandra Shekhar Murmu</name>
    </author>
    <author>
      <name>Suman Majumdar</name>
    </author>
    <author>
      <name>Samit Kumar Pal</name>
    </author>
    <author>
      <name>Saswata Dasgupta</name>
    </author>
    <author>
      <name>Satadru Bag</name>
    </author>
    <author>
      <name>Abhirup Datta</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16640v1</id>
    <title>Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval</title>
    <updated>2026-02-18T17:29:43Z</updated>
    <link href="https://arxiv.org/abs/2602.16640v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16640v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a "resource divide." State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes "lexical density" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T17:29:43Z</published>
    <arxiv:comment>5 pages, 2 tables</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Subrit Dikshit</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16620v1</id>
    <title>HOLISMOKES XX. Lens models of binary lens galaxies with five images of Supernova Winny</title>
    <updated>2026-02-18T17:15:40Z</updated>
    <link href="https://arxiv.org/abs/2602.16620v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16620v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Strongly lensed supernovae (SNe) provide a powerful way to study cosmology, SNe and galaxies. Modelling the lens system is key to extracting astrophysical and cosmological information. We present adaptive-optics-assisted high-resolution images of SN Winny (SN 2025wny) in the J and K filters obtained with the Large Binocular Telescope (LBT). The LBT imaging confirms the presence of a fifth point source, whose colour is consistent with that of the other SN images at similar phases, while lens modelling robustly supports its interpretation as an additional image of SN~Winny. We measure the positions of the five SN images with uncertainties varying between 1 and 14 milliarcseconds. We build the first mass models using lenstronomy and GLEE, and explore three classes of mass models for the two lens galaxies G1 and G2. The optimal model class of the three is a singular isothermal ellipsoid for G1, a singular isothermal sphere for G2, and an external shear. We infer the enclosed masses within the Einstein radius as 4.61^{+0.06}_{-0.04} \times 10^{11}\,M_\odot for G1 and 1.01\pm0.02 \times 10^{11}\,M_\odot for G2. The lensing configuration by the two lens galaxies can produce two additional magnified SN images beyond the five observed ones; the exclusion of such model configurations further constrains the lens model parameters. Our model fits to the observed image positions with an RMS of ~0.0012" - 0.0025", within the observed positional uncertainties. The predicted magnifications of the multiple images vary between ~1.6 (for the faintest fifth image E) to ~10 (for the brightest image A). The predicted relative lensing magnifications of the multiple images do not match that of the observed within 2σuncertainties. The differences in the relative magnifications could be due to millilensing/microlensing. Our mass models form the basis for future analyses of this unique system. (abridged)</summary>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T17:15:40Z</published>
    <arxiv:primary_category term="astro-ph.GA"/>
    <author>
      <name>L. R. Ecker</name>
    </author>
    <author>
      <name>A. G. Schweinfurth</name>
    </author>
    <author>
      <name>R. Saglia</name>
    </author>
    <author>
      <name>L. Deng</name>
    </author>
    <author>
      <name>S. H. Suyu</name>
    </author>
    <author>
      <name>C. Saulder</name>
    </author>
    <author>
      <name>J. Snigula</name>
    </author>
    <author>
      <name>R. Bender</name>
    </author>
    <author>
      <name>R. Cañameras</name>
    </author>
    <author>
      <name>T. -W. Chen</name>
    </author>
    <author>
      <name>A. Galan</name>
    </author>
    <author>
      <name>A. Halkola</name>
    </author>
    <author>
      <name>E. Mamuzic</name>
    </author>
    <author>
      <name>A. Melo</name>
    </author>
    <author>
      <name>S. Schuldt</name>
    </author>
    <author>
      <name>S. Taubenberger</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.16612v1</id>
    <title>Causal and Compositional Abstraction</title>
    <updated>2026-02-18T17:06:09Z</updated>
    <link href="https://arxiv.org/abs/2602.16612v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.16612v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.</summary>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-18T17:06:09Z</published>
    <arxiv:primary_category term="cs.LO"/>
    <author>
      <name>Robin Lorenz</name>
    </author>
    <author>
      <name>Sean Tull</name>
    </author>
  </entry>
</feed>
