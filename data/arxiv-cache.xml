<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-02T00:58:07Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-02T00:58:08Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>127398</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2511.23478v1</id>
    <title>Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</title>
    <updated>2025-11-28T18:59:58Z</updated>
    <link href="https://arxiv.org/abs/2511.23478v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23478v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:59:58Z</published>
    <arxiv:comment>Video-R2 Technical Report</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Muhammad Maaz</name>
    </author>
    <author>
      <name>Hanoona Rasheed</name>
    </author>
    <author>
      <name>Fahad Shahbaz Khan</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23473v1</id>
    <title>ThetaEvolve: Test-time Learning on Open Problems</title>
    <updated>2025-11-28T18:58:14Z</updated>
    <link href="https://arxiv.org/abs/2511.23473v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23473v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:58:14Z</published>
    <arxiv:comment>30 pages, link: https://github.com/ypwang61/ThetaEvolve</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yiping Wang</name>
    </author>
    <author>
      <name>Shao-Rong Su</name>
    </author>
    <author>
      <name>Zhiyuan Zeng</name>
    </author>
    <author>
      <name>Eva Xu</name>
    </author>
    <author>
      <name>Liliang Ren</name>
    </author>
    <author>
      <name>Xinyu Yang</name>
    </author>
    <author>
      <name>Zeyi Huang</name>
    </author>
    <author>
      <name>Xuehai He</name>
    </author>
    <author>
      <name>Luyao Ma</name>
    </author>
    <author>
      <name>Baolin Peng</name>
    </author>
    <author>
      <name>Hao Cheng</name>
    </author>
    <author>
      <name>Pengcheng He</name>
    </author>
    <author>
      <name>Weizhu Chen</name>
    </author>
    <author>
      <name>Shuohang Wang</name>
    </author>
    <author>
      <name>Simon Shaolei Du</name>
    </author>
    <author>
      <name>Yelong Shen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23470v1</id>
    <title>Spectral analysis of the Koopman operator recovers Hamiltonian parameters in open quantum systems</title>
    <updated>2025-11-28T18:57:15Z</updated>
    <link href="https://arxiv.org/abs/2511.23470v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23470v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>An accurate identification of Hamiltonian parameters is essential for modeling and control of open quantum systems. In this work, a novel data-driven method for inferring such parameters from first-moment dynamics is presented using an open 2D quantum harmonic oscillator as an example. The method relies on the discrete spectrum of the Koopman operator to obtain such parameters, which is obtained using the multichannel Hankel Alternative View of Koopman (mHAVOK) algorithm; a theoretical connection of such affirmation is presented. The method is tested on the expected value of noiseless quadratures, retrieving oscillation frequencies, damping rates, nonlinear Kerr shifts, qubit-photon coupling strengths of a Jaynes-Cummings interaction, and a modulated frequency of a time-dependent Hamiltonian. The majority of the recovered parameters remained within 5% of their true values. When compared to Fourier and matrix-pencil estimators, our approach yields lower errors for dynamics with strong dissipation. Overall, these findings suggest that Koopman operator theory provides a practical framework for studying quantum dynamical systems.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:57:15Z</published>
    <arxiv:comment>12 pages and 8 figures</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Jorge E. Pérez-García</name>
    </author>
    <author>
      <name>Carlos Colchero</name>
    </author>
    <author>
      <name>Julio C. Gutiérrez-Vega</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23463v1</id>
    <title>Kinetic Mixing and the Phantom Illusion: Axion-Dilaton Quintessence in Light of DESI DR2</title>
    <updated>2025-11-28T18:54:22Z</updated>
    <link href="https://arxiv.org/abs/2511.23463v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23463v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent results from DESI BAO analyses suggest that dark energy may not be a cosmological constant and is in fact dynamical. Furthermore, the data suggest that the equation of state may have been in the phantom regime in the distant past, recently undergoing a phantom crossing. In this work, we investigate whether this preference can be realized within a kinetically mixed axion-dilaton (KMIX) quintessence model, a string-motivated system in which an axion-like field couples exponentially to a dilaton-like (moduli) field. Crucially, KMIX can appear phantom in a standard Chevallier-Polarski-Linder (CPL) based analysis. To confront the model with data, we develop a fast pipeline based on normalizing flows that (i) learns a theory-informed prior on $(w_0,w_a)$ from KMIX realizations and (ii) provides an inverse mapping from CPL parameters back to the physical KMIX parameters. By importance-sampling pre-computed CPL chains using this framework, we effectively transform generic phenomenological constraints into direct, computationally efficient constraints on the underlying KMIX theory, avoiding the prohibitive cost of full parameter space exploration. Applied to Planck+DESI DR2 BAO measurements, our framework finds support for KMIX at $2.5σ$ compared to the base CPL fit at $3.1σ$, demonstrating that KMIX may account for the DESI preference without invoking true phantom behavior. When additionally including Type Ia supernovae data, we find that the preference remains above $3σ$ for Union3 and DES Y5, but drops to $2.1σ$ with Pantheon+. The latter, combined with the DESI full-shape power spectrum and bispectrum data, further reduces the preference to $1.7σ$. Ultimately, should the DESI deviation persist with future data, KMIX may offer a theoretically well-motivated explanation for the phantom-like signatures inferred from phenomenological fits.</summary>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:54:22Z</published>
    <arxiv:comment>15 pages, 8 figures, 3 tables</arxiv:comment>
    <arxiv:primary_category term="astro-ph.CO"/>
    <author>
      <name>Michael W. Toomey</name>
    </author>
    <author>
      <name>Ellie Hughes</name>
    </author>
    <author>
      <name>Mikhail M. Ivanov</name>
    </author>
    <author>
      <name>James M. Sullivan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23455v1</id>
    <title>The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference</title>
    <updated>2025-11-28T18:47:33Z</updated>
    <link href="https://arxiv.org/abs/2511.23455v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23455v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\times$ to $10\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:47:33Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Hans Gundlach</name>
    </author>
    <author>
      <name>Jayson Lynch</name>
    </author>
    <author>
      <name>Matthias Mertens</name>
    </author>
    <author>
      <name>Neil Thompson</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23440v1</id>
    <title>Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation</title>
    <updated>2025-11-28T18:35:20Z</updated>
    <link href="https://arxiv.org/abs/2511.23440v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23440v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:35:20Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Bernhard Klein</name>
    </author>
    <author>
      <name>Falk Selker</name>
    </author>
    <author>
      <name>Hendrik Borras</name>
    </author>
    <author>
      <name>Sophie Steger</name>
    </author>
    <author>
      <name>Franz Pernkopf</name>
    </author>
    <author>
      <name>Holger Fröning</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23436v1</id>
    <title>Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent</title>
    <updated>2025-11-28T18:32:49Z</updated>
    <link href="https://arxiv.org/abs/2511.23436v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23436v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:32:49Z</published>
    <arxiv:comment>15 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Jianzhe Lin</name>
    </author>
    <author>
      <name>Zeyu Pan</name>
    </author>
    <author>
      <name>Yun Zhu</name>
    </author>
    <author>
      <name>Ruiqi Song</name>
    </author>
    <author>
      <name>Jining Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23429v1</id>
    <title>Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</title>
    <updated>2025-11-28T18:26:39Z</updated>
    <link href="https://arxiv.org/abs/2511.23429v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23429v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as "open the door", "draw a torch", or "trigger an explosion".</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:26:39Z</published>
    <arxiv:comment>Technical Report, Project page:https://hunyuan-gamecraft-2.github.io/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Junshu Tang</name>
    </author>
    <author>
      <name>Jiacheng Liu</name>
    </author>
    <author>
      <name>Jiaqi Li</name>
    </author>
    <author>
      <name>Longhuang Wu</name>
    </author>
    <author>
      <name>Haoyu Yang</name>
    </author>
    <author>
      <name>Penghao Zhao</name>
    </author>
    <author>
      <name>Siruis Gong</name>
    </author>
    <author>
      <name>Xiang Yuan</name>
    </author>
    <author>
      <name>Shuai Shao</name>
    </author>
    <author>
      <name>Qinglin Lu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23419v1</id>
    <title>Getting it right: Methods for risk ratios and risk differences cluster randomized trials with a small number of clusters</title>
    <updated>2025-11-28T18:18:10Z</updated>
    <link href="https://arxiv.org/abs/2511.23419v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23419v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Most cluster randomized trials (CRTs) randomize fewer than 30-40 clusters in total. When performing inference for such ``small'' CRTs, it is important to use methods that appropriately account for the small sample size. When the generalized estimating equations (GEE) approach is used for analysis of ``small'' CRTs, the robust variance estimator from GEE is biased downward and therefore bias-corrected standard errors should be used. Moreover, in order to avoid inflated Type I error, an appropriate bias-corrected standard error should be paired with the t- rather than Z-statistic when making inference about a single-parameter intervention effect. Although several bias-correction methods (including Kauermann and Carroll (KC), Mancl and DeRouen (MD), Morel, Bokossa, and Neerchal (MBN), and the average of KC and MD (AVG)) have been evaluated for inference for odds ratios, their finite-sample behavior in ``small'' CRTs with few clusters has not been thoroughly investigated for risk ratios and risk differences. The current article aims to fill the gap by including analysis via binomial, Poisson and Gaussian models and for a broad spectrum of scenarios. Analysis is via binomial and Poisson models (using log and identity link for risk and differences measures, respectively). We additionally explore the use of Gaussian models with identity link for risk differences and adopt the "modified" approach for analysis with misspecified Poisson and Gaussian models. We consider a broad spectrum of scenarios including for rare outcomes, small cluster sizes, high intracluster correlations (ICCs), and high coefficients of variation (CVs) of cluster size.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:18:10Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Shifeng Sun</name>
    </author>
    <author>
      <name>Xueqi Wang</name>
    </author>
    <author>
      <name>Zhuoran Hou</name>
    </author>
    <author>
      <name>Elizabeth L. Turner</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.23417v1</id>
    <title>Detection of the Pairwise Kinematic Sunyaev-Zel'dovich Effect and Pairwise Velocity with DESI DR1 Galaxies and ACT DR6 and Planck CMB Data</title>
    <updated>2025-11-28T18:12:40Z</updated>
    <link href="https://arxiv.org/abs/2511.23417v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.23417v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a 9.3-sigma detection of the pairwise kinematic Sunyaev-Zeldovich (kSZ) effect by combining a sample of 913,286 Luminous Red Galaxies (LRGs) from the Dark Energy Spectroscopic Instrument Data Release 1 (DESI DR1) catalog and co-added Atacama Cosmology Telescope (ACT DR6) and Planck cosmic microwave background (CMB) temperature maps. This represents the highest-significance pairwise kSZ measurement to date. The analysis uses three ACT CMB temperature maps: co-added 150 GHz, total frequency maps, and a component-separated Internal Linear Combination (ILC) map, all of which cover 19,000 square degrees of the sky from Advanced ACTPol observations conducted between 2017 and 2022. Comparison of the results of these three maps serves as a consistency check for potential foreground contamination that may depend on the observation frequency. An estimate of the best-fit mass-averaged optical depth is obtained by comparing the pairwise kSZ curve with the linear-theory prediction of the pairwise velocity under the best-fit Planck cosmology, and is compared with predictions from simulations. This estimate serves as a reference point for future comparisons with thermal SZ-derived optical depth measurements for the same DESI cluster samples, which will be presented in a companion paper. Finally, we employ a machine-learning approach trained on simulations to estimate the optical depth for 456,803 DESI LRG-identified clusters within the simulated mass range (greater than about 1e13 solar masses). These are combined with the measured kSZ signal to infer the individual cluster peculiar velocities, providing the opportunity to constrain the behavior of gravity and the dark sector over a range of cosmic scales and epochs.</summary>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-28T18:12:40Z</published>
    <arxiv:comment>15 pages, 7 figures</arxiv:comment>
    <arxiv:primary_category term="astro-ph.CO"/>
    <author>
      <name>Yulin Gong</name>
    </author>
    <author>
      <name>Patricio A. Gallardo</name>
    </author>
    <author>
      <name>Rachel Bean</name>
    </author>
    <author>
      <name>Jenna Moore</name>
    </author>
    <author>
      <name>Eve M. Vavagiakis</name>
    </author>
    <author>
      <name>Nicholas Battaglia</name>
    </author>
    <author>
      <name>Boryana Hadzhiyska</name>
    </author>
    <author>
      <name>Yun-Hsin Hsu</name>
    </author>
    <author>
      <name>Jessica Nicole Aguilar</name>
    </author>
    <author>
      <name>Steven Ahlen</name>
    </author>
    <author>
      <name>Davide Bianchi</name>
    </author>
    <author>
      <name>David Brooks</name>
    </author>
    <author>
      <name>Todd Claybaugh</name>
    </author>
    <author>
      <name>Rebecca Canning</name>
    </author>
    <author>
      <name>Mark Devlin</name>
    </author>
    <author>
      <name>Peter Doel</name>
    </author>
    <author>
      <name>Axel de la Macorra</name>
    </author>
    <author>
      <name>Simone Ferraro</name>
    </author>
    <author>
      <name>Andreu Font-Ribera</name>
    </author>
    <author>
      <name>Jaime E. Forero-Romero</name>
    </author>
    <author>
      <name>Enrique Gaztañaga</name>
    </author>
    <author>
      <name>Gaston Gutierrez</name>
    </author>
    <author>
      <name>Satya Gontcho A Gontcho</name>
    </author>
    <author>
      <name>Julien Guy</name>
    </author>
    <author>
      <name>Klaus Honscheid</name>
    </author>
    <author>
      <name>Cullan Howlett</name>
    </author>
    <author>
      <name>R. Henry Liu</name>
    </author>
    <author>
      <name>Mustapha Ishak</name>
    </author>
    <author>
      <name>Dick Joyce</name>
    </author>
    <author>
      <name>Anthony Kremin</name>
    </author>
    <author>
      <name>Claire Lamman</name>
    </author>
    <author>
      <name>Michael Levi</name>
    </author>
    <author>
      <name>Martin Landriau</name>
    </author>
    <author>
      <name>Marc Manera</name>
    </author>
    <author>
      <name>Aaron Meisner</name>
    </author>
    <author>
      <name>Ramon Miquel</name>
    </author>
    <author>
      <name>Michael D. Niemack</name>
    </author>
    <author>
      <name>Seshadri Nadathur</name>
    </author>
    <author>
      <name>Will Percival</name>
    </author>
    <author>
      <name>Francisco Prada</name>
    </author>
    <author>
      <name>Graziano Rossi</name>
    </author>
    <author>
      <name>Bernardita Ried Guachalla</name>
    </author>
    <author>
      <name>Eusebio Sanchez</name>
    </author>
    <author>
      <name>Hee-Jong Seo</name>
    </author>
    <author>
      <name>David Sprayberry</name>
    </author>
    <author>
      <name>David Schlegel</name>
    </author>
    <author>
      <name>Cristóbal Sifón</name>
    </author>
    <author>
      <name>Michael Schubnell</name>
    </author>
    <author>
      <name>Joseph Harry Silber</name>
    </author>
    <author>
      <name>Gregory Tarlé</name>
    </author>
    <author>
      <name>Benjamin Alan Weaver</name>
    </author>
    <author>
      <name>Rongpu Zhou</name>
    </author>
    <author>
      <name>Hu Zou</name>
    </author>
  </entry>
</feed>
