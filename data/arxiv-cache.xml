<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-13T00:56:39Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-12T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">111961</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.06190v1</id>
    <updated>2025-05-09T17:00:28Z</updated>
    <published>2025-05-09T17:00:28Z</published>
    <title>Beyond the Mean: Limit Theory and Tests for Infinite-Mean Autoregressive
  Conditional Durations</title>
    <summary>  Integrated autoregressive conditional duration (ACD) models serve as natural
counterparts to the well-known integrated GARCH models used for financial
returns. However, despite their resemblance, asymptotic theory for ACD is
challenging and also not complete, in particular for integrated ACD. Central
challenges arise from the facts that (i) integrated ACD processes imply
durations with infinite expectation, and (ii) even in the non-integrated case,
conventional asymptotic approaches break down due to the randomness in the
number of durations within a fixed observation period. Addressing these
challenges, we provide here unified asymptotic theory for the (quasi-) maximum
likelihood estimator for ACD models; a unified theory which includes integrated
ACD models. Based on the new results, we also provide a novel framework for
hypothesis testing in duration models, enabling inference on a key empirical
question: whether durations possess a finite or infinite expectation. We apply
our results to high-frequency cryptocurrency ETF trading data. Motivated by
parameter estimates near the integrated ACD boundary, we assess whether
durations between trades in these markets have finite expectation, an
assumption often made implicitly in the literature on point process models. Our
empirical findings indicate infinite-mean durations for all the five
cryptocurrencies examined, with the integrated ACD hypothesis rejected --
against alternatives with tail index less than one -- for four out of the five
cryptocurrencies considered.
</summary>
    <author>
      <name>Giuseppe Cavaliere</name>
    </author>
    <author>
      <name>Thomas Mikosch</name>
    </author>
    <author>
      <name>Anders Rahbek</name>
    </author>
    <author>
      <name>Frederik Vilandt</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06175v1</id>
    <updated>2025-05-09T16:29:29Z</updated>
    <published>2025-05-09T16:29:29Z</published>
    <title>Turbo-ICL: In-Context Learning-Based Turbo Equalization</title>
    <summary>  This paper introduces a novel in-context learning (ICL) framework, inspired
by large language models (LLMs), for soft-input soft-output channel
equalization in coded multiple-input multiple-output (MIMO) systems. The
proposed approach learns to infer posterior symbol distributions directly from
a prompt of pilot signals and decoder feedback. A key innovation is the use of
prompt augmentation to incorporate extrinsic information from the decoder
output as additional context, enabling the ICL model to refine its symbol
estimates iteratively across turbo decoding iterations. Two model variants,
based on Transformer and state-space architectures, are developed and
evaluated. Extensive simulations demonstrate that, when traditional linear
assumptions break down, e.g., in the presence of low-resolution quantization,
ICL equalizers consistently outperform conventional model-based baselines, even
when the latter are provided with perfect channel state information. Results
also highlight the advantage of Transformer-based models under limited training
diversity, as well as the efficiency of state-space models in
resource-constrained scenarios.
</summary>
    <author>
      <name>Zihang Song</name>
    </author>
    <author>
      <name>Matteo Zecchin</name>
    </author>
    <author>
      <name>Bipin Rajendran</name>
    </author>
    <author>
      <name>Osvaldo Simeone</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06122v1</id>
    <updated>2025-05-09T15:25:48Z</updated>
    <published>2025-05-09T15:25:48Z</published>
    <title>Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled
  Systems via Particle Filter Reinforcement Learning</title>
    <summary>  This paper addresses the problem of parameter privacy-preserving data sharing
in coupled systems, where a data provider shares data with a data user but
wants to protect its sensitive parameters. The shared data affects not only the
data user's decision-making but also the data provider's operations through
system interactions. To trade off control performance and privacy, we propose
an interaction-aware privacy-preserving data sharing approach. Our approach
generates distorted data by minimizing a combination of (i) mutual information,
quantifying privacy leakage of sensitive parameters, and (ii) the impact of
distorted data on the data provider's control performance, considering the
interactions between stakeholders. The optimization problem is formulated into
a Bellman equation and solved by a particle filter reinforcement learning
(RL)-based approach. Compared to existing RL-based methods, our formulation
significantly reduces history dependency and efficiently handles scenarios with
continuous state space. Validated in a mixed-autonomy platoon scenario, our
method effectively protects sensitive driving behavior parameters of
human-driven vehicles (HDVs) against inference attacks while maintaining
negligible impact on fuel efficiency.
</summary>
    <author>
      <name>Haokun Yu</name>
    </author>
    <author>
      <name>Jingyuan Zhou</name>
    </author>
    <author>
      <name>Kaidi Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 8 figures, accepted at the 7th Annual Learning for Dynamics
  and Control (L4DC) Conference, 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.06122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06121v1</id>
    <updated>2025-05-09T15:22:21Z</updated>
    <published>2025-05-09T15:22:21Z</published>
    <title>Constraints to Lorentz violation and ultrahigh-energy electrons in
  D-foamy space-times</title>
    <summary>  We entertain the constraints that the absence of vacuum Cherenkov radiation
of ultrahigh-energy electrons inferred from LHAASO observations of the Crab
Nebula can impose on generic models in which Lorentz symmetry of the particle
vacuum is violated, as established by some recent studies in
\href{https://doi.org/10.1016/j.physletb.2022.137034}{\emph{Phys. Lett. B} {\bf
829} (2022) 137034}; \href{https://doi.org/10.1016/j.physletb.2022.137536}{{\bf
835} (2022) 137536};
\href{https://doi.org/10.1103/PhysRevD.108.063006}{\emph{Phys. Rev. D} {\bf108}
(2023) 063006}. We demonstrate in the present paper, that implementing a
phenomenological approach to the Lorentz violation, the rates of this vacuum
process are substantial such that one is justified in deriving bounds on the
violation scales from simple threshold analysis just as these works did. Albeit
such results are likely effective then, they do not apply in the same form
among scenarios. Specifically, we show that these Cherenkov constraints are
naturally evaded in models of space-time foam inspired from~(supercritical)
string theory, involving D-branes as space-time defects in a brane-world
scenario, in which subluminous energy-dependent refractive indices of light
have been suggested. We examine here two specific foam situations and find for
both cases~(though, for different reasons) the potentiality that charged quanta
such as electrons do \emph{not} radiate as they pass through the gravitational
vacuum `medium' despite moving faster than photons.
</summary>
    <author>
      <name>Chengyi Li</name>
    </author>
    <author>
      <name>Bo-Qiang Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, no figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.06121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06117v1</id>
    <updated>2025-05-09T15:16:42Z</updated>
    <published>2025-05-09T15:16:42Z</published>
    <title>Photovoltaic Defect Image Generator with Boundary Alignment Smoothing
  Constraint for Domain Shift Mitigation</title>
    <summary>  Accurate defect detection of photovoltaic (PV) cells is critical for ensuring
quality and efficiency in intelligent PV manufacturing systems. However, the
scarcity of rich defect data poses substantial challenges for effective model
training. While existing methods have explored generative models to augment
datasets, they often suffer from instability, limited diversity, and domain
shifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image
Generator based on Stable Diffusion (SD). PDIG leverages the strong priors
learned from large-scale datasets to enhance generation quality under limited
data. Specifically, we introduce a Semantic Concept Embedding (SCE) module that
incorporates text-conditioned priors to capture the relational concepts between
defect types and their appearances. To further enrich the domain distribution,
we design a Lightweight Industrial Style Adaptor (LISA), which injects
industrial defect characteristics into the SD model through cross-disentangled
attention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)
module, enforcing the quality of generated images via positional consistency
and spatial smoothing alignment. Extensive experiments demonstrate that PDIG
achieves superior realism and diversity compared to state-of-the-art methods.
Specifically, our approach improves Frechet Inception Distance (FID) by 19.16
points over the second-best method and significantly enhances the performance
of downstream defect detection tasks.
</summary>
    <author>
      <name>Dongying Li</name>
    </author>
    <author>
      <name>Binyi Su</name>
    </author>
    <author>
      <name>Hua Zhang</name>
    </author>
    <author>
      <name>Yong Li</name>
    </author>
    <author>
      <name>Haiyong Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06108v1</id>
    <updated>2025-05-09T15:05:57Z</updated>
    <published>2025-05-09T15:05:57Z</published>
    <title>LLMs Outperform Experts on Challenging Biology Benchmarks</title>
    <summary>  This study systematically evaluates 27 frontier Large Language Models on
eight diverse biology benchmarks spanning molecular biology, genetics, cloning,
virology, and biosecurity. Models from major AI developers released between
November 2022 and April 2025 were assessed through ten independent runs per
benchmark. The findings reveal dramatic improvements in biological
capabilities. Top model performance increased more than 4-fold on the
challenging text-only subset of the Virology Capabilities Test over the study
period, with the top model now performing twice as well as expert virologists.
Several models now match or exceed expert-level performance on other
challenging benchmarks, including LAB-Bench CloningScenarios and the biology
subsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not
substantially improve performance over zero-shot evaluation, while extended
reasoning features in o3-mini and Claude 3.7 Sonnet typically improved
performance as predicted by inference scaling. Benchmarks such as PubMedQA and
the MMLU and WMDP biology subsets exhibited performance plateaus well below
100%, suggesting benchmark saturation and errors in the underlying benchmark
data. The analysis highlights the need for more sophisticated evaluation
methodologies as AI systems continue to advance.
</summary>
    <author>
      <name>Lennart Justen</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06044v1</id>
    <updated>2025-05-09T13:40:37Z</updated>
    <published>2025-05-09T13:40:37Z</published>
    <title>Shadow-Based Framework for Estimating Transition Disk Geometries</title>
    <summary>  Some transition disks host misaligned inner disks with radii of several au.
Understanding the geometric and physical properties of these misaligned disks
is essential for advancing terrestrial planet formation models. This study
introduces a novel method to infer the three-dimensional structures of both
inner and outer disks by analyzing non-axisymmetric shadows and the horizon in
optical and infrared scattered light images of the outer disk. This method was
applied to the HD 100453 system, in which infrared scattered light images from
the Very Large Telescope revealed disk shadows. These results indicate that the
inner disk is misaligned by $\sim$70$^{\circ}$ relative to the outer disk,
which is consistent with the results of previous studies. The aspect ratio of
the inner disk surface was estimated to be 0.17, which may reflect the surface
height of the optically thick dusty component due to vertical lofting by MHD
winds or turbulence. In addition, the surface height distribution of the outer
disk was characterized, providing novel insights into its vertical structure.
</summary>
    <author>
      <name>Ryuta Orihara</name>
    </author>
    <author>
      <name>Munetake Momose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.06044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06020v1</id>
    <updated>2025-05-09T13:08:27Z</updated>
    <published>2025-05-09T13:08:27Z</published>
    <title>ArtRAG: Retrieval-Augmented Generation with Structured Context for
  Visual Art Understanding</title>
    <summary>  Understanding visual art requires reasoning across multiple perspectives --
cultural, historical, and stylistic -- beyond mere object recognition. While
recent multimodal large language models (MLLMs) perform well on general image
captioning, they often fail to capture the nuanced interpretations that fine
art demands. We propose ArtRAG, a novel, training-free framework that combines
structured knowledge with retrieval-augmented generation (RAG) for
multi-perspective artwork explanation. ArtRAG automatically constructs an Art
Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing
entities such as artists, movements, themes, and historical events into a rich,
interpretable graph. At inference time, a multi-granular structured retriever
selects semantically and topologically relevant subgraphs to guide generation.
This enables MLLMs to produce contextually grounded, culturally informed art
descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG
outperforms several heavily trained baselines. Human evaluations further
confirm that ArtRAG generates coherent, insightful, and culturally enriched
interpretations.
</summary>
    <author>
      <name>Shuai Wang</name>
    </author>
    <author>
      <name>Ivona Najdenkoska</name>
    </author>
    <author>
      <name>Hongyi Zhu</name>
    </author>
    <author>
      <name>Stevan Rudinac</name>
    </author>
    <author>
      <name>Monika Kackovic</name>
    </author>
    <author>
      <name>Nachoem Wijnberg</name>
    </author>
    <author>
      <name>Marcel Worring</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.05979v1</id>
    <updated>2025-05-09T12:06:46Z</updated>
    <published>2025-05-09T12:06:46Z</published>
    <title>Mixtures of multivariate linear asymmetric Laplace regressions with
  multiple asymmetric Laplace covariates</title>
    <summary>  In response to the challenge of accommodating non-Gaussian behaviour in data,
the shifted asymmetric Laplace (SAL) cluster-weighted model (SALCWM) is
introduced as a model-based method for jointly clustering responses and random
covariates that exhibit skewness. Within each cluster, the multivariate SAL
distribution is assumed for both the covariates and the responses given the
covariates. To mitigate the effect of possible atypical observations, a
heavy-tailed extension, the contaminated SALCWM (cSALCWM), is also proposed. In
addition to the SALCWM parameters, each mixture component has a parameter
controlling the proportion of outliers, one controlling the proportion of
leverage points, one specifying the degree of outlierness, and another
specifying the degree of leverage. The cSALCWM has the added benefit that once
the model parameters are estimated and the observations are assigned to
components, a more refined intra-group classification in typical points, (mild)
outliers, good leverage, and bad leverage points can be directly obtained. An
expectation-conditional maximization algorithm is developed for efficient
maximum likelihood parameter estimation under this framework. Theoretical
identifiability conditions are established, and empirical results from
simulation studies and validation via real-world applications demonstrate that
the cSALCWM not only preserves the modelling strengths of the SALCWM but also
significantly enhances outlier detection and overall inference reliability. The
methodology proposed in this paper has been implemented in an \texttt{R}
package, which is publicly available at https://github.com/arnootto/ALCWM.
</summary>
    <author>
      <name>Arnoldus F. Otto</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Statistics, University of Pretoria, Pretoria, South Africa</arxiv:affiliation>
    </author>
    <author>
      <name>Andriëtte Bekker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Statistics, University of Pretoria, Pretoria, South Africa</arxiv:affiliation>
    </author>
    <author>
      <name>Antonio Punzo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Economics and Business, University of Catania, Catania, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Johannes T. Ferreira</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Statistics and Actuarial Science, University of the Witwatersrand, Johannesburg, South Africa</arxiv:affiliation>
    </author>
    <author>
      <name>Cristina Tortora</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Mathematics and Statistics, San José State University, California, United States of America</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 4 figures, 13 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.05979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.05979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.05950v1</id>
    <updated>2025-05-09T10:53:47Z</updated>
    <published>2025-05-09T10:53:47Z</published>
    <title>FloE: On-the-Fly MoE Inference</title>
    <summary>  With the widespread adoption of Mixture-of-Experts (MoE) models, there is a
growing demand for efficient inference on memory-constrained devices. While
offloading expert parameters to CPU memory and loading activated experts on
demand has emerged as a potential solution, the large size of activated experts
overburdens the limited PCIe bandwidth, hindering the effectiveness in
latency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly
MoE inference system on memory-constrained GPUs. FloE is built on the insight
that there exists substantial untapped redundancy within sparsely activated
experts. It employs various compression techniques on the expert's internal
parameter matrices to reduce the data movement load, combined with low-cost
sparse prediction, achieving perceptible inference acceleration in wall-clock
time on resource-constrained devices. Empirically, FloE achieves a 9.3x
compression of parameters per expert in Mixtral-8x7B; enables deployment on a
GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and
delivers a 48.7x inference speedup compared to DeepSpeed-MII on a single
GeForce RTX 3090.
</summary>
    <author>
      <name>Yuxin Zhou</name>
    </author>
    <author>
      <name>Zheng Li</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <author>
      <name>Jue Wang</name>
    </author>
    <author>
      <name>Yiping Wang</name>
    </author>
    <author>
      <name>Zhongle Xie</name>
    </author>
    <author>
      <name>Ke Chen</name>
    </author>
    <author>
      <name>Lidan Shou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICML 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.05950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.05950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
