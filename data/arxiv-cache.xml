<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-11-28T00:55:30Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-28T00:55:30Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>127229</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2511.21691v1</id>
    <title>Canvas-to-Image: Compositional Image Generation with Multimodal Controls</title>
    <updated>2025-11-26T18:59:56Z</updated>
    <link href="https://arxiv.org/abs/2511.21691v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21691v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:59:56Z</published>
    <arxiv:comment>24 pages; webpage: https://snap-research.github.io/canvas-to-image/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yusuf Dalva</name>
    </author>
    <author>
      <name>Guocheng Gordon Qian</name>
    </author>
    <author>
      <name>Maya Goldenberg</name>
    </author>
    <author>
      <name>Tsai-Shien Chen</name>
    </author>
    <author>
      <name>Kfir Aberman</name>
    </author>
    <author>
      <name>Sergey Tulyakov</name>
    </author>
    <author>
      <name>Pinar Yanardag</name>
    </author>
    <author>
      <name>Kuan-Chieh Jackson Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21690v1</id>
    <title>TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</title>
    <updated>2025-11-26T18:59:55Z</updated>
    <link href="https://arxiv.org/abs/2511.21690v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21690v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:59:55Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Seungjae Lee</name>
    </author>
    <author>
      <name>Yoonkyo Jung</name>
    </author>
    <author>
      <name>Inkook Chun</name>
    </author>
    <author>
      <name>Yao-Chih Lee</name>
    </author>
    <author>
      <name>Zikui Cai</name>
    </author>
    <author>
      <name>Hongjia Huang</name>
    </author>
    <author>
      <name>Aayush Talreja</name>
    </author>
    <author>
      <name>Tan Dat Dao</name>
    </author>
    <author>
      <name>Yongyuan Liang</name>
    </author>
    <author>
      <name>Jia-Bin Huang</name>
    </author>
    <author>
      <name>Furong Huang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21686v1</id>
    <title>Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework</title>
    <updated>2025-11-26T18:59:28Z</updated>
    <link href="https://arxiv.org/abs/2511.21686v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21686v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:59:28Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Ansong Ni</name>
    </author>
    <author>
      <name>Ching-Feng Yeh</name>
    </author>
    <author>
      <name>Youssef Emad</name>
    </author>
    <author>
      <name>Xinjie Lei</name>
    </author>
    <author>
      <name>Liam Robbins</name>
    </author>
    <author>
      <name>Karthik Padthe</name>
    </author>
    <author>
      <name>Hu Xu</name>
    </author>
    <author>
      <name>Xian Li</name>
    </author>
    <author>
      <name>Asli Celikyilmaz</name>
    </author>
    <author>
      <name>Ramya Raghavendra</name>
    </author>
    <author>
      <name>Lifei Huang</name>
    </author>
    <author>
      <name>Carole-Jean Wu</name>
    </author>
    <author>
      <name>Shang-Wen Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21675v1</id>
    <title>On Evolution-Based Models for Experimentation Under Interference</title>
    <updated>2025-11-26T18:53:46Z</updated>
    <link href="https://arxiv.org/abs/2511.21675v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21675v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:53:46Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Sadegh Shirani</name>
    </author>
    <author>
      <name>Mohsen Bayati</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21669v1</id>
    <title>DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving</title>
    <updated>2025-11-26T18:47:25Z</updated>
    <link href="https://arxiv.org/abs/2511.21669v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21669v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:47:25Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Fengze Yu</name>
    </author>
    <author>
      <name>Leshu Li</name>
    </author>
    <author>
      <name>Brad McDanel</name>
    </author>
    <author>
      <name>Saiqian Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21653v1</id>
    <title>CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow</title>
    <updated>2025-11-26T18:25:41Z</updated>
    <link href="https://arxiv.org/abs/2511.21653v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21653v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:25:41Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ruisheng Han</name>
    </author>
    <author>
      <name>Kanglei Zhou</name>
    </author>
    <author>
      <name>Shuang Chen</name>
    </author>
    <author>
      <name>Amir Atapour-Abarghouei</name>
    </author>
    <author>
      <name>Hubert P. H. Shum</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21644v1</id>
    <title>Factorisation conditions and causality for local measurements in QFT</title>
    <updated>2025-11-26T18:15:21Z</updated>
    <link href="https://arxiv.org/abs/2511.21644v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21644v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Quantum operations that are perfectly admissible in non-relativistic quantum theory can enable signalling between spacelike separated regions when naively imported into quantum field theory (QFT). Prominent examples of such "impossible measurements", in the sense of Sorkin, include certain unitary kicks and projective measurements. It is generally accepted that only those quantum operations whose physical implementation arises from a fully relativistically covariant interaction, between the quantum field and a suitable probe, should be regarded as admissible. While this idea has been realised at the level of abstract algebraic QFT, or via particular measurement models, there is still no general set of operational criteria characterising which measurements are physically implementable. In this work we adopt the local S-matrix formalism, and make use of a hierarchy of factorisation conditions that exclude both superluminal signalling and retrocausality, thereby providing such a criterion. Realising the local S-matrices through explicit interactions between smeared field operators and a pointer degree of freedom, we further derive local causality conditions for the induced Kraus operators, which guarantee the absence of signalling in "impossible measurement" scenarios. Finally, we show that the accuracy with which local field observables can be measured is fundamentally limited by the retarded propagator of the field, which also plays an essential role in a factorisation identity we prove for the field Kraus operators.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:15:21Z</published>
    <arxiv:comment>23 pages, 8 figures</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Robin Simmons</name>
    </author>
    <author>
      <name>Maria Papageorgiou</name>
    </author>
    <author>
      <name>Marios Christodoulou</name>
    </author>
    <author>
      <name>ÄŒaslav Brukner</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21636v1</id>
    <title>Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling</title>
    <updated>2025-11-26T18:08:20Z</updated>
    <link href="https://arxiv.org/abs/2511.21636v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21636v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T18:08:20Z</published>
    <arxiv:comment>Presented at 43rd Conference of the International System Dynamics Society in Boston, United States</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Peter S. Hovmand</name>
    </author>
    <author>
      <name>Kari O'Donnell</name>
    </author>
    <author>
      <name>Callie Ogland-Hand</name>
    </author>
    <author>
      <name>Brian Biroscak</name>
    </author>
    <author>
      <name>Douglas D. Gunzler</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21610v1</id>
    <title>Auxiliary Metrics Help Decoding Skill Neurons in the Wild</title>
    <updated>2025-11-26T17:31:53Z</updated>
    <link href="https://arxiv.org/abs/2511.21610v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21610v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T17:31:53Z</published>
    <arxiv:comment>7 pages, 7 figures. Includes additional appendix</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yixiu Zhao</name>
    </author>
    <author>
      <name>Xiaozhi Wang</name>
    </author>
    <author>
      <name>Zijun Yao</name>
    </author>
    <author>
      <name>Lei Hou</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.21603v1</id>
    <title>Uniform inference for kernel instrumental variable regression</title>
    <updated>2025-11-26T17:18:21Z</updated>
    <link href="https://arxiv.org/abs/2511.21603v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.21603v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Instrumental variable regression is a foundational tool for causal analysis across the social and biomedical sciences. Recent advances use kernel methods to estimate nonparametric causal relationships, with general data types, while retaining a simple closed-form expression. Empirical researchers ultimately need reliable inference on causal estimates; however, uniform confidence sets for the method remain unavailable. To fill this gap, we develop valid and sharp confidence sets for kernel instrumental variable regression, allowing general nonlinearities and data types. Computationally, our bootstrap procedure requires only a single run of the kernel instrumental variable regression estimator. Theoretically, it relies on the same key assumptions. Overall, we provide a practical procedure for inference that substantially increases the value of kernel methods for causal analysis.</summary>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-26T17:18:21Z</published>
    <arxiv:primary_category term="math.ST"/>
    <author>
      <name>Marvin Lob</name>
    </author>
    <author>
      <name>Rahul Singh</name>
    </author>
    <author>
      <name>Suhas Vijaykumar</name>
    </author>
  </entry>
</feed>
