<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-10T00:58:22Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-09T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">114436</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.06275v1</id>
    <updated>2025-06-06T17:58:36Z</updated>
    <published>2025-06-06T17:58:36Z</published>
    <title>Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding</title>
    <summary>  Despite recent progress in vision-language models (VLMs), holistic
understanding of long-form video content remains a significant challenge,
partly due to limitations in current benchmarks. Many focus on peripheral,
``needle-in-a-haystack'' details, encouraging context-insensitive retrieval
over deep comprehension. Others rely on large-scale, semi-automatically
generated questions (often produced by language models themselves) that are
easier for models to answer but fail to reflect genuine understanding. In this
paper, we introduce MF$^2$, a new benchmark for evaluating whether models can
comprehend, consolidate, and recall key narrative information from full-length
movies (50-170 minutes long). MF$^2$ includes over 50 full-length,
open-licensed movies, each paired with manually constructed sets of claim pairs
-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.
These claims target core narrative elements such as character motivations and
emotions, causal chains, and event order, and refer to memorable moments that
humans can recall without rewatching the movie. Instead of multiple-choice
formats, we adopt a binary claim evaluation protocol: for each pair, models
must correctly identify both the true and false claims. This reduces biases
like answer ordering and enables a more precise assessment of reasoning. Our
experiments demonstrate that both open-weight and closed state-of-the-art
models fall well short of human performance, underscoring the relative ease of
the task for humans and their superior ability to retain and reason over
critical narrative information -- an ability current VLMs lack.
</summary>
    <author>
      <name>Emmanouil Zaranis</name>
    </author>
    <author>
      <name>António Farinhas</name>
    </author>
    <author>
      <name>Saul Santos</name>
    </author>
    <author>
      <name>Beatriz Canaverde</name>
    </author>
    <author>
      <name>Miguel Moura Ramos</name>
    </author>
    <author>
      <name>Aditya K Surikuchi</name>
    </author>
    <author>
      <name>André Viveiros</name>
    </author>
    <author>
      <name>Baohao Liao</name>
    </author>
    <author>
      <name>Elena Bueno-Benito</name>
    </author>
    <author>
      <name>Nithin Sivakumaran</name>
    </author>
    <author>
      <name>Pavlo Vasylenko</name>
    </author>
    <author>
      <name>Shoubin Yu</name>
    </author>
    <author>
      <name>Sonal Sannigrahi</name>
    </author>
    <author>
      <name>Wafaa Mohammed</name>
    </author>
    <author>
      <name>Ben Peters</name>
    </author>
    <author>
      <name>Danae Sánchez Villegas</name>
    </author>
    <author>
      <name>Elias Stengel-Eskin</name>
    </author>
    <author>
      <name>Giuseppe Attanasio</name>
    </author>
    <author>
      <name>Jaehong Yoon</name>
    </author>
    <author>
      <name>Stella Frank</name>
    </author>
    <author>
      <name>Alessandro Suglia</name>
    </author>
    <author>
      <name>Chrysoula Zerva</name>
    </author>
    <author>
      <name>Desmond Elliott</name>
    </author>
    <author>
      <name>Mariella Dimiccoli</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <author>
      <name>Oswald Lanz</name>
    </author>
    <author>
      <name>Raffaella Bernardi</name>
    </author>
    <author>
      <name>Raquel Fernández</name>
    </author>
    <author>
      <name>Sandro Pezzelle</name>
    </author>
    <author>
      <name>Vlad Niculae</name>
    </author>
    <author>
      <name>André F. T. Martins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.06275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06270v1</id>
    <updated>2025-06-06T17:53:02Z</updated>
    <published>2025-06-06T17:53:02Z</published>
    <title>RecGPT: A Foundation Model for Sequential Recommendation</title>
    <summary>  This work addresses a fundamental barrier in recommender systems: the
inability to generalize across domains without extensive retraining.
Traditional ID-based approaches fail entirely in cold-start and cross-domain
scenarios where new users or items lack sufficient interaction history.
Inspired by foundation models' cross-domain success, we develop a foundation
model for sequential recommendation that achieves genuine zero-shot
generalization capabilities. Our approach fundamentally departs from existing
ID-based methods by deriving item representations exclusively from textual
features. This enables immediate embedding of any new item without model
retraining. We introduce unified item tokenization with Finite Scalar
Quantization that transforms heterogeneous textual descriptions into
standardized discrete tokens. This eliminates domain barriers that plague
existing systems. Additionally, the framework features hybrid
bidirectional-causal attention that captures both intra-item token coherence
and inter-item sequential dependencies. An efficient catalog-aware beam search
decoder enables real-time token-to-item mapping. Unlike conventional approaches
confined to their training domains, RecGPT naturally bridges diverse
recommendation contexts through its domain-invariant tokenization mechanism.
Comprehensive evaluations across six datasets and industrial scenarios
demonstrate consistent performance advantages.
</summary>
    <author>
      <name>Yangqin Jiang</name>
    </author>
    <author>
      <name>Xubin Ren</name>
    </author>
    <author>
      <name>Lianghao Xia</name>
    </author>
    <author>
      <name>Da Luo</name>
    </author>
    <author>
      <name>Kangyi Lin</name>
    </author>
    <author>
      <name>Chao Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.06270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06267v1</id>
    <updated>2025-06-06T17:49:44Z</updated>
    <published>2025-06-06T17:49:44Z</published>
    <title>When Measurement Mediates the Effect of Interest</title>
    <summary>  Many health promotion strategies aim to improve reach into the target
population and outcomes among those reached. For example, an HIV prevention
strategy could expand the reach of risk screening and the delivery of
biomedical prevention to persons with HIV risk. This setting creates a complex
missing data problem: the strategy improves health outcomes directly and
indirectly through expanded reach, while outcomes are only measured among those
reached. To formally define the total causal effect in such settings, we use
Counterfactual Strata Effects: causal estimands where the outcome is only
relevant for a group whose membership is subject to missingness and/or impacted
by the exposure. To identify and estimate the corresponding statistical
estimand, we propose a novel extension of Two-Stage targeted minimum loss-based
estimation (TMLE). Simulations demonstrate the practical performance of our
approach as well as the limitations of existing approaches.
</summary>
    <author>
      <name>Joy Zora Nakato</name>
    </author>
    <author>
      <name>Janice Litunya</name>
    </author>
    <author>
      <name>Brian Beesiga</name>
    </author>
    <author>
      <name>Jane Kabami</name>
    </author>
    <author>
      <name>James Ayieko</name>
    </author>
    <author>
      <name>Moses R. Kamya</name>
    </author>
    <author>
      <name>Gabriel Chamie</name>
    </author>
    <author>
      <name>Laura B. Balzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures, to be published in 'Statistics in Medicine"</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.06267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06266v1</id>
    <updated>2025-06-06T17:48:23Z</updated>
    <published>2025-06-06T17:48:23Z</published>
    <title>Cartridges: Lightweight and general-purpose long context representations
  via self-study</title>
    <summary>  Large language models are often used to answer queries grounded in large text
corpora (e.g. codebases, legal documents, or chat histories) by placing the
entire corpus in the context window and leveraging in-context learning (ICL).
Although current models support contexts of 100K-1M tokens, this setup is
costly to serve because the memory consumption of the KV cache scales with
input length. We explore an alternative: training a smaller KV cache offline on
each corpus. At inference time, we load this trained KV cache, which we call a
Cartridge, and decode a response. Critically, the cost of training a Cartridge
can be amortized across all the queries referencing the same corpus. However,
we find that the naive approach of training the Cartridge with next-token
prediction on the corpus is not competitive with ICL. Instead, we propose
self-study, a training recipe in which we generate synthetic conversations
about the corpus and train the Cartridge with a context-distillation objective.
We find that Cartridges trained with self-study replicate the functionality of
ICL, while being significantly cheaper to serve. On challenging long-context
benchmarks, Cartridges trained with self-study match ICL performance while
using 38.6x less memory and enabling 26.4x higher throughput. Self-study also
extends the model's effective context length (e.g. from 128k to 484k tokens on
MTOB) and surprisingly, leads to Cartridges that can be composed at inference
time without retraining.
</summary>
    <author>
      <name>Sabri Eyuboglu</name>
    </author>
    <author>
      <name>Ryan Ehrlich</name>
    </author>
    <author>
      <name>Simran Arora</name>
    </author>
    <author>
      <name>Neel Guha</name>
    </author>
    <author>
      <name>Dylan Zinsley</name>
    </author>
    <author>
      <name>Emily Liu</name>
    </author>
    <author>
      <name>Will Tennien</name>
    </author>
    <author>
      <name>Atri Rudra</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <author>
      <name>Azalia Mirhoseini</name>
    </author>
    <author>
      <name>Christopher Re</name>
    </author>
    <link href="http://arxiv.org/abs/2506.06266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06259v1</id>
    <updated>2025-06-06T17:39:32Z</updated>
    <published>2025-06-06T17:39:32Z</published>
    <title>An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower
  Bounds</title>
    <summary>  Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for
characterizing the computational hard phases in statistical detection problems.
The FP criterion, based on an annealed version of the celebrated Franz-Parisi
potential from statistical physics, was shown to be equivalent to low-degree
polynomial (LDP) lower bounds for Gaussian additive models, thereby connecting
two distinct approaches to understanding the computational hardness in
statistical inference. In this paper, we propose a refined FP criterion that
aims to better capture the geometric ``overlap" structure of statistical
models. Our main result establishes that this optimized FP criterion is
equivalent to Statistical Query (SQ) lower bounds -- another foundational
framework in computational complexity of statistical inference. Crucially, this
equivalence holds under a mild, verifiable assumption satisfied by a broad
class of statistical models, including Gaussian additive models, planted sparse
models, as well as non-Gaussian component analysis (NGCA), single-index (SI)
models, and convex truncation detection settings. For instance, in the case of
convex truncation tasks, the assumption is equivalent with the Gaussian
correlation inequality (Royen, 2014) from convex geometry.
  In addition to the above, our equivalence not only unifies and simplifies the
derivation of several known SQ lower bounds -- such as for the NGCA model
(Diakonikolas et al., 2017) and the SI model (Damian et al., 2024) -- but also
yields new SQ lower bounds of independent interest, including for the
computational gaps in mixed sparse linear regression (Arpino et al., 2023) and
convex truncation (De et al., 2023).
</summary>
    <author>
      <name>Siyu Chen</name>
    </author>
    <author>
      <name>Theodor Misiakiewicz</name>
    </author>
    <author>
      <name>Ilias Zadik</name>
    </author>
    <author>
      <name>Peiyuan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.06259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06248v1</id>
    <updated>2025-06-06T17:17:40Z</updated>
    <published>2025-06-06T17:17:40Z</published>
    <title>Lagrangian-based Equilibrium Propagation: generalisation to arbitrary
  boundary conditions &amp; equivalence with Hamiltonian Echo Learning</title>
    <summary>  Equilibrium Propagation (EP) is a learning algorithm for training
Energy-based Models (EBMs) on static inputs which leverages the variational
description of their fixed points. Extending EP to time-varying inputs is a
challenging problem, as the variational description must apply to the entire
system trajectory rather than just fixed points, and careful consideration of
boundary conditions becomes essential. In this work, we present Generalized
Lagrangian Equilibrium Propagation (GLEP), which extends the variational
formulation of EP to time-varying inputs. We demonstrate that GLEP yields
different learning algorithms depending on the boundary conditions of the
system, many of which are impractical for implementation. We then show that
Hamiltonian Echo Learning (HEL) -- which includes the recently proposed
Recurrent HEL (RHEL) and the earlier known Hamiltonian Echo Backpropagation
(HEB) algorithms -- can be derived as a special case of GLEP. Notably, HEL is
the only instance of GLEP we found that inherits the properties that make EP a
desirable alternative to backpropagation for hardware implementations: it
operates in a "forward-only" manner (i.e. using the same system for both
inference and learning), it scales efficiently (requiring only two or more
passes through the system regardless of model size), and enables local
learning.
</summary>
    <author>
      <name>Guillaume Pourcel</name>
    </author>
    <author>
      <name>Debabrota Basu</name>
    </author>
    <author>
      <name>Maxence Ernoult</name>
    </author>
    <author>
      <name>Aditya Gilra</name>
    </author>
    <link href="http://arxiv.org/abs/2506.06248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06233v1</id>
    <updated>2025-06-06T16:53:16Z</updated>
    <published>2025-06-06T16:53:16Z</published>
    <title>Bayesian variable selection in a Cox proportional hazards model with the
  "Sum of Single Effects" prior</title>
    <summary>  Motivated by genetic fine-mapping applications, we introduce a new approach
to Bayesian variable selection regression (BVSR) for time-to-event (TTE)
outcomes. This new approach is designed to deal with the specific challenges
that arise in genetic fine-mapping, including: the presence of very strong
correlations among the covariates, often exceeding 0.99; very large data sets
containing potentially thousands of covariates and hundreds of thousands of
samples. We accomplish this by extending the "Sum of Single Effects" (SuSiE)
method to the Cox proportional hazards (CoxPH) model. We demonstrate the
benefits of the new method, "CoxPH-SuSiE", over existing BVSR methods for TTE
outcomes in simulated fine-mapping data sets. We also illustrate CoxPH-SuSiE on
real data by fine-mapping asthma loci using data from UK Biobank. This
fine-mapping identified 14 asthma risk SNPs in 8 asthma risk loci, among which
6 had strong evidence for being causal (posterior inclusion probability greater
than 50%). Two of the 6 putatively causal variants are known to be pathogenic,
and others lie within a genomic sequence that is known to regulate the
expression of GATA3.
</summary>
    <author>
      <name>Yunqi Yang</name>
    </author>
    <author>
      <name>Karl Tayeb</name>
    </author>
    <author>
      <name>Peter Carbonetto</name>
    </author>
    <author>
      <name>Xiaoyuan Zhong</name>
    </author>
    <author>
      <name>Carole Ober</name>
    </author>
    <author>
      <name>Matthew Stephens</name>
    </author>
    <link href="http://arxiv.org/abs/2506.06233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06211v1</id>
    <updated>2025-06-06T16:17:09Z</updated>
    <published>2025-06-06T16:17:09Z</published>
    <title>PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in
  Puzzlehunts</title>
    <summary>  Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined
problem definitions. In contrast to conventional reasoning benchmarks
consisting of tasks with clear instructions, puzzlehunts require models to
discover the underlying problem structure from multimodal evidence and
iterative reasoning, mirroring real-world domains such as scientific discovery,
exploratory data analysis, or investigative problem-solving. Despite recent
progress in foundation models, their performance on such open-ended settings
remains largely untested. In this paper, we introduce PuzzleWorld, a
large-scale benchmark of 667 puzzlehunt-style problems designed to assess
step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is
annotated with the final solution, detailed reasoning traces, and cognitive
skill labels, enabling holistic benchmarking and fine-grained diagnostic
analysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,
with the best model solving only 14% of puzzles and reaching 40% stepwise
accuracy. To demonstrate the value of our reasoning annotations, we show that
fine-tuning a small model on reasoning traces improves stepwise reasoning from
4% to 11%, while training on final answers alone degrades performance to near
zero. Our error analysis reveals that current models exhibit myopic reasoning,
are bottlenecked by the limitations of language-based inference, and lack
sketching capabilities crucial for visual and spatial reasoning. We release
PuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on
building more general, open-ended, and creative reasoning systems.
</summary>
    <author>
      <name>Hengzhi Li</name>
    </author>
    <author>
      <name>Brendon Jiang</name>
    </author>
    <author>
      <name>Alexander Naehu</name>
    </author>
    <author>
      <name>Regan Song</name>
    </author>
    <author>
      <name>Justin Zhang</name>
    </author>
    <author>
      <name>Megan Tjandrasuwita</name>
    </author>
    <author>
      <name>Chanakya Ekbote</name>
    </author>
    <author>
      <name>Steven-Shine Chen</name>
    </author>
    <author>
      <name>Adithya Balachandran</name>
    </author>
    <author>
      <name>Wei Dai</name>
    </author>
    <author>
      <name>Rebecca Chang</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.06211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06158v1</id>
    <updated>2025-06-06T15:25:14Z</updated>
    <published>2025-06-06T15:25:14Z</published>
    <title>ENMA: Tokenwise Autoregression for Generative Neural PDE Operators</title>
    <summary>  Solving time-dependent parametric partial differential equations (PDEs)
remains a fundamental challenge for neural solvers, particularly when
generalizing across a wide range of physical parameters and dynamics. When data
is uncertain or incomplete-as is often the case-a natural approach is to turn
to generative models. We introduce ENMA, a generative neural operator designed
to model spatio-temporal dynamics arising from physical phenomena. ENMA
predicts future dynamics in a compressed latent space using a generative masked
autoregressive transformer trained with flow matching loss, enabling tokenwise
generation. Irregularly sampled spatial observations are encoded into uniform
latent representations via attention mechanisms and further compressed through
a spatio-temporal convolutional encoder. This allows ENMA to perform in-context
learning at inference time by conditioning on either past states of the target
trajectory or auxiliary context trajectories with similar dynamics. The result
is a robust and adaptable framework that generalizes to new PDE regimes and
supports one-shot surrogate modeling of time-dependent parametric PDEs.
</summary>
    <author>
      <name>Armand Kassaï Koupaï</name>
    </author>
    <author>
      <name>Lise Le Boudec</name>
    </author>
    <author>
      <name>Louis Serrano</name>
    </author>
    <author>
      <name>Patrick Gallinari</name>
    </author>
    <link href="http://arxiv.org/abs/2506.06158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.06139v1</id>
    <updated>2025-06-06T14:56:24Z</updated>
    <published>2025-06-06T14:56:24Z</published>
    <title>Impact of initial mass function on the chemical evolution of
  high-redshift galaxies</title>
    <summary>  Star formation and metal enrichment in galaxies are regulated by supernova
(SN) explosions and metal yields from massive stars, which are sensitive to the
high-mass end of the initial mass function (IMF). Recent JWST observations have
reached a consensus on an invariant relation between stellar mass, metallicity,
and star formation rate up to $z\sim 8$ and its breakdown at higher redshifts.
It is crucial to understand the underlying physics, especially the role played
by the IMF. We explore the impact of IMF on the chemical evolution of
high-redshift galaxies and the interplay between IMF and galactic outflows. The
ultimate goal is to constrain the high-mass end of the IMF by the cosmic star
formation history and stellar mass-metallicity-star formation rate relation
(MZSFR) inferred from observations at $z\sim 4-10$. Using the semi-analytical
galaxy evolution code A-SLOTH, we follow galactic baryon cycles along merger
trees built from cosmological simulations. Stellar feedback is modeled with
up-to-date stellar evolution tracks covering the full metallicity range ($Z
\sim 10^{-11} - 0.03$) and a broad stellar mass range ($m_\star\sim2 - 600\ \rm
M_\odot$) including the metal yields from stellar winds, core-collapse SNe,
(pulsational) pair-instability SNe, and Type Ia SNe. Assuming that the IMF
follows a Kroupa-like shape with a varying upper mass limit $m_{\max}$, we find
$m_{\max} \gtrsim 200\ \rm M_\odot$ is required to reproduce the observed
MZSFR. Observational data at $z\gtrsim 6$ favor a galactic outflow model where
the outflow mass is proportional to the ratio of supernova energy to halo
binding energy. We conclude that very massive ($\gtrsim 200\ \rm M_\odot$)
stars can play important roles in the star formation and chemical enrichment
histories of high-$z$ galaxies. We also discuss their implications for
transient sources of both electromagnetic waves and gravitational waves.
</summary>
    <author>
      <name>Boyuan Liu</name>
    </author>
    <author>
      <name>Michela Mapelli</name>
    </author>
    <author>
      <name>Volker Bromm</name>
    </author>
    <author>
      <name>Ralf S. Klessen</name>
    </author>
    <author>
      <name>Lumen Boco</name>
    </author>
    <author>
      <name>Tilman Hartwig</name>
    </author>
    <author>
      <name>Simon C. O. Glover</name>
    </author>
    <author>
      <name>Veronika Lipatova</name>
    </author>
    <author>
      <name>Guglielmo Costa</name>
    </author>
    <author>
      <name>Marco Dall'Amico</name>
    </author>
    <author>
      <name>Giuliano Iorio</name>
    </author>
    <author>
      <name>Kendall Shepherd</name>
    </author>
    <author>
      <name>Alessandro Bressan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19+7 pages, 12+4 figures, to be submitted to A&amp;A, comments are
  welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.06139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.06139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
