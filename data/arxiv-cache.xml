<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-07T00:52:56Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">122706</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.03230v1</id>
    <updated>2025-10-03T17:59:34Z</updated>
    <published>2025-10-03T17:59:34Z</published>
    <title>Improving GUI Grounding with Explicit Position-to-Coordinate Mapping</title>
    <summary>  GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.
</summary>
    <author>
      <name>Suyuchen Wang</name>
    </author>
    <author>
      <name>Tianyu Zhang</name>
    </author>
    <author>
      <name>Ahmed Masry</name>
    </author>
    <author>
      <name>Christopher Pal</name>
    </author>
    <author>
      <name>Spandana Gella</name>
    </author>
    <author>
      <name>Bang Liu</name>
    </author>
    <author>
      <name>Perouz Taslakian</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03229v1</id>
    <updated>2025-10-03T17:59:03Z</updated>
    <published>2025-10-03T17:59:03Z</published>
    <title>Robust magnetic field estimates in star-forming galaxies with the
  equipartition formula in the absence of equipartition</title>
    <summary>  The equipartition model is widely used to estimate magnetic field strength
from synchrotron intensity in radio galaxies, yet the validity of its
underlying assumptions remains uncertain. Using an Arepo simulation which
incorporates a two-moment cosmic ray (CR) transport scheme and a multiphase
interstellar medium, we compare magnetic fields inferred from synthetic
synchrotron emission maps with the true fields in the simulation. Starting from
the derivation of the equipartition formula, we find that the deviation between
the equipartition magnetic field and the true magnetic field depends only
weakly on the ratio of the magnetic to the CR energy density. In practice, for
both face-on and edge-on projections, the equipartition model slightly
overestimates the total synchrotron-weighted magnetic field with mean offsets
of 32% (0.17 dex) and 36% (0.2 dex), even though the energy equipartition does
not hold locally. Beyond these average offsets, a clear trend emerges in
edge-on projections that the model underestimates the field in the disk and
overestimates it in the halo. Our results demonstrate that the validity of the
equipartition model depends only weakly on the strict fulfillment of energy
equipartition, and that the equipartition model remains a practical method for
estimating magnetic field strengths in face-on projection maps based on our
CR-magnetohydrodynamics simulation.
</summary>
    <author>
      <name>H. -H. Sandy Chiu</name>
    </author>
    <author>
      <name>Mateusz Ruszkowski</name>
    </author>
    <author>
      <name>Maria Werhahn</name>
    </author>
    <author>
      <name>Christoph Pfrommer</name>
    </author>
    <author>
      <name>Timon Thomas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; submitted to A&amp;A</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.03229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03223v1</id>
    <updated>2025-10-03T17:56:33Z</updated>
    <published>2025-10-03T17:56:33Z</published>
    <title>Self-Anchor: Large Language Model Reasoning via Step-by-step Attention
  Alignment</title>
    <summary>  To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.
</summary>
    <author>
      <name>Hongxiang Zhang</name>
    </author>
    <author>
      <name>Yuan Tian</name>
    </author>
    <author>
      <name>Tianyi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03221v1</id>
    <updated>2025-10-03T17:54:56Z</updated>
    <published>2025-10-03T17:54:56Z</published>
    <title>Inferring Stellar Densities with Flexible Models I: The Distribution of
  RR Lyrae in the Milky Way with $\textit{Gaia}$ DR3</title>
    <summary>  Understanding the formation and evolutionary history of the Milky Way
requires detailed mapping of its stellar components, which preserve fossil
records of the Galaxy's assembly through cosmic time. RR Lyrae stars are
particularly well-suited for this endeavor, as they are old, standard candle
variables that probe the Galaxy's earliest formation epochs. In this work, we
employ a hierarchical Bayesian Gaussian Mixture Model (GMM) to characterize the
three-dimensional density distribution of RR Lyrae stars in the Milky Way. This
approach provides a flexible framework for modeling complex stellar
distributions, particularly in the inner Galaxy where the bulge, disk, and halo
components overlap. Our analysis reveals that the inner Galaxy is dominated by
a distinct prolate stellar population with axis ratio $q$=1.30. Consistent with
previous work, we find the halo follows a $r^{-4}$ power-law profile that
flattens within 12 kpc of the Galactic center. We also confirm the halo is
oblate ($q$=0.62) with a tilt angle of $12.22^{\circ}$. We report for the first
time that this tilt aligns the halo major axis in the direction of the
Sagittarius dwarf galaxy. These results establish GMMs as an effective and
flexible tool for modeling Galactic structure and provide new constraints on
the distribution of old stars in the inner Galaxy.
</summary>
    <author>
      <name>Madeline Lucey</name>
    </author>
    <author>
      <name>Cecilia Mateu</name>
    </author>
    <author>
      <name>Adrian Price-Whelan</name>
    </author>
    <author>
      <name>David Hogg</name>
    </author>
    <author>
      <name>Hans-Walter Rix</name>
    </author>
    <author>
      <name>Robyn Sanderson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, submitted to ApJ, comments welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.03221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03199v1</id>
    <updated>2025-10-03T17:35:45Z</updated>
    <published>2025-10-03T17:35:45Z</published>
    <title>Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference
  Scaling</title>
    <summary>  LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.
</summary>
    <author>
      <name>Qiwei Di</name>
    </author>
    <author>
      <name>Kaixuan Ji</name>
    </author>
    <author>
      <name>Xuheng Li</name>
    </author>
    <author>
      <name>Heyang Zhao</name>
    </author>
    <author>
      <name>Quanquan Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.03199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03185v1</id>
    <updated>2025-10-03T17:09:03Z</updated>
    <published>2025-10-03T17:09:03Z</published>
    <title>PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning</title>
    <summary>  Benchmarks for competition-style reasoning have advanced evaluation in
mathematics and programming, yet physics remains comparatively explored. Most
existing physics benchmarks evaluate only final answers, which fail to capture
reasoning processes, while recent stepwise methods rely on heuristic
LLM-as-judge scoring or restrictive linear assumptions, limiting reliability
and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation
framework and benchmark for complex physics reasoning problems. Solutions are
represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding
causal dependencies among intermediate steps to enable fine-grained,
interpretable, and theoretically grounded scoring. We prove the optimality of
the DAG representation and the corresponding scoring policy. Combining with a
fully rule-based method for symbolic formula equivalence matching that we
developed, we ensure consistent validation across diverse formulations without
heuristic judgments. Results show that our evaluation framework is more aligned
with human experts' scoring. Experiments on state-of-the-art LLMs reveal
persistent reasoning failures in physics, while step-level scoring offers both
diagnostic insight and rich signals for later training. By combining structural
rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides
a principled foundation for advancing process-level evaluation and guiding the
development of models with deeper scientific reasoning capabilities.
</summary>
    <author>
      <name>Wanjia Zhao</name>
    </author>
    <author>
      <name>Qinwei Ma</name>
    </author>
    <author>
      <name>Jingzhe Shi</name>
    </author>
    <author>
      <name>Shirley Wu</name>
    </author>
    <author>
      <name>Jiaqi Han</name>
    </author>
    <author>
      <name>Yijia Xiao</name>
    </author>
    <author>
      <name>Si-Yuan Chen</name>
    </author>
    <author>
      <name>Xiao Luo</name>
    </author>
    <author>
      <name>Ludwig Schmidt</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03174v1</id>
    <updated>2025-10-03T16:48:32Z</updated>
    <published>2025-10-03T16:48:32Z</published>
    <title>Topic Modeling as Long-Form Generation: Can Long-Context LLMs
  revolutionize NTM via Zero-Shot Prompting?</title>
    <summary>  Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."
</summary>
    <author>
      <name>Xuan Xu</name>
    </author>
    <author>
      <name>Haolun Li</name>
    </author>
    <author>
      <name>Zhongliang Yang</name>
    </author>
    <author>
      <name>Beilin Chu</name>
    </author>
    <author>
      <name>Jia Song</name>
    </author>
    <author>
      <name>Moxuan Xu</name>
    </author>
    <author>
      <name>Linna Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03151v1</id>
    <updated>2025-10-03T16:24:50Z</updated>
    <published>2025-10-03T16:24:50Z</published>
    <title>Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory
  Perspective</title>
    <summary>  This paper uses classical high-rate quantization theory to provide new
insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is
defined by a segmentation of the input space to regions, each with a
single-parameter expert that acts as a constant predictor with zero-compute at
inference. Motivated by high-rate quantization theory assumptions, we assume
that the number of experts is sufficiently large to make their input-space
regions very small. This lets us to study the approximation error of our MoE
model class: (i) for one-dimensional inputs, we formulate the test error and
its minimizing segmentation and experts; (ii) for multidimensional inputs, we
formulate an upper bound for the test error and study its minimization.
Moreover, we consider the learning of the expert parameters from a training
dataset, given an input-space segmentation, and formulate their statistical
learning properties. This leads us to theoretically and empirically show how
the tradeoff between approximation and estimation errors in MoE learning
depends on the number of experts.
</summary>
    <author>
      <name>Yehuda Dar</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03141v1</id>
    <updated>2025-10-03T16:11:36Z</updated>
    <published>2025-10-03T16:11:36Z</published>
    <title>Nonmodal growth and optimal perturbations in magnetohydrodynamic shear
  flows</title>
    <summary>  In astrophysical shear flows, the Kelvin-Helmholtz (KH) instability is
generally suppressed by magnetic tension provided a sufficiently strong
streamwise magnetic field. This is often used to infer upper (or lower) bounds
on field strengths in systems where shear-driven fluctuations are (or are not)
observed, on the basis that fluctuations cannot grow in the absence of linear
instability. On the contrary, by calculating the maximum growth that
small-amplitude perturbations can achieve in finite time for such a system, we
show that perturbations can grow in energy by orders of magnitude even when the
flow is sub-Alfv\'enic, suggesting that shear-driven turbulence is possible
even in the presence of strong magnetic fields, and challenging inferences from
the observed presence or absence of shear-driven fluctuations. We further show
that magnetic fields introduce additional nonmodal growth mechanisms relative
to the hydrodynamic case, and that 2D simulations miss key aspects of these
growth mechanisms.
</summary>
    <author>
      <name>Adrian E. Fraser</name>
    </author>
    <author>
      <name>Alexis K. Kaminski</name>
    </author>
    <author>
      <name>Jeffrey S. Oishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, submitted to PRL</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.03141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03127v1</id>
    <updated>2025-10-03T15:53:28Z</updated>
    <published>2025-10-03T15:53:28Z</published>
    <title>A Study of Rule Omission in Raven's Progressive Matrices</title>
    <summary>  Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.
</summary>
    <author>
      <name>Binze Li</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
