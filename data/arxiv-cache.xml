<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-24T00:51:36Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-23T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">124382</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.19811v1</id>
    <updated>2025-10-22T17:48:23Z</updated>
    <published>2025-10-22T17:48:23Z</published>
    <title>Hubble: a Model Suite to Advance the Study of LLM Memorization</title>
    <summary>  We present Hubble, a suite of fully open-source large language models (LLMs)
for the scientific study of LLM memorization. Hubble models come in standard
and perturbed variants: standard models are pretrained on a large English
corpus, and perturbed models are trained in the same way but with controlled
insertion of text (e.g., book passages, biographies, and test sets) designed to
emulate key memorization risks. Our core release includes 8 models -- standard
and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B
tokens -- establishing that memorization risks are determined by the frequency
of sensitive data relative to size of the training corpus (i.e., a password
appearing once in a smaller corpus is memorized better than the same password
in a larger corpus). Our release also includes 6 perturbed models with text
inserted at different pretraining phases, showing that sensitive data without
continued exposure can be forgotten. These findings suggest two best practices
for addressing memorization risks: to dilute sensitive data by increasing the
size of the training corpus, and to order sensitive data to appear earlier in
training. Beyond these general empirical findings, Hubble enables a broad range
of memorization research; for example, analyzing the biographies reveals how
readily different types of private information are memorized. We also
demonstrate that the randomized insertions in Hubble make it an ideal testbed
for membership inference and machine unlearning, and invite the community to
further explore, benchmark, and build upon our work.
</summary>
    <author>
      <name>Johnny Tian-Zheng Wei</name>
    </author>
    <author>
      <name>Ameya Godbole</name>
    </author>
    <author>
      <name>Mohammad Aflah Khan</name>
    </author>
    <author>
      <name>Ryan Wang</name>
    </author>
    <author>
      <name>Xiaoyuan Zhu</name>
    </author>
    <author>
      <name>James Flemings</name>
    </author>
    <author>
      <name>Nitya Kashyap</name>
    </author>
    <author>
      <name>Krishna P. Gummadi</name>
    </author>
    <author>
      <name>Willie Neiswanger</name>
    </author>
    <author>
      <name>Robin Jia</name>
    </author>
    <link href="http://arxiv.org/abs/2510.19811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19798v1</id>
    <updated>2025-10-22T17:34:26Z</updated>
    <published>2025-10-22T17:34:26Z</published>
    <title>Detecting gravitational lensing by matter currents</title>
    <summary>  We explore the observational prospects for detecting gravitational lensing
induced by cosmological matter currents, a relativistic correction to the
standard scalar lensing effect arising from the motion of matter. We propose to
isolate this contribution by cross-correlating the weak-lensing convergence
field with a reconstructed cosmic momentum field inferred from galaxy surveys.
Using numerical simulations, we demonstrate that this reconstructed momentum
field is uncorrelated with the scalar lensing signal, enabling a clean
separation of the gravitomagnetic component. We then forecast the detectability
of this signal for upcoming wide-field galaxy and weak-lensing surveys, showing
that a statistically significant detection may be achievable under realistic
observational conditions. Such a measurement would provide the first direct
probe of the large-scale cosmic momentum field, offering a novel test of
general relativity and Lorentz invariance on cosmological scales.
</summary>
    <author>
      <name>C. Murray</name>
    </author>
    <author>
      <name>R. Kou</name>
    </author>
    <author>
      <name>J. G. Bartlett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.19798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19796v1</id>
    <updated>2025-10-22T17:30:39Z</updated>
    <published>2025-10-22T17:30:39Z</published>
    <title>Blackbox Model Provenance via Palimpsestic Membership Inference</title>
    <summary>  Suppose Alice trains an open-weight language model and Bob uses a blackbox
derivative of Alice's model to produce text. Can Alice prove that Bob is using
her model, either by querying Bob's derivative model (query setting) or from
the text alone (observational setting)? We formulate this question as an
independence testing problem--in which the null hypothesis is that Bob's model
or text is independent of Alice's randomized training run--and investigate it
through the lens of palimpsestic memorization in language models: models are
more likely to memorize data seen later in training, so we can test whether Bob
is using Alice's model using test statistics that capture correlation between
Bob's model or text and the ordering of training examples in Alice's training
run. If Alice has randomly shuffled her training data, then any significant
correlation amounts to exactly quantifiable statistical evidence against the
null hypothesis, regardless of the composition of Alice's training data. In the
query setting, we directly estimate (via prompting) the likelihood Bob's model
gives to Alice's training examples and order; we correlate the likelihoods of
over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to
12B parameters with the base model's training data order, achieving a p-value
on the order of at most 1e-8 in all but six cases. In the observational
setting, we try two approaches based on estimating 1) the likelihood of Bob's
text overlapping with spans of Alice's training examples and 2) the likelihood
of Bob's text with respect to different versions of Alice's model we obtain by
repeating the last phase (e.g., 1%) of her training run on reshuffled data. The
second approach can reliably distinguish Bob's text from as little as a few
hundred tokens; the first does not involve any retraining but requires many
more tokens (several hundred thousand) to achieve high power.
</summary>
    <author>
      <name>Rohith Kuditipudi</name>
    </author>
    <author>
      <name>Jing Huang</name>
    </author>
    <author>
      <name>Sally Zhu</name>
    </author>
    <author>
      <name>Diyi Yang</name>
    </author>
    <author>
      <name>Christopher Potts</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.19796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19788v2</id>
    <updated>2025-10-23T11:48:07Z</updated>
    <published>2025-10-22T17:23:18Z</published>
    <title>Benchmarking World-Model Learning</title>
    <summary>  Model-learning agents should gather information to learn world models that
support many downstream tasks and inferences, such as predicting unobserved
states, estimating near- and far-term consequences of actions, planning action
sequences, and detecting changes in dynamics. Current methods for learning and
evaluating world models diverge from this goal: training and evaluation are
anchored to next-frame prediction, and success is scored by reward maximization
in the same environment. We propose WorldTest, a protocol to evaluate
model-learning agents that separates reward-free interaction from a scored test
phase in a different but related environment. WorldTest is
open-ended$\unicode{x2014}$models should support many different tasks unknown
ahead of time$\unicode{x2014}$and agnostic to model representation, allowing
comparison across approaches. We instantiated WorldTest with AutumnBench, a
suite of 43 interactive grid-world environments and 129 tasks across three
families: masked-frame prediction, planning, and predicting changes to the
causal dynamics. We compared 517 human participants and three frontier models
on AutumnBench. We found that humans outperform the models, and scaling compute
improves performance only in some environments but not others. WorldTest
provides a novel template$\unicode{x2014}$reward-free exploration, derived
tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn
about environment dynamics, and AutumnBench exposes significant headroom in
world-model learning.
</summary>
    <author>
      <name>Archana Warrier</name>
    </author>
    <author>
      <name>Dat Nguyen</name>
    </author>
    <author>
      <name>Michelangelo Naim</name>
    </author>
    <author>
      <name>Moksh Jain</name>
    </author>
    <author>
      <name>Yichao Liang</name>
    </author>
    <author>
      <name>Karen Schroeder</name>
    </author>
    <author>
      <name>Cambridge Yang</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <author>
      <name>Sebastian Vollmer</name>
    </author>
    <author>
      <name>Kevin Ellis</name>
    </author>
    <author>
      <name>Zenna Tavares</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.19788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19784v1</id>
    <updated>2025-10-22T17:20:12Z</updated>
    <published>2025-10-22T17:20:12Z</published>
    <title>Environment Inference for Learning Generalizable Dynamical System</title>
    <summary>  Data-driven methods offer efficient and robust solutions for analyzing
complex dynamical systems but rely on the assumption of I.I.D. data, driving
the development of generalization techniques for handling environmental
differences. These techniques, however, are limited by their dependence on
environment labels, which are often unavailable during training due to data
acquisition challenges, privacy concerns, and environmental variability,
particularly in large public datasets and privacy-sensitive domains. In
response, we propose DynaInfer, a novel method that infers environment
specifications by analyzing prediction errors from fixed neural networks within
each training round, enabling environment assignments directly from data. We
prove our algorithm effectively solves the alternating optimization problem in
unlabeled scenarios and validate it through extensive experiments across
diverse dynamical systems. Results show that DynaInfer outperforms existing
environment assignment techniques, converges rapidly to true labels, and even
achieves superior performance when environment labels are available.
</summary>
    <author>
      <name>Shixuan Liu</name>
    </author>
    <author>
      <name>Yue He</name>
    </author>
    <author>
      <name>Haotian Wang</name>
    </author>
    <author>
      <name>Wenjing Yang</name>
    </author>
    <author>
      <name>Yunfei Wang</name>
    </author>
    <author>
      <name>Peng Cui</name>
    </author>
    <author>
      <name>Zhong Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2025 Spotlight</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.19784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19779v1</id>
    <updated>2025-10-22T17:13:00Z</updated>
    <published>2025-10-22T17:13:00Z</published>
    <title>AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
  Decoders</title>
    <summary>  Speculative Decoding (SD) accelerates large language model inference by
employing a small draft model to generate predictions, which are then verified
by a larger target model. The effectiveness of SD hinges on the alignment
between these models, which is typically enhanced by Knowledge Distillation
(KD). However, conventional KD methods aim to minimize the KL divergence
between the draft and target models across all tokens, a goal that is
misaligned with the true objective of SD, which is to maximize token acceptance
rate. Therefore, draft models often struggle to fully assimilate the target
model's knowledge due to capacity constraints, leading to suboptimal
performance. To address this challenge, we propose AdaSPEC, a novel method that
incorporates selective token filtering into the KD process. AdaSPEC utilizes a
reference model to identify and filter out difficult-to-fit tokens, enabling
the distillation of a draft model that better aligns with the target model on
simpler tokens. This approach improves the overall token acceptance rate
without compromising generation quality. We evaluate AdaSPEC across diverse
tasks, including arithmetic reasoning, instruction-following, coding, and
summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.
Our results demonstrate that AdaSPEC consistently outperforms the
state-of-the-art DistillSpec method, achieving higher acceptance rates across
all tasks (up to 15\%). The code is publicly available at
https://github.com/yuezhouhu/adaspec.
</summary>
    <author>
      <name>Yuezhou Hu</name>
    </author>
    <author>
      <name>Jiaxin Guo</name>
    </author>
    <author>
      <name>Xinyu Feng</name>
    </author>
    <author>
      <name>Tuo Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2510.19779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19773v1</id>
    <updated>2025-10-22T17:03:55Z</updated>
    <published>2025-10-22T17:03:55Z</published>
    <title>The Tail Tells All: Estimating Model-Level Membership Inference
  Vulnerability Without Reference Models</title>
    <summary>  Membership inference attacks (MIAs) have emerged as the standard tool for
evaluating the privacy risks of AI models. However, state-of-the-art attacks
require training numerous, often computationally expensive, reference models,
limiting their practicality. We present a novel approach for estimating
model-level vulnerability, the TPR at low FPR, to membership inference attacks
without requiring reference models. Empirical analysis shows loss distributions
to be asymmetric and heavy-tailed and suggests that most points at risk from
MIAs have moved from the tail (high-loss region) to the head (low-loss region)
of the distribution after training. We leverage this insight to propose a
method to estimate model-level vulnerability from the training and testing
distribution alone: using the absence of outliers from the high-loss region as
a predictor of the risk. We evaluate our method, the TNR of a simple loss
attack, across a wide range of architectures and datasets and show it to
accurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). We
also show our method to outperform both low-cost (few reference models) attacks
such as RMIA and other measures of distribution difference. We finally evaluate
the use of non-linear functions to evaluate risk and show the approach to be
promising to evaluate the risk in large-language models.
</summary>
    <author>
      <name>Euodia Dodd</name>
    </author>
    <author>
      <name>Nataša Krčo</name>
    </author>
    <author>
      <name>Igor Shilov</name>
    </author>
    <author>
      <name>Yves-Alexandre de Montjoye</name>
    </author>
    <link href="http://arxiv.org/abs/2510.19773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19767v1</id>
    <updated>2025-10-22T16:56:01Z</updated>
    <published>2025-10-22T16:56:01Z</published>
    <title>SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via
  Promoting Deeper Thought Exploration</title>
    <summary>  The long chain-of-thought (LongCoT) capability is central to the recent
breakthroughs achieved by large language models in complex reasoning tasks.
However, the accompanying issue of ''underthinking'', where models exhibit
shallow reasoning by frequently switching thoughts without sufficient
exploration, limits both performance and token efficiency. To address this
problem, we propose a simple yet effective reasoning strategy: the SmartSwitch
inference framework. This framework can be easily integrated into any large
language model as a plug-and-play solution, continuously monitoring the model's
reasoning process to detect underthinking and guide it toward deeper
exploration of promising but overlooked thoughts. Specifically, the perception
module identifies points where thoughts switch and evaluates the potential of
the preceding thought using an off-the-shelf process reward model (PRM). If a
high-potential thought is found to be prematurely abandoned, the intervention
module interrupts the ongoing inference, backtracks to the point before the
switch, and inserts a "deepening prompt" to encourage further exploration along
that promising path. Extensive experiments on challenging mathematical
reasoning benchmarks demonstrate that our method significantly enhances the
performance of various large language models of different sizes.
</summary>
    <author>
      <name>Xichen Zhang</name>
    </author>
    <author>
      <name>Sitong Wu</name>
    </author>
    <author>
      <name>Haoru Tan</name>
    </author>
    <author>
      <name>Shaozuo Yu</name>
    </author>
    <author>
      <name>Yinghao Zhu</name>
    </author>
    <author>
      <name>Ziyi He</name>
    </author>
    <author>
      <name>Jiaya Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/dvlab-research/SmartSwitch</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.19767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19764v1</id>
    <updated>2025-10-22T16:50:00Z</updated>
    <published>2025-10-22T16:50:00Z</published>
    <title>A flexible framework for structural plasticity in GPU-accelerated sparse
  spiking neural networks</title>
    <summary>  The majority of research in both training Artificial Neural Networks (ANNs)
and modeling learning in biological brains focuses on synaptic plasticity,
where learning equates to changing the strength of existing connections.
However, in biological brains, structural plasticity - where new connections
are created and others removed - is also vital, not only for effective learning
but also for recovery from damage and optimal resource usage. Inspired by
structural plasticity, pruning is often used in machine learning to remove weak
connections from trained models to reduce the computational requirements of
inference. However, the machine learning frameworks typically used for
backpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)
are optimized for dense connectivity, meaning that pruning does not help reduce
the training costs of ever-larger models. The GeNN simulator already supports
efficient GPU-accelerated simulation of sparse SNNs for computational
neuroscience and machine learning. Here, we present a new flexible framework
for implementing GPU-accelerated structural plasticity rules and demonstrate
this first using the e-prop supervised learning rule and DEEP R to train
efficient, sparse SNN classifiers and then, in an unsupervised learning
context, to learn topographic maps. Compared to baseline dense models, our
sparse classifiers reduce training time by up to 10x while the DEEP R rewiring
enables them to perform as well as the original models. We demonstrate
topographic map formation in faster-than-realtime simulations, provide insights
into the connectivity evolution, and measure simulation speed versus network
size. The proposed framework will enable further research into achieving and
maintaining sparsity in network structure and neural communication, as well as
exploring the computational benefits of sparsity in a range of neuromorphic
applications.
</summary>
    <author>
      <name>James C. Knight</name>
    </author>
    <author>
      <name>Johanna Senk</name>
    </author>
    <author>
      <name>Thomas Nowotny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.19764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.19755v2</id>
    <updated>2025-10-23T09:09:15Z</updated>
    <published>2025-10-22T16:46:05Z</published>
    <title>A Survey on Cache Methods in Diffusion Models: Toward Efficient
  Multi-Modal Generation</title>
    <summary>  Diffusion Models have become a cornerstone of modern generative AI for their
exceptional generation quality and controllability. However, their inherent
\textit{multi-step iterations} and \textit{complex backbone networks} lead to
prohibitive computational overhead and generation latency, forming a major
bottleneck for real-time applications. Although existing acceleration
techniques have made progress, they still face challenges such as limited
applicability, high training costs, or quality degradation.
  Against this backdrop, \textbf{Diffusion Caching} offers a promising
training-free, architecture-agnostic, and efficient inference paradigm. Its
core mechanism identifies and reuses intrinsic computational redundancies in
the diffusion process. By enabling feature-level cross-step reuse and
inter-layer scheduling, it reduces computation without modifying model
parameters. This paper systematically reviews the theoretical foundations and
evolution of Diffusion Caching and proposes a unified framework for its
classification and analysis.
  Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}. This trend enhances caching flexibility across diverse tasks and
enables integration with other acceleration techniques such as sampling
optimization and model distillation, paving the way for a unified, efficient
inference framework for future multimodal and interactive applications. We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.
</summary>
    <author>
      <name>Jiacheng Liu</name>
    </author>
    <author>
      <name>Xinyu Wang</name>
    </author>
    <author>
      <name>Yuqi Lin</name>
    </author>
    <author>
      <name>Zhikai Wang</name>
    </author>
    <author>
      <name>Peiru Wang</name>
    </author>
    <author>
      <name>Peiliang Cai</name>
    </author>
    <author>
      <name>Qinming Zhou</name>
    </author>
    <author>
      <name>Zhengan Yan</name>
    </author>
    <author>
      <name>Zexuan Yan</name>
    </author>
    <author>
      <name>Zhengyi Shi</name>
    </author>
    <author>
      <name>Chang Zou</name>
    </author>
    <author>
      <name>Yue Ma</name>
    </author>
    <author>
      <name>Linfeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages,2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.19755v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.19755v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
