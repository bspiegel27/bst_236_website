<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-23T00:56:37Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-22T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">112812</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.15818v1</id>
    <updated>2025-05-21T17:59:56Z</updated>
    <published>2025-05-21T17:59:56Z</published>
    <title>InstructSAM: A Training-Free Framework for Instruction-Oriented Remote
  Sensing Object Recognition</title>
    <summary>  Language-Guided object recognition in remote sensing imagery is crucial for
large-scale mapping and automated data annotation. However, existing
open-vocabulary and visual grounding methods rely on explicit category cues,
limiting their ability to handle complex or implicit queries that require
advanced reasoning. To address this issue, we introduce a new suite of tasks,
including Instruction-Oriented Object Counting, Detection, and Segmentation
(InstructCDS), covering open-vocabulary, open-ended, and open-subclass
scenarios. We further present EarthInstruct, the first InstructCDS benchmark
for earth observation. It is constructed from two diverse remote sensing
datasets with varying spatial resolutions and annotation rules across 20
categories, necessitating models to interpret dataset-specific instructions.
Given the scarcity of semantically rich labeled data in remote sensing, we
propose InstructSAM, a training-free framework for instruction-driven object
recognition. InstructSAM leverages large vision-language models to interpret
user instructions and estimate object counts, employs SAM2 for mask proposal,
and formulates mask-label assignment as a binary integer programming problem.
By integrating semantic similarity with counting constraints, InstructSAM
efficiently assigns categories to predicted masks without relying on confidence
thresholds. Experiments demonstrate that InstructSAM matches or surpasses
specialized baselines across multiple tasks while maintaining near-constant
inference time regardless of object count, reducing output tokens by 89% and
overall runtime by over 32% compared to direct generation approaches. We
believe the contributions of the proposed tasks, benchmark, and effective
approach will advance future research in developing versatile object
recognition systems.
</summary>
    <author>
      <name>Yijie Zheng</name>
    </author>
    <author>
      <name>Weijie Wu</name>
    </author>
    <author>
      <name>Qingyun Li</name>
    </author>
    <author>
      <name>Xuehui Wang</name>
    </author>
    <author>
      <name>Xu Zhou</name>
    </author>
    <author>
      <name>Aiai Ren</name>
    </author>
    <author>
      <name>Jun Shen</name>
    </author>
    <author>
      <name>Long Zhao</name>
    </author>
    <author>
      <name>Guoqing Li</name>
    </author>
    <author>
      <name>Xue Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15817v1</id>
    <updated>2025-05-21T17:59:54Z</updated>
    <published>2025-05-21T17:59:54Z</published>
    <title>Learning to Reason via Mixture-of-Thought for Logical Reasoning</title>
    <summary>  Human beings naturally utilize multiple reasoning modalities to learn and
solve logical problems, i.e., different representational formats such as
natural language, code, and symbolic logic. In contrast, most existing
LLM-based approaches operate with a single reasoning modality during training,
typically natural language. Although some methods explored modality selection
or augmentation at inference time, the training process remains modality-blind,
limiting synergy among modalities. To fill in this gap, we propose
Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three
complementary modalities: natural language, code, and a newly introduced
symbolic modality, truth-table, which systematically enumerates logical cases
and partially mitigates key failure modes in natural language reasoning. MoT
adopts a two-phase design: (1) self-evolving MoT training, which jointly learns
from filtered, self-generated rationales across modalities; and (2) MoT
inference, which fully leverages the synergy of three modalities to produce
better predictions. Experiments on logical reasoning benchmarks including FOLIO
and ProofWriter demonstrate that our MoT framework consistently and
significantly outperforms strong LLM baselines with single-modality
chain-of-thought approaches, achieving up to +11.7pp average accuracy gain.
Further analyses show that our MoT framework benefits both training and
inference stages; that it is particularly effective on harder logical reasoning
problems; and that different modalities contribute complementary strengths,
with truth-table reasoning helping to overcome key bottlenecks in natural
language inference.
</summary>
    <author>
      <name>Tong Zheng</name>
    </author>
    <author>
      <name>Lichang Chen</name>
    </author>
    <author>
      <name>Simeng Han</name>
    </author>
    <author>
      <name>R. Thomas McCoy</name>
    </author>
    <author>
      <name>Heng Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.15817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15807v1</id>
    <updated>2025-05-21T17:59:01Z</updated>
    <published>2025-05-21T17:59:01Z</published>
    <title>The Atlas of In-Context Learning: How Attention Heads Shape In-Context
  Retrieval Augmentation</title>
    <summary>  Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.
</summary>
    <author>
      <name>Patrick Kahardipraja</name>
    </author>
    <author>
      <name>Reduan Achtibat</name>
    </author>
    <author>
      <name>Thomas Wiegand</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <author>
      <name>Sebastian Lapuschkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.15807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15781v1</id>
    <updated>2025-05-21T17:32:10Z</updated>
    <published>2025-05-21T17:32:10Z</published>
    <title>dKV-Cache: The Cache for Diffusion Language Models</title>
    <summary>  Diffusion Language Models (DLMs) have been seen as a promising competitor for
autoregressive language models. However, diffusion language models have long
been constrained by slow inference. A core challenge is that their
non-autoregressive architecture and bidirectional attention preclude the
key-value cache that accelerates decoding. We address this bottleneck by
proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising
process of DLMs. Our approach is motivated by the observation that different
tokens have distinct representation dynamics throughout the diffusion process.
Accordingly, we propose a delayed and conditioned caching strategy for key and
value states. We design two complementary variants to cache key and value
step-by-step: (1) dKV-Cache-Decode, which provides almost lossless
acceleration, and even improves performance on long sequences, suggesting that
existing DLMs may under-utilise contextual information during inference. (2)
dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving
higher speed-ups with quadratic time complexity at the cost of some performance
degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,
largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on
several benchmarks, delivering acceleration across general language
understanding, mathematical, and code-generation benchmarks. Experiments
demonstrate that cache can also be used in DLMs, even in a training-free manner
from current DLMs.
</summary>
    <author>
      <name>Xinyin Ma</name>
    </author>
    <author>
      <name>Runpeng Yu</name>
    </author>
    <author>
      <name>Gongfan Fang</name>
    </author>
    <author>
      <name>Xinchao Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The code is available at https://github.com/horseee/dKV-Cache</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.15781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15777v1</id>
    <updated>2025-05-21T17:28:14Z</updated>
    <published>2025-05-21T17:28:14Z</published>
    <title>Projection-Based Correction for Enhancing Deep Inverse Networks</title>
    <summary>  Deep learning-based models have demonstrated remarkable success in solving
illposed inverse problems; however, many fail to strictly adhere to the
physical constraints imposed by the measurement process. In this work, we
introduce a projection-based correction method to enhance the inference of deep
inverse networks by ensuring consistency with the forward model. Specifically,
given an initial estimate from a learned reconstruction network, we apply a
projection step that constrains the solution to lie within the valid solution
space of the inverse problem. We theoretically demonstrate that if the recovery
model is a well-trained deep inverse network, the solution can be decomposed
into range-space and null-space components, where the projection-based
correction reduces to an identity transformation. Extensive simulations and
experiments validate the proposed method, demonstrating improved reconstruction
accuracy across diverse inverse problems and deep network architectures.
</summary>
    <author>
      <name>Jorge Bacca</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15774v1</id>
    <updated>2025-05-21T17:26:11Z</updated>
    <published>2025-05-21T17:26:11Z</published>
    <title>Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and
  Global Information Retention</title>
    <summary>  Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.
</summary>
    <author>
      <name>Huanxuan Liao</name>
    </author>
    <author>
      <name>Wen Hu</name>
    </author>
    <author>
      <name>Yao Xu</name>
    </author>
    <author>
      <name>Shizhu He</name>
    </author>
    <author>
      <name>Jun Zhao</name>
    </author>
    <author>
      <name>Kang Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15766v1</id>
    <updated>2025-05-21T17:13:09Z</updated>
    <published>2025-05-21T17:13:09Z</published>
    <title>Quasar radiation transforms the gas in a merging companion galaxy</title>
    <summary>  Quasars, powered by gas accretion onto supermassive black holes, rank among
the most energetic objects of the Universe. While they are thought to be
ignited by galaxy mergers and affect the surrounding gas, observational
constraints on both processes remain scarce. Here we unveil a major merging
system at redshift $z \approx 2.7$, and demonstrate that radiation from the
quasar in one galaxy directly alters the gas properties in the other galaxy.
Our findings reveal that the galaxies, with centroids separated by only a few
kiloparsecs and approaching each other at speed $\approx550\,$km$\,$s$^{-1}$,
are massive, form stars, and contain a substantial molecular mass. Yet, dusty
molecular gas seen in absorption against the quasar nucleus is highly excited
and confined within cloudlets with densities $\sim 10^5$ - $10^6$ cm$^{-3}$ and
sizes $&lt;$0.02 pc, several orders of magnitude more compact than those observed
in intervening (non-quasar) environments. This is also approximately 10$^5$
times smaller than currently resolvable through molecular-line emission at high
redshifts. We infer that, wherever exposed to the quasar radiation, molecular
gas is disrupted, leaving behind surviving dense clouds too small to give birth
to new stars. Our results not only underscore the role of major galaxy mergers
in triggering quasar activity, but also reveal localized negative feedback as a
profound alteration of internal gas structure which likely hampers star
formation.
</summary>
    <author>
      <name>Sergei Balashev</name>
    </author>
    <author>
      <name>Pasquier Noterdaeme</name>
    </author>
    <author>
      <name>Neeraj Gupta</name>
    </author>
    <author>
      <name>Jens-Kristian Krogager</name>
    </author>
    <author>
      <name>Francoise Combes</name>
    </author>
    <author>
      <name>Sebastian Lopez</name>
    </author>
    <author>
      <name>Patrick Petitjean</name>
    </author>
    <author>
      <name>Alain Omont</name>
    </author>
    <author>
      <name>Raghunathan Srianand</name>
    </author>
    <author>
      <name>Rodrigo Cuellar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41586-025-08966-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41586-025-08966-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 12 figures, published in Nature on May 21th, 2025,
  https://www.nature.com/articles/s41586-025-08966-4</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.15766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15759v1</id>
    <updated>2025-05-21T17:03:34Z</updated>
    <published>2025-05-21T17:03:34Z</published>
    <title>Estimating Associations Between Cumulative Exposure and Health via
  Generalized Distributed Lag Non-Linear Models using Penalized Splines</title>
    <summary>  Quantifying associations between short-term exposure to ambient air pollution
and health outcomes is an important public health priority. Many studies have
investigated the association considering delayed effects within the past few
days. Adaptive cumulative exposure distributed lag non-linear models
(ACE-DLNMs) quantify associations between health outcomes and cumulative
exposure that is specified in a data-adaptive way. While the ACE-DLNM framework
is highly interpretable, it is limited to continuous outcomes and does not
scale well to large datasets. Motivated by a large analysis of daily pollution
and respiratory hospitalization counts in Canada between 2001 and 2018, we
propose a generalized ACE-DLNM incorporating penalized splines, improving upon
existing ACE-DLNM methods to accommodate general response types. We then
develop a computationally efficient estimation strategy based on profile
likelihood and Laplace approximate marginal likelihood with Newton-type
methods. We demonstrate the performance and practical advantages of the
proposed method through simulations. In application to the motivating analysis,
the proposed method yields more stable inferences compared to generalized
additive models with fixed exposures, while retaining interpretability.
</summary>
    <author>
      <name>Tianyi Pan</name>
    </author>
    <author>
      <name>Hwashin Hyun Shin</name>
    </author>
    <author>
      <name>Glen McGee</name>
    </author>
    <author>
      <name>Alex Stringer</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15692v1</id>
    <updated>2025-05-21T16:06:10Z</updated>
    <published>2025-05-21T16:06:10Z</published>
    <title>Thought-Augmented Policy Optimization: Bridging External Guidance and
  Internal Capabilities</title>
    <summary>  Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
("thought patterns"). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.
</summary>
    <author>
      <name>Jinyang Wu</name>
    </author>
    <author>
      <name>Chonghua Liao</name>
    </author>
    <author>
      <name>Mingkuan Feng</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Zhengqi Wen</name>
    </author>
    <author>
      <name>Pengpeng Shao</name>
    </author>
    <author>
      <name>Huazhe Xu</name>
    </author>
    <author>
      <name>Jianhua Tao</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15690v1</id>
    <updated>2025-05-21T16:05:29Z</updated>
    <published>2025-05-21T16:05:29Z</published>
    <title>Toward Open Earth Science as Fast and Accessible as Natural Language</title>
    <summary>  Is natural-language-driven earth observation data analysis now feasible with
the assistance of Large Language Models (LLMs)? For open science in service of
public interest, feasibility requires reliably high accuracy, interactive
latencies, low (sustainable) costs, open LLMs, and openly maintainable software
-- hence, the challenge. What are the techniques and programming system
requirements necessary for satisfying these constraints, and what is the
corresponding development and maintenance burden in practice? This study lays
the groundwork for exploring these questions, introducing an impactful earth
science use-case, and providing a software framework with evaluation data and
metrics, along with initial results from employing model scaling,
prompt-optimization, and inference-time scaling optimization techniques. While
we attain high accuracy (near 100%) across 10 of 11 metrics, the analysis
further considers cost (token-spend), latency, and maintainability across this
space of techniques. Finally, we enumerate opportunities for further research,
general programming and evaluation framework development, and ongoing work for
a comprehensive, deployable solution. This is a call for collaboration and
contribution.
</summary>
    <author>
      <name>Marquita Ellis</name>
    </author>
    <author>
      <name>Iksha Gurung</name>
    </author>
    <author>
      <name>Muthukumaran Ramasubramanian</name>
    </author>
    <author>
      <name>Rahul Ramachandran</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.2; H.5.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
