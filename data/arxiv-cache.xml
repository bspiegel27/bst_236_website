<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-04T00:57:57Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-03T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">113805</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.24873v1</id>
    <updated>2025-05-30T17:59:45Z</updated>
    <published>2025-05-30T17:59:45Z</published>
    <title>MiniMax-Remover: Taming Bad Noise Helps Video Object Removal</title>
    <summary>  Recent advances in video diffusion models have driven rapid progress in video
editing techniques. However, video object removal, a critical subtask of video
editing, remains challenging due to issues such as hallucinated objects and
visual artifacts. Furthermore, existing methods often rely on computationally
expensive sampling procedures and classifier-free guidance (CFG), resulting in
slow inference. To address these limitations, we propose MiniMax-Remover, a
novel two-stage video object removal approach. Motivated by the observation
that text condition is not best suited for this task, we simplify the
pretrained video generation model by removing textual input and cross-attention
layers, resulting in a more lightweight and efficient model architecture in the
first stage. In the second stage, we distilled our remover on successful videos
produced by the stage-1 model and curated by human annotators, using a minimax
optimization strategy to further improve editing quality and inference speed.
Specifically, the inner maximization identifies adversarial input noise ("bad
noise") that makes failure removals, while the outer minimization step trains
the model to generate high-quality removal results even under such challenging
conditions. As a result, our method achieves a state-of-the-art video object
removal results with as few as 6 sampling steps and doesn't rely on CFG,
significantly improving inference efficiency. Extensive experiments demonstrate
the effectiveness and superiority of MiniMax-Remover compared to existing
methods. Codes and Videos are available at: https://minimax-remover.github.io.
</summary>
    <author>
      <name>Bojia Zi</name>
    </author>
    <author>
      <name>Weixuan Peng</name>
    </author>
    <author>
      <name>Xianbiao Qi</name>
    </author>
    <author>
      <name>Jianan Wang</name>
    </author>
    <author>
      <name>Shihao Zhao</name>
    </author>
    <author>
      <name>Rong Xiao</name>
    </author>
    <author>
      <name>Kam-Fai Wong</name>
    </author>
    <link href="http://arxiv.org/abs/2505.24873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24872v1</id>
    <updated>2025-05-30T17:59:43Z</updated>
    <published>2025-05-30T17:59:43Z</published>
    <title>ProxyThinker: Test-Time Guidance through Small Visual Reasoners</title>
    <summary>  Recent advancements in reinforcement learning with verifiable rewards have
pushed the boundaries of the visual reasoning capabilities in large
vision-language models (LVLMs). However, training LVLMs with reinforcement
fine-tuning (RFT) is computationally expensive, posing a significant challenge
to scaling model size. In this work, we propose ProxyThinker, an inference-time
technique that enables large models to inherit the visual reasoning
capabilities from small, slow-thinking visual reasoners without any training.
By subtracting the output distributions of base models from those of RFT
reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits
the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors
such as self-verification and self-correction. ProxyThinker consistently boosts
performance on challenging visual benchmarks on spatial, mathematical, and
multi-disciplinary reasoning, enabling untuned base models to compete with the
performance of their full-scale RFT counterparts. Furthermore, our
implementation efficiently coordinates multiple language models with
parallelism techniques and achieves up to 38 $\times$ faster inference compared
to previous decoding-time methods, paving the way for the practical deployment
of ProxyThinker. Code is available at
https://github.com/MrZilinXiao/ProxyThinker.
</summary>
    <author>
      <name>Zilin Xiao</name>
    </author>
    <author>
      <name>Jaywon Koo</name>
    </author>
    <author>
      <name>Siru Ouyang</name>
    </author>
    <author>
      <name>Jefferson Hernandez</name>
    </author>
    <author>
      <name>Yu Meng</name>
    </author>
    <author>
      <name>Vicente Ordonez</name>
    </author>
    <link href="http://arxiv.org/abs/2505.24872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24869v1</id>
    <updated>2025-05-30T17:59:19Z</updated>
    <published>2025-05-30T17:59:19Z</published>
    <title>SiLVR: A Simple Language-based Video Reasoning Framework</title>
    <summary>  Recent advances in test-time optimization have led to remarkable reasoning
capabilities in Large Language Models (LLMs), enabling them to solve highly
complex problems in math and coding. However, the reasoning capabilities of
multimodal LLMs (MLLMs) still significantly lag, especially for complex
video-language tasks. To address this issue, we present SiLVR, a Simple
Language-based Video Reasoning framework that decomposes complex video
understanding into two stages. In the first stage, SiLVR transforms raw video
into language-based representations using multisensory inputs, such as short
clip captions and audio/speech subtitles. In the second stage, language
descriptions are fed into a powerful reasoning LLM to solve complex
video-language understanding tasks. To handle long-context multisensory inputs,
we use an adaptive token reduction scheme, which dynamically determines the
temporal granularity with which to sample the tokens. Our simple, modular, and
training-free video reasoning framework achieves the best-reported results on
Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.
Furthermore, our empirical study focused on video reasoning capabilities shows
that, despite not being explicitly trained on video, strong reasoning LLMs can
effectively aggregate multisensory input information from video, speech, and
audio for complex temporal, causal, long-context, and knowledge acquisition
reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.
</summary>
    <author>
      <name>Ce Zhang</name>
    </author>
    <author>
      <name>Yan-Bo Lin</name>
    </author>
    <author>
      <name>Ziyang Wang</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <author>
      <name>Gedas Bertasius</name>
    </author>
    <link href="http://arxiv.org/abs/2505.24869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24859v1</id>
    <updated>2025-05-30T17:57:15Z</updated>
    <published>2025-05-30T17:57:15Z</published>
    <title>Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive
  Free-Form Summarization</title>
    <summary>  Steering vectors are a lightweight method for controlling text properties by
adding a learned bias to language model activations at inference time. So far,
steering vectors have predominantly been evaluated in multiple-choice settings,
while their effectiveness in free-form generation tasks remains understudied.
Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of
steering vectors in adaptively controlling topical focus, sentiment, toxicity,
and readability in abstractive summaries of the NEWTS dataset. We find that
steering effectively controls the targeted summary properties, but high
steering strengths consistently degrade both intrinsic and extrinsic text
quality. Compared to steering, prompting offers weaker control, while
preserving text quality. Combining steering and prompting yields the strongest
control over text properties and offers the most favorable efficacy-quality
trade-off at moderate steering strengths. Our results underscore the practical
trade-off between control strength and text quality preservation when applying
steering vectors to free-form generation tasks.
</summary>
    <author>
      <name>Joschka Braun</name>
    </author>
    <author>
      <name>Carsten Eickhoff</name>
    </author>
    <author>
      <name>Seyed Ali Bahrainian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 21 figures, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.24859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24855v1</id>
    <updated>2025-05-30T17:51:29Z</updated>
    <published>2025-05-30T17:51:29Z</published>
    <title>Active Gaussian Network Model: a non-equilibrium description of protein
  fluctuations and allosteric behavior</title>
    <summary>  Understanding the link between structure and function in proteins is
fundamental in molecular biology and proteomics. A central question in this
context is whether allostery - where the binding of a molecule at one site
affects the activity of a distant site - emerges as a further manifestation of
the intricate interplay between structure, function, and intrinsic dynamics.
This study explores how allosteric regulation is modified when intrinsic
protein dynamics operate under out-of-equilibrium conditions. To this purpose,
we introduce a simple nonequilibrium model of protein dynamics, inspired by
active matter systems, by generalizing the widely employed Gaussian Network
Model (GNM) to incorporate non-thermal effects. Our approach underscores the
advantage of framing allostery as a causal process by using, as a benchmark
system, the second PDZ domain of the human phosphatase hPT1E that mediates
protein-protein interactions. We employ causal indicators, such as response
functions and transfer entropy, to identify the network of PDZ2 residues
through which the allosteric signal propagates across the protein structure.
These indicators reveal specific regions that align well with experimental
observations. Furthermore, our results suggest that deviations from purely
thermal fluctuations can significantly influence allosteric communication by
introducing distinct timescales and memory effects. This influence is
particularly relevant when the allosteric response unfolds on timescales
incompatible with relaxation to equilibrium. Accordingly, non-thermal
fluctuations may become essential for accurately describing protein responses
to ligand binding and developing a comprehensive understanding of allosteric
regulation.
</summary>
    <author>
      <name>Giulio Costantini</name>
    </author>
    <author>
      <name>Lorenzo Caprini</name>
    </author>
    <author>
      <name>Umberto Marini Bettolo Marconi</name>
    </author>
    <author>
      <name>Fabio Cecconi</name>
    </author>
    <link href="http://arxiv.org/abs/2505.24855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24852v1</id>
    <updated>2025-05-30T17:49:30Z</updated>
    <published>2025-05-30T17:49:30Z</published>
    <title>Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for
  End-to-End Few-Shot and Continual Learning from Sequential Data</title>
    <summary>  On-device learning at the edge enables low-latency, private personalization
with improved long-term robustness and reduced maintenance costs. Yet,
achieving scalable, low-power end-to-end on-chip learning, especially from
real-world sequential data with a limited number of examples, is an open
challenge. Indeed, accelerators supporting error backpropagation optimize for
learning performance at the expense of inference efficiency, while simplified
learning algorithms often fail to reach acceptable accuracy targets. In this
work, we present Chameleon, leveraging three key contributions to solve these
challenges. (i) A unified learning and inference architecture supports few-shot
learning (FSL), continual learning (CL) and inference at only 0.5% area
overhead to the inference logic. (ii) Long temporal dependencies are
efficiently captured with temporal convolutional networks (TCNs), enabling the
first demonstration of end-to-end on-chip FSL and CL on sequential data and
inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free
compute array allows either matching the power consumption of state-of-the-art
inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$
higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records
on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way
5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),
while maintaining an inference accuracy of 93.3% on the 12-class Google Speech
Commands dataset at an extreme-edge power budget of 3.1 $\mu$W.
</summary>
    <author>
      <name>Douwe den Blanken</name>
    </author>
    <author>
      <name>Charlotte Frenkel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.24852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; B.6.0; B.7.0; I.2.6; B.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24843v1</id>
    <updated>2025-05-30T17:42:32Z</updated>
    <published>2025-05-30T17:42:32Z</published>
    <title>From Invariant Representations to Invariant Data: Provable Robustness to
  Spurious Correlations via Noisy Counterfactual Matching</title>
    <summary>  Spurious correlations can cause model performance to degrade in new
environments. Prior causality-inspired works aim to learn invariant
representations (e.g., IRM) but typically underperform empirical risk
minimization (ERM). Recent alternatives improve robustness by leveraging
test-time data, but such data may be unavailable in practice. To address these
issues, we take a data-centric approach by leveraging invariant data pairs,
pairs of samples that would have the same prediction with the optimally robust
classifier. We prove that certain counterfactual pairs will naturally satisfy
this invariance property and introduce noisy counterfactual matching (NCM), a
simple constraint-based method for leveraging invariant pairs for enhanced
robustness, even with a small set of noisy pairs-in the ideal case, each pair
can eliminate one spurious feature. For linear causal models, we prove that the
test domain error can be upper bounded by the in-domain error and a term that
depends on the counterfactuals' diversity and quality. We validate on a
synthetic dataset and demonstrate on real-world benchmarks that linear probing
on a pretrained backbone improves robustness.
</summary>
    <author>
      <name>Ruqi Bai</name>
    </author>
    <author>
      <name>Yao Ji</name>
    </author>
    <author>
      <name>Zeyu Zhou</name>
    </author>
    <author>
      <name>David I. Inouye</name>
    </author>
    <link href="http://arxiv.org/abs/2505.24843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24832v2</id>
    <updated>2025-06-02T14:13:41Z</updated>
    <published>2025-05-30T17:34:03Z</published>
    <title>How much do language models memorize?</title>
    <summary>  We propose a new method for estimating how much a model ``knows'' about a
datapoint and use it to measure the capacity of modern language models. Prior
studies of language model memorization have struggled to disentangle
memorization from generalization. We formally separate memorization into two
components: \textit{unintended memorization}, the information a model contains
about a specific dataset, and \textit{generalization}, the information a model
contains about the true data-generation process. When we completely eliminate
generalization, we can compute the total memorization, which provides an
estimate of model capacity: our measurements estimate that GPT-style models
have a capacity of approximately 3.6 bits per parameter. We train language
models on datasets of increasing size and observe that models memorize until
their capacity fills, at which point ``grokking'' begins, and unintended
memorization decreases as models begin to generalize. We train hundreds of
transformer language models ranging from $500K$ to $1.5B$ parameters and
produce a series of scaling laws relating model capacity and data size to
membership inference.
</summary>
    <author>
      <name>John X. Morris</name>
    </author>
    <author>
      <name>Chawin Sitawarin</name>
    </author>
    <author>
      <name>Chuan Guo</name>
    </author>
    <author>
      <name>Narine Kokhlikyan</name>
    </author>
    <author>
      <name>G. Edward Suh</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <author>
      <name>Saeed Mahloujifar</name>
    </author>
    <link href="http://arxiv.org/abs/2505.24832v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24832v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24816v1</id>
    <updated>2025-05-30T17:19:52Z</updated>
    <published>2025-05-30T17:19:52Z</published>
    <title>CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free
  Class-Incremental Learning</title>
    <summary>  Class-Incremental Learning (CIL) aims to learn new classes sequentially while
retaining the knowledge of previously learned classes. Recently, pre-trained
models (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown
remarkable performance in rehearsal-free CIL without requiring exemplars from
previous tasks. However, existing adapter-based methods, which incorporate
lightweight learnable modules into PTMs for CIL, create new adapters for each
new task, leading to both parameter redundancy and failure to leverage shared
knowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation
(CL-LoRA), which introduces a novel dual-adapter architecture combining
\textbf{task-shared adapters} to learn cross-task knowledge and
\textbf{task-specific adapters} to capture unique features of each new task.
Specifically, the shared adapters utilize random orthogonal matrices and
leverage knowledge distillation with gradient reassignment to preserve
essential shared knowledge. In addition, we introduce learnable block-wise
weights for task-specific adapters, which mitigate inter-task interference
while maintaining the model's plasticity. We demonstrate CL-LoRA consistently
achieves promising performance under multiple benchmarks with reduced training
and inference computation, establishing a more efficient and scalable paradigm
for continual learning with pre-trained models.
</summary>
    <author>
      <name>Jiangpeng He</name>
    </author>
    <author>
      <name>Zhihao Duan</name>
    </author>
    <author>
      <name>Fengqing Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.24816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24795v1</id>
    <updated>2025-05-30T16:56:47Z</updated>
    <published>2025-05-30T16:56:47Z</published>
    <title>Early warning signals in rumor models</title>
    <summary>  We study the emission and control of a rumor using the modified Maki-Thomson
model. A key challenge in social networks is distinguishing between natural
increases in transmissibility and artificial injections of rumor spreaders,
such as through broadcast events or astroturfing. Using stochastic simulations,
we compare two scenarios: one with organic growth in transmissibility, and
another with externally injected spreaders. Although both lead to high
autocorrelation, only the organic growth produces oscillatory patterns in
autocorrelation at multiple lags, an effect we can analytically explain using
the N-intertwined mean-field (NIMFA) approximation. This distinction offers a
practical tool to identify the origin of rumor virality and also infer its
transmissibility. Our approach is validated analytically and tested on
real-world data from Twitter during the announcement of the Higgs boson
discovery. In addition to detection, we also explore control strategies. We
show that the average lifetime of a rumor can be manipulated through targeted
interventions: placing spreaders at specific locations in the network.
Depending on their placement, these interventions can either extend or shorten
the lifespan of the rumor.
</summary>
    <author>
      <name>Eva Rif√†</name>
    </author>
    <author>
      <name>Julian Vicens</name>
    </author>
    <author>
      <name>Emanuele Cozzo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.24795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
