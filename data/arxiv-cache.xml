<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-30T00:53:39Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">121961</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.22642v1</id>
    <updated>2025-09-26T17:59:07Z</updated>
    <published>2025-09-26T17:59:07Z</published>
    <title>WoW: Towards a World omniscient World model Through Embodied Interaction</title>
    <summary>  Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.
</summary>
    <author>
      <name>Xiaowei Chi</name>
    </author>
    <author>
      <name>Peidong Jia</name>
    </author>
    <author>
      <name>Chun-Kai Fan</name>
    </author>
    <author>
      <name>Xiaozhu Ju</name>
    </author>
    <author>
      <name>Weishi Mi</name>
    </author>
    <author>
      <name>Kevin Zhang</name>
    </author>
    <author>
      <name>Zhiyuan Qin</name>
    </author>
    <author>
      <name>Wanxin Tian</name>
    </author>
    <author>
      <name>Kuangzhi Ge</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Zezhong Qian</name>
    </author>
    <author>
      <name>Anthony Chen</name>
    </author>
    <author>
      <name>Qiang Zhou</name>
    </author>
    <author>
      <name>Yueru Jia</name>
    </author>
    <author>
      <name>Jiaming Liu</name>
    </author>
    <author>
      <name>Yong Dai</name>
    </author>
    <author>
      <name>Qingpo Wuwu</name>
    </author>
    <author>
      <name>Chengyu Bai</name>
    </author>
    <author>
      <name>Yu-Kai Wang</name>
    </author>
    <author>
      <name>Ying Li</name>
    </author>
    <author>
      <name>Lizhang Chen</name>
    </author>
    <author>
      <name>Yong Bao</name>
    </author>
    <author>
      <name>Zhiyuan Jiang</name>
    </author>
    <author>
      <name>Jiacheng Zhu</name>
    </author>
    <author>
      <name>Kai Tang</name>
    </author>
    <author>
      <name>Ruichuan An</name>
    </author>
    <author>
      <name>Yulin Luo</name>
    </author>
    <author>
      <name>Qiuxuan Feng</name>
    </author>
    <author>
      <name>Siyuan Zhou</name>
    </author>
    <author>
      <name>Chi-min Chan</name>
    </author>
    <author>
      <name>Chengkai Hou</name>
    </author>
    <author>
      <name>Wei Xue</name>
    </author>
    <author>
      <name>Sirui Han</name>
    </author>
    <author>
      <name>Yike Guo</name>
    </author>
    <author>
      <name>Shanghang Zhang</name>
    </author>
    <author>
      <name>Jian Tang</name>
    </author>
    <link href="http://arxiv.org/abs/2509.22642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22637v1</id>
    <updated>2025-09-26T17:58:10Z</updated>
    <published>2025-09-26T17:58:10Z</published>
    <title>Variational Reasoning for Language Models</title>
    <summary>  We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.
</summary>
    <author>
      <name>Xiangxin Zhou</name>
    </author>
    <author>
      <name>Zichen Liu</name>
    </author>
    <author>
      <name>Haonan Wang</name>
    </author>
    <author>
      <name>Chao Du</name>
    </author>
    <author>
      <name>Min Lin</name>
    </author>
    <author>
      <name>Chongxuan Li</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <author>
      <name>Tianyu Pang</name>
    </author>
    <link href="http://arxiv.org/abs/2509.22637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22636v1</id>
    <updated>2025-09-26T17:58:04Z</updated>
    <published>2025-09-26T17:58:04Z</published>
    <title>Scale-Wise VAR is Secretly Discrete Diffusion</title>
    <summary>  Autoregressive (AR) transformers have emerged as a powerful paradigm for
visual generation, largely due to their scalability, computational efficiency
and unified architecture with language and vision. Among them, next scale
prediction Visual Autoregressive Generation (VAR) has recently demonstrated
remarkable performance, even surpassing diffusion-based models. In this work,
we revisit VAR and uncover a theoretical insight: when equipped with a
Markovian attention mask, VAR is mathematically equivalent to a discrete
diffusion. We term this reinterpretation as Scalable Visual Refinement with
Discrete Diffusion (SRDD), establishing a principled bridge between AR
transformers and diffusion models. Leveraging this new perspective, we show how
one can directly import the advantages of diffusion such as iterative
refinement and reduce architectural inefficiencies into VAR, yielding faster
convergence, lower inference cost, and improved zero-shot reconstruction.
Across multiple datasets, we show that the diffusion based perspective of VAR
leads to consistent gains in efficiency and generation.
</summary>
    <author>
      <name>Amandeep Kumar</name>
    </author>
    <author>
      <name>Nithin Gopalakrishnan Nair</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Reports</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.22636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22622v1</id>
    <updated>2025-09-26T17:48:24Z</updated>
    <published>2025-09-26T17:48:24Z</published>
    <title>LongLive: Real-time Interactive Long Video Generation</title>
    <summary>  We present LongLive, a frame-level autoregressive (AR) framework for
real-time and interactive long video generation. Long video generation presents
challenges in both efficiency and quality. Diffusion and Diffusion-Forcing
models can produce high-quality videos but suffer from low efficiency due to
bidirectional attention. Causal attention AR models support KV caching for
faster inference, but often degrade in quality on long videos due to memory
challenges during long-video training. In addition, beyond static prompt-based
generation, interactive capabilities, such as streaming prompt inputs, are
critical for dynamic content creation, enabling users to guide narratives in
real time. This interactive requirement significantly increases complexity,
especially in ensuring visual consistency and semantic coherence during prompt
transitions. To address these challenges, LongLive adopts a causal, frame-level
AR design that integrates a KV-recache mechanism that refreshes cached states
with new prompts for smooth, adherent switches; streaming long tuning to enable
long video training and to align training and inference (train-long-test-long);
and short window attention paired with a frame-level attention sink, shorten as
frame sink, preserving long-range consistency while enabling faster generation.
With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model
to minute-long generation in just 32 GPU-days. At inference, LongLive sustains
20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both
short and long videos. LongLive supports up to 240-second videos on a single
H100 GPU. LongLive further supports INT8-quantized inference with only marginal
quality loss.
</summary>
    <author>
      <name>Shuai Yang</name>
    </author>
    <author>
      <name>Wei Huang</name>
    </author>
    <author>
      <name>Ruihang Chu</name>
    </author>
    <author>
      <name>Yicheng Xiao</name>
    </author>
    <author>
      <name>Yuyang Zhao</name>
    </author>
    <author>
      <name>Xianbang Wang</name>
    </author>
    <author>
      <name>Muyang Li</name>
    </author>
    <author>
      <name>Enze Xie</name>
    </author>
    <author>
      <name>Yingcong Chen</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Yukang Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code, model, and demos are available at
  https://github.com/NVlabs/LongLive</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.22622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22621v1</id>
    <updated>2025-09-26T17:46:32Z</updated>
    <published>2025-09-26T17:46:32Z</published>
    <title>IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning</title>
    <summary>  Supervised Fine-Tuning (SFT) is used to specialize model behavior by training
weights to produce intended target responses for queries. In contrast,
In-Context Learning (ICL) adapts models during inference with instructions or
demonstrations in the prompt. ICL can offer better generalizability and more
calibrated responses compared to SFT in data scarce settings, at the cost of
more inference compute. In this work, we ask the question: Can ICL's internal
computations be used to improve the qualities of SFT? We first show that ICL
and SFT produce distinct activation patterns, indicating that the two methods
achieve adaptation through different functional mechanisms. Motivated by this
observation and to use ICL's rich functionality, we introduce ICL Activation
Alignment (IA2), a self-distillation technique which aims to replicate ICL's
activation patterns in SFT models and incentivizes ICL-like internal reasoning.
Performing IA2 as a priming step before SFT significantly improves the accuracy
and calibration of model outputs, as shown by our extensive empirical results
on 12 popular benchmarks and 2 model families. This finding is not only
practically useful, but also offers a conceptual window into the inner
mechanics of model adaptation.
</summary>
    <author>
      <name>Aayush Mishra</name>
    </author>
    <author>
      <name>Daniel Khashabi</name>
    </author>
    <author>
      <name>Anqi Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2509.22621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22597v1</id>
    <updated>2025-09-26T17:17:14Z</updated>
    <published>2025-09-26T17:17:14Z</published>
    <title>A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse
  Problem</title>
    <summary>  The stochastic inverse problem is a key ingredient in making inferences,
predictions, and decisions for complex science and engineering systems. We
formulate and analyze a nonparametric Bayesian solution for the stochastic
inverse problem. Key properties of the solution are proved and the convergence
and error of a computational solution obtained by random sampling is analyzed.
Several applications illustrate the results.
</summary>
    <author>
      <name>Haiyi Shi</name>
    </author>
    <author>
      <name>Lei Yang</name>
    </author>
    <author>
      <name>Jiarui Chi</name>
    </author>
    <author>
      <name>Troy Butler</name>
    </author>
    <author>
      <name>Haonan Wang</name>
    </author>
    <author>
      <name>Derek Bingham</name>
    </author>
    <author>
      <name>Don Estep</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.22597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62G05, 65C60 Secondary 62P30, 62P35, 60D05, 60A10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22592v1</id>
    <updated>2025-09-26T17:12:19Z</updated>
    <published>2025-09-26T17:12:19Z</published>
    <title>Transport Based Mean Flows for Generative Modeling</title>
    <summary>  Flow-matching generative models have emerged as a powerful paradigm for
continuous data generation, achieving state-of-the-art results across domains
such as images, 3D shapes, and point clouds. Despite their success, these
models suffer from slow inference due to the requirement of numerous sequential
sampling steps. Recent work has sought to accelerate inference by reducing the
number of sampling steps. In particular, Mean Flows offer a one-step generation
approach that delivers substantial speedups while retaining strong generative
performance. Yet, in many continuous domains, Mean Flows fail to faithfully
approximate the behavior of the original multi-step flow-matching process. In
this work, we address this limitation by incorporating optimal transport-based
sampling strategies into the Mean Flow framework, enabling one-step generators
that better preserve the fidelity and diversity of the original multi-step flow
process. Experiments on controlled low-dimensional settings and on
high-dimensional tasks such as image generation, image-to-image translation,
and point cloud generation demonstrate that our approach achieves superior
inference accuracy in one-step generative modeling.
</summary>
    <author>
      <name>Elaheh Akbari</name>
    </author>
    <author>
      <name>Ping He</name>
    </author>
    <author>
      <name>Ahmadreza Moradipari</name>
    </author>
    <author>
      <name>Yikun Bai</name>
    </author>
    <author>
      <name>Soheil Kolouri</name>
    </author>
    <link href="http://arxiv.org/abs/2509.22592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22572v1</id>
    <updated>2025-09-26T16:49:10Z</updated>
    <published>2025-09-26T16:49:10Z</published>
    <title>Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs
  at Test Time</title>
    <summary>  Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.
</summary>
    <author>
      <name>Yixuan Han</name>
    </author>
    <author>
      <name>Fan Ma</name>
    </author>
    <author>
      <name>Ruijie Quan</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2509.22572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22561v1</id>
    <updated>2025-09-26T16:41:11Z</updated>
    <published>2025-09-26T16:41:11Z</published>
    <title>Likelihood-free inference for gravitational-wave data analysis and
  public alerts</title>
    <summary>  Rapid and reliable detection and dissemination of source parameter estimation
data products from gravitational-wave events, especially sky localization, is
critical for maximizing the potential of multi-messenger astronomy. Machine
learning based detection and parameter estimation algorithms are emerging as
production ready alternatives to traditional approaches. Here, we report
validation studies of AMPLFI, a likelihood-free inference solution to
low-latency parameter estimation of binary black holes. We use simulated
signals added into data from the LIGO-Virgo-KAGRA's (LVK's) third observing run
(O3) to compare sky localization performance with BAYESTAR, the algorithm
currently in production for rapid sky localization of candidates from
matched-filter pipelines. We demonstrate sky localization performance, measured
by searched area and volume, to be equivalent with BAYESTAR. We show accurate
reconstruction of source parameters with uncertainties for use distributing
low-latency coarse-grained chirp mass information. In addition, we analyze
several candidate events reported by the LVK in the third gravitational-wave
transient catalog (GWTC-3) and show consistency with the LVK's analysis.
Altogether, we demonstrate AMPLFI's ability to produce data products for
low-latency public alerts.
</summary>
    <author>
      <name>Ethan Marx</name>
    </author>
    <author>
      <name>Deep Chatterjee</name>
    </author>
    <author>
      <name>Malina Desai</name>
    </author>
    <author>
      <name>Ravi Kumar</name>
    </author>
    <author>
      <name>William Benoit</name>
    </author>
    <author>
      <name>Argyro Sasli</name>
    </author>
    <author>
      <name>Leo Singer</name>
    </author>
    <author>
      <name>Michael W. Coughlin</name>
    </author>
    <author>
      <name>Philip Harris</name>
    </author>
    <author>
      <name>Erik Katsavounidis</name>
    </author>
    <link href="http://arxiv.org/abs/2509.22561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.22558v1</id>
    <updated>2025-09-26T16:39:10Z</updated>
    <published>2025-09-26T16:39:10Z</published>
    <title>StepORLM: A Self-Evolving Framework With Generative Process Supervision
  For Operations Research Language Models</title>
    <summary>  Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.
</summary>
    <author>
      <name>Chenyu Zhou</name>
    </author>
    <author>
      <name>Tianyi Xu</name>
    </author>
    <author>
      <name>Jianghao Lin</name>
    </author>
    <author>
      <name>Dongdong Ge</name>
    </author>
    <link href="http://arxiv.org/abs/2509.22558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.22558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
