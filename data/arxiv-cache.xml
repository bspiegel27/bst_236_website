<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-14T01:04:50Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-14T01:04:50Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>128470</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.10959v1</id>
    <title>StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</title>
    <updated>2025-12-11T18:59:59Z</updated>
    <link href="https://arxiv.org/abs/2512.10959v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10959v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp &amp; inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:59:59Z</published>
    <arxiv:comment>Project page: https://hf.co/spaces/prs-eth/stereospace_web</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tjark Behrens</name>
    </author>
    <author>
      <name>Anton Obukhov</name>
    </author>
    <author>
      <name>Bingxin Ke</name>
    </author>
    <author>
      <name>Fabio Tosi</name>
    </author>
    <author>
      <name>Matteo Poggi</name>
    </author>
    <author>
      <name>Konrad Schindler</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10953v1</id>
    <title>Bidirectional Normalizing Flow: From Data to Noise and Back</title>
    <updated>2025-12-11T18:59:55Z</updated>
    <link href="https://arxiv.org/abs/2512.10953v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10953v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:59:55Z</published>
    <arxiv:comment>Tech report</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yiyang Lu</name>
    </author>
    <author>
      <name>Qiao Sun</name>
    </author>
    <author>
      <name>Xianbang Wang</name>
    </author>
    <author>
      <name>Zhicheng Jiang</name>
    </author>
    <author>
      <name>Hanhong Zhao</name>
    </author>
    <author>
      <name>Kaiming He</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10954v1</id>
    <title>Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration</title>
    <updated>2025-12-11T18:59:55Z</updated>
    <link href="https://arxiv.org/abs/2512.10954v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10954v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:59:55Z</published>
    <arxiv:comment>Project Page: https://sichengmo.github.io/GroupDiff/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Sicheng Mo</name>
    </author>
    <author>
      <name>Thao Nguyen</name>
    </author>
    <author>
      <name>Richard Zhang</name>
    </author>
    <author>
      <name>Nick Kolkin</name>
    </author>
    <author>
      <name>Siddharth Srinivasan Iyer</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Krishna Kumar Singh</name>
    </author>
    <author>
      <name>Yong Jae Lee</name>
    </author>
    <author>
      <name>Bolei Zhou</name>
    </author>
    <author>
      <name>Yuheng Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10950v1</id>
    <title>E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</title>
    <updated>2025-12-11T18:59:53Z</updated>
    <link href="https://arxiv.org/abs/2512.10950v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10950v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:59:53Z</published>
    <arxiv:comment>Project website: https://qitaozhao.github.io/E-RayZer</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Qitao Zhao</name>
    </author>
    <author>
      <name>Hao Tan</name>
    </author>
    <author>
      <name>Qianqian Wang</name>
    </author>
    <author>
      <name>Sai Bi</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Kalyan Sunkavalli</name>
    </author>
    <author>
      <name>Shubham Tulsiani</name>
    </author>
    <author>
      <name>Hanwen Jiang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10947v1</id>
    <title>Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving</title>
    <updated>2025-12-11T18:59:46Z</updated>
    <link href="https://arxiv.org/abs/2512.10947v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10947v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:59:46Z</published>
    <arxiv:comment>Project Page: https://jiawei-yang.github.io/Flex/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jiawei Yang</name>
    </author>
    <author>
      <name>Ziyu Chen</name>
    </author>
    <author>
      <name>Yurong You</name>
    </author>
    <author>
      <name>Yan Wang</name>
    </author>
    <author>
      <name>Yiming Li</name>
    </author>
    <author>
      <name>Yuxiao Chen</name>
    </author>
    <author>
      <name>Boyi Li</name>
    </author>
    <author>
      <name>Boris Ivanovic</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10946v1</id>
    <title>ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning</title>
    <updated>2025-12-11T18:59:46Z</updated>
    <link href="https://arxiv.org/abs/2512.10946v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10946v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:59:46Z</published>
    <arxiv:comment>Project page: https://implicit-rdp.github.io</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Wendi Chen</name>
    </author>
    <author>
      <name>Han Xue</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <author>
      <name>Fangyuan Zhou</name>
    </author>
    <author>
      <name>Jun Lv</name>
    </author>
    <author>
      <name>Yang Jin</name>
    </author>
    <author>
      <name>Shirun Tang</name>
    </author>
    <author>
      <name>Chuan Wen</name>
    </author>
    <author>
      <name>Cewu Lu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10942v1</id>
    <title>VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</title>
    <updated>2025-12-11T18:59:22Z</updated>
    <link href="https://arxiv.org/abs/2512.10942v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10942v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:59:22Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Delong Chen</name>
    </author>
    <author>
      <name>Mustafa Shukor</name>
    </author>
    <author>
      <name>Theo Moutakanni</name>
    </author>
    <author>
      <name>Willy Chung</name>
    </author>
    <author>
      <name>Jade Yu</name>
    </author>
    <author>
      <name>Tejaswi Kasarla</name>
    </author>
    <author>
      <name>Allen Bolourchi</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Pascale Fung</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10937v1</id>
    <title>On Decision-Making Agents and Higher-Order Causal Processes</title>
    <updated>2025-12-11T18:58:33Z</updated>
    <link href="https://arxiv.org/abs/2512.10937v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10937v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:58:33Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Matt Wilson</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10927v1</id>
    <title>FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos</title>
    <updated>2025-12-11T18:53:15Z</updated>
    <link href="https://arxiv.org/abs/2512.10927v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10927v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:53:15Z</published>
    <arxiv:comment>Code is available at https://github.com/Wolfv0/FoundationMotion/tree/main</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yulu Gan</name>
    </author>
    <author>
      <name>Ligeng Zhu</name>
    </author>
    <author>
      <name>Dandan Shan</name>
    </author>
    <author>
      <name>Baifeng Shi</name>
    </author>
    <author>
      <name>Hongxu Yin</name>
    </author>
    <author>
      <name>Boris Ivanovic</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <author>
      <name>Boyi Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.10912v1</id>
    <title>Imprint of the black hole interior on thermal four-point correlators</title>
    <updated>2025-12-11T18:40:27Z</updated>
    <link href="https://arxiv.org/abs/2512.10912v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.10912v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We consider correlators smeared against directed wavepackets over a thermal state dual to a single-sided planar AdS black hole. In the large frequency limit, our measurement is simplified using a bulk WKB description. We propose a dictionary that maps the action of smeared boundary operators to flat-space oscillators near an interior bulk point on the thermal state, by analytically continuing late-time operators from the right to the left boundary via an integral transform. Using the dictionary the smeared correlator factorizes to a flat-space like scattering amplitude about the interior event. Our transformed correlators describe local physics in the two-sided black hole interior, while incurring a suppression of $\mathcal{O}(e^{-βω/ 2})$. These measurements necessitate a non-trivial time ordering of operators living on boundary hyperboloids which are causally connected to the past light cone of the bulk point, as well as on a corresponding future branch.</summary>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-11T18:40:27Z</published>
    <arxiv:primary_category term="hep-th"/>
    <author>
      <name>Joydeep Chakravarty</name>
    </author>
  </entry>
</feed>
