<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-11T00:53:52Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-10T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">110376</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.07084v1</id>
    <updated>2025-04-09T17:56:58Z</updated>
    <published>2025-04-09T17:56:58Z</published>
    <title>A geometric ensemble method for Bayesian inference</title>
    <summary>  Conventional approximations to Bayesian inference rely on either
approximations by statistics such as mean and covariance or by point particles.
Recent advances such as the ensemble Gaussian mixture filter have generalized
these notions to sums of parameterized distributions. This work presents a new
methodology for approximating Bayesian inference by sums of uniform
distributions on convex polytopes. The methodology presented herein is
developed from the simplest convex polytope filter that takes advantage of
uniform prior and measurement uncertainty, to an operationally viable ensemble
filter with Kalmanized approximations to updating convex polytopes. Numerical
results on the Ikeda map show the viability of this methodology in the
low-dimensional setting, and numerical results on the Lorenz '96 equations
similarly show viability in the high-dimensional setting.
</summary>
    <author>
      <name>Andrey A Popov</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G25, 62L12, 62M20, 93E11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07081v1</id>
    <updated>2025-04-09T17:54:22Z</updated>
    <published>2025-04-09T17:54:22Z</published>
    <title>Self-Steering Language Models</title>
    <summary>  While test-time reasoning enables language models to tackle complex tasks,
searching or planning in natural language can be slow, costly, and error-prone.
But even when LMs struggle to emulate the precise reasoning steps needed to
solve a problem, they often excel at describing its abstract structure--both
how to verify solutions and how to search for them. This paper introduces
DisCIPL, a method for "self-steering" LMs where a Planner model generates a
task-specific inference program that is executed by a population of Follower
models. Our approach equips LMs with the ability to write recursive search
procedures that guide LM inference, enabling new forms of verifiable and
efficient reasoning. When instantiated with a small Follower (e.g.,
Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,
including GPT-4o and o1, on challenging constrained generation tasks. In
decoupling planning from execution, our work opens up a design space of
highly-parallelized Monte Carlo inference strategies that outperform standard
best-of-N sampling, require no finetuning, and can be implemented automatically
by existing LMs.
</summary>
    <author>
      <name>Gabriel Grand</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <author>
      <name>Vikash K. Mansinghka</name>
    </author>
    <author>
      <name>Alexander K. Lew</name>
    </author>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07080v1</id>
    <updated>2025-04-09T17:53:55Z</updated>
    <published>2025-04-09T17:53:55Z</published>
    <title>DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning</title>
    <summary>  Despite great performance on Olympiad-level reasoning problems, frontier
large language models can still struggle on high school math when presented
with novel problems outside standard benchmarks. Going beyond final accuracy,
we propose a deductive consistency metric to analyze chain-of-thought output
from language models (LMs).Formally, deductive reasoning involves two subtasks:
understanding a set of input premises and inferring the conclusions that follow
from them. The proposed metric studies LMs' performance on these subtasks, with
the goal of explaining LMs' reasoning errors on novel problems: how well do LMs
understand input premises with increasing context lengths, and how well can
they infer conclusions over multiple reasoning hops? Since existing benchmarks
may be memorized, we develop a pipeline to evaluate LMs' deductive consistency
on novel, perturbed versions of benchmark problems. On novel grade school math
problems (GSM-8k), we find that LMs are fairly robust to increasing number of
input premises, but suffer significant accuracy decay as the number of
reasoning hops is increased. Interestingly, these errors are masked in the
original benchmark as all models achieve near 100% accuracy. As we increase the
number of solution steps using a synthetic dataset, prediction over multiple
hops still remains the major source of error compared to understanding input
premises. Other factors, such as shifts in language style or natural
propagation of early errors do not explain the trends. Our analysis provides a
new view to characterize LM reasoning -- as computations over a window of input
premises and reasoning hops -- that can provide unified evaluation across
problem domains.
</summary>
    <author>
      <name>Atharva Pandey</name>
    </author>
    <author>
      <name>Kshitij Dubey</name>
    </author>
    <author>
      <name>Rahul Sharma</name>
    </author>
    <author>
      <name>Amit Sharma</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07070v1</id>
    <updated>2025-04-09T17:39:58Z</updated>
    <published>2025-04-09T17:39:58Z</published>
    <title>A Survey on Personalized and Pluralistic Preference Alignment in Large
  Language Models</title>
    <summary>  Personalized preference alignment for large language models (LLMs), the
process of tailoring LLMs to individual users' preferences, is an emerging
research direction spanning the area of NLP and personalization. In this
survey, we present an analysis of works on personalized alignment and modeling
for LLMs. We introduce a taxonomy of preference alignment techniques, including
training time, inference time, and additionally, user-modeling based methods.
We provide analysis and discussion on the strengths and limitations of each
group of techniques and then cover evaluation, benchmarks, as well as open
problems in the field.
</summary>
    <author>
      <name>Zhouhang Xie</name>
    </author>
    <author>
      <name>Junda Wu</name>
    </author>
    <author>
      <name>Yiran Shen</name>
    </author>
    <author>
      <name>Yu Xia</name>
    </author>
    <author>
      <name>Xintong Li</name>
    </author>
    <author>
      <name>Aaron Chang</name>
    </author>
    <author>
      <name>Ryan Rossi</name>
    </author>
    <author>
      <name>Sachin Kumar</name>
    </author>
    <author>
      <name>Bodhisattwa Prasad Majumder</name>
    </author>
    <author>
      <name>Jingbo Shang</name>
    </author>
    <author>
      <name>Prithviraj Ammanabrolu</name>
    </author>
    <author>
      <name>Julian McAuley</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07069v1</id>
    <updated>2025-04-09T17:39:41Z</updated>
    <published>2025-04-09T17:39:41Z</published>
    <title>HalluciNot: Hallucination Detection Through Context and Common Knowledge
  Verification</title>
    <summary>  This paper introduces a comprehensive system for detecting hallucinations in
large language model (LLM) outputs in enterprise settings. We present a novel
taxonomy of LLM responses specific to hallucination in enterprise applications,
categorizing them into context-based, common knowledge, enterprise-specific,
and innocuous statements. Our hallucination detection model HDM-2 validates LLM
responses with respect to both context and generally known facts (common
knowledge). It provides both hallucination scores and word-level annotations,
enabling precise identification of problematic content. To evaluate it on
context-based and common-knowledge hallucinations, we introduce a new dataset
HDMBench. Experimental results demonstrate that HDM-2 out-performs existing
approaches across RagTruth, TruthfulQA, and HDMBench datasets. This work
addresses the specific challenges of enterprise deployment, including
computational efficiency, domain specialization, and fine-grained error
identification. Our evaluation dataset, model weights, and inference code are
publicly available.
</summary>
    <author>
      <name>Bibek Paudel</name>
    </author>
    <author>
      <name>Alexander Lyzhov</name>
    </author>
    <author>
      <name>Preetam Joshi</name>
    </author>
    <author>
      <name>Puneet Anand</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07055v1</id>
    <updated>2025-04-09T17:16:23Z</updated>
    <published>2025-04-09T17:16:23Z</published>
    <title>$Π$-NeSy: A Possibilistic Neuro-Symbolic Approach</title>
    <summary>  In this article, we introduce a neuro-symbolic approach that combines a
low-level perception task performed by a neural network with a high-level
reasoning task performed by a possibilistic rule-based system. The goal is to
be able to derive for each input instance the degree of possibility that it
belongs to a target (meta-)concept. This (meta-)concept is connected to
intermediate concepts by a possibilistic rule-based system. The probability of
each intermediate concept for the input instance is inferred using a neural
network. The connection between the low-level perception task and the
high-level reasoning task lies in the transformation of neural network outputs
modeled by probability distributions (through softmax activation) into
possibility distributions. The use of intermediate concepts is valuable for the
explanation purpose: using the rule-based system, the classification of an
input instance as an element of the (meta-)concept can be justified by the fact
that intermediate concepts have been recognized.
  From the technical side, our contribution consists of the design of efficient
methods for defining the matrix relation and the equation system associated
with a possibilistic rule-based system. The corresponding matrix and equation
are key data structures used to perform inferences from a possibilistic
rule-based system and to learn the values of the rule parameters in such a
system according to a training data sample. Furthermore, leveraging recent
results on the handling of inconsistent systems of fuzzy relational equations,
an approach for learning rule parameters according to multiple training data
samples is presented. Experiments carried out on the MNIST addition problems
and the MNIST Sudoku puzzles problems highlight the effectiveness of our
approach compared with state-of-the-art neuro-symbolic ones.
</summary>
    <author>
      <name>Ismaïl Baaj</name>
    </author>
    <author>
      <name>Pierre Marquis</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07029v1</id>
    <updated>2025-04-09T16:44:19Z</updated>
    <published>2025-04-09T16:44:19Z</published>
    <title>Distilling Textual Priors from LLM to Efficient Image Fusion</title>
    <summary>  Multi-modality image fusion aims to synthesize a single, comprehensive image
from multiple source inputs. Traditional approaches, such as CNNs and GANs,
offer efficiency but struggle to handle low-quality or complex inputs. Recent
advances in text-guided methods leverage large model priors to overcome these
limitations, but at the cost of significant computational overhead, both in
memory and inference time. To address this challenge, we propose a novel
framework for distilling large model priors, eliminating the need for text
guidance during inference while dramatically reducing model size. Our framework
utilizes a teacher-student architecture, where the teacher network incorporates
large model priors and transfers this knowledge to a smaller student network
via a tailored distillation process. Additionally, we introduce spatial-channel
cross-fusion module to enhance the model's ability to leverage textual priors
across both spatial and channel dimensions. Our method achieves a favorable
trade-off between computational efficiency and fusion quality. The distilled
network, requiring only 10\% of the parameters and inference time of the
teacher network, retains 90\% of its performance and outperforms existing SOTA
methods. Extensive experiments demonstrate the effectiveness of our approach.
The implementation will be made publicly available as an open-source resource.
</summary>
    <author>
      <name>Ran Zhang</name>
    </author>
    <author>
      <name>Xuanhua He</name>
    </author>
    <author>
      <name>Ke Cao</name>
    </author>
    <author>
      <name>Liu Liu</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Man Zhou</name>
    </author>
    <author>
      <name>Jie Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06982v1</id>
    <updated>2025-04-09T15:38:18Z</updated>
    <published>2025-04-09T15:38:18Z</published>
    <title>SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets</title>
    <summary>  3D human digitization has long been a highly pursued yet challenging task.
Existing methods aim to generate high-quality 3D digital humans from single or
multiple views, but remain primarily constrained by current paradigms and the
scarcity of 3D human assets. Specifically, recent approaches fall into several
paradigms: optimization-based and feed-forward (both single-view regression and
multi-view generation with reconstruction). However, they are limited by slow
speed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional
planes to high-dimensional space due to occlusion and invisibility,
respectively. Furthermore, existing 3D human assets remain small-scale,
insufficient for large-scale training. To address these challenges, we propose
a latent space generation paradigm for 3D human digitization, which involves
compressing multi-view images into Gaussians via a UV-structured VAE, along
with DiT-based conditional generation, we transform the ill-posed
low-to-high-dimensional mapping problem into a learnable distribution shift,
which also supports end-to-end inference. In addition, we employ the multi-view
optimization approach combined with synthetic data to construct the HGS-1M
dataset, which contains $1$ million 3D Gaussian assets to support the
large-scale training. Experimental results demonstrate that our paradigm,
powered by large-scale training, produces high-quality 3D human Gaussians with
intricate textures, facial details, and loose clothing deformation.
</summary>
    <author>
      <name>Yuhang Yang</name>
    </author>
    <author>
      <name>Fengqi Liu</name>
    </author>
    <author>
      <name>Yixing Lu</name>
    </author>
    <author>
      <name>Qin Zhao</name>
    </author>
    <author>
      <name>Pingyu Wu</name>
    </author>
    <author>
      <name>Wei Zhai</name>
    </author>
    <author>
      <name>Ran Yi</name>
    </author>
    <author>
      <name>Yang Cao</name>
    </author>
    <author>
      <name>Lizhuang Ma</name>
    </author>
    <author>
      <name>Zheng-Jun Zha</name>
    </author>
    <author>
      <name>Junting Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">project page:https://yyvhang.github.io/SIGMAN_3D/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06975v1</id>
    <updated>2025-04-09T15:30:09Z</updated>
    <published>2025-04-09T15:30:09Z</published>
    <title>AWDIT: An Optimal Weak Database Isolation Tester</title>
    <summary>  In order to achieve low latency, high throughput, and partition tolerance,
modern databases forgo strong transaction isolation for weak isolation
guarantees. However, several production databases have been found to suffer
from isolation bugs, breaking their data-consistency contract. Black-box
testing is a prominent technique for detecting isolation bugs, by checking
whether histories of database transactions adhere to a prescribed isolation
level.
  Testing databases on realistic workloads of large size requires isolation
testers to be as efficient as possible, a requirement that has initiated a
study of the complexity of isolation testing. Although testing strong isolation
has been known to be NP-complete, weak isolation levels were recently shown to
be testable in polynomial time, which has propelled the scalability of testing
tools. However, existing testers have a large polynomial complexity,
restricting testing to workloads of only moderate size, which is not typical of
large-scale databases.
  In this work, we develop AWDIT, a highly-efficient and provably optimal
tester for weak database isolation. Given a history $H$ of size $n$ and $k$
sessions, AWDIT tests whether H satisfies the most common weak isolation levels
of Read Committed (RC), Read Atomic (RA), and Causal Consistency (CC) in time
$O(n^{3/2})$, $O(n^{3/2})$, and $O(n \cdot k)$, respectively, improving
significantly over the state of the art. Moreover, we prove that AWDIT is
essentially optimal, in the sense that there is a conditional lower bound of
$n^{3/2}$ for any weak isolation level between RC and CC. Our experiments show
that AWDIT is significantly faster than existing, highly optimized testers;
e.g., for the $\sim$20% largest histories, AWDIT obtains an average speedup of
$245\times$, $193\times$, and $62\times$ for RC, RA, and CC, respectively, over
the best baseline.
</summary>
    <author>
      <name>Lasse Møldrup</name>
    </author>
    <author>
      <name>Andreas Pavlogiannis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3729339</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3729339" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 9 figures, 1 table; accepted for PLDI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; D.2.5; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06973v1</id>
    <updated>2025-04-09T15:28:36Z</updated>
    <published>2025-04-09T15:28:36Z</published>
    <title>HIP 15429: a newborn Be star on an eccentric binary orbit</title>
    <summary>  We identified a new post-interaction binary, HIP 15429, consisting of a
stripped star and a recently formed, rapidly rotating Be star companion ($v
\sin i \approx 270$ km/s) sharing many similarities with recently identified
bloated stripped stars. From orbital fitting of multi-epoch radial velocities
we find a 221-day period. We also find an eccentricity of $e=0.52$, which is
unexpectedly high as tides are expected to have circularised the orbit
efficiently during the presumed recent mass transfer. The formation of a
circumbinary disk during the mass transfer phase or the presence of an unseen
tertiary companion might explain the orbit's high eccentricity. We determined
physical parameters for both stars by fitting the spectra of the disentangled
binary components and multi-band photometry. The stripped nature of the donor
star is affirmed by its high luminosity at a low inferred mass ($\lesssim 1
\mathrm{M}_\odot$) and imprints of CNO-processed material in the surface
abundances. The donor's relatively large radius and cool temperature
($T_{\mathrm{eff}} = 13.5 \pm 0.5$ kK) suggest that it has only recently ceased
mass transfer. Evolutionary models assuming a 5-6 $\mathrm{M}_\odot$ progenitor
can reproduce these parameters and imply that the binary is currently evolving
towards a stage where the donor becomes a subdwarf orbiting a Be star. The
remarkably high eccentricity of HIP 15429 challenges standard tidal evolution
models, suggesting either inefficient tidal dissipation or external influences,
such as a tertiary companion or circumbinary disk. This underscores the need to
identify and characterise more post-mass transfer binaries to benchmark and
refine theoretical models of binary evolution.
</summary>
    <author>
      <name>Johanna Müller-Horn</name>
    </author>
    <author>
      <name>Kareem El-Badry</name>
    </author>
    <author>
      <name>Hans-Walter Rix</name>
    </author>
    <author>
      <name>Tomer Shenar</name>
    </author>
    <author>
      <name>Rhys Seeburger</name>
    </author>
    <author>
      <name>Jaime Villasenor</name>
    </author>
    <author>
      <name>Julia Bodensteiner</name>
    </author>
    <author>
      <name>W. David Latham</name>
    </author>
    <author>
      <name>Allyson Bieryla</name>
    </author>
    <author>
      <name>A. Lars Buchhave</name>
    </author>
    <author>
      <name>Howard Isaacson</name>
    </author>
    <author>
      <name>W. Andrew Howard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Astronomy &amp; Astrophysics</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
