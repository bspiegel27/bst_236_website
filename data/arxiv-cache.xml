<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-11T01:00:16Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-10T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">116616</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.07077v1</id>
    <updated>2025-07-09T17:35:58Z</updated>
    <published>2025-07-09T17:35:58Z</published>
    <title>Reading a Ruler in the Wild</title>
    <summary>  Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.
</summary>
    <author>
      <name>Yimu Pan</name>
    </author>
    <author>
      <name>Manas Mehta</name>
    </author>
    <author>
      <name>Gwen Sincerbeaux</name>
    </author>
    <author>
      <name>Jeffery A. Goldstein</name>
    </author>
    <author>
      <name>Alison D. Gernand</name>
    </author>
    <author>
      <name>James Z. Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2507.07077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07067v1</id>
    <updated>2025-07-09T17:27:51Z</updated>
    <published>2025-07-09T17:27:51Z</published>
    <title>How to Bridge the Sim-to-Real Gap in Digital Twin-Aided
  Telecommunication Networks</title>
    <summary>  Training effective artificial intelligence models for telecommunications is
challenging due to the scarcity of deployment-specific data. Real data
collection is expensive, and available datasets often fail to capture the
unique operational conditions and contextual variability of the network
environment. Digital twinning provides a potential solution to this problem, as
simulators tailored to the current network deployment can generate
site-specific data to augment the available training datasets. However, there
is a need to develop solutions to bridge the inherent simulation-to-reality
(sim-to-real) gap between synthetic and real-world data. This paper reviews
recent advances on two complementary strategies: 1) the calibration of digital
twins (DTs) through real-world measurements, and 2) the use of sim-to-real
gap-aware training strategies to robustly handle residual discrepancies between
digital twin-generated and real data. For the latter, we evaluate two
conceptually distinct methods that model the sim-to-real gap either at the
level of the environment via Bayesian learning or at the level of the training
loss via prediction-powered inference.
</summary>
    <author>
      <name>Clement Ruah</name>
    </author>
    <author>
      <name>Houssem Sifaou</name>
    </author>
    <author>
      <name>Osvaldo Simeone</name>
    </author>
    <author>
      <name>Bashir M. Al-Hashimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07031v1</id>
    <updated>2025-07-09T17:03:21Z</updated>
    <published>2025-07-09T17:03:21Z</published>
    <title>ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel
  Proof Accumulation</title>
    <summary>  As AI models become ubiquitous in our daily lives, there has been an
increasing demand for transparency in ML services. However, the model owner
does not want to reveal the weights, as they are considered trade secrets. To
solve this problem, researchers have turned to zero-knowledge proofs of ML
model inference. These proofs convince the user that the ML model output is
correct, without revealing the weights of the model to the user. Past work on
these provers can be placed into two categories. The first method compiles the
ML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The
second method uses custom cryptographic protocols designed only for a specific
class of models. Unfortunately, the first method is highly inefficient, making
it impractical for the large models used today, and the second method does not
generalize well, making it difficult to update in the rapidly changing field of
machine learning. To solve this, we propose ZKTorch, an open source end-to-end
proving system that compiles ML models into base cryptographic operations
called basic blocks, each proved using specialized protocols. ZKTorch is built
on top of a novel parallel extension to the Mira accumulation scheme, enabling
succinct proofs with minimal accumulation overhead. These contributions allow
ZKTorch to achieve at least a $3\times$ reduction in the proof size compared to
specialized protocols and up to a $6\times$ speedup in proving time over a
general-purpose ZKML framework.
</summary>
    <author>
      <name>Bing-Jyue Chen</name>
    </author>
    <author>
      <name>Lilia Tang</name>
    </author>
    <author>
      <name>Daniel Kang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07024v1</id>
    <updated>2025-07-09T16:54:21Z</updated>
    <published>2025-07-09T16:54:21Z</published>
    <title>FlexOlmo: Open Language Models for Flexible Data Use</title>
    <summary>  We introduce FlexOlmo, a new class of language models (LMs) that supports (1)
distributed training without data sharing, where different model parameters are
independently trained on closed datasets, and (2) data-flexible inference,
where these parameters along with their associated data can be flexibly
included or excluded from model inferences with no further training. FlexOlmo
employs a mixture-of-experts (MoE) architecture where each expert is trained
independently on closed datasets and later integrated through a new
domain-informed routing without any joint training. FlexOlmo is trained on
FlexMix, a corpus we curate comprising publicly available datasets alongside
seven domain-specific sets, representing realistic approximations of closed
sets. We evaluate models with up to 37 billion parameters (20 billion active)
on 31 diverse downstream tasks. We show that a general expert trained on public
data can be effectively combined with independently trained experts from other
data owners, leading to an average 41% relative improvement while allowing
users to opt out of certain data based on data licensing or permission
requirements. Our approach also outperforms prior model merging methods by
10.1% on average and surpasses the standard MoE trained without data
restrictions using the same training FLOPs. Altogether, this research presents
a solution for both data owners and researchers in regulated industries with
sensitive or protected data. FlexOlmo enables benefiting from closed data while
respecting data owners' preferences by keeping their data local and supporting
fine-grained control of data access during inference.
</summary>
    <author>
      <name>Weijia Shi</name>
    </author>
    <author>
      <name>Akshita Bhagia</name>
    </author>
    <author>
      <name>Kevin Farhat</name>
    </author>
    <author>
      <name>Niklas Muennighoff</name>
    </author>
    <author>
      <name>Pete Walsh</name>
    </author>
    <author>
      <name>Jacob Morrison</name>
    </author>
    <author>
      <name>Dustin Schwenk</name>
    </author>
    <author>
      <name>Shayne Longpre</name>
    </author>
    <author>
      <name>Jake Poznanski</name>
    </author>
    <author>
      <name>Allyson Ettinger</name>
    </author>
    <author>
      <name>Daogao Liu</name>
    </author>
    <author>
      <name>Margaret Li</name>
    </author>
    <author>
      <name>Dirk Groeneveld</name>
    </author>
    <author>
      <name>Mike Lewis</name>
    </author>
    <author>
      <name>Wen-tau Yih</name>
    </author>
    <author>
      <name>Luca Soldaini</name>
    </author>
    <author>
      <name>Kyle Lo</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <author>
      <name>Pang Wei Koh</name>
    </author>
    <author>
      <name>Hannaneh Hajishirzi</name>
    </author>
    <author>
      <name>Ali Farhadi</name>
    </author>
    <author>
      <name>Sewon Min</name>
    </author>
    <link href="http://arxiv.org/abs/2507.07024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.06986v1</id>
    <updated>2025-07-09T16:08:58Z</updated>
    <published>2025-07-09T16:08:58Z</published>
    <title>BarkBeetle: Stealing Decision Tree Models with Fault Injection</title>
    <summary>  Machine learning models, particularly decision trees (DTs), are widely
adopted across various domains due to their interpretability and efficiency.
However, as ML models become increasingly integrated into privacy-sensitive
applications, concerns about their confidentiality have grown, particularly in
light of emerging threats such as model extraction and fault injection attacks.
Assessing the vulnerability of DTs under such attacks is therefore important.
In this work, we present BarkBeetle, a novel attack that leverages fault
injection to extract internal structural information of DT models. BarkBeetle
employs a bottom-up recovery strategy that uses targeted fault injection at
specific nodes to efficiently infer feature splits and threshold values. Our
proof-of-concept implementation demonstrates that BarkBeetle requires
significantly fewer queries and recovers more structural information compared
to prior approaches, when evaluated on DTs trained with public UCI datasets. To
validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi
RP2350 board and perform fault injections using the Faultier voltage glitching
tool. As BarkBeetle targets general DT models, we also provide an in-depth
discussion on its applicability to a broader range of tree-based applications,
including data stream classification, DT variants, and cryptography schemes.
</summary>
    <author>
      <name>Qifan Wang</name>
    </author>
    <author>
      <name>Jonas Sander</name>
    </author>
    <author>
      <name>Minmin Jiang</name>
    </author>
    <author>
      <name>Thomas Eisenbarth</name>
    </author>
    <author>
      <name>David Oswald</name>
    </author>
    <link href="http://arxiv.org/abs/2507.06986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.06986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.06976v1</id>
    <updated>2025-07-09T16:05:25Z</updated>
    <published>2025-07-09T16:05:25Z</published>
    <title>DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via
  Joint LiDAR-Based 3D Object Detection and Denoising</title>
    <summary>  While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.
</summary>
    <author>
      <name>Sven Teufel</name>
    </author>
    <author>
      <name>Dominique Mayer</name>
    </author>
    <author>
      <name>Jörg Gamerdinger</name>
    </author>
    <author>
      <name>Oliver Bringmann</name>
    </author>
    <link href="http://arxiv.org/abs/2507.06976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.06976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.06969v1</id>
    <updated>2025-07-09T15:59:30Z</updated>
    <published>2025-07-09T15:59:30Z</published>
    <title>Unifying Re-Identification, Attribute Inference, and Data Reconstruction
  Risks in Differential Privacy</title>
    <summary>  Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.
</summary>
    <author>
      <name>Bogdan Kulynych</name>
    </author>
    <author>
      <name>Juan Felipe Gomez</name>
    </author>
    <author>
      <name>Georgios Kaissis</name>
    </author>
    <author>
      <name>Jamie Hayes</name>
    </author>
    <author>
      <name>Borja Balle</name>
    </author>
    <author>
      <name>Flavio du Pin Calmon</name>
    </author>
    <author>
      <name>Jean Louis Raisaro</name>
    </author>
    <link href="http://arxiv.org/abs/2507.06969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.06969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.06964v2</id>
    <updated>2025-07-10T05:41:21Z</updated>
    <published>2025-07-09T15:52:34Z</published>
    <title>IdentityByDescentDispersal.jl: Inferring dispersal rates with
  identity-by-descent blocks</title>
    <summary>  The population density and per-generation dispersal rate of a population are
central parameters in the study of evolution and ecology. The distribution of
recent coalescent events between individuals in space can be used to estimate
such quantities through the distribution of identity-by-descent (IBD) blocks.
An IBD block is defined as a segment of DNA that has been inherited by a pair
of individuals from a common ancestor without being broken by recombination. We
introduce IdentityByDescentDispersal.jl, a Julia package for estimating
effective population densities and dispersal rates from observed spatial
patterns of IBD shared blocks. It implements the inference scheme proposed by
Ringbauer, Coop, and Barton (2017). The package provides a user-friendly
interface, supports efficient gradient-based optimization and accommodates
arbitrary user-defined demographic models through numerical integration. This
software aims to encourage a wider audience to utilize spatial genetic data for
estimating dispersal rates, thereby motivating further research and expanding
its applications.
</summary>
    <author>
      <name>Francisco Campuzano Jiménez</name>
    </author>
    <author>
      <name>Arthur Zwaenepoel</name>
    </author>
    <author>
      <name>Els Lea R De Keyzer</name>
    </author>
    <author>
      <name>Hannes Svardal</name>
    </author>
    <link href="http://arxiv.org/abs/2507.06964v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.06964v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.06961v1</id>
    <updated>2025-07-09T15:46:39Z</updated>
    <published>2025-07-09T15:46:39Z</published>
    <title>Off-Policy Evaluation Under Nonignorable Missing Data</title>
    <summary>  Off-Policy Evaluation (OPE) aims to estimate the value of a target policy
using offline data collected from potentially different policies. In real-world
applications, however, logged data often suffers from missingness. While OPE
has been extensively studied in the literature, a theoretical understanding of
how missing data affects OPE results remains unclear. In this paper, we
investigate OPE in the presence of monotone missingness and theoretically
demonstrate that the value estimates remain unbiased under ignorable
missingness but can be biased under nonignorable (informative) missingness. To
retain the consistency of value estimation, we propose an inverse probability
weighted value estimator and conduct statistical inference to quantify the
uncertainty of the estimates. Through a series of numerical experiments, we
empirically demonstrate that our proposed estimator yields a more reliable
value inference under missing data.
</summary>
    <author>
      <name>Han Wang</name>
    </author>
    <author>
      <name>Yang Xu</name>
    </author>
    <author>
      <name>Wenbin Lu</name>
    </author>
    <author>
      <name>Rui Song</name>
    </author>
    <link href="http://arxiv.org/abs/2507.06961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.06961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.06956v1</id>
    <updated>2025-07-09T15:39:17Z</updated>
    <published>2025-07-09T15:39:17Z</published>
    <title>Investigating the Robustness of Retrieval-Augmented Generation at the
  Query Level</title>
    <summary>  Large language models (LLMs) are very costly and inefficient to update with
new information. To address this limitation, retrieval-augmented generation
(RAG) has been proposed as a solution that dynamically incorporates external
knowledge during inference, improving factual consistency and reducing
hallucinations. Despite its promise, RAG systems face practical challenges-most
notably, a strong dependence on the quality of the input query for accurate
retrieval. In this paper, we investigate the sensitivity of different
components in the RAG pipeline to various types of query perturbations. Our
analysis reveals that the performance of commonly used retrievers can degrade
significantly even under minor query variations. We study each module in
isolation as well as their combined effect in an end-to-end question answering
setting, using both general-domain and domain-specific datasets. Additionally,
we propose an evaluation framework to systematically assess the query-level
robustness of RAG pipelines and offer actionable recommendations for
practitioners based on the results of more than 1092 experiments we performed.
</summary>
    <author>
      <name>Sezen Perçin</name>
    </author>
    <author>
      <name>Xin Su</name>
    </author>
    <author>
      <name>Qutub Sha Syed</name>
    </author>
    <author>
      <name>Phillip Howard</name>
    </author>
    <author>
      <name>Aleksei Kuvshinov</name>
    </author>
    <author>
      <name>Leo Schwinn</name>
    </author>
    <author>
      <name>Kay-Ulrich Scholl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Generation, Evaluation &amp; Metrics (GEM) Workshop at ACL
  2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.06956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.06956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
