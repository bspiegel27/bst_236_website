<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-26T00:59:52Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-25T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117562</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.18606v1</id>
    <updated>2025-07-24T17:42:30Z</updated>
    <published>2025-07-24T17:42:30Z</published>
    <title>Hybrid quantum-classical algorithm for near-optimal planning in POMDPs</title>
    <summary>  Reinforcement learning (RL) provides a principled framework for
decision-making in partially observable environments, which can be modeled as
Markov decision processes and compactly represented through dynamic decision
Bayesian networks. Recent advances demonstrate that inference on sparse
Bayesian networks can be accelerated using quantum rejection sampling combined
with amplitude amplification, leading to a computational speedup in estimating
acceptance probabilities.\\ Building on this result, we introduce Quantum
Bayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead
algorithm for model-based RL in partially observable environments. We present a
rigorous, oracle-free time complexity analysis under fault-tolerant assumptions
for the quantum device. Unlike standard treatments that assume a black-box
oracle, we explicitly specify the inference process, allowing our bounds to
more accurately reflect the true computational cost. We show that, for
environments whose dynamics form a sparse Bayesian network, horizon-based
near-optimal planning can be achieved sub-quadratically faster through
quantum-enhanced belief updates.
  Furthermore, we present numerical experiments benchmarking QBRL against its
classical counterpart on simple yet illustrative decision-making tasks. Our
results offer a detailed analysis of how the quantum computational advantage
translates into decision-making performance, highlighting that the magnitude of
the advantage can vary significantly across different deployment settings.
</summary>
    <author>
      <name>Gilberto Cunha</name>
    </author>
    <author>
      <name>Alexandra Ramôa</name>
    </author>
    <author>
      <name>André Sequeira</name>
    </author>
    <author>
      <name>Michael de Oliveira</name>
    </author>
    <author>
      <name>Luís Barbosa</name>
    </author>
    <link href="http://arxiv.org/abs/2507.18606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18595v1</id>
    <updated>2025-07-24T17:26:30Z</updated>
    <published>2025-07-24T17:26:30Z</published>
    <title>Investigating Mobility in Spatial Biodiversity Models through Recurrence
  Quantification Analysis</title>
    <summary>  Recurrence plots and their associated quantifiers provide a robust framework
for detecting and characterising complex patterns in non-linear time-series. In
this paper, we employ recurrence quantification analysis to investigate the
dynamics of the cyclic, non-hierarchical May-Leonard model, also referred to as
rock--paper--scissors systems, that describes competitive interactions among
three species. A crucial control parameter in these systems is the species'
mobility $m$, which governs the spatial displacement of individuals and
profoundly influences the resulting dynamics. By systematically varying $m$ and
constructing suitable recurrence plots from numerical simulations, we explore
how recurrence quantifiers reflect distinct dynamical features associated with
different ecological states. We then introduce an ensemble-based approach that
leverages statistical distributions of recurrence quantifiers, computed from
numerous independent realisations, allowing us to identify dynamical outliers
as significant deviations from typical system behaviour. Through detailed
numerical analyses, we demonstrate that these outliers correspond to divergent
ecological regimes associated with specific mobility values, providing also a
robust manner to infer the mobility parameter from observed numerical data. Our
results highlight the potential of recurrence-based methods as diagnostic tools
for analysing spatial ecological systems and extracting ecologically relevant
information from their non-linear dynamical patterns.
</summary>
    <author>
      <name>Matheus Palmero</name>
    </author>
    <author>
      <name>Matheus Bongestab</name>
    </author>
    <link href="http://arxiv.org/abs/2507.18595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18580v1</id>
    <updated>2025-07-24T16:56:38Z</updated>
    <published>2025-07-24T16:56:38Z</published>
    <title>System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese
  Hate Speech Recognition</title>
    <summary>  This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.
</summary>
    <author>
      <name>Jiahao Wang</name>
    </author>
    <author>
      <name>Ramen Liu</name>
    </author>
    <author>
      <name>Longhui Zhang</name>
    </author>
    <author>
      <name>Jing Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, accepted as oral presentation at CCL25-Eval</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.18580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18578v1</id>
    <updated>2025-07-24T16:51:33Z</updated>
    <published>2025-07-24T16:51:33Z</published>
    <title>Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective
  DLLMs</title>
    <summary>  Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.
</summary>
    <author>
      <name>Feng Hong</name>
    </author>
    <author>
      <name>Geng Yu</name>
    </author>
    <author>
      <name>Yushi Ye</name>
    </author>
    <author>
      <name>Haicheng Huang</name>
    </author>
    <author>
      <name>Huangjie Zheng</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <author>
      <name>Yanfeng Wang</name>
    </author>
    <author>
      <name>Jiangchao Yao</name>
    </author>
    <link href="http://arxiv.org/abs/2507.18578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18576v1</id>
    <updated>2025-07-24T16:49:19Z</updated>
    <published>2025-07-24T16:49:19Z</published>
    <title>SafeWork-R1: Coevolving Safety and Intelligence under the
  AI-45$^{\circ}$ Law</title>
    <summary>  We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.
</summary>
    <author>
      <name>Shanghai AI Lab</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Yicheng Bao</name>
    </author>
    <author>
      <name>Guanxu Chen</name>
    </author>
    <author>
      <name>Mingkang Chen</name>
    </author>
    <author>
      <name>Yunhao Chen</name>
    </author>
    <author>
      <name>Chiyu Chen</name>
    </author>
    <author>
      <name>Lingjie Chen</name>
    </author>
    <author>
      <name>Sirui Chen</name>
    </author>
    <author>
      <name>Xinquan Chen</name>
    </author>
    <author>
      <name>Jie Cheng</name>
    </author>
    <author>
      <name>Yu Cheng</name>
    </author>
    <author>
      <name>Dengke Deng</name>
    </author>
    <author>
      <name>Yizhuo Ding</name>
    </author>
    <author>
      <name>Dan Ding</name>
    </author>
    <author>
      <name>Xiaoshan Ding</name>
    </author>
    <author>
      <name>Yi Ding</name>
    </author>
    <author>
      <name>Zhichen Dong</name>
    </author>
    <author>
      <name>Lingxiao Du</name>
    </author>
    <author>
      <name>Yuyu Fan</name>
    </author>
    <author>
      <name>Xinshun Feng</name>
    </author>
    <author>
      <name>Yanwei Fu</name>
    </author>
    <author>
      <name>Yuxuan Gao</name>
    </author>
    <author>
      <name>Ruijun Ge</name>
    </author>
    <author>
      <name>Tianle Gu</name>
    </author>
    <author>
      <name>Lujun Gui</name>
    </author>
    <author>
      <name>Jiaxuan Guo</name>
    </author>
    <author>
      <name>Qianxi He</name>
    </author>
    <author>
      <name>Yuenan Hou</name>
    </author>
    <author>
      <name>Xuhao Hu</name>
    </author>
    <author>
      <name>Hong Huang</name>
    </author>
    <author>
      <name>Kaichen Huang</name>
    </author>
    <author>
      <name>Shiyang Huang</name>
    </author>
    <author>
      <name>Yuxian Jiang</name>
    </author>
    <author>
      <name>Shanzhe Lei</name>
    </author>
    <author>
      <name>Jie Li</name>
    </author>
    <author>
      <name>Lijun Li</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Xiangtian Li</name>
    </author>
    <author>
      <name>Yafu Li</name>
    </author>
    <author>
      <name>Lingyu Li</name>
    </author>
    <author>
      <name>Xueyan Li</name>
    </author>
    <author>
      <name>Haotian Liang</name>
    </author>
    <author>
      <name>Dongrui Liu</name>
    </author>
    <author>
      <name>Qihua Liu</name>
    </author>
    <author>
      <name>Zhixuan Liu</name>
    </author>
    <author>
      <name>Bangwei Liu</name>
    </author>
    <author>
      <name>Huacan Liu</name>
    </author>
    <author>
      <name>Yuexiao Liu</name>
    </author>
    <author>
      <name>Zongkai Liu</name>
    </author>
    <author>
      <name>Chaochao Lu</name>
    </author>
    <author>
      <name>Yudong Lu</name>
    </author>
    <author>
      <name>Xiaoya Lu</name>
    </author>
    <author>
      <name>Zhenghao Lu</name>
    </author>
    <author>
      <name>Qitan Lv</name>
    </author>
    <author>
      <name>Caoyuan Ma</name>
    </author>
    <author>
      <name>Jiachen Ma</name>
    </author>
    <author>
      <name>Xiaoya Ma</name>
    </author>
    <author>
      <name>Zhongtian Ma</name>
    </author>
    <author>
      <name>Lingyu Meng</name>
    </author>
    <author>
      <name>Ziqi Miao</name>
    </author>
    <author>
      <name>Yazhe Niu</name>
    </author>
    <author>
      <name>Yuezhang Peng</name>
    </author>
    <author>
      <name>Yuan Pu</name>
    </author>
    <author>
      <name>Han Qi</name>
    </author>
    <author>
      <name>Chen Qian</name>
    </author>
    <author>
      <name>Xingge Qiao</name>
    </author>
    <author>
      <name>Jingjing Qu</name>
    </author>
    <author>
      <name>Jiashu Qu</name>
    </author>
    <author>
      <name>Wanying Qu</name>
    </author>
    <author>
      <name>Wenwen Qu</name>
    </author>
    <author>
      <name>Xiaoye Qu</name>
    </author>
    <author>
      <name>Qihan Ren</name>
    </author>
    <author>
      <name>Qingnan Ren</name>
    </author>
    <author>
      <name>Qingyu Ren</name>
    </author>
    <author>
      <name>Jing Shao</name>
    </author>
    <author>
      <name>Wenqi Shao</name>
    </author>
    <author>
      <name>Shuai Shao</name>
    </author>
    <author>
      <name>Dongxing Shi</name>
    </author>
    <author>
      <name>Xin Song</name>
    </author>
    <author>
      <name>Xinhao Song</name>
    </author>
    <author>
      <name>Yan Teng</name>
    </author>
    <author>
      <name>Xuan Tong</name>
    </author>
    <author>
      <name>Yingchun Wang</name>
    </author>
    <author>
      <name>Xuhong Wang</name>
    </author>
    <author>
      <name>Shujie Wang</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Yige Wang</name>
    </author>
    <author>
      <name>Yixu Wang</name>
    </author>
    <author>
      <name>Yuanfu Wang</name>
    </author>
    <author>
      <name>Futing Wang</name>
    </author>
    <author>
      <name>Ruofan Wang</name>
    </author>
    <author>
      <name>Wenjie Wang</name>
    </author>
    <author>
      <name>Yajie Wang</name>
    </author>
    <author>
      <name>Muhao Wei</name>
    </author>
    <author>
      <name>Xiaoyu Wen</name>
    </author>
    <author>
      <name>Fenghua Weng</name>
    </author>
    <author>
      <name>Yuqi Wu</name>
    </author>
    <author>
      <name>Yingtong Xiong</name>
    </author>
    <author>
      <name>Xingcheng Xu</name>
    </author>
    <author>
      <name>Chao Yang</name>
    </author>
    <author>
      <name>Yue Yang</name>
    </author>
    <author>
      <name>Yang Yao</name>
    </author>
    <author>
      <name>Yulei Ye</name>
    </author>
    <author>
      <name>Zhenyun Yin</name>
    </author>
    <author>
      <name>Yi Yu</name>
    </author>
    <author>
      <name>Bo Zhang</name>
    </author>
    <author>
      <name>Qiaosheng Zhang</name>
    </author>
    <author>
      <name>Jinxuan Zhang</name>
    </author>
    <author>
      <name>Yexin Zhang</name>
    </author>
    <author>
      <name>Yinqiang Zheng</name>
    </author>
    <author>
      <name>Hefeng Zhou</name>
    </author>
    <author>
      <name>Zhanhui Zhou</name>
    </author>
    <author>
      <name>Pengyu Zhu</name>
    </author>
    <author>
      <name>Qingzi Zhu</name>
    </author>
    <author>
      <name>Yubo Zhu</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages, 18 figures, authors are listed in alphabetical order by
  their last names</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.18576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18562v1</id>
    <updated>2025-07-24T16:36:47Z</updated>
    <published>2025-07-24T16:36:47Z</published>
    <title>GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation</title>
    <summary>  Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.
</summary>
    <author>
      <name>Jiafeng Xiong</name>
    </author>
    <author>
      <name>Yuting Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2507.18562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18557v1</id>
    <updated>2025-07-24T16:30:46Z</updated>
    <published>2025-07-24T16:30:46Z</published>
    <title>Deep Learning for Blood-Brain Barrier Permeability Prediction</title>
    <summary>  Predicting whether a molecule can cross the blood-brain barrier (BBB) is a
key step in early-stage neuropharmaceutical development, directly influencing
both research efficiency and success rates in drug discovery. Traditional
empirical methods based on physicochemical properties are prone to systematic
misjudgements due to their reliance on static rules. Early machine learning
models, although data-driven, often suffer from limited capacity, poor
generalization, and insufficient interpretability. In recent years, artificial
intelligence (AI) methods have become essential tools for predicting BBB
permeability and guiding related drug design, owing to their ability to model
molecular structures and capture complex biological mechanisms. This article
systematically reviews the evolution of this field-from deep neural networks to
graph-based structural modeling-highlighting the advantages of multi-task and
multimodal learning strategies in identifying mechanism-relevant variables. We
further explore the emerging potential of generative models and causal
inference methods for integrating permeability prediction with mechanism-aware
drug design. BBB modeling is in the transition from static classification
toward mechanistic perception and structure-function modeling. This paradigm
shift provides a methodological foundation and future roadmap for the
integration of AI into neuropharmacological development.
</summary>
    <author>
      <name>Zihan Yang</name>
    </author>
    <author>
      <name>Haipeng Gong</name>
    </author>
    <link href="http://arxiv.org/abs/2507.18557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18554v1</id>
    <updated>2025-07-24T16:23:11Z</updated>
    <published>2025-07-24T16:23:11Z</published>
    <title>How weak are weak factors? Uniform inference for signal strength in
  signal plus noise models</title>
    <summary>  The paper analyzes four classical signal-plus-noise models: the factor model,
spiked sample covariance matrices, the sum of a Wigner matrix and a low-rank
perturbation, and canonical correlation analysis with low-rank dependencies.
The objective is to construct confidence intervals for the signal strength that
are uniformly valid across all regimes - strong, weak, and critical signals. We
demonstrate that traditional Gaussian approximations fail in the critical
regime. Instead, we introduce a universal transitional distribution that
enables valid inference across the entire spectrum of signal strengths. The
approach is illustrated through applications in macroeconomics and finance.
</summary>
    <author>
      <name>Anna Bykhovskaya</name>
    </author>
    <author>
      <name>Vadim Gorin</name>
    </author>
    <author>
      <name>Sasha Sodin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">75 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.18554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18551v1</id>
    <updated>2025-07-24T16:19:08Z</updated>
    <published>2025-07-24T16:19:08Z</published>
    <title>A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration</title>
    <summary>  Intraoperative registration of real-time ultrasound (iUS) to preoperative
Magnetic Resonance Imaging (MRI) remains an unsolved problem due to severe
modality-specific differences in appearance, resolution, and field-of-view. To
address this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS
matching and registration. Our approach employs a patient-specific
matching-by-synthesis approach, generating synthetic iUS volumes from
preoperative MRI. This enables supervised contrastive training to learn a
shared descriptor space.
  A probabilistic keypoint detection strategy is then employed to identify
anatomically salient and modality-consistent locations. During training, a
curriculum-based triplet loss with dynamic hard negative mining is used to
learn descriptors that are i) robust to iUS artifacts such as speckle noise and
limited coverage, and ii) rotation-invariant . At inference, the method detects
keypoints in MR and real iUS images and identifies sparse matches, which are
then used to perform rigid registration. Our approach is evaluated using 3D
MRI-iUS pairs from the ReMIND dataset. Experiments show that our approach
outperforms state-of-the-art keypoint matching methods across 11 patients, with
an average precision of $69.8\%$. For image registration, our method achieves a
competitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg
benchmark.
  Compared to existing iUS-MR registration approach, our framework is
interpretable, requires no manual initialization, and shows robustness to iUS
field-of-view variation. Code is available at
https://github.com/morozovdd/CrossKEY.
</summary>
    <author>
      <name>Daniil Morozov</name>
    </author>
    <author>
      <name>Reuben Dorent</name>
    </author>
    <author>
      <name>Nazim Haouchine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.18551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18544v1</id>
    <updated>2025-07-24T16:10:20Z</updated>
    <published>2025-07-24T16:10:20Z</published>
    <title>EP250108a/SN2025kg: A Magnetar-powered Gamma-Ray Burst Supernova
  Originating from a Close Helium-star Binary via Isolated Binary Evolution</title>
    <summary>  SN\,2025kg, linked to EP250108a, is among the brightest broad-lined Type Ic
supernova (SN Ic-BL) known, showing unique helium absorptions, a late-time
broad H$\alpha$, and an early bump. In this {\em{Letter}}, we propose a
jet-cocoon origin to explain EP250108a as off-axis cooling emission from a
mildly relativistic inner cocoon viewed at $\sim45^\circ$ and the early bump of
SN\,2025kg as the outer cocoon cooling emission, both constraining an energy of
$\sim(1-2)\times10^{52}{\rm{erg}}$ and a progenitor radius of $\sim5\,R_\odot$.
To explain SN\,2025kg's exceptionally luminous peak, potential energy injection
into the $\sim2.5\,M_\odot$ ejecta from a magnetar with initial period
$\sim1.7\,{\rm{ms}}$ and magnetic field $\sim2\times10^{15}{\rm{G}}$ may be
required, implying a rapidly rotating $\sim4\,M_\odot$ progenitor. Thus, the
progenitor may be a low-mass helium star with an extended helium envelope,
supported by helium absorption lines and an inferred weak pre-SN wind.
Hydrogen-rich material may reside in the inner ejecta layers, as suggested by
the late-time broad H$\alpha$, possibly originating from main-sequence
companion material evaporated by the magnetar wind. Since the observed
near-solar metallicity challenges the popular quasi-chemically homogeneous
evolution channel, the rapidly rotating helium-star progenitor of
EP250108a/SN\,2025kg might attain angular momentum by being tidally spun up by
a main-sequence companion in a close binary formed through isolated binary
evolution.
</summary>
    <author>
      <name>Jin-Ping Zhu</name>
    </author>
    <author>
      <name>Jian-He Zheng</name>
    </author>
    <author>
      <name>Bing Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.18544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
