<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-20T01:07:41Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-19T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117118</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.13345v1</id>
    <updated>2025-07-17T17:59:47Z</updated>
    <published>2025-07-17T17:59:47Z</published>
    <title>Imbalance in Balance: Online Concept Balancing in Generation Models</title>
    <summary>  In visual generation tasks, the responses and combinations of complex
concepts often lack stability and are error-prone, which remains an
under-explored area. In this paper, we attempt to explore the causal factors
for poor concept responses through elaborately designed experiments. We also
design a concept-wise equalization loss function (IMBA loss) to address this
issue. Our proposed method is online, eliminating the need for offline dataset
processing, and requires minimal code changes. In our newly proposed complex
concept benchmark Inert-CompBench and two other public test sets, our method
significantly enhances the concept response capability of baseline models and
yields highly competitive results with only a few codes.
</summary>
    <author>
      <name>Yukai Shi</name>
    </author>
    <author>
      <name>Jiarong Ou</name>
    </author>
    <author>
      <name>Rui Chen</name>
    </author>
    <author>
      <name>Haotian Yang</name>
    </author>
    <author>
      <name>Jiahao Wang</name>
    </author>
    <author>
      <name>Xin Tao</name>
    </author>
    <author>
      <name>Pengfei Wan</name>
    </author>
    <author>
      <name>Di Zhang</name>
    </author>
    <author>
      <name>Kun Gai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICCV2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13343v1</id>
    <updated>2025-07-17T17:59:10Z</updated>
    <published>2025-07-17T17:59:10Z</published>
    <title>Taming Diffusion Transformer for Real-Time Mobile Video Generation</title>
    <summary>  Diffusion Transformers (DiT) have shown strong performance in video
generation tasks, but their high computational cost makes them impractical for
resource-constrained devices like smartphones, and real-time generation is even
more challenging. In this work, we propose a series of novel optimizations to
significantly accelerate video generation and enable real-time performance on
mobile platforms. First, we employ a highly compressed variational autoencoder
(VAE) to reduce the dimensionality of the input data without sacrificing visual
quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning
strategy to shrink the model size to suit mobile platform while preserving
critical performance characteristics. Third, we develop an adversarial step
distillation technique tailored for DiT, which allows us to reduce the number
of inference steps to four. Combined, these optimizations enable our model to
achieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,
demonstrating the feasibility of real-time, high-quality video generation on
mobile devices.
</summary>
    <author>
      <name>Yushu Wu</name>
    </author>
    <author>
      <name>Yanyu Li</name>
    </author>
    <author>
      <name>Anil Kag</name>
    </author>
    <author>
      <name>Ivan Skorokhodov</name>
    </author>
    <author>
      <name>Willi Menapace</name>
    </author>
    <author>
      <name>Ke Ma</name>
    </author>
    <author>
      <name>Arpit Sahni</name>
    </author>
    <author>
      <name>Ju Hu</name>
    </author>
    <author>
      <name>Aliaksandr Siarohin</name>
    </author>
    <author>
      <name>Dhritiman Sagar</name>
    </author>
    <author>
      <name>Yanzhi Wang</name>
    </author>
    <author>
      <name>Sergey Tulyakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13339v1</id>
    <updated>2025-07-17T17:57:18Z</updated>
    <published>2025-07-17T17:57:18Z</published>
    <title>SpectraLift: Physics-Guided Spectral-Inversion Network for
  Self-Supervised Hyperspectral Image Super-Resolution</title>
    <summary>  High-spatial-resolution hyperspectral images (HSI) are essential for
applications such as remote sensing and medical imaging, yet HSI sensors
inherently trade spatial detail for spectral richness. Fusing
high-spatial-resolution multispectral images (HR-MSI) with
low-spatial-resolution hyperspectral images (LR-HSI) is a promising route to
recover fine spatial structures without sacrificing spectral fidelity. Most
state-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)
calibration or ground truth high resolution HSI (HR-HSI), both of which are
impractical to obtain in real world settings. We present SpectraLift, a fully
self-supervised framework that fuses LR-HSI and HR-MSI inputs using only the
MSI's Spectral Response Function (SRF). SpectraLift trains a lightweight
per-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic
low-spatial-resolution multispectral image (LR-MSI) obtained by applying the
SRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an
$\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as
the optimization objective. At inference, SpectraLift uses the trained network
to map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in
minutes, is agnostic to spatial blur and resolution, and outperforms
state-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.
</summary>
    <author>
      <name>Ritik Shah</name>
    </author>
    <author>
      <name>Marco F. Duarte</name>
    </author>
    <link href="http://arxiv.org/abs/2507.13339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13334v1</id>
    <updated>2025-07-17T17:50:36Z</updated>
    <published>2025-07-17T17:50:36Z</published>
    <title>A Survey of Context Engineering for Large Language Models</title>
    <summary>  The performance of Large Language Models (LLMs) is fundamentally determined
by the contextual information provided during inference. This survey introduces
Context Engineering, a formal discipline that transcends simple prompt design
to encompass the systematic optimization of information payloads for LLMs. We
present a comprehensive taxonomy decomposing Context Engineering into its
foundational components and the sophisticated implementations that integrate
them into intelligent systems. We first examine the foundational components:
context retrieval and generation, context processing and context management. We
then explore how these components are architecturally integrated to create
sophisticated system implementations: retrieval-augmented generation (RAG),
memory systems and tool-integrated reasoning, and multi-agent systems. Through
this systematic analysis of over 1300 research papers, our survey not only
establishes a technical roadmap for the field but also reveals a critical
research gap: a fundamental asymmetry exists between model capabilities. While
current models, augmented by advanced context engineering, demonstrate
remarkable proficiency in understanding complex contexts, they exhibit
pronounced limitations in generating equally sophisticated, long-form outputs.
Addressing this gap is a defining priority for future research. Ultimately,
this survey provides a unified framework for both researchers and engineers
advancing context-aware AI.
</summary>
    <author>
      <name>Lingrui Mei</name>
    </author>
    <author>
      <name>Jiayu Yao</name>
    </author>
    <author>
      <name>Yuyao Ge</name>
    </author>
    <author>
      <name>Yiwei Wang</name>
    </author>
    <author>
      <name>Baolong Bi</name>
    </author>
    <author>
      <name>Yujun Cai</name>
    </author>
    <author>
      <name>Jiazhi Liu</name>
    </author>
    <author>
      <name>Mingyu Li</name>
    </author>
    <author>
      <name>Zhong-Zhi Li</name>
    </author>
    <author>
      <name>Duzhen Zhang</name>
    </author>
    <author>
      <name>Chenlin Zhou</name>
    </author>
    <author>
      <name>Jiayi Mao</name>
    </author>
    <author>
      <name>Tianze Xia</name>
    </author>
    <author>
      <name>Jiafeng Guo</name>
    </author>
    <author>
      <name>Shenghua Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ongoing work; 165 pages, 1401 citations</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13326v1</id>
    <updated>2025-07-17T17:45:09Z</updated>
    <published>2025-07-17T17:45:09Z</published>
    <title>A Real-Time System for Egocentric Hand-Object Interaction Detection in
  Industrial Domains</title>
    <summary>  Hand-object interaction detection remains an open challenge in real-time
applications, where intuitive user experiences depend on fast and accurate
detection of interactions with surrounding objects. We propose an efficient
approach for detecting hand-objects interactions from streaming egocentric
vision that operates in real time. Our approach consists of an action
recognition module and an object detection module for identifying active
objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as
backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark
at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object.
We implement our models in a cascaded architecture where the action recognition
and object detection modules operate sequentially. When the action recognition
predicts a contact state, it activates the object detection module, which in
turn performs inference on the relevant frame to detect and classify the active
object.
</summary>
    <author>
      <name>Antonio Finocchiaro</name>
    </author>
    <author>
      <name>Alessandro Sebastiano Catinello</name>
    </author>
    <author>
      <name>Michele Mazzamuto</name>
    </author>
    <author>
      <name>Rosario Leonardi</name>
    </author>
    <author>
      <name>Antonino Furnari</name>
    </author>
    <author>
      <name>Giovanni Maria Farinella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures, In International Conference on Image Analysis
  and Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13317v1</id>
    <updated>2025-07-17T17:36:13Z</updated>
    <published>2025-07-17T17:36:13Z</published>
    <title>Testing halo models for constraining astrophysical feedback with
  multi-probe modeling: I. 3D Power spectra and mass fractions</title>
    <summary>  Upcoming Stage-IV surveys will deliver measurements of distribution of matter
with unprecedented precision, demanding highly accurate theoretical models for
cosmological parameter inference. A major source of modeling uncertainty lies
in astrophysical processes associated with galaxy formation and evolution,
which remain poorly understood. Probes such as the thermal and kinematic
Sunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio
bursts offer a promising avenue for mapping the distribution and thermal
properties of cosmic baryons. A unified analytical framework capable of jointly
modeling these observables is essential for fully harnessing the complementary
information while mitigating probe-specific systematics. In this work, we
present a detailed assessment of existing analytical models, which differ in
their assumptions and prescriptions for simultaneously describing the
distribution of matter and baryons in the universe. Using the Magneticum
hydrodynamical simulation, we test these models by jointly analyzing the 3D
auto- and cross-power spectra of the matter and baryonic fields that underpin
the above probes. We find that all models can reproduce the power spectra at
sub-percent to few-percent accuracy, depending on the tracer combination and
number of free parameters. Their ability to recover underlying halo properties,
such as the evolution of gas abundance and thermodynamic profiles with halo
mass, varies considerably. Our results suggest that these models require
further refinement and testing for reliable interpretation of multi-wavelength
datasets.
</summary>
    <author>
      <name>Pranjal R. S.</name>
    </author>
    <author>
      <name>Shivam Pandey</name>
    </author>
    <author>
      <name>Dhayaa Anbajagane</name>
    </author>
    <author>
      <name>Elisabeth Krause</name>
    </author>
    <author>
      <name>Klaus Dolag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Comments welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13305v1</id>
    <updated>2025-07-17T17:22:41Z</updated>
    <published>2025-07-17T17:22:41Z</published>
    <title>Boosting Team Modeling through Tempo-Relational Representation Learning</title>
    <summary>  Team modeling remains a fundamental challenge at the intersection of
Artificial Intelligence and the Social Sciences. Social Science research
emphasizes the need to jointly model dynamics and relations, while practical
applications demand unified models capable of inferring multiple team
constructs simultaneously, providing interpretable insights and actionable
recommendations to enhance team performance. However, existing works do not
meet these practical demands. To bridge this gap, we present TRENN, a novel
tempo-relational architecture that integrates: (i) an automatic temporal graph
extractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct
prediction, and (iv) two complementary explainability modules. TRENN jointly
captures relational and temporal team dynamics, providing a solid foundation
for MT-TRENN, which extends TReNN by replacing the decoder with a multi-task
head, enabling the model to learn shared Social Embeddings and simultaneously
predict multiple team constructs, including Emergent Leadership, Leadership
Style, and Teamwork components. Experimental results demonstrate that our
approach significantly outperforms approaches that rely exclusively on temporal
or relational information. Additionally, experimental evaluation has shown that
the explainability modules integrated in MT-TRENN yield interpretable insights
and actionable suggestions to support team improvement. These capabilities make
our approach particularly well-suited for Human-Centered AI applications, such
as intelligent decision-support systems in high-stakes collaborative
environments.
</summary>
    <author>
      <name>Vincenzo Marco De Luca</name>
    </author>
    <author>
      <name>Giovanna Varni</name>
    </author>
    <author>
      <name>Andrea Passerini</name>
    </author>
    <link href="http://arxiv.org/abs/2507.13305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13301v1</id>
    <updated>2025-07-17T17:11:07Z</updated>
    <published>2025-07-17T17:11:07Z</published>
    <title>mNARX+: A surrogate model for complex dynamical systems using
  manifold-NARX and automatic feature selection</title>
    <summary>  We propose an automatic approach for manifold nonlinear autoregressive with
exogenous inputs (mNARX) modeling that leverages the feature-based structure of
functional-NARX (F-NARX) modeling. This novel approach, termed mNARX+,
preserves the key strength of the mNARX framework, which is its expressivity
allowing it to model complex dynamical systems, while simultaneously addressing
a key limitation: the heavy reliance on domain expertise to identify relevant
auxiliary quantities and their causal ordering. Our method employs a
data-driven, recursive algorithm that automates the construction of the mNARX
model sequence. It operates by sequentially selecting temporal features based
on their correlation with the model prediction residuals, thereby automatically
identifying the most critical auxiliary quantities and the order in which they
should be modeled. This procedure significantly reduces the need for prior
system knowledge. We demonstrate the effectiveness of the mNARX+ algorithm on
two case studies: a Bouc-Wen oscillator with strong hysteresis and a complex
aero-servo-elastic wind turbine simulator. The results show that the algorithm
provides a systematic, data-driven method for creating accurate and stable
surrogate models for complex dynamical systems.
</summary>
    <author>
      <name>S. Sch√§r</name>
    </author>
    <author>
      <name>S. Marelli</name>
    </author>
    <author>
      <name>B. Sudret</name>
    </author>
    <link href="http://arxiv.org/abs/2507.13301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13255v1</id>
    <updated>2025-07-17T16:04:55Z</updated>
    <published>2025-07-17T16:04:55Z</published>
    <title>Automating Steering for Safe Multimodal Large Language Models</title>
    <summary>  Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.
</summary>
    <author>
      <name>Lyucheng Wu</name>
    </author>
    <author>
      <name>Mengru Wang</name>
    </author>
    <author>
      <name>Ziwen Xu</name>
    </author>
    <author>
      <name>Tri Cao</name>
    </author>
    <author>
      <name>Nay Oo</name>
    </author>
    <author>
      <name>Bryan Hooi</name>
    </author>
    <author>
      <name>Shumin Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working in progress. 22 pages (8+ for main); 25 figures; 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13249v1</id>
    <updated>2025-07-17T15:58:39Z</updated>
    <published>2025-07-17T15:58:39Z</published>
    <title>Comparing astrophysical models to gravitational-wave data in the
  observable space</title>
    <summary>  Comparing population-synthesis models to the results of hierarchical Bayesian
inference in gravitational-wave astronomy requires a careful understanding of
the domain of validity of the models fitted to data. This comparison is usually
done using the inferred astrophysical distribution: from the data that were
collected, one deconvolves selection effects to reconstruct the generating
population distribution. In this letter, we demonstrate the benefits of instead
comparing observable populations directly. In this approach, the domain of
validity of the models is trivially respected, such that only the relevant
parameter space regions as predicted by the astrophysical models of interest
contribute to the comparison. We clarify that unbiased inference of the
observable compact-binary population is indeed possible. Crucially, this
approach still requires incorporating selection effects, but in a manner that
differs from the standard implementation. We apply our observable-space
reconstruction to LIGO-Virgo-KAGRA data from their third observing run and
illustrate its potential by comparing the results to the predictions of a
fiducial population-synthesis model.
</summary>
    <author>
      <name>Alexandre Toubiana</name>
    </author>
    <author>
      <name>Davide Gerosa</name>
    </author>
    <author>
      <name>Matthew Mould</name>
    </author>
    <author>
      <name>Stefano Rinaldi</name>
    </author>
    <author>
      <name>Manuel Arca Sedda</name>
    </author>
    <author>
      <name>Tristan Bruel</name>
    </author>
    <author>
      <name>Riccardo Buscicchio</name>
    </author>
    <author>
      <name>Jonathan Gair</name>
    </author>
    <author>
      <name>Lavinia Paiella</name>
    </author>
    <author>
      <name>Filippo Santoliquido</name>
    </author>
    <author>
      <name>Rodrigo Tenorio</name>
    </author>
    <author>
      <name>Cristiano Ugolini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 11 with biblio and appendix, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
