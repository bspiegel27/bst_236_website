<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-25T00:53:06Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-24T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">121623</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.19296v1</id>
    <updated>2025-09-23T17:58:01Z</updated>
    <published>2025-09-23T17:58:01Z</published>
    <title>Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model
  Self-Distillation</title>
    <summary>  The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.
</summary>
    <author>
      <name>Sherwin Bahmani</name>
    </author>
    <author>
      <name>Tianchang Shen</name>
    </author>
    <author>
      <name>Jiawei Ren</name>
    </author>
    <author>
      <name>Jiahui Huang</name>
    </author>
    <author>
      <name>Yifeng Jiang</name>
    </author>
    <author>
      <name>Haithem Turki</name>
    </author>
    <author>
      <name>Andrea Tagliasacchi</name>
    </author>
    <author>
      <name>David B. Lindell</name>
    </author>
    <author>
      <name>Zan Gojcic</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <author>
      <name>Huan Ling</name>
    </author>
    <author>
      <name>Jun Gao</name>
    </author>
    <author>
      <name>Xuanchi Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.19296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19284v1</id>
    <updated>2025-09-23T17:50:54Z</updated>
    <published>2025-09-23T17:50:54Z</published>
    <title>What Characterizes Effective Reasoning? Revisiting Length, Review, and
  Structure of CoT</title>
    <summary>  Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.
</summary>
    <author>
      <name>Yunzhen Feng</name>
    </author>
    <author>
      <name>Julia Kempe</name>
    </author>
    <author>
      <name>Cheng Zhang</name>
    </author>
    <author>
      <name>Parag Jain</name>
    </author>
    <author>
      <name>Anthony Hartshorn</name>
    </author>
    <link href="http://arxiv.org/abs/2509.19284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19276v1</id>
    <updated>2025-09-23T17:41:43Z</updated>
    <published>2025-09-23T17:41:43Z</published>
    <title>A Gradient Flow Approach to Solving Inverse Problems with Latent
  Diffusion Models</title>
    <summary>  Solving ill-posed inverse problems requires powerful and flexible priors. We
propose leveraging pretrained latent diffusion models for this task through a
new training-free approach, termed Diffusion-regularized Wasserstein Gradient
Flow (DWGF). Specifically, we formulate the posterior sampling problem as a
regularized Wasserstein gradient flow of the Kullback-Leibler divergence in the
latent space. We demonstrate the performance of our method on standard
benchmarks using StableDiffusion (Rombach et al., 2022) as the prior.
</summary>
    <author>
      <name>Tim Y. J. Wang</name>
    </author>
    <author>
      <name>O. Deniz Akyildiz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 2nd Workshop on Frontiers in Probabilistic Inference:
  Sampling Meets Learning, 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.19276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19275v1</id>
    <updated>2025-09-23T17:40:49Z</updated>
    <published>2025-09-23T17:40:49Z</published>
    <title>A Novel Site-Specific Inference Model for Urban Canyon Channels: From
  Measurements to Modeling</title>
    <summary>  With the rapid development of intelligent transportation and smart city
applications, urban canyon has become a critical scenario for the design and
evaluation of wireless communication systems. Due to its unique environmental
layout, the channel characteristics in urban canyon are strongly a street
geometry and building distribution, thereby exhibiting significant
site-specific channel condition. However, this feature has not been well
captured in existing channel models. In this paper, we propose a site-specific
channel inference model based on environmental geometry, the model is
parameterized using sub-6GHz channel measurements. Multipath components (MPCs)
are extracted and clustered according to geometric propagation, which are
explicitly derived from the influence of canyon width, thereby establishing an
interpretable mapping between the physical environment and statistical
characteristics of MPCs. A step-by-step implementation scheme is presented.
Subsequently, the proposed site-specific channel inference model is validated
by comparing second-order statistics of channels, derived from the model and
measurements. The results show that the proposed model achieves high accuracy
and robustness in different urban canyon scenarios.
</summary>
    <author>
      <name>Junzhe Song</name>
    </author>
    <author>
      <name>Ruisi He</name>
    </author>
    <author>
      <name>Mi Yang</name>
    </author>
    <author>
      <name>Zhengyu Zhang</name>
    </author>
    <author>
      <name>Xinwen Chen</name>
    </author>
    <author>
      <name>Xiaoying Zhang</name>
    </author>
    <author>
      <name>Bo Ai</name>
    </author>
    <link href="http://arxiv.org/abs/2509.19275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19247v1</id>
    <updated>2025-09-23T17:08:30Z</updated>
    <published>2025-09-23T17:08:30Z</published>
    <title>Exploring the Sub-Neptune Frontier with JWST</title>
    <summary>  Sub-Neptune planets, with sizes and masses between those of Earth and
Neptune, dominate the exoplanet population. Sub-Neptunes are expected to be the
most diverse family of the exoplanet population, potentially including rocky
gas dwarfs, water worlds, and mini-Neptunes, with a wide range of atmospheric,
surface and interior conditions. With no analogue in the solar system, these
planets open fundamental questions in planetary processes, origins, and
habitability, and present new avenues in the search for life elsewhere.
Atmospheric observations with the James Webb Space Telescope (JWST) are
enabling unprecedented characterization of sub-Neptunes, starting with the
first detections of carbon-bearing molecules in the habitable zone sub-Neptune
K2-18 b. We survey the present landscape of JWST observations and atmospheric
inferences of sub-Neptunes, which in turn provide key insights into their
atmospheric processes, internal structures, surface conditions, formation
pathways and potential habitability. The atmospheric abundance constraints
reveal evidence of chemical disequilibria, and insights into the planetary
mass-metallicity relation in the sub-Neptune regime. Similarly, for
sub-Neptunes with H$_2$O-rich interiors, increasing atmospheric H$_2$O
abundances with the equilibrium temperature may indicate the existence of a
critical temperature for transition from H$_2$ dominated atmospheres with
tropospheric cold traps to those with steamy atmospheres. The chemical
abundances also provide initial evidence for diverse planet types, from
potentially habitable hycean worlds to steam worlds with super critical water
layers. These planet types serve as benchmarks for an emerging taxonomy of
volatile-rich sub-Neptunes as a function of their equilibrium temperature and
atmospheric extent, heralding a new era of chemical classification of low-mass
exoplanets with JWST.
</summary>
    <author>
      <name>Nikku Madhusudhan</name>
    </author>
    <author>
      <name>MÃ¥ns Holmberg</name>
    </author>
    <author>
      <name>Savvas Constantinou</name>
    </author>
    <author>
      <name>Gregory J. Cooke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in PNAS special feature on Characterization
  of Exoplanets in the JWST Era</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.19247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19244v2</id>
    <updated>2025-09-24T09:38:15Z</updated>
    <published>2025-09-23T17:05:46Z</published>
    <title>Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal
  Understanding and Generation</title>
    <summary>  We propose Lavida-O, a unified Masked Diffusion Model (MDM) for multimodal
understanding and generation. Unlike existing multimodal MDMs such as MMaDa and
Muddit which only support simple image-level understanding tasks and
low-resolution image generation, Lavida-O presents a single framework that
enables image-level understanding, object grounding, image editing, and
high-resolution (1024px) text-to-image synthesis. Lavida-O incorporates a novel
Elastic Mixture-of-Transformers (Elastic-MoT) architecture that couples a
lightweight generation branch with a larger understanding branch, supported by
token compression, universal text conditioning and stratified sampling for
efficient and high-quality generation. Lavida-O further incorporates planning
and iterative self-reflection in image generation and editing tasks, seamlessly
boosting generation quality with its understanding capabilities. Lavida-O
achieves state-of-the-art performance on a wide range of benchmarks including
RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image
editing, outperforming existing autoregressive models and continuous diffusion
models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable
speedup at inference. These advances establish Lavida-O as a new paradigm for
scalable multimodal reasoning and generation.
</summary>
    <author>
      <name>Shufan Li</name>
    </author>
    <author>
      <name>Jiuxiang Gu</name>
    </author>
    <author>
      <name>Kangning Liu</name>
    </author>
    <author>
      <name>Zhe Lin</name>
    </author>
    <author>
      <name>Zijun Wei</name>
    </author>
    <author>
      <name>Aditya Grover</name>
    </author>
    <author>
      <name>Jason Kuen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.19244v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19244v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19227v1</id>
    <updated>2025-09-23T16:49:25Z</updated>
    <published>2025-09-23T16:49:25Z</published>
    <title>MsFIN: Multi-scale Feature Interaction Network for Traffic Accident
  Anticipation</title>
    <summary>  With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.
</summary>
    <author>
      <name>Tongshuai Wu</name>
    </author>
    <author>
      <name>Chao Lu</name>
    </author>
    <author>
      <name>Ze Song</name>
    </author>
    <author>
      <name>Yunlong Lin</name>
    </author>
    <author>
      <name>Sizhe Fan</name>
    </author>
    <author>
      <name>Xuemei Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2509.19227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19215v1</id>
    <updated>2025-09-23T16:35:38Z</updated>
    <published>2025-09-23T16:35:38Z</published>
    <title>PPG-Distill: Efficient Photoplethysmography Signals Analysis via
  Foundation Model Distillation</title>
    <summary>  Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables
</summary>
    <author>
      <name>Juntong Ni</name>
    </author>
    <author>
      <name>Saurabh Kataria</name>
    </author>
    <author>
      <name>Shengpu Tang</name>
    </author>
    <author>
      <name>Carl Yang</name>
    </author>
    <author>
      <name>Xiao Hu</name>
    </author>
    <author>
      <name>Wei Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NeurIPS 2025 Workshop on Learning from Time Series for
  Health</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.19215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19180v1</id>
    <updated>2025-09-23T15:52:18Z</updated>
    <published>2025-09-23T15:52:18Z</published>
    <title>Bayesian Neural Networks versus deep ensembles for uncertainty
  quantification in machine learning interatomic potentials</title>
    <summary>  Neural-network-based machine learning interatomic potentials have emerged as
powerful tools for predicting atomic energies and forces, enabling accurate and
efficient simulations in atomistic modeling. A key limitation of traditional
deep learning approaches, however, is their inability to provide reliable
estimates of predictive uncertainty. Such uncertainty quantification is
critical for assessing model reliability, especially in materials science,
where often the model is applied on out-of-distribution data. Different
strategies have been proposed to address this challenge, with deep ensembles
and Bayesian neural networks being among the most widely used. In this work, we
introduce an implementation of Bayesian neural networks with variational
inference in the aenet-PyTorch framework. To evaluate their applicability to
machine learning interatomic potentials, we systematically compare the
performance of variational BNNs and deep ensembles on a dataset of 7,815
TiO$_{2}$ structures. The models are trained on both the full dataset and a
subset to assess how variations in data representation influence predictive
accuracy and uncertainty estimation. This analysis provides insights into the
strengths and limitations of each approach, offering practical guidance for the
development of uncertainty-aware machine learning interatomic potentials.
</summary>
    <author>
      <name>Riccardo Farris</name>
    </author>
    <author>
      <name>Emanuele Telari</name>
    </author>
    <author>
      <name>Nongnuch Artrith</name>
    </author>
    <author>
      <name>Konstantin Neyman</name>
    </author>
    <author>
      <name>Albert Bruix</name>
    </author>
    <link href="http://arxiv.org/abs/2509.19180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.19170v2</id>
    <updated>2025-09-24T11:28:42Z</updated>
    <published>2025-09-23T15:43:47Z</published>
    <title>Soft Tokens, Hard Truths</title>
    <summary>  The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.
</summary>
    <author>
      <name>Natasha Butt</name>
    </author>
    <author>
      <name>Ariel Kwiatkowski</name>
    </author>
    <author>
      <name>Ismail Labiad</name>
    </author>
    <author>
      <name>Julia Kempe</name>
    </author>
    <author>
      <name>Yann Ollivier</name>
    </author>
    <link href="http://arxiv.org/abs/2509.19170v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.19170v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
