<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-01-30T01:11:43Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-01-30T01:11:44Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>131996</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2601.20864v1</id>
    <title>Bottom-heavy initial mass functions reveal hidden mass in early galaxies</title>
    <updated>2026-01-28T18:59:58Z</updated>
    <link href="https://arxiv.org/abs/2601.20864v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20864v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>JWST observations have revealed that massive galaxies formed and evolved far faster than predicted by galaxy formation models, with many having already assembled a large mass in stars $\sim12$ billion years ago [1-7]. However, masses of distant galaxies are highly uncertain, as they assume a distribution of stellar birth masses (the initial mass function [IMF]) similar to that in the Milky Way (MW). Specifically, the contribution from low-mass stars, which make up the bulk of stellar mass, is not directly observed, but inferred based on an extrapolation of the MW IMF. Here, we provide the first robust measurements of the IMF beyond the local Universe. Using ultra-deep spectra of nine massive, quiescent galaxies at $z\sim0.7$ from the ambitious JWST-IMFERNO program, extended to bluer wavelengths with deep spectra from LEGA-C [8], we find that these distant galaxies have excess low-mass stars. In other words, they have more bottom-heavy IMFs than the MW. For the oldest two galaxies, which are direct descendants of JWST's "impossibly early" galaxies, the bottom-heavy IMFs increase their stellar masses by a factor of $3-4$. These galaxies thus amplify the tension with galaxy formation models.</summary>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T18:59:58Z</published>
    <arxiv:comment>29 pages, 7 figures, under peer review at Nature Astronomy</arxiv:comment>
    <arxiv:primary_category term="astro-ph.GA"/>
    <author>
      <name>Chloe M. Cheng</name>
    </author>
    <author>
      <name>Martje Slob</name>
    </author>
    <author>
      <name>Mariska Kriek</name>
    </author>
    <author>
      <name>Aliza G. Beverage</name>
    </author>
    <author>
      <name>Pieter G. van Dokkum</name>
    </author>
    <author>
      <name>Rachel Bezanson</name>
    </author>
    <author>
      <name>Gabriel Brammer</name>
    </author>
    <author>
      <name>Charlie Conroy</name>
    </author>
    <author>
      <name>Anna de Graaff</name>
    </author>
    <author>
      <name>Elham Eftekhari</name>
    </author>
    <author>
      <name>Robert Feldmann</name>
    </author>
    <author>
      <name>Wout M. Goesaert</name>
    </author>
    <author>
      <name>Meng Gu</name>
    </author>
    <author>
      <name>Joel Leja</name>
    </author>
    <author>
      <name>Brian Lorenz</name>
    </author>
    <author>
      <name>Pavel E. Mancera Piña</name>
    </author>
    <author>
      <name>Ignacio Martín-Navarro</name>
    </author>
    <author>
      <name>Andrew B. Newman</name>
    </author>
    <author>
      <name>Sedona H. Price</name>
    </author>
    <author>
      <name>Alice E. Shapley</name>
    </author>
    <author>
      <name>Piyush Sharda</name>
    </author>
    <author>
      <name>Katherine A. Suess</name>
    </author>
    <author>
      <name>Arjen van der Wel</name>
    </author>
    <author>
      <name>Daniel R. Weisz</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20819v1</id>
    <title>Demystifying Prediction Powered Inference</title>
    <updated>2026-01-28T18:16:02Z</updated>
    <link href="https://arxiv.org/abs/2601.20819v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20819v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from large unlabeled datasets to improve statistical efficiency while maintaining valid inference through explicit bias correction using a smaller labeled subset. Despite its potential, the growing PPI variants and the subtle distinctions between them have made it challenging for practitioners to determine when and how to apply these methods responsibly. This paper demystifies PPI by synthesizing its theoretical foundations, methodological extensions, connections to existing statistics literature, and diagnostic tools into a unified practical workflow. Using the Mosaiks housing price data, we show that PPI variants produce tighter confidence intervals than complete-case analysis, but that double-dipping, i.e. reusing training data for inference, leads to anti-conservative confidence intervals and coverages. Under missing-not-at-random mechanisms, all methods, including classical inference using only labeled data, yield biased estimates. We provide a decision flowchart linking assumption violations to appropriate PPI variants, a summary table of selective methods, and practical diagnostic strategies for evaluating core assumptions. By framing PPI as a general recipe rather than a single estimator, this work bridges methodological innovation and applied practice, helping researchers responsibly integrate predictions into valid inference.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T18:16:02Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Yilin Song</name>
    </author>
    <author>
      <name>Dan M. Kluger</name>
    </author>
    <author>
      <name>Harsh Parikh</name>
    </author>
    <author>
      <name>Tian Gu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20815v1</id>
    <title>GNN Explanations that do not Explain and How to find Them</title>
    <updated>2026-01-28T18:05:17Z</updated>
    <link href="https://arxiv.org/abs/2601.20815v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20815v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T18:05:17Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Steve Azzolin</name>
    </author>
    <author>
      <name>Stefano Teso</name>
    </author>
    <author>
      <name>Bruno Lepri</name>
    </author>
    <author>
      <name>Andrea Passerini</name>
    </author>
    <author>
      <name>Sagar Malhotra</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20809v1</id>
    <title>Joint estimation of the basic reproduction number and serial interval using Sequential Bayes</title>
    <updated>2026-01-28T17:58:20Z</updated>
    <link href="https://arxiv.org/abs/2601.20809v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20809v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Early in an infectious disease outbreak, timely and accurate estimation of the basic reproduction number ($R_0$) and the serial interval (SI) is critical for understanding transmission dynamics and informing public health responses. While many methods estimate these quantities separately, and a small number jointly estimate them from incidence data, existing joint approaches are largely likelihood-based and do not fully exploit prior information. We propose a novel Bayesian framework for the joint estimation of $R_0$ and the serial interval using only case count data, implemented through a sequential Bayes approach. Our method assumes an SIR model and employs a mildly informative joint prior constructed by linking log-Gamma marginal distributions for $R_0$ and the SI via a Gaussian copula, explicitly accounting for their dependence. The prior is updated sequentially as new incidence data become available, allowing for real-time inference. We assess the performance of the proposed estimator through extensive simulation studies under correct model specification as well as under model misspecification, including when the true data come from an SEIR or SEAIR model, and under varying degrees of prior misspecification. Comparisons with the widely used White and Pagano likelihood-based joint estimator show that our approach yields substantially more precise and stable estimates of $R_0$, with comparable or improved bias, particularly in the early stages of an outbreak. Estimation of the SI is more sensitive to prior misspecification; however, when prior information is reasonably accurate, our method provides reliable SI estimates and remains more stable than the competing approach. We illustrate the practical utility of the proposed method using Canadian COVID-19 incidence data at both national and provincial levels.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T17:58:20Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Tatiana Krikella</name>
    </author>
    <author>
      <name>Jane M. Heffernan</name>
    </author>
    <author>
      <name>Hanna Jankowski</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20784v1</id>
    <title>REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence</title>
    <updated>2026-01-28T17:17:21Z</updated>
    <link href="https://arxiv.org/abs/2601.20784v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20784v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.
  This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T17:17:21Z</published>
    <arxiv:comment>16 pages, 13 figures, 5 tables, 2026 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Zishen Wan</name>
    </author>
    <author>
      <name>Che-Kai Liu</name>
    </author>
    <author>
      <name>Jiayi Qian</name>
    </author>
    <author>
      <name>Hanchen Yang</name>
    </author>
    <author>
      <name>Arijit Raychowdhury</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20772v1</id>
    <title>COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI</title>
    <updated>2026-01-28T16:59:56Z</updated>
    <link href="https://arxiv.org/abs/2601.20772v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20772v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T16:59:56Z</published>
    <arxiv:comment>Preprint. Submitted to an IEEE conference. 6 pages, 6 figures, 2 tables</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shakhyar Gogoi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20755v1</id>
    <title>ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler</title>
    <updated>2026-01-28T16:39:38Z</updated>
    <link href="https://arxiv.org/abs/2601.20755v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20755v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T16:39:38Z</published>
    <arxiv:comment>Accepted in the 9th Annual Conference on Machine Learning and Systems (MLSys 2026)</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Bohua Zou</name>
    </author>
    <author>
      <name>Debayan Roy</name>
    </author>
    <author>
      <name>Dhimankumar Yogesh Airao</name>
    </author>
    <author>
      <name>Weihao Xu</name>
    </author>
    <author>
      <name>Binqi Sun</name>
    </author>
    <author>
      <name>Yutao Liu</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20725v1</id>
    <title>Comparing causal estimands from sequential nested versus single point target trials: A simulation study</title>
    <updated>2026-01-28T16:00:00Z</updated>
    <link href="https://arxiv.org/abs/2601.20725v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20725v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sequential nested trial (SNT) emulation is a powerful approach for maximizing precision and avoiding time-related biases. However, there exists little discussion about the implied causal estimands in comparison to a real-world single point trial. We used Monte Carlo simulation to compare treatment effect estimates from an SNT emulation that re-indexed patients annually and a SNT emulation with a treatment decision design to the estimates from a single point trial. We generated 5,000 cohorts of 5,000 people with 3 years of follow-up. For the single point trial, patients were randomized to initiate or not initiate treatment at Visit 1. For the SNT emulations, simulated patients could contribute up to two index dates. When disease severity did not modify the treatment effect, both SNT approaches returned treatment effect estimates identical to the single point trial. In the presence of treatment effect modification by disease severity, both SNT approaches returned treatment effect estimates that diverged from the single point trial even after confounding-adjustment. These findings underscore the difficulties of interpreting causal estimands from a SNT emulation: the target population does not correspond to a single time point trial. Such implications are important for communicating study results for evidence-based decision-making.</summary>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T16:00:00Z</published>
    <arxiv:comment>32 pages, 3 main figures, 3 supplemental figures,</arxiv:comment>
    <arxiv:primary_category term="stat.AP"/>
    <author>
      <name>Catherine Wiener</name>
    </author>
    <author>
      <name>Chase D. Latour</name>
    </author>
    <author>
      <name>Kathleen Hurwitz</name>
    </author>
    <author>
      <name>Xiaojuan Li</name>
    </author>
    <author>
      <name>Catherine R. Lesko</name>
    </author>
    <author>
      <name>Alexander Breskin</name>
    </author>
    <author>
      <name>M. Alan Brookhart</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20724v1</id>
    <title>Pricing Catastrophe: How Extreme Political Shocks Reprice Sovereign Risk, Beliefs, and Growth Expectations</title>
    <updated>2026-01-28T15:58:00Z</updated>
    <link href="https://arxiv.org/abs/2601.20724v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20724v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Extreme political shocks may reshape economies not only through contemporaneous disruption but by altering beliefs about the distribution of future states. We study how such belief ruptures affect the cost of capital, expectations, and macroeconomic dynamics, using the October 7, 2023 attack on Israel as a precisely timed shock. Leveraging monthly data from 2008 to 2025 and a donor pool of advanced economies, we estimate counterfactual paths using a matrix completion design with rolling-window cross-validation and placebo-based inference, corroborated by synthetic difference-in-differences. We document three core findings. First, long-horizon sovereign risk of Israel is persistently repriced. Ten-year yields and spreads relative to the United States rise sharply and remain elevated. Second, household welfare beliefs deteriorate durably, as reflected in consumer confidence. Third, medium-run momentum improves, captured by a strong rise in the OECD composite leading indicator. These patterns reveal risk-growth decoupling where tail-risk premia rise even as medium-horizon activity expectations strengthen. Our results highlight belief-driven channels as a central mechanism through which extreme ruptures shape macro-financial outcomes.</summary>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T15:58:00Z</published>
    <arxiv:primary_category term="econ.GN"/>
    <author>
      <name>Riste Ichev</name>
    </author>
    <author>
      <name>Rok Spruk</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.20723v1</id>
    <title>Distributed Learning over Noisy Communication Networks</title>
    <updated>2026-01-28T15:57:53Z</updated>
    <link href="https://arxiv.org/abs/2601.20723v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.20723v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study binary coordination games over graphs under log-linear learning when neighbor actions are conveyed through explicit noisy communication links. Each edge is modeled as either a binary symmetric channel (BSC) or a binary erasure channel (BEC). We analyze two operational regimes. For binary symmetric and binary erasure channels, we provide a structural characterization of the induced learning dynamics. In a fast-communication regime, agents update using channel-averaged payoffs; the resulting learning dynamics coincide with a Gibbs sampler for a scaled coordination potential, where channel reliability enters only through a scalar attenuation coefficient. In a snapshot regime, agents update from a single noisy realization and ignore channel statistics; the induced Markov chain is generally nonreversible, but admits a high-temperature expansion whose drift matches that of the fast Gibbs sampler with the same attenuation. We further formalize a finite-$K$ communication budget, which interpolates between snapshot and fast behavior as the number of channel uses per update grows. This viewpoint yields a communication-theoretic interpretation in terms of retransmissions and repetition coding, and extends naturally to heterogeneous link reliabilities via effective edge weights. Numerical experiments illustrate the theory and quantify the tradeoff between communication resources and steady-state coordination quality.</summary>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-28T15:57:53Z</published>
    <arxiv:comment>draft, submitted to IEEE JSAC Special Issue on Distributed Optimization, Learning, and Inference over Communication-Constrained Networks</arxiv:comment>
    <arxiv:primary_category term="eess.SY"/>
    <author>
      <name>Emrah Akyol</name>
    </author>
    <author>
      <name>Marcos Vasconcelos</name>
    </author>
  </entry>
</feed>
