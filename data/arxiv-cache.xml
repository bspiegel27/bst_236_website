<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-31T00:55:33Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-30T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">124962</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.25769v1</id>
    <updated>2025-10-29T17:59:06Z</updated>
    <published>2025-10-29T17:59:06Z</published>
    <title>Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE
  Solutions</title>
    <summary>  Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.
</summary>
    <author>
      <name>Naoki Kiyohara</name>
    </author>
    <author>
      <name>Edward Johns</name>
    </author>
    <author>
      <name>Yingzhen Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2025 (poster). Project page:
  https://nkiyohara.github.io/nsf-neurips2025/</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25743v1</id>
    <updated>2025-10-29T17:46:07Z</updated>
    <published>2025-10-29T17:46:07Z</published>
    <title>Agentic Economic Modeling</title>
    <summary>  We introduce Agentic Economic Modeling (AEM), a framework that aligns
synthetic LLM choices with small-sample human evidence for reliable econometric
inference. AEM first generates task-conditioned synthetic choices via LLMs,
then learns a bias-correction mapping from task features and raw LLM choices to
human-aligned choices, upon which standard econometric estimators perform
inference to recover demand elasticities and treatment effects.We validate AEM
in two experiments. In a large scale conjoint study with millions of
observations, using only 10% of the original data to fit the correction model
lowers the error of the demand-parameter estimates, while uncorrected LLM
choices even increase the errors. In a regional field experiment, a mixture
model calibrated on 10% of geographic regions estimates an out-of-domain
treatment effect of -65\pm10 bps, closely matching the full human experiment
(-60\pm8 bps).Under time-wise extrapolation, training with only day-one human
data yields -24 bps (95% CI: [-26, -22], p&lt;1e-5),improving over the human-only
day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results
demonstrate AEM's potential to improve RCT efficiency and establish a
foundation method for LLM-based counterfactual generation.
</summary>
    <author>
      <name>Bohan Zhang</name>
    </author>
    <author>
      <name>Jiaxuan Li</name>
    </author>
    <author>
      <name>Ali Horta√ßsu</name>
    </author>
    <author>
      <name>Xiaoyang Ye</name>
    </author>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Angelo Ni</name>
    </author>
    <author>
      <name>Edward Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.25743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25739v1</id>
    <updated>2025-10-29T17:43:31Z</updated>
    <published>2025-10-29T17:43:31Z</published>
    <title>Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image
  Generation</title>
    <summary>  Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.
</summary>
    <author>
      <name>Zhi-Kai Chen</name>
    </author>
    <author>
      <name>Jun-Peng Jiang</name>
    </author>
    <author>
      <name>Han-Jia Ye</name>
    </author>
    <author>
      <name>De-Chuan Zhan</name>
    </author>
    <link href="http://arxiv.org/abs/2510.25739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25709v1</id>
    <updated>2025-10-29T17:19:29Z</updated>
    <published>2025-10-29T17:19:29Z</published>
    <title>A new conformal gauge theory of vector-spinors and spin-3/2 particles</title>
    <summary>  The unique off-shell fermionic gauge invariance of a vector-spinor field
theory is found, and the invariant action is derived. The latter is Weyl
invariant in any dimension in the massless limit, and it coincides with the
singular point of the one-parameter family of Rarita-Schwinger Lagrangians.
Pure gauge configurations are represented by gamma-trace vector-spinors, which
can be gauged away in a global way. Previous claims that this theory is
inconsistent are shown to be flawed, and the Velo-Zwanziger instability is
proved to be absent. The theory propagates a massive spin-3/2 particle together
with a spin-1/2 state whose mass is twice that of the j=3/2 mode. This physical
prediction is derived in a consistent fashion both from the classical field
equations in d=4 and from the causal construction of a gamma-traceless quantum
vector-spinor field. The conformal anomaly is derived using known results for
the heat kernel of non-minimal second-order operators, and it is shown that the
a-anomaly has an opposite sign w.r.t. known results for lower spin fields.
</summary>
    <author>
      <name>Dario Sauro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25706v1</id>
    <updated>2025-10-29T17:17:14Z</updated>
    <published>2025-10-29T17:17:14Z</published>
    <title>Cosmological Constraints from Dark Energy Survey Year 1 Cluster Lensing
  and Abundances with Simulation-based Forward-Modeling</title>
    <summary>  We present a simulation-based forward-modeling framework for cosmological
inference from optical galaxy-cluster samples, and apply it to the abundance
and weak-lensing signals of DES-Y1 redMaPPer clusters. The model embeds
cosmology-dependent optical selection using a counts-in-cylinders approach,
while also accounting for cluster miscentering and baryonic feedback in
lensing. Applied to DES-Y1, and assuming a flat $\Lambda$CDM cosmology, we
obtain $\Omega_m=0.254^{+0.026}_{-0.020}$ and
$\sigma_8=0.826^{+0.030}_{-0.034}$, consistent with a broad suite of
low-redshift structure measurements, including recent full-shape analyses, the
DES/KiDS/HSC 3$\times$2 results, and most cluster-abundance studies. Our
results are also consistent with \textit{Planck}, with the difference being
significant at $2.58\sigma$. These results establish simulation-based
forward-modeling of cluster abundances as a promising new tool for precision
cosmology with Stage~IV survey data.
</summary>
    <author>
      <name>Andr√©s N. Salcedo</name>
    </author>
    <author>
      <name>Eduardo Rozo</name>
    </author>
    <author>
      <name>Hao-Yi Wu</name>
    </author>
    <author>
      <name>David H. Weinberg</name>
    </author>
    <author>
      <name>Pranav Chiploonkar</name>
    </author>
    <author>
      <name>Chun-Hao To</name>
    </author>
    <author>
      <name>Shulei Cao</name>
    </author>
    <author>
      <name>Eli S. Rykoff</name>
    </author>
    <author>
      <name>Nicole Marcelina Gountanis</name>
    </author>
    <author>
      <name>Conghao Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 8 main text figures and 1 table, 2 appendix figures and 1
  table, submitted to PRD</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25700v1</id>
    <updated>2025-10-29T17:04:34Z</updated>
    <published>2025-10-29T17:04:34Z</published>
    <title>Timelike Holographic Complexity</title>
    <summary>  Motivated by the pseudo-entropy program, we investigate timelike subregion
complexity within the holographic ``Complexity=Volume'' framework, extending
the usual spatial constructions to Lorentzian boundary intervals. For
hyperbolic timelike regions in AdS geometries, we compute the corresponding
bulk volumes and demonstrate that, despite the Lorentzian embedding, the
resulting subregion complexity remains purely real. We further generalize our
analysis to AdS black brane geometries, where the extremal surfaces can either
be constant-time hypersurfaces or penetrate the horizon. In all cases, the
computed complexity exhibits the same universal UV divergences as in the
spacelike case but shows no imaginary contribution, underscoring its causal and
geometric origin. This stands in sharp contrast with the complex-valued
pseudo-entropy and suggests that holographic complexity preserves a genuinely
geometric and real character even under Lorentzian continuation.
</summary>
    <author>
      <name>Mohsen Alishahiha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25697v2</id>
    <updated>2025-10-30T12:22:50Z</updated>
    <published>2025-10-29T17:01:51Z</published>
    <title>Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related
  to Metal Casting</title>
    <summary>  We formulate mold filling in metal casting as a 2D neural operator learning
problem that maps geometry and boundary data on an unstructured mesh to time
resolved flow quantities, replacing expensive transient CFD. In the proposed
method, a graph based encoder aggregates local neighborhood information on the
input mesh and encodes geometry and boundary data, a Fourier spectral core
operates on a regular latent grid to capture global interactions across the
domain, and a graph based decoder projects the latent fields to a target mesh.
The model is trained to jointly predict velocity components, pressure, and
liquid volume fraction over a fixed rollout horizon and generalizes across
different ingate locations and process settings. On held out geometries and
inlet conditions, it reproduces large scale advection and the fluid-air
interface evolution with localized errors near steep gradients. The mean
relative L2 error is about 5% across all fields, and inference is two to three
orders of magnitude faster than conventional CFD, enabling design in the loop
exploration. Ablation studies show monotonic accuracy degradation under
stronger spatial subsampling of input vertices and a smoother decline under
temporal subsampling. Halving the training set yields only a small increase in
error. These results establish neural operators as accurate and data efficient
surrogates for 2D mold filling and enable rapid optimization of gating systems
in casting workflows.
</summary>
    <author>
      <name>Edgard Moreira Minete</name>
    </author>
    <author>
      <name>Mathis Immertreu</name>
    </author>
    <author>
      <name>Fabian Teichmann</name>
    </author>
    <author>
      <name>Sebastian M√ºller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 figures, 5 tables, 63 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3, J.2, I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25683v1</id>
    <updated>2025-10-29T16:47:24Z</updated>
    <published>2025-10-29T16:47:24Z</published>
    <title>Graph Network-based Structural Simulator: Graph Neural Networks for
  Structural Dynamics</title>
    <summary>  Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.
</summary>
    <author>
      <name>Alessandro Lucchetti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Politecnico di Milano, Department of Mechanical Engineering, Milano, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Francesco Cadini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Politecnico di Milano, Department of Mechanical Engineering, Milano, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Giglio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Politecnico di Milano, Department of Mechanical Engineering, Milano, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Luca Lomazzi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Politecnico di Milano, Department of Mechanical Engineering, Milano, Italy</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25677v1</id>
    <updated>2025-10-29T16:43:07Z</updated>
    <published>2025-10-29T16:43:07Z</published>
    <title>ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective
  Abstention and Zero-Knowledge Attestation</title>
    <summary>  ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a
large-model encoder for Wi-Fi channel state information (and optionally mmWave
radar or RFID) with a policy-grounded decision layer and end-to-end
zero-knowledge proofs of inference. The encoder uses masked spectral
pretraining with phase-consistency regularization, plus a light cross-modal
alignment that ties RF features to compact, human-interpretable policy tokens.
To reduce unsafe actions under distribution shift, we add a calibrated
selective-abstention head; the chosen risk-coverage operating point is
registered and bound into the proof. We implement a four-stage proving
pipeline: (C1) feature sanity and commitment, (C2) threshold and version
binding, (C3) time-window binding, and (C4) PLONK-style proofs that the
quantized network, given the committed window, produced the logged action and
confidence. Micro-batched proving amortizes cost across adjacent windows, and a
gateway option offloads proofs from low-power devices. The system integrates
with differentially private federated learning and on-device personalization
without weakening verifiability: model hashes and the registered threshold are
part of each public statement. Across activity, presence or intrusion,
respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1
and calibration, yields favorable coverage-risk curves under perturbations, and
rejects tamper and replay with compact proofs and fast verification.
</summary>
    <author>
      <name>Hasan Akgul</name>
    </author>
    <author>
      <name>Mari Eplik</name>
    </author>
    <author>
      <name>Javier Rojas</name>
    </author>
    <author>
      <name>Aina Binti Abdullah</name>
    </author>
    <author>
      <name>Pieter van der Merwe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1; D.4.6; E.3; I.2.6; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.25674v1</id>
    <updated>2025-10-29T16:42:07Z</updated>
    <published>2025-10-29T16:42:07Z</published>
    <title>Mechanistic Interpretability of RNNs emulating Hidden Markov Models</title>
    <summary>  Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.
</summary>
    <author>
      <name>Elia Torre</name>
    </author>
    <author>
      <name>Michele Viscione</name>
    </author>
    <author>
      <name>Lucas Pompe</name>
    </author>
    <author>
      <name>Benjamin F Grewe</name>
    </author>
    <author>
      <name>Valerio Mante</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NeurIPS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.25674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.25674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
