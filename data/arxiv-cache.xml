<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-01T00:59:32Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-30T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">122222</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.25131v1</id>
    <updated>2025-09-29T17:48:28Z</updated>
    <published>2025-09-29T17:48:28Z</published>
    <title>MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech</title>
    <summary>  We present MGM-Omni, a unified Omni LLM for omni-modal understanding and
expressive, long-horizon speech generation. Unlike cascaded pipelines that
isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a
dual-track, token-based architecture that cleanly decouples multimodal
reasoning from real-time speech generation. This design enables efficient
cross-modal interaction and low-latency, streaming speech generation. For
understanding, a unified training strategy coupled with a dual audio encoder
design enables long-form audio perception across diverse acoustic conditions.
For generation, a chunk-based parallel decoding scheme narrows the text speech
token-rate gap, accelerating inference and supporting streaming zero-shot voice
cloning with stable timbre over extended durations. Compared to concurrent
work, MGM-Omni achieves these capabilities with markedly data-efficient
training. Extensive experiments demonstrate that MGM-Omni outperforms existing
open source models in preserving timbre identity across extended sequences,
producing natural and context-aware speech, and achieving superior long-form
audio and omnimodal understanding. MGM-Omni establishes an efficient,
end-to-end paradigm for omnimodal understanding and controllable, personalised
long-horizon speech generation.
</summary>
    <author>
      <name>Chengyao Wang</name>
    </author>
    <author>
      <name>Zhisheng Zhong</name>
    </author>
    <author>
      <name>Bohao Peng</name>
    </author>
    <author>
      <name>Senqiao Yang</name>
    </author>
    <author>
      <name>Yuqi Liu</name>
    </author>
    <author>
      <name>Haokun Gui</name>
    </author>
    <author>
      <name>Bin Xia</name>
    </author>
    <author>
      <name>Jingyao Li</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Jiaya Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at https://github.com/dvlab-research/MGM-Omni</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.25131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25123v1</id>
    <updated>2025-09-29T17:44:27Z</updated>
    <published>2025-09-29T17:44:27Z</published>
    <title>From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by
  Composing Old Ones</title>
    <summary>  Does RL teach LLMs genuinely new skills, or does it merely activate existing
ones? This question lies at the core of ongoing debates about the role of RL in
LLM post-training. On one side, strong empirical results can be achieved with
RL even without preceding supervised finetuning; on the other, critics argue
that RL contributes little beyond reweighting existing reasoning strategies.
This work provides concrete evidence that LLMs can acquire genuinely new skills
during RL by composing existing ones, mirroring one of the central mechanisms
by which humans acquire new cognitive skills. To mitigate data contamination
and other confounding factors, and to allow precise control over task
complexity, we develop a synthetic framework for our investigation.
Specifically, we define a skill as the ability to infer the output of a string
transformation function f(x) given x. When an LLM has already learned f and g
prior to RL, our experiments reveal that RL enables it to learn unseen
compositions of them h(x)=g(f(x)). Further, this compositional ability
generalizes to more difficult problems such as compositions of &gt;2 functions
unseen during RL training. Surprisingly, our experiments show that
compositional skill acquired on a source task transfers to a different target
task. This transfer happens even without compositional training on the target,
requiring only prior knowledge of the target's atomic skills. Our qualitative
analysis shows that RL fundamentally changes the reasoning behaviors of the
models. In contrast, next-token training with the same data yields none of
these findings. Our systematic experiments provide fresh insights into LLM
learning, suggesting the value of first building base models with basic skills,
then using RL to incentivize advanced, generalizable skills for complex
problems.
</summary>
    <author>
      <name>Lifan Yuan</name>
    </author>
    <author>
      <name>Weize Chen</name>
    </author>
    <author>
      <name>Yuchen Zhang</name>
    </author>
    <author>
      <name>Ganqu Cui</name>
    </author>
    <author>
      <name>Hanbin Wang</name>
    </author>
    <author>
      <name>Ziming You</name>
    </author>
    <author>
      <name>Ning Ding</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <author>
      <name>Hao Peng</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25121v1</id>
    <updated>2025-09-29T17:43:32Z</updated>
    <published>2025-09-29T17:43:32Z</published>
    <title>Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs</title>
    <summary>  Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as
unstructured graphs, achieving state of the art performance in computer vision
tasks such as image classification, object detection, and instance
segmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by
connecting patches (nodes) based on feature similarity, and is dynamically
repeated in each ViG layer following GNN based patch (node) feature updates.
However, DIGC constitutes over 50% of end to end ViG inference latency, rising
to 95% at high image resolutions, making it the dominant computational
bottleneck. While hardware acceleration holds promise, prior works primarily
optimize graph construction algorithmically, often compromising DIGC
flexibility, accuracy, or generality. To address these limitations, we propose
a streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip
buffers that process input features in small, uniform blocks. Our design
minimizes external memory traffic via localized computation and performs
efficient parallel sorting with local merge sort and global k way merging
directly on streaming input blocks via heap insertion. This modular
architecture scales seamlessly across image resolutions, ViG layer types, and
model sizes and variants, and supports DIGC across diverse ViG based vision
backbones. The design achieves high clock frequencies post place and route due
to the statically configured parallelism minimizing critical path delay and
delivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC
baselines.
</summary>
    <author>
      <name>Anvitha Ramachandran</name>
    </author>
    <author>
      <name>Dhruv Parikh</name>
    </author>
    <author>
      <name>Viktor Prasanna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE HPEC 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.25121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25110v1</id>
    <updated>2025-09-29T17:39:51Z</updated>
    <published>2025-09-29T17:39:51Z</published>
    <title>gCAMB: A GPU-accelerated Boltzmann solver for next-generation
  cosmological surveys</title>
    <summary>  Inferring cosmological parameters from Cosmic Microwave Background (CMB) data
requires repeated and computationally expensive calculations of theoretical
angular power spectra using Boltzmann solvers like CAMB. This creates a
significant bottleneck, particularly for non-standard cosmological models and
the high-accuracy demands of future surveys. While emulators based on deep
neural networks can accelerate this process by several orders of magnitude,
they first require large, pre-computed training datasets, which are costly to
generate and model-specific. To address this challenge, we introduce gCAMB, a
version of the CAMB code ported to GPUs, which preserves all the features of
the original CPU-only code. By offloading the most computationally intensive
modules to the GPU, gCAMB significantly accelerates the generation of power
spectra, saving massive computational time, halving the power consumption in
high-accuracy settings and, among other purposes, facilitating the creation of
extensive training sets needed for robust cosmological analyses. We make the
gCAMB software available to the community at
https://github.com/lstorchi/CAMB/tree/gpuport.
</summary>
    <author>
      <name>L. Storchi</name>
    </author>
    <author>
      <name>P. Campeti</name>
    </author>
    <author>
      <name>M. Lattanzi</name>
    </author>
    <author>
      <name>N. Antonini</name>
    </author>
    <author>
      <name>E. Calore</name>
    </author>
    <author>
      <name>P. Lubrano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code available at https://github.com/lstorchi/CAMB/tree/gpuport.
  Submitted to Astronomy &amp; Computing. 7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.25110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25096v1</id>
    <updated>2025-09-29T17:30:47Z</updated>
    <published>2025-09-29T17:30:47Z</published>
    <title>Estimating high-resolution albedo for urban applications</title>
    <summary>  Implementation of cool roofs is a high-impact pathway for mitigating heat at
both global and city scales. However, while albedo estimates derived from
Sentinel-2 are free and globally-available, the 10 m resolution is insufficient
to resolve individual roofs. We present methods for increasing the resolution
of Sentinel-2 albedo using high-resolution satellite imagery to produce albedo
inferences at a 30-cm scale. Validating against high-resolution aerial albedo
measurements over Boulder, CO we find improved precision and accuracy relative
to Sentinel-2 with an RMSE of 0.04. Applying these methods to 12 global cities,
we evaluate the impacts of three cool roof implementation scenarios. We find
that cities can see up to a 0.5{\deg}C cooling effect from full scale
implementation of cool roofs and prioritizing the largest buildings for
implementation is a highly effective policy pathway. While Sentinel-2 produces
accurate estimates of albedo change at larger scales, high-resolution
inferences are required for prioritizing buildings based on their solar
radiation management potential. This research demonstrates a scalable
implementation of targeted cool roof interventions in neighborhoods with the
greatest potential for heat mitigation by enabling actionable, building-level
insights.
</summary>
    <author>
      <name>David Fork</name>
    </author>
    <author>
      <name>Elizabeth Jane Wesley</name>
    </author>
    <author>
      <name>Salil Banerjee</name>
    </author>
    <author>
      <name>Vishal Batchu</name>
    </author>
    <author>
      <name>Aniruddh Chennapragada</name>
    </author>
    <author>
      <name>Kevin Crossan</name>
    </author>
    <author>
      <name>Bryce Cronkite-Ratcliff</name>
    </author>
    <author>
      <name>Ellie Delich</name>
    </author>
    <author>
      <name>Tristan Goulden</name>
    </author>
    <author>
      <name>Mansi Kansal</name>
    </author>
    <author>
      <name>Jonas Kemp</name>
    </author>
    <author>
      <name>Eric Mackres</name>
    </author>
    <author>
      <name>Yael Mayer</name>
    </author>
    <author>
      <name>Becca Milman</name>
    </author>
    <author>
      <name>John C. Platt</name>
    </author>
    <author>
      <name>Shruthi Prabhakara</name>
    </author>
    <author>
      <name>Gautam Prasad</name>
    </author>
    <author>
      <name>Shravya Shetty</name>
    </author>
    <author>
      <name>Charlotte Stanton</name>
    </author>
    <author>
      <name>Wayne Sun</name>
    </author>
    <author>
      <name>Lucy R. Hutyra</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25085v1</id>
    <updated>2025-09-29T17:23:54Z</updated>
    <published>2025-09-29T17:23:54Z</published>
    <title>jina-reranker-v3: Last but Not Late Interaction for Document Reranking</title>
    <summary>  jina-reranker-v3 is a 0.6B parameter multilingual document reranker that
introduces a novel last but not late interaction. Unlike late interaction
models such as ColBERT that perform separate encoding followed by multi-vector
matching, our approach conducts causal self-attention between query and
documents within the same context window, enabling rich cross-document
interactions before extracting contextual embeddings from the last token of
each document. This compact architecture achieves state-of-the-art BEIR
performance with 61.94 nDCG@10 while being ten times smaller than generative
listwise rerankers.
</summary>
    <author>
      <name>Feng Wang</name>
    </author>
    <author>
      <name>Yuqing Li</name>
    </author>
    <author>
      <name>Han Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">early draft, CoIR table needs to be updated</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.25085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25072v1</id>
    <updated>2025-09-29T17:16:51Z</updated>
    <published>2025-09-29T17:16:51Z</published>
    <title>Optimizing Privacy-Preserving Primitives to Support LLM-Scale
  Applications</title>
    <summary>  Privacy-preserving technologies have introduced a paradigm shift that allows
for realizable secure computing in real-world systems. The significant barrier
to the practical adoption of these primitives is the computational and
communication overhead that is incurred when applied at scale. In this paper,
we present an overview of our efforts to bridge the gap between this overhead
and practicality for privacy-preserving learning systems using multi-party
computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic
encryption (FHE). Through meticulous hardware/software/algorithm co-design, we
show progress towards enabling LLM-scale applications in privacy-preserving
settings. We demonstrate the efficacy of our solutions in several contexts,
including DNN IP ownership, ethical LLM usage enforcement, and transformer
inference.
</summary>
    <author>
      <name>Yaman Jandali</name>
    </author>
    <author>
      <name>Ruisi Zhang</name>
    </author>
    <author>
      <name>Nojan Sheybani</name>
    </author>
    <author>
      <name>Farinaz Koushanfar</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25065v1</id>
    <updated>2025-09-29T17:13:08Z</updated>
    <published>2025-09-29T17:13:08Z</published>
    <title>Cause-and-effect approach to turbulence forecasting</title>
    <summary>  Traditional approaches to turbulence forecasting often rely on
correlation-based criteria for input selection. These methods may select
variables that correlate with the target without truly driving its dynamics,
which limits interpretability, generalization, and efficiency. In this work, we
introduce a causality-based approach for input selection in turbulence
forecasting based on the Synergistic-Unique-Redundant Decomposition (SURD) of
causality. This method decomposes the information from candidate inputs into
unique, redundant, and synergistic causal contributions and links them to the
fundamental limits of predictive accuracy achievable by any model. In practice,
we implement the approach using neural mutual-information estimators and
demonstrate its application to wall-shear-stress forecasting from direct
numerical simulation data of turbulent channel flow. Our findings show that
input variables with strong unique or synergistic causal contributions enable
compact forecasting models with high predictive power, whereas redundant
variables can be excluded without degrading accuracy. We first validate these
capabilities in two benchmark cases involving collider effects, and then apply
the methodology to three turbulent flow configurations with different
interaction types. In each case, we demonstrate how SURD causalities guide
optimal input selection by constructing forecasting models based on various
input combinations. We also compare the results with standard space-time
correlation analysis and show that SURD provides a more reliable basis for
input selection, as it captures nonlinear dependencies, distinguishes
redundant, unique, and synergistic interactions, and remains invariant under
invertible transformations of the variables. Overall, we believe this enables
more interpretable and compact models by reducing input dimensionality without
sacrificing performance.
</summary>
    <author>
      <name>Álvaro Martínez-Sánchez</name>
    </author>
    <author>
      <name>Adrián Lozano-Durán</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25063v1</id>
    <updated>2025-09-29T17:12:18Z</updated>
    <published>2025-09-29T17:12:18Z</published>
    <title>Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for
  Survey Non-response in the German Longitudinal Election Study</title>
    <summary>  Survey researchers face two key challenges: the rising costs of probability
samples and missing data (e.g., non-response or attrition), which can undermine
inference and increase the use of convenience samples. Recent work explores
using large language models (LLMs) to simulate respondents via persona-based
prompts, often without labeled data. We study a more practical setting where
partial survey responses exist: we fine-tune LLMs on available data to impute
self-reported vote choice under both random and systematic nonresponse, using
the German Longitudinal Election Study. We compare zero-shot prompting and
supervised fine-tuning against tabular classifiers (e.g., CatBoost) and test
how different convenience samples (e.g., students) used for fine-tuning affect
generalization.
  Our results show that when data are missing completely at random, fine-tuned
LLMs match tabular classifiers but outperform zero-shot approaches. When only
biased convenience samples are available, fine-tuning small (3B to 8B)
open-source LLMs can recover both individual-level predictions and
population-level distributions more accurately than zero-shot and often better
than tabular methods. This suggests fine-tuned LLMs offer a promising strategy
for researchers working with non-probability samples or systematic missingness,
and may enable new survey designs requiring only easily accessible
subpopulations.
</summary>
    <author>
      <name>Tobias Holtdirk</name>
    </author>
    <author>
      <name>Dennis Assenmacher</name>
    </author>
    <author>
      <name>Arnim Bleier</name>
    </author>
    <author>
      <name>Claudia Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25045v1</id>
    <updated>2025-09-29T16:59:07Z</updated>
    <published>2025-09-29T16:59:07Z</published>
    <title>Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic
  Architectures</title>
    <summary>  Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.
</summary>
    <author>
      <name>Marco Bronzini</name>
    </author>
    <author>
      <name>Carlo Nicolini</name>
    </author>
    <author>
      <name>Bruno Lepri</name>
    </author>
    <author>
      <name>Jacopo Staiano</name>
    </author>
    <author>
      <name>Andrea Passerini</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
