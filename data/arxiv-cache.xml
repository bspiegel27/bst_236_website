<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-02-02T01:15:51Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-02-02T01:15:51Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>132191</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2601.22157v1</id>
    <title>Discovering Hidden Gems in Model Repositories</title>
    <updated>2026-01-29T18:59:55Z</updated>
    <link href="https://arxiv.org/abs/2601.22157v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22157v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:59:55Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jonathan Kahana</name>
    </author>
    <author>
      <name>Eliahu Horwitz</name>
    </author>
    <author>
      <name>Yedid Hoshen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22156v1</id>
    <title>Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</title>
    <updated>2026-01-29T18:59:53Z</updated>
    <link href="https://arxiv.org/abs/2601.22156v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22156v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:59:53Z</published>
    <arxiv:comment>20 pages, 8 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yingfa Chen</name>
    </author>
    <author>
      <name>Zhen Leng Thai</name>
    </author>
    <author>
      <name>Zihan Zhou</name>
    </author>
    <author>
      <name>Zhu Zhang</name>
    </author>
    <author>
      <name>Xingyu Shen</name>
    </author>
    <author>
      <name>Shuo Wang</name>
    </author>
    <author>
      <name>Chaojun Xiao</name>
    </author>
    <author>
      <name>Xu Han</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22153v1</id>
    <title>DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</title>
    <updated>2026-01-29T18:59:51Z</updated>
    <link href="https://arxiv.org/abs/2601.22153v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22153v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:59:51Z</published>
    <arxiv:comment>Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Haozhe Xie</name>
    </author>
    <author>
      <name>Beichen Wen</name>
    </author>
    <author>
      <name>Jiarui Zheng</name>
    </author>
    <author>
      <name>Zhaoxi Chen</name>
    </author>
    <author>
      <name>Fangzhou Hong</name>
    </author>
    <author>
      <name>Haiwen Diao</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22132v1</id>
    <title>Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</title>
    <updated>2026-01-29T18:52:54Z</updated>
    <link href="https://arxiv.org/abs/2601.22132v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22132v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:52:54Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ziming Dong</name>
    </author>
    <author>
      <name>Hardik Sharma</name>
    </author>
    <author>
      <name>Evan O'Toole</name>
    </author>
    <author>
      <name>Jaya Prakash Champati</name>
    </author>
    <author>
      <name>Kui Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22123v1</id>
    <title>Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics</title>
    <updated>2026-01-29T18:47:46Z</updated>
    <link href="https://arxiv.org/abs/2601.22123v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22123v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:47:46Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Winfried Ripken</name>
    </author>
    <author>
      <name>Michael Plainer</name>
    </author>
    <author>
      <name>Gregor Lied</name>
    </author>
    <author>
      <name>Thorben Frank</name>
    </author>
    <author>
      <name>Oliver T. Unke</name>
    </author>
    <author>
      <name>Stefan Chmiela</name>
    </author>
    <author>
      <name>Frank Noé</name>
    </author>
    <author>
      <name>Klaus Robert Müller</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22114v1</id>
    <title>SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</title>
    <updated>2026-01-29T18:41:52Z</updated>
    <link href="https://arxiv.org/abs/2601.22114v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22114v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:41:52Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Saoud Aldowaish</name>
    </author>
    <author>
      <name>Yashwanth Karumanchi</name>
    </author>
    <author>
      <name>Kai-Chen Chiang</name>
    </author>
    <author>
      <name>Soroosh Noorzad</name>
    </author>
    <author>
      <name>Morteza Fayazi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22106v1</id>
    <title>Information-geometry-driven graph sequential growth</title>
    <updated>2026-01-29T18:37:08Z</updated>
    <link href="https://arxiv.org/abs/2601.22106v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22106v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We investigate the properties of a class of regularisation-free approaches for Gaussian graphical inference based on the information-geometry-driven sequential growth of initially edgeless graphs. Relating the growth of a graph to a coordinate descent process, we characterise the fully-corrective descents corresponding to information-optimal growths, and propose numerically efficient strategies for their approximation. We demonstrate the ability of the proposed procedures to reliably extract sparse graphical models while limiting the number of false detections, and illustrate how activation ranks can provide insight into the informational relevance of edge sets. The considered approaches are tuning-parameter-free and have complexities akin to coordinate descents.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:37:08Z</published>
    <arxiv:comment>23 pages, 10 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Harry T. Bond</name>
    </author>
    <author>
      <name>Bertrand Gauthier</name>
    </author>
    <author>
      <name>Kirstin Strokorb</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22093v1</id>
    <title>Investigating Associational Biases in Inter-Model Communication of Large Generative Models</title>
    <updated>2026-01-29T18:29:55Z</updated>
    <link href="https://arxiv.org/abs/2601.22093v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22093v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:29:55Z</published>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Fethiye Irmak Dogan</name>
    </author>
    <author>
      <name>Yuval Weiss</name>
    </author>
    <author>
      <name>Kajal Patel</name>
    </author>
    <author>
      <name>Jiaee Cheong</name>
    </author>
    <author>
      <name>Hatice Gunes</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22079v1</id>
    <title>The Economics of No-regret Learning Algorithms</title>
    <updated>2026-01-29T18:19:22Z</updated>
    <link href="https://arxiv.org/abs/2601.22079v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22079v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A fundamental challenge for modern economics is to understand what happens when actors in an economy are replaced with algorithms. Like rationality has enabled understanding of outcomes of classical economic actors, no-regret can enable the understanding of outcomes of algorithmic actors. This review article covers the classical computer science literature on no-regret algorithms to provide a foundation for an overview of the latest economics research on no-regret algorithms, focusing on the emerging topics of manipulation, statistical inference, and algorithmic collusion.</summary>
    <category term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:19:22Z</published>
    <arxiv:comment>Accepted to Advances in Economics and Econometrics: Thirteenth World Congress, Volume 2</arxiv:comment>
    <arxiv:primary_category term="econ.TH"/>
    <author>
      <name>Jason Hartline</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.22076v1</id>
    <title>Where Do the Joules Go? Diagnosing Inference Energy Consumption</title>
    <updated>2026-01-29T18:16:45Z</updated>
    <link href="https://arxiv.org/abs/2601.22076v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.22076v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-29T18:16:45Z</published>
    <arxiv:comment>The ML.ENERGY Leaderboard v3.0 is open https://ml.energy/leaderboard</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jae-Won Chung</name>
    </author>
    <author>
      <name>Ruofan Wu</name>
    </author>
    <author>
      <name>Jeff J. Ma</name>
    </author>
    <author>
      <name>Mosharaf Chowdhury</name>
    </author>
  </entry>
</feed>
