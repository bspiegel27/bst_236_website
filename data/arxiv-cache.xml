<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-22T01:01:37Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-21T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117187</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.14136v1</id>
    <updated>2025-07-18T17:59:24Z</updated>
    <published>2025-07-18T17:59:24Z</published>
    <title>Missing baryons recovered: a measurement of the gas fraction in galaxies
  and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing</title>
    <summary>  We present new constraints on the halo masses and matter density profiles of
DESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs)
and Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing
convergence map from ACT DR6. This provides an independent, lensing-based
calibration of halo masses, complementary to methods relying on clustering or
dynamics. We derive constraints on the mean halo mass for three DESI-selected
samples, finding $\log(M_{\rm halo}/(M_\odot/h)) \approx 13.18$, 13.03 and
13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a
halo model approach, we also compare the projected galaxy-matter density
profiles with previously reported gas profiles inferred from measurements of
the kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the
key uncertainties in interpreting kSZ signals -- the unknown host halo mass
distribution -- by providing an independent and consistent mass calibration.
The agreement between the gas and total mass profiles at large aperture
suggests that sufficiently far from the group center (2--3 virial radii), we
recover all the baryons, offering a resolution to the 'missing baryon' problem.
We further study the cumulative gas fractions for all galaxies as well as for
the most massive galaxy groups in the sample ($\log(M_{\rm halo}/(M_\odot/h))
\approx 13.5$), finding values that are physically sensible and in agreement
with previous findings using kSZ and X-ray data: compared to the TNG300
simulation, the observed gas fractions are systematically lower at fixed radius
by $\gtrsim$4$\sigma$, providing compelling, independent evidence for stronger
baryonic feedback in the real Universe. These findings highlight the power of
combining CMB lensing with galaxy surveys to probe the interplay between
baryons and dark matter in group-sized halos.
</summary>
    <author>
      <name>Boryana Hadzhiyska</name>
    </author>
    <author>
      <name>Simone Ferraro</name>
    </author>
    <author>
      <name>Gerrit S. Farren</name>
    </author>
    <author>
      <name>Noah Sailer</name>
    </author>
    <author>
      <name>Rongpu Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.14136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14126v1</id>
    <updated>2025-07-18T17:55:42Z</updated>
    <published>2025-07-18T17:55:42Z</published>
    <title>Toward Temporal Causal Representation Learning with Tensor Decomposition</title>
    <summary>  Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.
</summary>
    <author>
      <name>Jianhong Chen</name>
    </author>
    <author>
      <name>Meng Zhao</name>
    </author>
    <author>
      <name>Mostafa Reisi Gahrooei</name>
    </author>
    <author>
      <name>Xubo Yue</name>
    </author>
    <link href="http://arxiv.org/abs/2507.14126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14069v1</id>
    <updated>2025-07-18T16:47:52Z</updated>
    <published>2025-07-18T16:47:52Z</published>
    <title>Edge Intelligence with Spiking Neural Networks</title>
    <summary>  The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.
</summary>
    <author>
      <name>Shuiguang Deng</name>
    </author>
    <author>
      <name>Di Yu</name>
    </author>
    <author>
      <name>Changze Lv</name>
    </author>
    <author>
      <name>Xin Du</name>
    </author>
    <author>
      <name>Linshan Jiang</name>
    </author>
    <author>
      <name>Xiaofan Zhao</name>
    </author>
    <author>
      <name>Wentao Tong</name>
    </author>
    <author>
      <name>Xiaoqing Zheng</name>
    </author>
    <author>
      <name>Weijia Fang</name>
    </author>
    <author>
      <name>Peng Zhao</name>
    </author>
    <author>
      <name>Gang Pan</name>
    </author>
    <author>
      <name>Schahram Dustdar</name>
    </author>
    <author>
      <name>Albert Y. Zomaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to Proceeding of IEEE for possible
  publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.14069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14049v1</id>
    <updated>2025-07-18T16:15:09Z</updated>
    <published>2025-07-18T16:15:09Z</published>
    <title>EdgeVLA: Efficient Vision-Language-Action Models</title>
    <summary>  Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.
</summary>
    <author>
      <name>Paweł Budzianowski</name>
    </author>
    <author>
      <name>Wesley Maa</name>
    </author>
    <author>
      <name>Matthew Freed</name>
    </author>
    <author>
      <name>Jingxiang Mo</name>
    </author>
    <author>
      <name>Winston Hsiao</name>
    </author>
    <author>
      <name>Aaron Xie</name>
    </author>
    <author>
      <name>Tomasz Młoduchowski</name>
    </author>
    <author>
      <name>Viraj Tipnis</name>
    </author>
    <author>
      <name>Benjamin Bolte</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IROS-MoMA3 Workshop 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2507.14049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14045v1</id>
    <updated>2025-07-18T16:13:35Z</updated>
    <published>2025-07-18T16:13:35Z</published>
    <title>Evaluating the Effectiveness of Cost-Efficient Large Language Models in
  Benchmark Biomedical Tasks</title>
    <summary>  This paper presents a comprehensive evaluation of cost-efficient Large
Language Models (LLMs) for diverse biomedical tasks spanning both text and
image modalities. We evaluated a range of closed-source and open-source LLMs on
tasks such as biomedical text classification and generation, question
answering, and multimodal image processing. Our experimental findings indicate
that there is no single LLM that can consistently outperform others across all
tasks. Instead, different LLMs excel in different tasks. While some
closed-source LLMs demonstrate strong performance on specific tasks, their
open-source counterparts achieve comparable results (sometimes even better),
with additional benefits like faster inference and enhanced privacy. Our
experimental results offer valuable insights for selecting models that are
optimally suited for specific biomedical applications.
</summary>
    <author>
      <name>Israt Jahan</name>
    </author>
    <author>
      <name>Md Tahmid Rahman Laskar</name>
    </author>
    <author>
      <name>Chun Peng</name>
    </author>
    <author>
      <name>Jimmy Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Canadian AI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.14045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14041v1</id>
    <updated>2025-07-18T16:11:12Z</updated>
    <published>2025-07-18T16:11:12Z</published>
    <title>Extreme value distribution for GRB prompt data -- How unexpected was the
  BOAT event?</title>
    <summary>  Gamma-Ray Bursts (GRBs) are known to be unpredictable in time and position. A
few (observationally) exceptional events have been observed, as GRB221009A that
stands out for its fluence and peak flux, being orders of magnitude higher than
what measured so far. Analyzing the observed fluence, peak flux or duration
distributions typically requires one to assume some scenarios, and the
consistency of the observed data with the predictions turns out to be an
important model diagnostic. However, it is also of interest to model these
distributions using general statistical properties that do not rely on specific
model assumptions, allowing one to derive inferences only based on the
consistency of the observed distributions with the hypothesis of one single
population of events that generate them. We obtained fluences, peak fluxes and
durations from the catalogues of GRBs observed by the CGRO-BATSE and Fermi-GBM
instruments. We selected the extreme values in slots of equal duration and
modelled their distributions by the generalized extreme value (GEV) formalism.
The GEV distribution is a limit distribution naturally arising when the number
of observations is large and is essentially independent of the phenomena
producing the observed data. The distributions of extreme values for fluences,
peak fluxes and durations are consistent with being extracted from a single
population of events but the fluence and peak flux recorded for GRB221009A
constitutes a striking exception. The probability to observe such an event,
assuming it is a cosmological GRB, is low, with a median value of about one
event per millennium for the fluence and about one event per century for the
peak flux.
</summary>
    <author>
      <name>Stefano Covino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A&amp;A, in press</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.14041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14035v1</id>
    <updated>2025-07-18T16:07:36Z</updated>
    <published>2025-07-18T16:07:36Z</published>
    <title>Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and
  Software for Port Selection and Beamforming</title>
    <summary>  This paper proposes a hardware-software co-design approach to efficiently
optimize beamforming and port selection in fluid antenna systems (FASs). To
begin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input
multiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)
maximization problem is formulated. Second, a method that integrates graph
neural networks (GNNs) with random port selection (RPS) is proposed to jointly
optimize beamforming and port selection, while also assessing the benefits and
limitations of random selection. Third, an instruction-driven deep learning
accelerator based on a field-programmable gate array (FPGA) is developed to
minimize inference latency. To further enhance efficiency, a scheduling
algorithm is introduced to reduce redundant computations and minimize the idle
time of computing cores. Simulation results demonstrate that the proposed
GNN-RPS approach achieves competitive communication performance. Furthermore,
experimental evaluations indicate that the FPGA-based accelerator maintains low
latency while simultaneously executing beamforming inference for multiple port
selections.
</summary>
    <author>
      <name>Sai Xu</name>
    </author>
    <author>
      <name>Kai-Kit Wong</name>
    </author>
    <author>
      <name>Yanan Du</name>
    </author>
    <author>
      <name>Hanjiang Hong</name>
    </author>
    <author>
      <name>Chan-Byoung Chae</name>
    </author>
    <author>
      <name>Baiyang Liu</name>
    </author>
    <author>
      <name>Kin-Fai Tong</name>
    </author>
    <link href="http://arxiv.org/abs/2507.14035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14031v1</id>
    <updated>2025-07-18T15:57:53Z</updated>
    <published>2025-07-18T15:57:53Z</published>
    <title>QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest
  Electrical Impedance Tomography</title>
    <summary>  Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.
</summary>
    <author>
      <name>Hao Fang</name>
    </author>
    <author>
      <name>Sihao Teng</name>
    </author>
    <author>
      <name>Hao Yu</name>
    </author>
    <author>
      <name>Siyi Yuan</name>
    </author>
    <author>
      <name>Huaiwu He</name>
    </author>
    <author>
      <name>Zhe Liu</name>
    </author>
    <author>
      <name>Yunjie Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.14031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14019v1</id>
    <updated>2025-07-18T15:35:17Z</updated>
    <published>2025-07-18T15:35:17Z</published>
    <title>On the importance of tail assumptions in climate extreme event
  attribution</title>
    <summary>  Extreme weather events are becoming more frequent and intense, posing serious
threats to human life, biodiversity, and ecosystems. A key objective of extreme
event attribution (EEA) is to assess whether and to what extent anthropogenic
climate change influences such events. Central to EEA is the accurate
statistical characterization of atmospheric extremes, which are inherently
multivariate or spatial due to their measurement over high-dimensional grids.
Within the counterfactual causal inference framework of Pearl, we evaluate how
tail assumptions affect attribution conclusions by comparing three multivariate
modeling approaches for estimating causation metrics. These include: (i) the
multivariate generalized Pareto distribution, which imposes an invariant tail
dependence structure; (ii) the factor copula model of Castro-Camilo and Huser
(2020), which offers flexible subasymptotic behavior; and (iii) the model of
Huser and Wadsworth (2019), which smoothly transitions between different forms
of extremal dependence. We assess the implications of these modeling choices in
both simulated scenarios (under varying forms of model misspecification) and
real data applications, using weekly winter maxima over Europe from the
M\'et\'eo-France CNRM model and daily precipitation from the ACCESS-CM2 model
over the U.S. Our findings highlight that tail assumptions critically shape
causality metrics in EEA. Misspecification of the extremal dependence structure
can lead to substantially different and potentially misleading attribution
conclusions, underscoring the need for careful model selection and evaluation
when quantifying the influence of climate change on extreme events.
</summary>
    <author>
      <name>Mengran Li</name>
    </author>
    <author>
      <name>Daniela Castro-Camilo</name>
    </author>
    <link href="http://arxiv.org/abs/2507.14019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G32, 62H10, 62P12, 62H20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14000v1</id>
    <updated>2025-07-18T15:14:56Z</updated>
    <published>2025-07-18T15:14:56Z</published>
    <title>Photonic Fabric Platform for AI Accelerators</title>
    <summary>  This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM
(PFA), a photonic-enabled switch and memory subsystem that delivers low
latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth
HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D
electro-optical system-in-package, the PFA offers up to 32 TB of shared memory
alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM
enables distributed AI training and inference to execute parallelism strategies
more efficiently. The Photonic Fabric removes the silicon beachfront constraint
that limits the fixed memory-to-compute ratio observed in virtually all current
XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet
that connects to the Photonic Fabric increases its memory capacity and
correspondingly its memory bandwidth by offering a flexible path to scaling
well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a
lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It
is used to evaluate the performance of LLM reference and energy savings on PFA,
without any significant change to the GPU core design. With the PFA, the
simulation results show that up to 3.66x throughput and 1.40x latency
improvements in LLM inference at 405B parameters, up to 7.04x throughput and
1.41x latency improvements at 1T parameters, and 60-90% energy savings in data
movement for heavy collective operations in all LLM training scenarios. While
these results are shown for NVIDIA GPUs, they can be applied similarly to other
AI accelerator designs (XPUs) that share the same fundamental limitation of
fixed memory to compute.
</summary>
    <author>
      <name>Jing Ding</name>
    </author>
    <author>
      <name>Trung Diep</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.14000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
