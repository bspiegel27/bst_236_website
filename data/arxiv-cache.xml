<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-21T00:52:35Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-20T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">109159</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.15463v1</id>
    <updated>2025-03-19T17:41:46Z</updated>
    <published>2025-03-19T17:41:46Z</published>
    <title>From 1,000,000 Users to Every User: Scaling Up Personalized Preference
  for User-level Alignment</title>
    <summary>  Large language models (LLMs) have traditionally been aligned through
one-size-fits-all approaches that assume uniform human preferences,
fundamentally overlooking the diversity in user values and needs. This paper
introduces a comprehensive framework for scalable personalized alignment of
LLMs. We establish a systematic preference space characterizing psychological
and behavioral dimensions, alongside diverse persona representations for robust
preference inference in real-world scenarios. Building upon this foundation, we
introduce \textsc{AlignX}, a large-scale dataset of over 1.3 million
personalized preference examples, and develop two complementary alignment
approaches: \textit{in-context alignment} directly conditioning on persona
representations and \textit{preference-bridged alignment} modeling intermediate
preference distributions. Extensive experiments demonstrate substantial
improvements over existing methods, with an average 17.06\% accuracy gain
across four benchmarks while exhibiting a strong adaptation capability to novel
preferences, robustness to limited user data, and precise preference
controllability. These results validate our framework's effectiveness,
advancing toward truly user-adaptive AI systems.
</summary>
    <author>
      <name>Jia-Nan Li</name>
    </author>
    <author>
      <name>Jian Guan</name>
    </author>
    <author>
      <name>Songhao Wu</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Rui Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15457v1</id>
    <updated>2025-03-19T17:36:54Z</updated>
    <published>2025-03-19T17:36:54Z</published>
    <title>Di$\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step
  Generator</title>
    <summary>  Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling
technique. Despite their remarkable results, they typically suffer from slow
inference with several steps. In this paper, we propose Di$\mathtt{[M]}$O, a
novel approach that distills masked diffusion models into a one-step generator.
Di$\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using
intermediate-step information for one-step generation, which we solve through
token-level distribution matching that optimizes model output logits by an
'on-policy framework' with the help of an auxiliary model; and (2) the lack of
entropy in the initial distribution, which we address through a token
initialization strategy that injects randomness while maintaining similarity to
teacher training distribution. We show Di$\mathtt{[M]}$O's effectiveness on
both class-conditional and text-conditional image generation, impressively
achieving performance competitive to multi-step teacher outputs while
drastically reducing inference time. To our knowledge, we are the first to
successfully achieve one-step distillation of masked diffusion models and the
first to apply discrete distillation to text-to-image generation, opening new
paths for efficient generative modeling.
</summary>
    <author>
      <name>Yuanzhi Zhu</name>
    </author>
    <author>
      <name>Xi Wang</name>
    </author>
    <author>
      <name>Stéphane Lathuilière</name>
    </author>
    <author>
      <name>Vicky Kalogeiton</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15451v1</id>
    <updated>2025-03-19T17:32:24Z</updated>
    <published>2025-03-19T17:32:24Z</published>
    <title>MotionStreamer: Streaming Motion Generation via Diffusion-based
  Autoregressive Model in Causal Latent Space</title>
    <summary>  This paper addresses the challenge of text-conditioned streaming motion
generation, which requires us to predict the next-step human pose based on
variable-length historical motions and incoming texts. Existing methods
struggle to achieve streaming motion generation, e.g., diffusion models are
constrained by pre-defined motion lengths, while GPT-based methods suffer from
delayed response and error accumulation problem due to discretized non-causal
tokenization. To solve these problems, we propose MotionStreamer, a novel
framework that incorporates a continuous causal latent space into a
probabilistic autoregressive model. The continuous latents mitigate information
loss caused by discretization and effectively reduce error accumulation during
long-term autoregressive generation. In addition, by establishing temporal
causal dependencies between current and historical motion latents, our model
fully utilizes the available information to achieve accurate online motion
decoding. Experiments show that our method outperforms existing approaches
while offering more applications, including multi-round generation, long-term
generation, and dynamic motion composition. Project Page:
https://zju3dv.github.io/MotionStreamer/
</summary>
    <author>
      <name>Lixing Xiao</name>
    </author>
    <author>
      <name>Shunlin Lu</name>
    </author>
    <author>
      <name>Huaijin Pi</name>
    </author>
    <author>
      <name>Ke Fan</name>
    </author>
    <author>
      <name>Liang Pan</name>
    </author>
    <author>
      <name>Yueer Zhou</name>
    </author>
    <author>
      <name>Ziyong Feng</name>
    </author>
    <author>
      <name>Xiaowei Zhou</name>
    </author>
    <author>
      <name>Sida Peng</name>
    </author>
    <author>
      <name>Jingbo Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://zju3dv.github.io/MotionStreamer/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.15451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15436v1</id>
    <updated>2025-03-19T17:18:18Z</updated>
    <published>2025-03-19T17:18:18Z</published>
    <title>An extensive simulation study evaluating the interaction of resampling
  techniques across multiple causal discovery contexts</title>
    <summary>  Despite the accelerating presence of exploratory causal analysis in modern
science and medicine, the available non-experimental methods for validating
causal models are not well characterized. One of the most popular methods is to
evaluate the stability of model features after resampling the data, similar to
resampling methods for estimating confidence intervals in statistics. Many
aspects of this approach have received little to no attention, however, such as
whether the choice of resampling method should depend on the sample size,
algorithms being used, or algorithm tuning parameters. We present theoretical
results proving that certain resampling methods closely emulate the assignment
of specific values to algorithm tuning parameters. We also report the results
of extensive simulation experiments, which verify the theoretical result and
provide substantial data to aid researchers in further characterizing
resampling in the context of causal discovery analysis. Together, the
theoretical work and simulation results provide specific guidance on how
resampling methods and tuning parameters should be selected in practice.
</summary>
    <author>
      <name>Ritwick Banerjee</name>
    </author>
    <author>
      <name>Bryan Andrews</name>
    </author>
    <author>
      <name>Erich Kummerfeld</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15422v1</id>
    <updated>2025-03-19T17:01:40Z</updated>
    <published>2025-03-19T17:01:40Z</published>
    <title>Limits on the Ejecta Mass During the Search for Kilonovae Associated
  with Neutron Star-Black Hole Mergers: A case study of S230518h, GW230529,
  S230627c and the Low-Significance Candidate S240422ed</title>
    <summary>  Neutron star-black hole (NSBH) mergers, detectable via their
gravitational-wave (GW) emission, are expected to produce kilonovae (KNe). Four
NSBH candidates have been identified and followed-up by more than fifty
instruments since the start of the fourth GW Observing Run (O4), in May 2023,
up to July 2024; however, no confirmed associated KN has been detected. This
study evaluates ejecta properties from multi-messenger observations to
understand the absence of detectable KN: we use GW public information and joint
observations taken from 05.2023 to 07.2024 (LVK, ATLAS, DECam, GECKO, GOTO,
GRANDMA, SAGUARO, TESS, WINTER, ZTF). First, our analysis on follow-up
observation strategies shows that, on average, more than 50% of the simulated
KNe associated with NSBH mergers reach their peak luminosity around one day
after merger in the $g,r,i$- bands, which is not necessarily covered for each
NSBH GW candidate. We also analyze the trade-off between observation efficiency
and the intrinsic properties of the KN emission, to understand the impact on
how these constraints affect our ability to detect the KN, and underlying
ejecta properties for each GW candidate. In particular, we can only confirm the
kilonova was not missed for 1% of the GW230529 and S230627c sky localization
region, given the large sky localization error of GW230529 and the large
distance for S230627c and, their respective KN faint luminosities. More
constraining, for S230518h, we infer the dynamical ejecta and post-merger disk
wind ejecta $m_{dyn}, m_{wind}$ $&lt;$ $0.03$ $M_\odot$ and the viewing angle
$\theta&gt;25^\circ$. Similarly, the non-astrophysical origin of S240422ed is
likely further confirmed by the fact that we would have detected even a faint
KN at the time and presumed distance of the S240422ed event candidate, within a
minimum 45% credible region of the sky area, that can be larger depending on
the KN scenario.
</summary>
    <author>
      <name>M. Pillas</name>
    </author>
    <author>
      <name>S. Antier</name>
    </author>
    <author>
      <name>K. Ackley</name>
    </author>
    <author>
      <name>T. Ahumada</name>
    </author>
    <author>
      <name>D. Akl</name>
    </author>
    <author>
      <name>L. de Almeida</name>
    </author>
    <author>
      <name>S. Anand</name>
    </author>
    <author>
      <name>C. Andrade</name>
    </author>
    <author>
      <name>I. Andreoni</name>
    </author>
    <author>
      <name>K. A. Bostroem</name>
    </author>
    <author>
      <name>M. Bulla</name>
    </author>
    <author>
      <name>E. Burns</name>
    </author>
    <author>
      <name>T. Cabrera</name>
    </author>
    <author>
      <name>S. Chang</name>
    </author>
    <author>
      <name>H. Choi</name>
    </author>
    <author>
      <name>B. O'Connor</name>
    </author>
    <author>
      <name>M. W. Coughlin</name>
    </author>
    <author>
      <name>W. Corradi</name>
    </author>
    <author>
      <name>A. R. Gibbs</name>
    </author>
    <author>
      <name>T. Dietrich</name>
    </author>
    <author>
      <name>D. Dornic</name>
    </author>
    <author>
      <name>J. -G. Ducoin</name>
    </author>
    <author>
      <name>P. -A. Duverne</name>
    </author>
    <author>
      <name>M. Dyer</name>
    </author>
    <author>
      <name>H. -B. Eggenstein</name>
    </author>
    <author>
      <name>M. Freeberg</name>
    </author>
    <author>
      <name>M. Fausnaugh</name>
    </author>
    <author>
      <name>W. Fong</name>
    </author>
    <author>
      <name>F. Foucart</name>
    </author>
    <author>
      <name>D. Frostig</name>
    </author>
    <author>
      <name>N. Guessoum</name>
    </author>
    <author>
      <name>V. Gupta</name>
    </author>
    <author>
      <name>P. Hello</name>
    </author>
    <author>
      <name>G. Hosseinzadeh</name>
    </author>
    <author>
      <name>L. Hu</name>
    </author>
    <author>
      <name>T. Hussenot-Desenonges</name>
    </author>
    <author>
      <name>M. Im</name>
    </author>
    <author>
      <name>R. Jayaraman</name>
    </author>
    <author>
      <name>M. Jeong</name>
    </author>
    <author>
      <name>V. Karambelkar</name>
    </author>
    <author>
      <name>S. Karpov</name>
    </author>
    <author>
      <name>M. Kasliwal</name>
    </author>
    <author>
      <name>C. D. Kilpatrick</name>
    </author>
    <author>
      <name>S. Kim</name>
    </author>
    <author>
      <name>N. Kochiashvili</name>
    </author>
    <author>
      <name>K. Kunnumkai</name>
    </author>
    <author>
      <name>M. Lamoureux</name>
    </author>
    <author>
      <name>C. U. Lee</name>
    </author>
    <author>
      <name>N. Lourie</name>
    </author>
    <author>
      <name>J. Lyman</name>
    </author>
    <author>
      <name>F. Magnani</name>
    </author>
    <author>
      <name>M. Masek</name>
    </author>
    <author>
      <name>G. Mo</name>
    </author>
    <author>
      <name>M. Molham</name>
    </author>
    <author>
      <name>F. Navarete</name>
    </author>
    <author>
      <name>D. O'Neill</name>
    </author>
    <author>
      <name>M. Nicholl</name>
    </author>
    <author>
      <name>A. H. Nitz</name>
    </author>
    <author>
      <name>K. Noysena</name>
    </author>
    <author>
      <name>G. S. H. Paek</name>
    </author>
    <author>
      <name>A. Palmese</name>
    </author>
    <author>
      <name>R. Poggiani</name>
    </author>
    <author>
      <name>T. Pradier</name>
    </author>
    <author>
      <name>O. Pyshna</name>
    </author>
    <author>
      <name>Y. Rajabov</name>
    </author>
    <author>
      <name>J. C. Rastinejad</name>
    </author>
    <author>
      <name>D. J. Sand</name>
    </author>
    <author>
      <name>P. Shawhan</name>
    </author>
    <author>
      <name>M. Shrestha</name>
    </author>
    <author>
      <name>R. Simcoe</name>
    </author>
    <author>
      <name>S. J. Smartt</name>
    </author>
    <author>
      <name>D. Steeghs</name>
    </author>
    <author>
      <name>R. Stein</name>
    </author>
    <author>
      <name>H. F. Stevance</name>
    </author>
    <author>
      <name>M. Sun</name>
    </author>
    <author>
      <name>A. Takey</name>
    </author>
    <author>
      <name>A. Toivonen</name>
    </author>
    <author>
      <name>D. Turpin</name>
    </author>
    <author>
      <name>K. Ulaczyk</name>
    </author>
    <author>
      <name>A. Wold</name>
    </author>
    <author>
      <name>T. Wouters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.15422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15420v1</id>
    <updated>2025-03-19T17:00:58Z</updated>
    <published>2025-03-19T17:00:58Z</published>
    <title>LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding</title>
    <summary>  Implicit Neural Representations (INRs) are proving to be a powerful paradigm
in unifying task modeling across diverse data domains, offering key advantages
such as memory efficiency and resolution independence. Conventional deep
learning models are typically modality-dependent, often requiring custom
architectures and objectives for different types of signals. However, existing
INR frameworks frequently rely on global latent vectors or exhibit
computational inefficiencies that limit their broader applicability. We
introduce LIFT, a novel, high-performance framework that addresses these
challenges by capturing multiscale information through meta-learning. LIFT
leverages multiple parallel localized implicit functions alongside a
hierarchical latent generator to produce unified latent representations that
span local, intermediate, and global features. This architecture facilitates
smooth transitions across local regions, enhancing expressivity while
maintaining inference efficiency. Additionally, we introduce ReLIFT, an
enhanced variant of LIFT that incorporates residual connections and expressive
frequency encodings. With this straightforward approach, ReLIFT effectively
addresses the convergence-capacity gap found in comparable methods, providing
an efficient yet powerful solution to improve capacity and speed up
convergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)
performance in generative modeling and classification tasks, with notable
reductions in computational costs. Moreover, in single-task settings, the
streamlined ReLIFT architecture proves effective in signal representations and
inverse problem tasks.
</summary>
    <author>
      <name>Amirhossein Kazerouni</name>
    </author>
    <author>
      <name>Soroush Mehraban</name>
    </author>
    <author>
      <name>Michael Brudno</name>
    </author>
    <author>
      <name>Babak Taati</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15415v1</id>
    <updated>2025-03-19T16:57:00Z</updated>
    <published>2025-03-19T16:57:00Z</published>
    <title>Automated Processing of eXplainable Artificial Intelligence Outputs in
  Deep Learning Models for Fault Diagnostics of Large Infrastructures</title>
    <summary>  Deep Learning (DL) models processing images to recognize the health state of
large infrastructure components can exhibit biases and rely on non-causal
shortcuts. eXplainable Artificial Intelligence (XAI) can address these issues
but manually analyzing explanations generated by XAI techniques is
time-consuming and prone to errors. This work proposes a novel framework that
combines post-hoc explanations with semi-supervised learning to automatically
identify anomalous explanations that deviate from those of correctly classified
images and may therefore indicate model abnormal behaviors. This significantly
reduces the workload for maintenance decision-makers, who only need to manually
reclassify images flagged as having anomalous explanations. The proposed
framework is applied to drone-collected images of insulator shells for power
grid infrastructure monitoring, considering two different Convolutional Neural
Networks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly
Detection. The average classification accuracy on two faulty classes is
improved by 8% and maintenance operators are required to manually reclassify
only 15% of the images. We compare the proposed framework with a
state-of-the-art approach based on the faithfulness metric: the experimental
results obtained demonstrate that the proposed framework consistently achieves
F_1 scores larger than those of the faithfulness-based approach. Additionally,
the proposed framework successfully identifies correct classifications that
result from non-causal shortcuts, such as the presence of ID tags printed on
insulator shells.
</summary>
    <author>
      <name>Giovanni Floreale</name>
    </author>
    <author>
      <name>Piero Baraldi</name>
    </author>
    <author>
      <name>Enrico Zio</name>
    </author>
    <author>
      <name>Olga Fink</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15410v1</id>
    <updated>2025-03-19T16:52:28Z</updated>
    <published>2025-03-19T16:52:28Z</published>
    <title>Loophole-free argument for physicality of electromagnetic potential from
  causal structure of flux quantization</title>
    <summary>  Recent work by Vaidman [Phys. Rev. A 86,040101 (2012)] showed that
Aharonov-Bohm effect can be explained in terms of local fields, thus
effectively restating an old problem of physicality of potentials. In this
work, we propose an argument demonstrating the physicality of electromagnetic
potential (upon the assumption of locality) based on the causal structure in
flux quantization setup. Crucially, we discuss the fundamental difference
between the considered setup and the Aharonov-Bohm experiment that allows for
avoiding Vaidman's loophole in our scenario.
</summary>
    <author>
      <name>Konrad Schlichtholz</name>
    </author>
    <author>
      <name>Marcin Markiewicz</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15395v1</id>
    <updated>2025-03-19T16:35:05Z</updated>
    <published>2025-03-19T16:35:05Z</published>
    <title>Making Online Polls More Accurate: Statistical Methods Explained</title>
    <summary>  Online data has the potential to transform how researchers and companies
produce election forecasts. Social media surveys, online panels and even
comments scraped from the internet can offer valuable insights into political
preferences. However, such data is often affected by significant selection
bias, as online respondents may not be representative of the overall
population. At the same time, traditional data collection methods are becoming
increasingly cost-prohibitive. In this scenario, scientists need instruments to
be able to draw the most accurate estimate possible from samples drawn online.
This paper provides an introduction to key statistical methods for mitigating
bias and improving inference in such cases, with a focus on electoral polling.
Specifically, it presents the main statistical techniques, categorized into
weighting, modeling and other approaches. It also offers practical
recommendations for drawing estimates with measures of uncertainty. Designed
for both researchers and industry practitioners, this introduction takes a
hands-on approach, with code available for implementing the main methods.
</summary>
    <author>
      <name>Alberto Arletti</name>
    </author>
    <author>
      <name>Maria Letizia Tanturri</name>
    </author>
    <author>
      <name>Omar Paccagnella</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15393v1</id>
    <updated>2025-03-19T16:33:08Z</updated>
    <published>2025-03-19T16:33:08Z</published>
    <title>Orbital eccentricity in a neutron star - black hole binary</title>
    <summary>  The observation of gravitational waves from merging black holes and neutron
stars provides a unique opportunity to discern information about their
astrophysical environment. Two signatures that are considered powerful tracers
to distinguish between different binary formation channels are
general-relativistic spin-induced orbital precession and orbital eccentricity.
Both effects leave characteristic imprints in the gravitational-wave signal
that can be extracted from observations. To date, neither precession nor
eccentricity have been discerned in neutron star - black hole binaries. Here we
report the first measurement of orbital eccentricity in a neutron star - black
hole binary. Using, for the first time, a waveform model that incorporates
precession and eccentricity, we perform Bayesian inference on the event
GW200105 and infer a median orbital eccentricity of $e_{20}\sim 0.145$ at an
orbital period of $0.1$s, excluding zero at more than $99\%$ confidence. We
find inconclusive evidence for the presence of precession, consistent with
previous, non-eccentric results. Our result implies a fraction of these
binaries will exhibit orbital eccentricity even at small separations,
suggesting formation through mechanisms involving dynamical interactions beyond
isolated binary evolution. Future observations will reveal the contribution of
eccentric neutron star - black hole binaries to the total merger rate across
cosmic time.
</summary>
    <author>
      <name>Gonzalo Morras</name>
    </author>
    <author>
      <name>Geraint Pratten</name>
    </author>
    <author>
      <name>Patricia Schmidt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.15393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
