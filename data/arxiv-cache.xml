<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-23T00:54:47Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-22T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">111016</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.15268v1</id>
    <updated>2025-04-21T17:52:36Z</updated>
    <published>2025-04-21T17:52:36Z</published>
    <title>Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios,
  and Stress Testing for Financial Portfolios</title>
    <summary>  We live in a multivariate world, and effective modeling of financial
portfolios, including their construction, allocation, forecasting, and risk
analysis, simply is not possible without explicitly modeling the dependence
structure of their assets. Dependence structure can drive portfolio results
more than many other parameters in investment and risk models, sometimes even
more than their combined effects, but the literature provides relatively little
to define the finite-sample distributions of dependence measures in useable and
useful ways under challenging, real-world financial data conditions. Yet this
is exactly what is needed to make valid inferences about their estimates, and
to use these inferences for a myriad of essential purposes, such as hypothesis
testing, dynamic monitoring, realistic and granular scenario and reverse
scenario analyses, and mitigating the effects of correlation breakdowns during
market upheavals (which is when we need valid inferences the most). This work
develops a new and straightforward method, Nonparametric Angles-based
Correlation (NAbC), for defining the finite-sample distributions of any
dependence measure whose matrix of pairwise associations is positive definite
(e.g. Pearsons, Kendalls Tau, Spearmans Rho, Chatterjees, Lancasters, Szekelys,
and their many variants). The solution remains valid under marginal asset
distributions characterized by notably different and varying degrees of serial
correlation, non-stationarity, heavy-tailedness, and asymmetry. Notably, NAbCs
p-values and confidence intervals remain analytically consistent at both the
matrix level and the pairwise cell level. Finally, NAbC maintains validity even
when selected cells in the matrix are frozen for a given scenario or stress
test, that is, unaffected by the scenario, thus enabling flexible, granular,
and realistic scenarios.
</summary>
    <author>
      <name>JD Opdyke</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15255v1</id>
    <updated>2025-04-21T17:34:55Z</updated>
    <published>2025-04-21T17:34:55Z</published>
    <title>Population Models for Star Formation Timescales in Early Galaxies: The
  First Step Towards Solving Outshining in Star Formation History Inference</title>
    <summary>  JWST have revealed temporarily-quenched and ultraviolet-luminous galaxies in
the early universe, suggesting enhanced star formation stochasticity. Verifying
this hypothesis is critical, yet challenging; outshining, wherein light from
young stars dominates the spectral energy distribution, represents perhaps the
greatest challenge in inferring the formation histories of unresolved galaxies.
In this paper, we take a simple model of burstiness and show that
state-of-the-art inference methods with flexible star formation histories
(SFHs) and neutral priors, while recovering average star formation rates (SFRs;
$\sim0.1$ dex median offset), fail to recover the complexities of fluctuations
on tens of Myr timescales, and typically underestimate masses in bursty systems
($\sim0.15$ dex). Surprisingly, detailed SFH recovery is still sensitive to
priors even when data quality is optimal, e.g., including high signal-to-noise
($\rm20~pixel^{-1}$) spectroscopy with wide coverage (rest-frame
$0.12-1.06~\mu$m). Crucially, however, refitting the same data with a prior
correctly encoding the bursty expectation eliminates these biases: median
offsets in mass and SFRs decrease to $\sim 0.04$ dex and $\sim 0.05$ dex,
respectively. Under the assumption that current population burstiness predicts
past SFH, the solution to outshining in modeling statistical samples is
empirically measuring recent galaxy SFHs with population modeling. A prototype
is H$\alpha$/UV: while helpful, it is insufficient to constrain the expected
complex burstiness. To this end, we introduce a more complete, quantitative
population-level approach and demonstrate that it promises to recover the
typical amplitude, timescale, and slope of the recent SFH to high accuracy.
This approach thus has the strong potential to solve outshining using
observations from JWST.
</summary>
    <author>
      <name>Bingjie Wang</name>
    </author>
    <author>
      <name>Joel Leja</name>
    </author>
    <author>
      <name>Hakim Atek</name>
    </author>
    <author>
      <name>Rachel Bezanson</name>
    </author>
    <author>
      <name>Emilie Burnham</name>
    </author>
    <author>
      <name>Pratika Dayal</name>
    </author>
    <author>
      <name>Robert Feldmann</name>
    </author>
    <author>
      <name>Jenny E. Greene</name>
    </author>
    <author>
      <name>Benjamin D. Johnson</name>
    </author>
    <author>
      <name>Ivo Labbe</name>
    </author>
    <author>
      <name>Michael V. Maseda</name>
    </author>
    <author>
      <name>Themiya Nanayakkara</name>
    </author>
    <author>
      <name>Sedona H. Price</name>
    </author>
    <author>
      <name>Katherine A. Suess</name>
    </author>
    <author>
      <name>John R. Weaver</name>
    </author>
    <author>
      <name>Katherine E. Whitaker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ApJ on 12/18/24; this version corresponds to a revised
  version after a reviewer's report. 27 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15253v1</id>
    <updated>2025-04-21T17:33:23Z</updated>
    <published>2025-04-21T17:33:23Z</published>
    <title>Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as
  Test-Time Scaling Evaluators</title>
    <summary>  Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.
</summary>
    <author>
      <name>Yilun Zhou</name>
    </author>
    <author>
      <name>Austin Xu</name>
    </author>
    <author>
      <name>Peifeng Wang</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Shafiq Joty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally. The codebase is at
  https://github.com/SalesforceAIResearch/jetts-benchmark</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15220v1</id>
    <updated>2025-04-21T16:46:07Z</updated>
    <published>2025-04-21T16:46:07Z</published>
    <title>Fully Bayesian Approaches to Topics over Time</title>
    <summary>  The Topics over Time (ToT) model captures thematic changes in timestamped
datasets by explicitly modeling publication dates jointly with word
co-occurrence patterns. However, ToT was not approached in a fully Bayesian
fashion, a flaw that makes it susceptible to stability problems. To address
this issue, we propose a fully Bayesian Topics over Time (BToT) model via the
introduction of a conjugate prior to the Beta distribution. This prior acts as
a regularization that prevents the online version of the algorithm from
unstable updates when a topic is poorly represented in a mini-batch. The
characteristics of this prior to the Beta distribution are studied here for the
first time. Still, this model suffers from a difference in scale between the
single-time observations and the multiplicity of words per document. A
variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a
solution. In WBToT, publication dates are repeated a certain number of times
per document, which balances the relative influence of words and timestamps
along the inference process. We have tested our models on two datasets: a
collection of over 200 years of US state-of-the-union (SOTU) addresses and a
large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that
WBToT captures events better than Latent Dirichlet Allocation and other SOTA
topic models like BERTopic: the median absolute deviation of the topic presence
over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also
demonstrate the superior coherence of WBToT over BToT, which highlights the
importance of balancing the time and word modalities. Finally, we illustrate
the stability of the online optimization algorithm in WBToT, which allows the
application of WBToT to problems that are intractable for standard ToT.
</summary>
    <author>
      <name>Juli√°n Cendrero</name>
    </author>
    <author>
      <name>Julio Gonzalo</name>
    </author>
    <author>
      <name>Ivar Zapata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15211v1</id>
    <updated>2025-04-21T16:31:15Z</updated>
    <published>2025-04-21T16:31:15Z</published>
    <title>Position: Bayesian Statistics Facilitates Stakeholder Participation in
  Evaluation of Generative AI</title>
    <summary>  The evaluation of Generative AI (GenAI) systems plays a critical role in
public policy and decision-making, yet existing methods are often limited by
reliance on benchmark-driven, point-estimate comparisons that fail to capture
uncertainty and broader societal impacts. This paper argues for the use of
Bayesian statistics as a principled framework to address these challenges.
Bayesian methods enable the integration of domain expertise through prior
elicitation, allow for continuous learning from new data, and provide robust
uncertainty quantification via posterior inference. We demonstrate how Bayesian
inference can be applied to GenAI evaluation, particularly in incorporating
stakeholder perspectives to enhance fairness, transparency, and reliability.
Furthermore, we discuss Bayesian workflows as an iterative process for model
validation and refinement, ensuring robust assessments of GenAI systems in
dynamic, real-world contexts.
</summary>
    <author>
      <name>Yanan Long</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at ACM CHI 2025 workshop STAIG</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15209v1</id>
    <updated>2025-04-21T16:27:16Z</updated>
    <published>2025-04-21T16:27:16Z</published>
    <title>A Causal Convolutional Low-rank Representation Model for Imputation of
  Water Quality Data</title>
    <summary>  The monitoring of water quality is a crucial part of environmental
protection, and a large number of monitors are widely deployed to monitor water
quality. Due to unavoidable factors such as data acquisition breakdowns,
sensors and communication failures, water quality monitoring data suffers from
missing values over time, resulting in High-Dimensional and Sparse (HDS) Water
Quality Data (WQD). The simple and rough filling of the missing values leads to
inaccurate results and affects the implementation of relevant measures.
Therefore, this paper proposes a Causal convolutional Low-rank Representation
(CLR) model for imputing missing WQD to improve the completeness of the WQD,
which employs a two-fold idea: a) applying causal convolutional operation to
consider the temporal dependence of the low-rank representation, thus
incorporating temporal information to improve the imputation accuracy; and b)
implementing a hyperparameters adaptation scheme to automatically adjust the
best hyperparameters during model training, thereby reducing the tedious manual
adjustment of hyper-parameters. Experimental studies on three real-world water
quality datasets demonstrate that the proposed CLR model is superior to some of
the existing state-of-the-art imputation models in terms of imputation accuracy
and time cost, as well as indicating that the proposed model provides more
reliable decision support for environmental monitoring.
</summary>
    <author>
      <name>Xin Liao</name>
    </author>
    <author>
      <name>Bing Yang</name>
    </author>
    <author>
      <name>Tan Dongli</name>
    </author>
    <author>
      <name>Cai Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07 (Primary) 62M10, 65C60 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15178v1</id>
    <updated>2025-04-21T15:40:10Z</updated>
    <published>2025-04-21T15:40:10Z</published>
    <title>Time-Series Analysis on Edge-AI Hardware for Healthcare Monitoring</title>
    <summary>  This project addresses the need for efficient, real-time analysis of
biomedical signals such as electrocardiograms (ECG) and electroencephalograms
(EEG) for continuous health monitoring. Traditional methods rely on
long-duration data recording followed by offline analysis, which is
power-intensive and delays responses to critical symptoms such as arrhythmia.
To overcome these limitations, a time-domain ECG analysis model based on a
novel dynamically-biased Long Short-Term Memory (DB-LSTM) neural network is
proposed. This model supports simultaneous ECG forecasting and classification
with high performance-achieving over 98% accuracy and a normalized mean square
error below 1e-3 for forecasting, and over 97% accuracy with faster convergence
and fewer training parameters for classification. To enable edge deployment,
the model is hardware-optimized by quantizing weights to INT4 or INT3 formats,
resulting in only a 2% and 6% drop in classification accuracy during training
and inference, respectively, while maintaining full accuracy for forecasting.
Extensive simulations using multiple ECG datasets confirm the model's
robustness. Future work includes implementing the algorithm on FPGA and CMOS
circuits for practical cardiac monitoring, as well as developing a digital
hardware platform that supports flexible neural network configurations and
on-chip online training for personalized healthcare applications.
</summary>
    <author>
      <name>Jinhai Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 20 figures, Progress report for qualification cum PhD
  confirmation exercise</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15166v1</id>
    <updated>2025-04-21T15:16:30Z</updated>
    <published>2025-04-21T15:16:30Z</published>
    <title>Simulating biochemical reactions: The Linear Noise Approximation can
  capture non-linear dynamics</title>
    <summary>  There is a plethora of highly stochastic non-linear dynamical systems in
fields such as molecular biology, chemistry, epidemiology, and ecology. Yet,
none of the currently available stochastic models are both accurate and
computationally efficient for long-term predictions of large systems. The
Linear Noise Approximation (LNA) model for biochemical reaction networks is
analytically tractable, which makes it computationally efficient for
simulation, analysis, and inference. However, it is only accurate for linear
systems and short-time transitions. Other methods can achieve greater accuracy
across a wider range of systems, including non-linear ones, but lack analytical
tractability. This paper seeks to challenge the prevailing view by
demonstrating that the Linear Noise Approximation can indeed capture non-linear
dynamics after certain modifications. We introduce a new framework that
utilises centre manifold theory allowing us to identify simple interventions to
the LNA that do not significantly compromise its computational efficiency. We
develop specific algorithms for systems that exhibit oscillations or
bi-stability and demonstrate their accuracy and computational efficiency across
multiple examples.
</summary>
    <author>
      <name>Frederick Truman-Williams</name>
    </author>
    <author>
      <name>Giorgos Minas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92C40, 82C31 (Primary) 60J20, 92C42 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15162v1</id>
    <updated>2025-04-21T15:08:01Z</updated>
    <published>2025-04-21T15:08:01Z</published>
    <title>To Offload or Not To Offload: Model-driven Comparison of Edge-native and
  On-device Processing</title>
    <summary>  Computational offloading is a promising approach for overcoming resource
constraints on client devices by moving some or all of an application's
computations to remote servers. With the advent of specialized hardware
accelerators, client devices are now able to perform fast local processing of
specific tasks, such as machine learning inference, reducing the need for
offloading computations. However, edge servers with accelerators also offer
faster processing for offloaded tasks than was previously possible. In this
paper, we present an analytic and experimental comparison of on-device
processing and edge offloading for a range of accelerator, network, and
application workload scenarios, with the goal of understanding when to use
local on-device processing and when to offload computations. We present models
that leverage analytical queuing results to capture the effects of dynamic
factors such as the performance gap between the device and edge server, network
variability, server load, and multi-tenancy on the edge server. We
experimentally demonstrate the accuracy of our models for a range of hardware
and application scenarios and show that our models achieve a mean absolute
percentage error of 2.2% compared to observed latencies. We use our models to
develop an adaptive resource manager for intelligent offloading and show its
efficacy in the presence of variable network conditions and dynamic
multi-tenant edge settings.
</summary>
    <author>
      <name>Nathan Ng</name>
    </author>
    <author>
      <name>David Irwin</name>
    </author>
    <author>
      <name>Ananthram Swami</name>
    </author>
    <author>
      <name>Don Towsley</name>
    </author>
    <author>
      <name>Prashant Shenoy</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15149v1</id>
    <updated>2025-04-21T14:52:56Z</updated>
    <published>2025-04-21T14:52:56Z</published>
    <title>Cosmological Constraints with Void Lensing I: the Simulation-Based
  Inference Framework</title>
    <summary>  We present a Simulation-Based Inference (SBI) framework for cosmological
parameter estimation via void lensing analysis. Despite the absence of an
analytical model of void lensing, SBI can effectively learn posterior
distributions through forward modeling of mock data. We develop a forward
modeling pipeline that accounts for both cosmology and the galaxy-halo
connection. By training a neural density estimator on simulated data, we infer
the posteriors of two cosmological parameters, $\Omega_m$ and $S_8$. Validation
tests are conducted on posteriors derived from different cosmological
parameters and a fiducial sample. The results demonstrate that SBI provides
unbiased estimates of mean values and accurate uncertainties. These findings
highlight the potential to apply void lensing analysis to observational data
even without an analytical void lensing model.
</summary>
    <author>
      <name>Chen Su</name>
    </author>
    <author>
      <name>Huanyuan Shan</name>
    </author>
    <author>
      <name>Cheng Zhao</name>
    </author>
    <author>
      <name>Wenshuo Xu</name>
    </author>
    <author>
      <name>Jiajun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
