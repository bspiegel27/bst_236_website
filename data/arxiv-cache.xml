<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-16T00:57:02Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-15T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">112218</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.09614v1</id>
    <updated>2025-05-14T17:59:35Z</updated>
    <published>2025-05-14T17:59:35Z</published>
    <title>Language Agents Mirror Human Causal Reasoning Biases. How Can We Help
  Them Think Like Scientists?</title>
    <summary>  Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.
</summary>
    <author>
      <name>Anthony GX-Chen</name>
    </author>
    <author>
      <name>Dongyan Lin</name>
    </author>
    <author>
      <name>Mandana Samiei</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <author>
      <name>Blake A. Richards</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <author>
      <name>Kenneth Marino</name>
    </author>
    <link href="http://arxiv.org/abs/2505.09614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09612v1</id>
    <updated>2025-05-14T17:59:17Z</updated>
    <published>2025-05-14T17:59:17Z</published>
    <title>Adaptively-weighted Nearest Neighbors for Matrix Completion</title>
    <summary>  In this technical note, we introduce and analyze AWNN: an adaptively weighted
nearest neighbor method for performing matrix completion. Nearest neighbor (NN)
methods are widely used in missing data problems across multiple disciplines
such as in recommender systems and for performing counterfactual inference in
panel data settings. Prior works have shown that in addition to being very
intuitive and easy to implement, NN methods enjoy nice theoretical guarantees.
However, the performance of majority of the NN methods rely on the appropriate
choice of the radii and the weights assigned to each member in the nearest
neighbor set and despite several works on nearest neighbor methods in the past
two decades, there does not exist a systematic approach of choosing the radii
and the weights without relying on methods like cross-validation. AWNN
addresses this challenge by judiciously balancing the bias variance trade off
inherent in weighted nearest-neighbor regression. We provide theoretical
guarantees for the proposed method under minimal assumptions and support the
theory via synthetic experiments.
</summary>
    <author>
      <name>Tathagata Sadhukhan</name>
    </author>
    <author>
      <name>Manit Paul</name>
    </author>
    <author>
      <name>Raaz Dwivedi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09608v1</id>
    <updated>2025-05-14T17:57:27Z</updated>
    <published>2025-05-14T17:57:27Z</published>
    <title>LightLab: Controlling Light Sources in Images with Diffusion Models</title>
    <summary>  We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.
</summary>
    <author>
      <name>Nadav Magar</name>
    </author>
    <author>
      <name>Amir Hertz</name>
    </author>
    <author>
      <name>Eric Tabellion</name>
    </author>
    <author>
      <name>Yael Pritch</name>
    </author>
    <author>
      <name>Alex Rav-Acha</name>
    </author>
    <author>
      <name>Ariel Shamir</name>
    </author>
    <author>
      <name>Yedid Hoshen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3721238.3730696</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3721238.3730696" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://nadmag.github.io/LightLab/</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09600v1</id>
    <updated>2025-05-14T17:50:28Z</updated>
    <published>2025-05-14T17:50:28Z</published>
    <title>Accelerating the time-domain LISA response model with central finite
  differences and hybridization techniques</title>
    <summary>  Accurate and efficient modeling of the Laser Interferometer Space Antenna
(LISA) response is crucial for gravitational-wave (GW) data analysis. A key
computational challenge lies in evaluating time-delay interferometry (TDI)
variables, which require projecting GW polarizations onto the LISA arms at
different retarded times. Without approximations, the full LISA response is
computationally expensive, and traditional approaches, such as the
long-wavelength approximation, accelerate the response calculation at the cost
of reducing accuracy at high frequencies. In this work, we introduce a novel
hybrid time-domain response for LISA that balances computational efficiency and
accuracy across the binary's evolution. Our method implements a fast
low-frequency approximation during the early inspiral$\unicode{x2013}$where
most binaries spend most of the time in the sensitive frequency band of
LISA$\unicode{x2013}$while reserving the computationally intensive
full-response calculations for the late inspiral, merger, and ringdown phases.
The low-frequency approximation (LFA) is based on Taylor expanding the response
quantities around a chosen evaluation time such that time delays correspond to
central finite differences. Our hybrid approach supports CPU and GPU
implementations, TDI generations 1.5 and 2.0, and flexible time-delay
complexity, and has the potential to accelerate parts of the global fit and
reduce energy consumption. We also test our LFA and hybrid responses on
eccentric binaries, and we perform parameter estimation for a "golden" binary.
Additionally, we assess the efficacy of our low-frequency response for "deep
alerts" by performing inspiral-only Bayesian inference.
</summary>
    <author>
      <name>Jorge Valencia</name>
    </author>
    <author>
      <name>Sascha Husa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09598v1</id>
    <updated>2025-05-14T17:47:00Z</updated>
    <published>2025-05-14T17:47:00Z</published>
    <title>How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of
  LLM Inference</title>
    <summary>  As large language models (LLMs) spread across industries, understanding their
environmental footprint at the inference level is no longer optional; it is
essential. However, most existing studies exclude proprietary models, overlook
infrastructural variability and overhead, or focus solely on training, even as
inference increasingly dominates AI's environmental impact. To bridge this gap,
this paper introduces a novel infrastructure-aware benchmarking framework for
quantifying the environmental footprint of LLM inference across 30
state-of-the-art models as deployed in commercial data centers. Our framework
combines public API performance data with region-specific environmental
multipliers and statistical inference of hardware configurations. We
additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank
models by performance relative to environmental cost. Our results show that o3
and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33
Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and
that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short
GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results
in substantial annual environmental impacts. These include electricity use
comparable to 35,000 U.S. homes, freshwater evaporation matching the annual
drinking needs of 1.2 million people, and carbon emissions requiring a
Chicago-sized forest to offset. These findings illustrate a growing paradox:
although individual queries are efficient, their global scale drives
disproportionate resource consumption. Our study provides a standardized,
empirically grounded methodology for benchmarking the sustainability of LLM
deployments, laying a foundation for future environmental accountability in AI
development and sustainability standards.
</summary>
    <author>
      <name>Nidhal Jegham</name>
    </author>
    <author>
      <name>Marwen Abdelatti</name>
    </author>
    <author>
      <name>Lassad Elmoubarki</name>
    </author>
    <author>
      <name>Abdeltawab Hendawi</name>
    </author>
    <link href="http://arxiv.org/abs/2505.09598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09578v1</id>
    <updated>2025-05-14T17:30:15Z</updated>
    <published>2025-05-14T17:30:15Z</published>
    <title>The vertical structure of debris discs and the role of disc gravity</title>
    <summary>  Debris discs provide valuable insights into the formation and evolution of
exoplanetary systems. Their structures are commonly attributed to planetary
perturbations, serving as probes of as-yet-undetected planets. However, most
studies of planet-debris disc interactions ignore the disc's gravity, treating
it as a collection of massless planetesimals. Here, using an analytical model,
we investigate how the vertical structure of a back-reacting debris disc
responds to secular perturbations from an inner, inclined planet. Considering
the disc's axisymmetric potential, we identify two dynamical regimes:
planet-dominated and disc-dominated, which may coexist, separated by a
secular-inclination resonance. In the planet-dominated regime ($M_d/m_p\ll1$),
we recover the classical result: a transient warp propagates outward until the
disc settles into a box-like structure centered around the planetary orbit's
initial inclination $I_p(0)$, with a distance-independent aspect ratio
$\mathcal{H}(R)\approx I_p(0)$. In contrast, in the disc-dominated regime
($M_d/m_p\gtrsim1$), the disc exhibits dynamical rigidity, remaining thin and
misaligned, with significantly suppressed inclinations and a sharply declining
aspect ratio, $\mathcal{H}(R)\propto I_p(0)R^{-7/2}$. In the intermediate
regime ($M_d/m_p\lesssim1$), the system exhibits a secular-inclination
resonance, leading to long-lived, warp-like structures and a bimodal
inclination distribution, containing both dynamically hot and cold populations.
We provide analytic formulae describing these effects as a function of system
parameters. We also find that the vertical density profile is intrinsically
non-Gaussian and recommend fitting observations with non-zero slopes of
$\mathcal{H}(R)$. Our results may be used to infer planetary parameters and
debris disc masses based on observed warps and scale heights, as demonstrated
for HD110058 and $\beta$ Pic.
</summary>
    <author>
      <name>Antranik A. Sefilian</name>
    </author>
    <author>
      <name>Kaitlin M. Kratter</name>
    </author>
    <author>
      <name>Mark C. Wyatt</name>
    </author>
    <author>
      <name>Cristobal Petrovich</name>
    </author>
    <author>
      <name>Philippe Thébault</name>
    </author>
    <author>
      <name>Renu Malhotra</name>
    </author>
    <author>
      <name>Virginie Faramaz-Gorka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to MNRAS (28 Pages, 13 Figures, 1 Table). Comments are
  welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09561v1</id>
    <updated>2025-05-14T17:00:47Z</updated>
    <published>2025-05-14T17:00:47Z</published>
    <title>Learning Long-Context Diffusion Policies via Past-Token Prediction</title>
    <summary>  Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.
</summary>
    <author>
      <name>Marcel Torne</name>
    </author>
    <author>
      <name>Andy Tang</name>
    </author>
    <author>
      <name>Yuejiang Liu</name>
    </author>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Videos are available at https://long-context-dp.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09551v1</id>
    <updated>2025-05-14T16:49:47Z</updated>
    <published>2025-05-14T16:49:47Z</published>
    <title>Fast Learning in Quantitative Finance with Extreme Learning Machine</title>
    <summary>  This paper demonstrates that a broad class of problems in quantitative
finance, including those previously addressed using deep neural networks, can
be efficiently solved using single-layer neural networks without iterative
gradient-based training, namely extreme learning machine (ELM). ELM utilizes a
single-layer network with randomly initialized hidden nodes and analytically
computed output weights obtained via convex optimization, enabling rapid
training and inference. Both supervised and unsupervised learning tasks are
explored.
  In supervised learning, ELM is employed to learn parametric option pricing
functions, predict intraday stock returns, and complete implied volatility
surfaces. Compared with deep neural networks, Gaussian process regression, and
logistic regression, ELM achieves higher computational speed, comparable
accuracy, and superior generalization.
  In unsupervised learning, ELM numerically solves Black-Scholes-type PDEs, and
outperforms Physics-Informed Neural Networks in training speed without losing
precision. The approximation and generalization abilities of ELM are briefly
discussed.
  The findings establish ELM as a practical and efficient tool for various
tasks in quantitative finance.
</summary>
    <author>
      <name>Liexin Cheng</name>
    </author>
    <author>
      <name>Xue Cheng</name>
    </author>
    <author>
      <name>Shuaiqiang Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.09551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09544v1</id>
    <updated>2025-05-14T16:42:36Z</updated>
    <published>2025-05-14T16:42:36Z</published>
    <title>Constraining Superluminal Einstein-Æther Gravity through
  Gravitational Memory</title>
    <summary>  Every emission of radiation in gravity also includes a non-wave-like
component that leaves a permanent change in proper distances of the spacetime
it travels through. This phenomenon is known as gravitational displacement
memory. Building up on a recently developed computation framework that
harnesses Isaacson's insights on a fundamental definition of gravitational
waves, we compute the leading displacement memory formula in Einstein-Aether
gravity. Our analysis represents the first direct calculation of gravitational
memory in a metric theory with non-trivial asymptotic vector field value. We
find that an emission of scalar and vector aether waves at a propagation speed
greater than the speed of tensor radiation features unprotected causal
directions with a priori unbound memory build-up. Based on the results, we
conjecture a stringent exclusion of the superluminal parameter space of
Einstein-Aether gravity.
</summary>
    <author>
      <name>Lavinia Heisenberg</name>
    </author>
    <author>
      <name>Benedetta Rosatello</name>
    </author>
    <author>
      <name>Guangzi Xu</name>
    </author>
    <author>
      <name>Jann Zosso</name>
    </author>
    <link href="http://arxiv.org/abs/2505.09544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09450v1</id>
    <updated>2025-05-14T15:01:59Z</updated>
    <published>2025-05-14T15:01:59Z</published>
    <title>MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating
  Motion during Ultrasound-Guided Aspiration Biopsy</title>
    <summary>  Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.
</summary>
    <author>
      <name>Yuelin Zhang</name>
    </author>
    <author>
      <name>Qingpeng Ding</name>
    </author>
    <author>
      <name>Long Lei</name>
    </author>
    <author>
      <name>Yongxuan Feng</name>
    </author>
    <author>
      <name>Raymond Shing-Yan Tang</name>
    </author>
    <author>
      <name>Shing Shin Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Early Accepted by MICCAI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
