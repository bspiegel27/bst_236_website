<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-10T00:53:16Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-09T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">110315</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.06266v1</id>
    <updated>2025-04-08T17:59:59Z</updated>
    <published>2025-04-08T17:59:59Z</published>
    <title>Constraining the [CII] luminosity function from the power spectrum of
  line intensity maps at redshift 3.6</title>
    <summary>  Forthcoming measurements of the line-intensity-mapping power spectrum (PS)
are expected to set precious constraints on several quantities of astrophysical
and cosmological interest. Our study targets the [CII] luminosity function (LF)
at high redshift, which is still highly uncertain, in particular at the faint
end. As an example of future opportunities, we present forecasts for the Deep
Spectroscopic Survey (DSS) that will be conducted with the Fred Young
Submillimeter Telescope at $z \simeq 3.6$ and also make predictions for
eventual $10\times$ wider and/or $\sqrt{10}\times$ more sensitive surveys. The
halo-occupation properties of [CII] emitters in the MARIGOLD simulations
provide us with the motivation to abundance match two versions of the ALPINE LF
against the halo mass function. We employ the resulting luminosity-mass
relation within the halo model to predict the expected PS signal and its
uncertainty. Finally, we use Bayesian inference to analyse mock PS data and
forecast what constraints could be achieved on the first two moments of the LF
and on Schechter fits. Depending on the actual LF, the DSS will measure the
clustering and shot-noise amplitudes of the PS with a signal-to-noise ratio of
$\sim 3$ or higher. However, degeneracies with the bias parameter and
redshift-space distortions make it unfeasible to extract the first moment of
the LF. Even the widest and most sensitive survey we consider can only
constrain it with a $50\%$ uncertainty. By jointly fitting the PS and the LF,
we directly constrain Schechter-function parameters. We find that the
normalisation and the cutoff luminosity are precisely and accurately measured
while the faint-end slope remains highly uncertain (unless the true value
approaches $-2$). Overall, increasing the survey sensitivity at fixed sky
coverage yields greater improvements than covering a larger area at fixed
sensitivity.
</summary>
    <author>
      <name>Elena Marcuzzo</name>
    </author>
    <author>
      <name>Cristiano Porciani</name>
    </author>
    <author>
      <name>Emilio Romano-Díaz</name>
    </author>
    <author>
      <name>Prachi Khatri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 18 figures, 5 tables. Submitted to A&amp;A</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06261v1</id>
    <updated>2025-04-08T17:59:41Z</updated>
    <published>2025-04-08T17:59:41Z</published>
    <title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
    <summary>  Large Language Models (LLMs) have demonstrated the ability to tackle
increasingly complex tasks through advanced reasoning, long-form content
generation, and tool use. Solving these tasks often involves long
inference-time computations. In human problem solving, a common strategy to
expedite work is collaboration: by dividing the problem into sub-tasks,
exploring different strategies concurrently, etc. Recent research has shown
that LLMs can also operate in parallel by implementing explicit cooperation
frameworks, such as voting mechanisms or the explicit creation of independent
sub-tasks that can be executed in parallel. However, each of these frameworks
may not be suitable for all types of tasks, which can hinder their
applicability. In this work, we propose a different design approach: we run LLM
"workers" in parallel , allowing them to synchronize via a concurrently-updated
attention cache and prompt these workers to decide how best to collaborate. Our
approach allows the instances to come up with their own collaboration strategy
for the problem at hand, all the while "seeing" each other's partial progress
in the concurrent cache. We implement this approach via Hogwild! Inference: a
parallel LLM inference engine where multiple instances of the same LLM run in
parallel with the same attention cache, with "instant" access to each other's
generated tokens. Hogwild! inference takes advantage of Rotary Position
Embeddings (RoPE) to avoid recomputation while improving parallel hardware
utilization. We find that modern reasoning-capable LLMs can perform inference
with shared Key-Value cache out of the box, without additional fine-tuning.
</summary>
    <author>
      <name>Gleb Rodionov</name>
    </author>
    <author>
      <name>Roman Garipov</name>
    </author>
    <author>
      <name>Alina Shutova</name>
    </author>
    <author>
      <name>George Yakushev</name>
    </author>
    <author>
      <name>Vage Egiazarian</name>
    </author>
    <author>
      <name>Anton Sinitsin</name>
    </author>
    <author>
      <name>Denis Kuznedelev</name>
    </author>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint, work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06252v1</id>
    <updated>2025-04-08T17:57:10Z</updated>
    <published>2025-04-08T17:57:10Z</published>
    <title>A systematic method to identify runaways from star clusters produced
  from single-binary interactions: A case study of M67</title>
    <summary>  One hypothesis for runaway stars (RSs) is that they are ejected from star
clusters with high velocities relative to the cluster center-of-mass motion.
There are two competing mechanisms for their production: supernova-based
ejections in binaries, where one companion explodes, leaves no remnant, and
launches the other companion at the instantaneous orbital velocity, and the
disintegration of triples (or higher-order multiples), which produces a
recoiled runaway binary (RB) and an RS. We search for RS candidates using data
from the Gaia DR3 survey with a focus on triple disintegration since in this
case the product is always a binary and a single star that should be moving in
opposite directions. We created a systematic methodology to look for candidate
RS-RB runaway pairs produced from the disintegration of bound three-body
systems formed from single-binary interactions based on momentum conservation
and causality. The method we use is general and can be applied to any cluster
with a 5D kinematic data set. We used our criteria to search for these pairs in
a 150 pc circular field of view surrounding the open cluster M67, which we used
as a benchmark cluster to test the robustness of our method. Our results reveal
only one RS-RB pair that is consistent with all of our selection criteria out
of an initial sample of $10^8$ pairs.
</summary>
    <author>
      <name>A. Herrera-Urquieta</name>
    </author>
    <author>
      <name>N. Leigh</name>
    </author>
    <author>
      <name>J. Pinto</name>
    </author>
    <author>
      <name>G. Díaz-Cerda</name>
    </author>
    <author>
      <name>S. M. Grondin</name>
    </author>
    <author>
      <name>J. J. Webb</name>
    </author>
    <author>
      <name>R. Mathieu</name>
    </author>
    <author>
      <name>T. Ryu</name>
    </author>
    <author>
      <name>A. Geller</name>
    </author>
    <author>
      <name>M. Kounkel</name>
    </author>
    <author>
      <name>S. Toonen</name>
    </author>
    <author>
      <name>M. Vilaxa-Campos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06235v1</id>
    <updated>2025-04-08T17:32:56Z</updated>
    <published>2025-04-08T17:32:56Z</published>
    <title>Decentralized Federated Domain Generalization with Style Sharing: A
  Formal Modeling and Convergence Analysis</title>
    <summary>  Much of the federated learning (FL) literature focuses on settings where
local dataset statistics remain the same between training and testing time.
Recent advances in domain generalization (DG) aim to use data from source
(training) domains to train a model that generalizes well to data from unseen
target (testing) domains. In this paper, we are motivated by two major gaps in
existing work on FL and DG: (1) the lack of formal mathematical analysis of DG
objectives and training processes; and (2) DG research in FL being limited to
the conventional star-topology architecture. Addressing the second gap, we
develop $\textit{Decentralized Federated Domain Generalization with Style
Sharing}$ ($\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to
allow devices in a peer-to-peer network to achieve DG based on sharing style
information inferred from their datasets. Additionally, we fill the first gap
by providing the first systematic approach to mathematically analyzing
style-based DG training optimization. We cast existing centralized DG
algorithms within our framework, and employ their formalisms to model
$\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which
a sub-linear convergence rate of $\texttt{StyleDDG}$ can be obtained. Through
experiments on two popular DG datasets, we demonstrate that $\texttt{StyleDDG}$
can obtain significant improvements in accuracy across target domains with
minimal added communication overhead compared to decentralized gradient methods
that do not employ style sharing.
</summary>
    <author>
      <name>Shahryar Zehtabi</name>
    </author>
    <author>
      <name>Dong-Jun Han</name>
    </author>
    <author>
      <name>Seyyedali Hosseinalipour</name>
    </author>
    <author>
      <name>Christopher G. Brinton</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06225v1</id>
    <updated>2025-04-08T17:13:41Z</updated>
    <published>2025-04-08T17:13:41Z</published>
    <title>Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via
  Adaptation</title>
    <summary>  While decoder-only large language models (LLMs) have shown impressive
results, encoder-decoder models are still widely adopted in real-world
applications for their inference efficiency and richer encoder representation.
In this paper, we study a novel problem: adapting pretrained decoder-only LLMs
to encoder-decoder, with the goal of leveraging the strengths of both
approaches to achieve a more favorable quality-efficiency trade-off. We argue
that adaptation not only enables inheriting the capability of decoder-only LLMs
but also reduces the demand for computation compared to pretraining from
scratch. We rigorously explore different pretraining objectives and parameter
initialization/optimization techniques. Through extensive experiments based on
Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to
1.6B), we demonstrate the effectiveness of adaptation and the advantage of
encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs
achieve comparable (often better) pretraining performance but substantially
better finetuning performance than their decoder-only counterpart. For example,
Gemma 2B-2B outperforms Gemma 2B by $\sim$7\% after instruction tuning.
Encoder-decoder adaptation also allows for flexible combination of
different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B
by $&gt;$3\%. The adapted encoder representation also yields better results on
SuperGLUE. We will release our checkpoints to facilitate future research.
</summary>
    <author>
      <name>Biao Zhang</name>
    </author>
    <author>
      <name>Fedor Moiseev</name>
    </author>
    <author>
      <name>Joshua Ainslie</name>
    </author>
    <author>
      <name>Paul Suganthan</name>
    </author>
    <author>
      <name>Min Ma</name>
    </author>
    <author>
      <name>Surya Bhupatiraju</name>
    </author>
    <author>
      <name>Fede Lebron</name>
    </author>
    <author>
      <name>Orhan Firat</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Zhe Dong</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06215v1</id>
    <updated>2025-04-08T17:00:42Z</updated>
    <published>2025-04-08T17:00:42Z</published>
    <title>Randomization Inference in Two-Sided Market Experiments</title>
    <summary>  Randomized experiments are increasingly employed in two-sided markets, such
as buyer-seller platforms, to evaluate treatment effects from marketplace
interventions. These experiments must reflect the underlying two-sided market
structure in their design (e.g., sellers and buyers), making them particularly
challenging to analyze. In this paper, we propose a randomization inference
framework to analyze outcomes from such two-sided experiments. Our approach is
finite-sample valid under sharp null hypotheses for any test statistic and
maintains asymptotic validity under weak null hypotheses through
studentization. Moreover, we provide heuristic guidance for choosing among
multiple valid randomization tests to enhance statistical power, which we
demonstrate empirically. Finally, we demonstrate the performance of our
methodology through a series of simulation studies.
</summary>
    <author>
      <name>Jizhou Liu</name>
    </author>
    <author>
      <name>Azeem M. Shaikh</name>
    </author>
    <author>
      <name>Panos Toulis</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06214v1</id>
    <updated>2025-04-08T16:58:58Z</updated>
    <published>2025-04-08T16:58:58Z</published>
    <title>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language
  Models</title>
    <summary>  Long-context capabilities are essential for a wide range of applications,
including document and video understanding, in-context learning, and
inference-time scaling, all of which require models to process and reason over
long sequences of text and multimodal data. In this work, we introduce a
efficient training recipe for building ultra-long context LLMs from aligned
instruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,
and 4M tokens. Our approach leverages efficient continued pretraining
strategies to extend the context window and employs effective instruction
tuning to maintain the instruction-following and reasoning abilities. Our
UltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves
state-of-the-art performance across a diverse set of long-context benchmarks.
Importantly, models trained with our approach maintain competitive performance
on standard benchmarks, demonstrating balanced improvements for both long and
short context tasks. We further provide an in-depth analysis of key design
choices, highlighting the impacts of scaling strategies and data composition.
Our findings establish a robust framework for efficiently scaling context
lengths while preserving general model capabilities. We release all model
weights at: https://ultralong.github.io/.
</summary>
    <author>
      <name>Chejian Xu</name>
    </author>
    <author>
      <name>Wei Ping</name>
    </author>
    <author>
      <name>Peng Xu</name>
    </author>
    <author>
      <name>Zihan Liu</name>
    </author>
    <author>
      <name>Boxin Wang</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06185v1</id>
    <updated>2025-04-08T16:25:59Z</updated>
    <published>2025-04-08T16:25:59Z</published>
    <title>WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and
  Real-World Wound Care</title>
    <summary>  Chronic wounds affect a large population, particularly the elderly and
diabetic patients, who often exhibit limited mobility and co-existing health
conditions. Automated wound monitoring via mobile image capture can reduce
in-person physician visits by enabling remote tracking of wound size. Semantic
segmentation is key to this process, yet wound segmentation remains
underrepresented in medical imaging research. To address this, we benchmark
state-of-the-art deep learning models from general-purpose vision, medical
imaging, and top methods from public wound challenges. For fair comparison, we
standardize training, data augmentation, and evaluation, conducting
cross-validationto minimize partitioning bias. We also assess real-world
deployment aspects, including generalization to an out-of-distribution wound
dataset, computational efficiency, and interpretability. Additionally, we
propose a reference object-based approach to convert AI-generated masks into
clinically relevant wound size estimates, and evaluate this, along with mask
quality, for the best models based on physician assessments. Overall, the
transformer-based TransNeXt showed the highest levels of generalizability.
Despite variations in inference times, all models processed at least one image
per second on the CPU, which is deemed adequate for the intended application.
Interpretability analysis typically revealed prominent activations in wound
regions, emphasizing focus on clinically relevant features. Expert evaluation
showed high mask approval for all analyzed models, with VWFormer and ConvNeXtS
backbone performing the best. Size retrieval accuracy was similar across
models, and predictions closely matched expert annotations. Finally, we
demonstrate how our AI-driven wound size estimation framework, WoundAmbit, can
be integrated into a custom telehealth system. Our code will be made available
on GitHub upon publication.
</summary>
    <author>
      <name>Vanessa Borst</name>
    </author>
    <author>
      <name>Timo Dittus</name>
    </author>
    <author>
      <name>Tassilo Dege</name>
    </author>
    <author>
      <name>Astrid Schmieder</name>
    </author>
    <author>
      <name>Samuel Kounev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Main paper: 17 pages; supplementary material: 16 pages; paper
  submitted to the application track of the European Conference on Machine
  Learning and Principles and Practice of Knowledge Discovery in Databases
  (ECML PKDD 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06174v1</id>
    <updated>2025-04-08T16:18:39Z</updated>
    <published>2025-04-08T16:18:39Z</published>
    <title>On Soft Clustering For Correlation Estimators: Model Uncertainty,
  Differentiability, and Surrogates</title>
    <summary>  Properly estimating correlations between objects at different spatial scales
necessitates $\mathcal{O}(n^2)$ distance calculations. For this reason, most
widely adopted packages for estimating correlations use clustering algorithms
to approximate local trends. However, methods for quantifying the error
introduced by this clustering have been understudied. In response, we present
an algorithm for estimating correlations that is probabilistic in the way that
it clusters objects, enabling us to quantify the uncertainty caused by
clustering simply through model inference. These soft clustering assignments
enable correlation estimators that are theoretically differentiable with
respect to their input catalogs. Thus, we also build a theoretical framework
for differentiable correlation functions and describe their utility in
comparison to existing surrogate models. Notably, we find that repeated
normalization and distance function calls slow gradient calculations and that
sparse Jacobians destabilize precision, pointing towards either approximate or
surrogate methods as a necessary solution to exact gradients from correlation
functions. To that end, we close with a discussion of surrogate models as
proxies for correlation functions. We provide an example that demonstrates the
efficacy of surrogate models to enable gradient-based optimization of
astrophysical model parameters, successfully minimizing a correlation function
output. Our numerical experiments cover science cases across cosmology, from
point spread function (PSF) modeling efforts to gravitational simulations to
galaxy intrinsic alignment (IA).
</summary>
    <author>
      <name>Edward Berman</name>
    </author>
    <author>
      <name>Sneh Pandya</name>
    </author>
    <author>
      <name>Jacqueline McCleary</name>
    </author>
    <author>
      <name>Marko Shuntov</name>
    </author>
    <author>
      <name>Caitlin Casey</name>
    </author>
    <author>
      <name>Nicole Drakos</name>
    </author>
    <author>
      <name>Andreas Faisst</name>
    </author>
    <author>
      <name>Steven Gillman</name>
    </author>
    <author>
      <name>Ghassem Gozaliasl</name>
    </author>
    <author>
      <name>Natalie Hogg</name>
    </author>
    <author>
      <name>Jeyhan Kartaltepe</name>
    </author>
    <author>
      <name>Anton Koekemoer</name>
    </author>
    <author>
      <name>Wilfried Mercier</name>
    </author>
    <author>
      <name>Diana Scognamiglio</name>
    </author>
    <author>
      <name> COSMOS-Web</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>The JWST Cosmic Origins Survey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to OjA. Code available at
  https://github.com/EdwardBerman/cosmo-corr</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06159v1</id>
    <updated>2025-04-08T15:55:33Z</updated>
    <published>2025-04-08T15:55:33Z</published>
    <title>Enhancing Primary Teacher Training through Academic Portfolios in
  Advanced Mathematics Courses</title>
    <summary>  The gap between theory and practice in mathematics education, particularly in
primary-teacher education, necessitates innovative teaching methodologies. This
paper explores the implementation of academic portfolios as a teaching
innovation in Algebra and Number Systems I and II courses within the primary
teacher education programme at Pontificia Universidad Cat\'olica de Chile. The
methodology involved integrating academic portfolios to align course content
with essential learning outcomes for future teaching roles. Implementation
begins with a negotiation between students and teachers to establish a learning
contract, followed by an overview of course rules, content, objectives,
materials, and grading rubrics. Preliminary findings indicate that this
innovative method enhances engagement with mathematical concepts, improves
assessment efficacy in teacher training, and may contribute to enhanced
preparation of primary mathematics teachers. The study highlights the role of
portfolios in making students active participants in their learning,
significantly enhancing the educational experience of teacher candidates. These
findings suggest a promising avenue for future educational assessments and
methodologies in mathematics, indicating that academic portfolios can bridge
the gap between theoretical knowledge and practical applications in mathematics
teacher education, potentially enhancing teacher preparation. While this study
shows promising results, further research with larger samples and longer
timeframes would be beneficial to establish causality and long-term impacts.
</summary>
    <author>
      <name>Carlos Rojas Bruna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Mathematics Education Assessment, Teacher Training
  Outcomes, Academic Portfolio, Teaching Innovation</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
