<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-04T00:52:48Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-03T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">109989</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.01959v1</id>
    <updated>2025-04-02T17:59:45Z</updated>
    <published>2025-04-02T17:59:45Z</published>
    <title>Slot-Level Robotic Placement via Visual Imitation from Single Human
  Video</title>
    <summary>  The majority of modern robot learning methods focus on learning a set of
pre-defined tasks with limited or no generalization to new tasks. Extending the
robot skillset to novel tasks involves gathering an extensive amount of
training data for additional tasks. In this paper, we address the problem of
teaching new tasks to robots using human demonstration videos for repetitive
tasks (e.g., packing). This task requires understanding the human video to
identify which object is being manipulated (the pick object) and where it is
being placed (the placement slot). In addition, it needs to re-identify the
pick object and the placement slots during inference along with the relative
poses to enable robot execution of the task. To tackle this, we propose SLeRP,
a modular system that leverages several advanced visual foundation models and a
novel slot-level placement detector Slot-Net, eliminating the need for
expensive video demonstrations for training. We evaluate our system using a new
benchmark of real-world videos. The evaluation results show that SLeRP
outperforms several baselines and can be deployed on a real robot.
</summary>
    <author>
      <name>Dandan Shan</name>
    </author>
    <author>
      <name>Kaichun Mo</name>
    </author>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Yu-Wei Chao</name>
    </author>
    <author>
      <name>David Fouhey</name>
    </author>
    <author>
      <name>Dieter Fox</name>
    </author>
    <author>
      <name>Arsalan Mousavian</name>
    </author>
    <link href="http://arxiv.org/abs/2504.01959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01956v2</id>
    <updated>2025-04-03T14:07:13Z</updated>
    <published>2025-04-02T17:59:21Z</published>
    <title>VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in
  One Step</title>
    <summary>  Recovering 3D scenes from sparse views is a challenging task due to its
inherent ill-posed problem. Conventional methods have developed specialized
solutions (e.g., geometry regularization or feed-forward deterministic model)
to mitigate the issue. However, they still suffer from performance degradation
by minimal overlap across input views with insufficient visual information.
Fortunately, recent video generative models show promise in addressing this
challenge as they are capable of generating video clips with plausible 3D
structures. Powered by large pretrained video diffusion models, some pioneering
research start to explore the potential of video generative prior and create 3D
scenes from sparse views. Despite impressive improvements, they are limited by
slow inference time and the lack of 3D constraint, leading to inefficiencies
and reconstruction artifacts that do not align with real-world geometry
structure. In this paper, we propose VideoScene to distill the video diffusion
model to generate 3D scenes in one step, aiming to build an efficient and
effective tool to bridge the gap from video to 3D. Specifically, we design a
3D-aware leap flow distillation strategy to leap over time-consuming redundant
information and train a dynamic denoising policy network to adaptively
determine the optimal leap timestep during inference. Extensive experiments
demonstrate that our VideoScene achieves faster and superior 3D scene
generation results than previous video diffusion models, highlighting its
potential as an efficient tool for future video to 3D applications. Project
Page: https://hanyang-21.github.io/VideoScene
</summary>
    <author>
      <name>Hanyang Wang</name>
    </author>
    <author>
      <name>Fangfu Liu</name>
    </author>
    <author>
      <name>Jiawei Chi</name>
    </author>
    <author>
      <name>Yueqi Duan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2025; Project Page:
  https://hanyang-21.github.io/VideoScene</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.01956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01951v1</id>
    <updated>2025-04-02T17:56:08Z</updated>
    <published>2025-04-02T17:56:08Z</published>
    <title>The LLM Wears Prada: Analysing Gender Bias and Stereotypes through
  Online Shopping Data</title>
    <summary>  With the wide and cross-domain adoption of Large Language Models, it becomes
crucial to assess to which extent the statistical correlations in training
data, which underlie their impressive performance, hide subtle and potentially
troubling biases. Gender bias in LLMs has been widely investigated from the
perspectives of works, hobbies, and emotions typically associated with a
specific gender. In this study, we introduce a novel perspective. We
investigate whether LLMs can predict an individual's gender based solely on
online shopping histories and whether these predictions are influenced by
gender biases and stereotypes. Using a dataset of historical online purchases
from users in the United States, we evaluate the ability of six LLMs to
classify gender and we then analyze their reasoning and products-gender
co-occurrences. Results indicate that while models can infer gender with
moderate accuracy, their decisions are often rooted in stereotypical
associations between product categories and gender. Furthermore, explicit
instructions to avoid bias reduce the certainty of model predictions, but do
not eliminate stereotypical patterns. Our findings highlight the persistent
nature of gender biases in LLMs and emphasize the need for robust
bias-mitigation strategies.
</summary>
    <author>
      <name>Massimiliano Luca</name>
    </author>
    <author>
      <name>Ciro Beneduce</name>
    </author>
    <author>
      <name>Bruno Lepri</name>
    </author>
    <author>
      <name>Jacopo Staiano</name>
    </author>
    <link href="http://arxiv.org/abs/2504.01951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01935v1</id>
    <updated>2025-04-02T17:45:58Z</updated>
    <published>2025-04-02T17:45:58Z</published>
    <title>Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning
  Length?</title>
    <summary>  Large language models (LLMs) often benefit from verbalized reasoning at
inference time, but it remains unclear which aspects of task difficulty these
extra reasoning tokens address. To investigate this question, we formalize a
framework using deterministic finite automata (DFAs). DFAs offer a formalism
through which we can characterize task complexity through measurable properties
such as run length (number of reasoning steps required) and state-space size
(decision complexity). We first show that across different tasks and models of
different sizes and training paradigms, there exists an optimal amount of
reasoning tokens such that the probability of producing a correct solution is
maximized. We then investigate which properties of complexity govern this
critical length: we find that task instances with longer corresponding
underlying DFA runs (i.e. demand greater latent state-tracking requirements)
correlate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.
state-space complexity) does not. We then demonstrate an implication of these
findings: being able to predict the optimal number of reasoning tokens for new
problems and filtering out non-optimal length answers results in consistent
accuracy improvements.
</summary>
    <author>
      <name>Celine Lee</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <author>
      <name>Keyon Vafa</name>
    </author>
    <link href="http://arxiv.org/abs/2504.01935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01933v1</id>
    <updated>2025-04-02T17:42:31Z</updated>
    <published>2025-04-02T17:42:31Z</published>
    <title>Hessian-aware Training for Enhancing DNNs Resilience to Parameter
  Corruptions</title>
    <summary>  Deep neural networks are not resilient to parameter corruptions: even a
single-bitwise error in their parameters in memory can cause an accuracy drop
of over 10%, and in the worst cases, up to 99%. This susceptibility poses great
challenges in deploying models on computing platforms, where adversaries can
induce bit-flips through software or bitwise corruptions may occur naturally.
Most prior work addresses this issue with hardware or system-level approaches,
such as integrating additional hardware components to verify a model's
integrity at inference. However, these methods have not been widely deployed as
they require infrastructure or platform-wide modifications.
  In this paper, we propose a new approach to addressing this issue: training
models to be more resilient to bitwise corruptions to their parameters. Our
approach, Hessian-aware training, promotes models with $flatter$ loss surfaces.
We show that, while there have been training methods, designed to improve
generalization through Hessian-based approaches, they do not enhance resilience
to parameter corruptions. In contrast, models trained with our method
demonstrate increased resilience to parameter corruptions, particularly with a
20$-$50% reduction in the number of bits whose individual flipping leads to a
90$-$100% accuracy drop. Moreover, we show the synergy between ours and
existing hardware and system-level defenses.
</summary>
    <author>
      <name>Tahmid Hasan Prato</name>
    </author>
    <author>
      <name>Seijoon Kim</name>
    </author>
    <author>
      <name>Lizhong Chen</name>
    </author>
    <author>
      <name>Sanghyun Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.01933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01931v1</id>
    <updated>2025-04-02T17:40:47Z</updated>
    <published>2025-04-02T17:40:47Z</published>
    <title>Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents
  with Dynamic Evaluation and Selection</title>
    <summary>  While AI agents have shown remarkable performance at various tasks, they
still struggle with complex multi-modal applications, structured generation and
strategic planning. Improvements via standard fine-tuning is often impractical,
as solving agentic tasks usually relies on black box API access without control
over model parameters. Inference-time methods such as Best-of-N (BON) sampling
offer a simple yet effective alternative to improve performance. However, BON
lacks iterative feedback integration mechanism. Hence, we propose Iterative
Agent Decoding (IAD) which combines iterative refinement with dynamic candidate
evaluation and selection guided by a verifier. IAD differs in how feedback is
designed and integrated, specifically optimized to extract maximal signal from
reward scores. We conduct a detailed comparison of baselines across key metrics
on Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms
baselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and
without LLM judges) and 8--10% gains on Webshop across multiple metrics. To
better understand the source of IAD's gains, we perform controlled experiments
to disentangle the effect of adaptive feedback from stochastic sampling, and
find that IAD's improvements are primarily driven by verifier-guided
refinement, not merely sampling diversity. We also show that both IAD and BON
exhibit inference-time scaling with increased compute when guided by an optimal
verifier. Our analysis highlights the critical role of verifier quality in
effective inference-time optimization and examines the impact of noisy and
sparse rewards on scaling behavior. Together, these findings offer key insights
into the trade-offs and principles of effective inference-time optimization.
</summary>
    <author>
      <name>Souradip Chakraborty</name>
    </author>
    <author>
      <name>Mohammadreza Pourreza</name>
    </author>
    <author>
      <name>Ruoxi Sun</name>
    </author>
    <author>
      <name>Yiwen Song</name>
    </author>
    <author>
      <name>Nino Scherrer</name>
    </author>
    <author>
      <name>Jindong Gu</name>
    </author>
    <author>
      <name>Furong Huang</name>
    </author>
    <author>
      <name>Amrit Singh Bedi</name>
    </author>
    <author>
      <name>Ahmad Beirami</name>
    </author>
    <author>
      <name>Hamid Palangi</name>
    </author>
    <author>
      <name>Tomas Pfister</name>
    </author>
    <link href="http://arxiv.org/abs/2504.01931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01928v1</id>
    <updated>2025-04-02T17:38:03Z</updated>
    <published>2025-04-02T17:38:03Z</published>
    <title>Is the Reversal Curse a Binding Problem? Uncovering Limitations of
  Transformers from a Basic Generalization Failure</title>
    <summary>  Despite their impressive capabilities, LLMs exhibit a basic generalization
failure known as the Reversal Curse, where they struggle to learn reversible
factual associations. Understanding why this occurs could help identify
weaknesses in current models and advance their generalization and robustness.
In this paper, we conjecture that the Reversal Curse in LLMs is a manifestation
of the long-standing binding problem in cognitive science, neuroscience and AI.
Specifically, we identify two primary causes of the Reversal Curse stemming
from transformers' limitations in conceptual binding: the inconsistency and
entanglements of concept representations. We perform a series of experiments
that support these conjectures. Our exploration leads to a model design based
on JEPA (Joint-Embedding Predictive Architecture) that for the first time
breaks the Reversal Curse without side-stepping it with specialized data
augmentation or non-causal masking, and moreover, generalization could be
further improved by incorporating special memory layers that support
disentangled concept representations. We demonstrate that the skill of reversal
unlocks a new kind of memory integration that enables models to solve
large-scale arithmetic reasoning problems via parametric forward-chaining,
outperforming frontier LLMs based on non-parametric memory and prolonged
explicit reasoning.
</summary>
    <author>
      <name>Boshi Wang</name>
    </author>
    <author>
      <name>Huan Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code and data:
  https://github.com/OSU-NLP-Group/reversal-curse-binding</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.01928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01913v1</id>
    <updated>2025-04-02T17:13:59Z</updated>
    <published>2025-04-02T17:13:59Z</published>
    <title>Representing Flow Fields with Divergence-Free Kernels for Reconstruction</title>
    <summary>  Accurately reconstructing continuous flow fields from sparse or indirect
measurements remains an open challenge, as existing techniques often suffer
from oversmoothing artifacts, reliance on heterogeneous architectures, and the
computational burden of enforcing physics-informed losses in implicit neural
representations (INRs). In this paper, we introduce a novel flow field
reconstruction framework based on divergence-free kernels (DFKs), which
inherently enforce incompressibility while capturing fine structures without
relying on hierarchical or heterogeneous representations. Through qualitative
analysis and quantitative ablation studies, we identify the matrix-valued
radial basis functions derived from Wendland's $\mathcal{C}^4$ polynomial
(DFKs-Wen4) as the optimal form of analytically divergence-free approximation
for velocity fields, owing to their favorable numerical properties, including
compact support, positive definiteness, and second-order differentiablility.
Experiments across various reconstruction tasks, spanning data compression,
inpainting, super-resolution, and time-continuous flow inference, has
demonstrated that DFKs-Wen4 outperform INRs and other divergence-free
representations in both reconstruction accuracy and computational efficiency
while requiring the fewest trainable parameters.
</summary>
    <author>
      <name>Xingyu Ni</name>
    </author>
    <author>
      <name>Jingrui Xing</name>
    </author>
    <author>
      <name>Xingqiao Li</name>
    </author>
    <author>
      <name>Bin Wang</name>
    </author>
    <author>
      <name>Baoquan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2504.01913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01894v1</id>
    <updated>2025-04-02T16:54:47Z</updated>
    <published>2025-04-02T16:54:47Z</published>
    <title>Multi-fidelity Parameter Estimation Using Conditional Diffusion Models</title>
    <summary>  We present a multi-fidelity method for uncertainty quantification of
parameter estimates in complex systems, leveraging generative models trained to
sample the target conditional distribution. In the Bayesian inference setting,
traditional parameter estimation methods rely on repeated simulations of
potentially expensive forward models to determine the posterior distribution of
the parameter values, which may result in computationally intractable
workflows. Furthermore, methods such as Markov Chain Monte Carlo (MCMC)
necessitate rerunning the entire algorithm for each new data observation,
further increasing the computational burden. Hence, we propose a novel method
for efficiently obtaining posterior distributions of parameter estimates for
high-fidelity models given data observations of interest. The method first
constructs a low-fidelity, conditional generative model capable of amortized
Bayesian inference and hence rapid posterior density approximation over a
wide-range of data observations. When higher accuracy is needed for a specific
data observation, the method employs adaptive refinement of the density
approximation. It uses outputs from the low-fidelity generative model to refine
the parameter sampling space, ensuring efficient use of the computationally
expensive high-fidelity solver. Subsequently, a high-fidelity, unconditional
generative model is trained to achieve greater accuracy in the target posterior
distribution. Both low- and high- fidelity generative models enable efficient
sampling from the target posterior and do not require repeated simulation of
the high-fidelity forward model. We demonstrate the effectiveness of the
proposed method on several numerical examples, including cases with multi-modal
densities, as well as an application in plasma physics for a runaway electron
simulation model.
</summary>
    <author>
      <name>Caroline Tatsuoka</name>
    </author>
    <author>
      <name>Minglei Yang</name>
    </author>
    <author>
      <name>Dongbin Xiu</name>
    </author>
    <author>
      <name>Guannan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2504.01894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01872v1</id>
    <updated>2025-04-02T16:27:44Z</updated>
    <published>2025-04-02T16:27:44Z</published>
    <title>CoMatcher: Multi-View Collaborative Feature Matching</title>
    <summary>  This paper proposes a multi-view collaborative matching strategy for reliable
track construction in complex scenarios. We observe that the pairwise matching
paradigms applied to image set matching often result in ambiguous estimation
when the selected independent pairs exhibit significant occlusions or extreme
viewpoint changes. This challenge primarily stems from the inherent uncertainty
in interpreting intricate 3D structures based on limited two-view observations,
as the 3D-to-2D projection leads to significant information loss. To address
this, we introduce CoMatcher, a deep multi-view matcher to (i) leverage
complementary context cues from different views to form a holistic 3D scene
understanding and (ii) utilize cross-view projection consistency to infer a
reliable global solution. Building on CoMatcher, we develop a groupwise
framework that fully exploits cross-view relationships for large-scale matching
tasks. Extensive experiments on various complex scenarios demonstrate the
superiority of our method over the mainstream two-view matching paradigm.
</summary>
    <author>
      <name>Jintao Zhang</name>
    </author>
    <author>
      <name>Zimin Xia</name>
    </author>
    <author>
      <name>Mingyue Dong</name>
    </author>
    <author>
      <name>Shuhan Shen</name>
    </author>
    <author>
      <name>Linwei Yue</name>
    </author>
    <author>
      <name>Xianwei Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, to be published in CVPR 2025</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2504.01872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.2.10; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
