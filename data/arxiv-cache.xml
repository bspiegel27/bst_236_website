<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-11-04T00:55:59Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-11-03T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">125144</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.27690v1</id>
    <updated>2025-10-31T17:59:03Z</updated>
    <published>2025-10-31T17:59:03Z</published>
    <title>Soft Gravitons, Hard Truths: Infrared Safety of Particle Processes in a
  Gravitational-Wave Background</title>
    <summary>  Gravitational waves are thought to propagate unattenuated through matter due
to a cancellation between graviton absorption and stimulated emission inferred
from leading-order soft-graviton arguments. We revisit this reasoning and show
that it fails for the converse problem: the effect of a gravitational-wave
background on matter. For unstable particles, real graviton emission \emph{and}
absorption appear to enhance decay rates. By extending the soft-graviton
framework describing real and virtual processes in a gravitational wave
background, and resumming them to all orders, we show that inclusive decay
rates remain essentially unchanged. The mutual transparency between matter and
gravitational radiation thus follows from infrared safety, and not from a
fortuitous cancellation in the lowest-order approximation of exclusive rates.
</summary>
    <author>
      <name>Wen-Yuan Ai</name>
    </author>
    <author>
      <name>Sebastian A. R. Ellis</name>
    </author>
    <author>
      <name>Josef Pradler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, revtex format</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.27690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27681v1</id>
    <updated>2025-10-31T17:49:50Z</updated>
    <published>2025-10-31T17:49:50Z</published>
    <title>Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in
  Creative Work</title>
    <summary>  As AI becomes more deeply embedded in knowledge work, building assistants
that support human creativity and expertise becomes more important. Yet
achieving synergy in human-AI collaboration is not easy. Providing AI with
detailed information about a user's demographics, psychological attributes,
divergent thinking, and domain expertise may improve performance by scaffolding
more effective multi-turn interactions. We implemented a personalized LLM-based
assistant, informed by users' psychometric profiles and an AI-guided interview
about their work style, to help users complete a marketing task for a fictional
startup. We randomized 331 participants to work with AI that was either generic
(n = 116), partially personalized (n = 114), or fully personalized (n=101).
Participants working with personalized AI produce marketing campaigns of
significantly higher quality and creativity, beyond what AI alone could have
produced. Compared to generic AI, personalized AI leads to higher self-reported
levels of assistance and feedback, while also increasing participant trust and
confidence. Causal mediation analysis shows that personalization improves
performance indirectly by enhancing collective memory, attention, and reasoning
in the human-AI interaction. These findings provide a theory-driven framework
in which personalization functions as external scaffolding that builds common
ground and shared partner models, reducing uncertainty and enhancing joint
cognition. This informs the design of future AI assistants that maximize
synergy and support human creative potential while limiting negative
homogenization.
</summary>
    <author>
      <name>Sean Kelley</name>
    </author>
    <author>
      <name>David De Cremer</name>
    </author>
    <author>
      <name>Christoph Riedl</name>
    </author>
    <link href="http://arxiv.org/abs/2510.27681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27656v1</id>
    <updated>2025-10-31T17:28:22Z</updated>
    <published>2025-10-31T17:28:22Z</published>
    <title>RDMA Point-to-Point Communication for LLM Systems</title>
    <summary>  Emerging Large Language Model (LLM) system patterns, such as disaggregated
inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement
fine-tuning, require flexible point-to-point communication beyond simple
collectives. Existing implementations are locked to specific Network Interface
Controllers (NICs), hindering integration into inference engines and
portability across hardware providers. We present TransferEngine, which bridges
the functionality of common NICs to expose a uniform interface. TransferEngine
exposes one-sided WriteImm operations with a ImmCounter primitive for
completion notification, without ordering assumptions of network transport,
transparently managing multiple NICs per GPU. We demonstrate peak throughput of
400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We
showcase TransferEngine through three production systems: (1) KvCache transfer
for disaggregated inference with dynamic scaling, (2) RL weight updates
achieving 1.3 seconds for trillion-parameter models, and (3) MoE
dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,
with the first viable latencies on EFA. We demonstrate that our portable
point-to-point communication complements collectives while avoiding lock-in.
</summary>
    <author>
      <name>Nandor Licker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Perplexity AI</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Hu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Perplexity AI</arxiv:affiliation>
    </author>
    <author>
      <name>Vladimir Zaytsev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Perplexity AI</arxiv:affiliation>
    </author>
    <author>
      <name>Lequn Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Perplexity AI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2510.27656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27655v1</id>
    <updated>2025-10-31T17:27:56Z</updated>
    <published>2025-10-31T17:27:56Z</published>
    <title>Community Detection on Model Explanation Graphs for Explainable AI</title>
    <summary>  Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions
but often miss higher-order structure: sets of features that act in concert. We
propose Modules of Influence (MoI), a framework that (i) constructs a model
explanation graph from per-instance attributions, (ii) applies community
detection to find feature modules that jointly affect predictions, and (iii)
quantifies how these modules relate to bias, redundancy, and causality
patterns. Across synthetic and real datasets, MoI uncovers correlated feature
groups, improves model debugging via module-level ablations, and localizes bias
exposure to specific modules. We release stability and synergy metrics, a
reference implementation, and evaluation protocols to benchmark module
discovery in XAI.
</summary>
    <author>
      <name>Ehsan Moradi</name>
    </author>
    <link href="http://arxiv.org/abs/2510.27655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27641v1</id>
    <updated>2025-10-31T17:12:34Z</updated>
    <published>2025-10-31T17:12:34Z</published>
    <title>SpecAttn: Speculating Sparse Attention</title>
    <summary>  Large Language Models (LLMs) face significant computational bottlenecks
during inference due to the quadratic complexity of self-attention mechanisms,
particularly as context lengths increase. We introduce SpecAttn, a novel
training-free approach that seamlessly integrates with existing speculative
decoding techniques to enable efficient sparse attention in pre-trained
transformers. Our key insight is to exploit the attention weights already
computed by the draft model during speculative decoding to identify important
tokens for the target model, eliminating redundant computation while
maintaining output quality. SpecAttn employs three core techniques: KL
divergence-based layer alignment between draft and target models, a
GPU-optimized sorting-free algorithm for top-p token selection from draft
attention patterns, and dynamic key-value cache pruning guided by these
predictions. By leveraging the computational work already performed in standard
speculative decoding pipelines, SpecAttn achieves over 75% reduction in
key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19
dataset, significantly outperforming existing sparse attention methods. Our
approach demonstrates that speculative execution can be enhanced to provide
approximate verification without significant performance degradation.
</summary>
    <author>
      <name>Harsh Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NeurIPS 2025 Workshop on Structured Probabilistic
  Inference &amp; Generative Modeling</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.27641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27633v1</id>
    <updated>2025-10-31T17:05:23Z</updated>
    <published>2025-10-31T17:05:23Z</published>
    <title>Testing Inequalities Linear in Nuisance Parameters</title>
    <summary>  This paper proposes a new test for inequalities that are linear in possibly
partially identified nuisance parameters. This type of hypothesis arises in a
broad set of problems, including subvector inference for linear unconditional
moment (in)equality models, specification testing of such models, and inference
for parameters bounded by linear programs. The new test uses a two-step test
statistic and a chi-squared critical value with data-dependent degrees of
freedom that can be calculated by an elementary formula. Its simple structure
and tuning-parameter-free implementation make it attractive for practical use.
We establish uniform asymptotic validity of the test, demonstrate its
finite-sample size and power in simulations, and illustrate its use in an
empirical application that analyzes women's labor supply in response to a
welfare policy reform.
</summary>
    <author>
      <name>Gregory Fletcher Cox</name>
    </author>
    <author>
      <name>Xiaoxia Shi</name>
    </author>
    <author>
      <name>Yuya Shimizu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">None</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.27633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27619v1</id>
    <updated>2025-10-31T16:43:12Z</updated>
    <published>2025-10-31T16:43:12Z</published>
    <title>Equation-of-state-informed pulse profile modeling</title>
    <summary>  NICER has enabled mass-radius inferences for pulsars using pulse profile
modeling, providing constraints on the equation of state (EOS) of cold, dense
matter. To date, PPM and EOS inference have been carried out as two separate
steps, with the former using EOS-agnostic priors. This approach has several
drawbacks. Ideally, one would perform a fully hierarchical Bayesian inference
where the pulse profile and EOS model parameters are jointly fit, but
implementing such a framework is complex and computationally demanding. Here,
we present an intermediate solution introducing an EOS-informed prior on
mass-radius into the existing PPM pipeline using normalizing flows. By focusing
on the parameter space consistent with certain EOSs, this approach both
tightens constraints on neutron star parameters while reducing computational
costs and requiring minimal additional implementation effort. We test this
approach on two pulsars, PSR J0740+6620 and PSR J0437-4715, and with two EOS
model families: a model based on the speed of sound inside the neutron star
interior (CS) and a piecewise-polytropic (PP) model. Both EOS models implement
constraints from chiral effective field theory calculations of dense matter.
For both pulsar datasets, the inferred radius credible intervals are narrower
than in the EOS-agnostic case, with CS favoring smaller radii and PP favoring
larger radii. For PSR J0437-4715, the EOS-informed priors reveal a new, more
extreme geometric mode that is statistically favored but physically
questionable. Including the PPM posteriors in the subsequent EOS inference
further tightens the mass-radius posteriors through the chiral effective field
theory constraints. However, there is also a sensitivity to the high-density
extensions, where the PP (CS) model produces a shift towards larger (smaller)
radii and corresponding stiffening (softening) of the pressure-energy density
relation.
</summary>
    <author>
      <name>Mariska Hoogkamer</name>
    </author>
    <author>
      <name>Nathan Rutherford</name>
    </author>
    <author>
      <name>Daniela Huppenkothen</name>
    </author>
    <author>
      <name>Benjamin Ricketts</name>
    </author>
    <author>
      <name>Anna L. Watts</name>
    </author>
    <author>
      <name>Melissa Mendes</name>
    </author>
    <author>
      <name>Isak Svensson</name>
    </author>
    <author>
      <name>Achim Schwenk</name>
    </author>
    <author>
      <name>Michael Kramer</name>
    </author>
    <author>
      <name>Kai Hebeler</name>
    </author>
    <author>
      <name>Tuomo Salmi</name>
    </author>
    <author>
      <name>Devarshi Choudhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to PRD</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.27619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27605v1</id>
    <updated>2025-10-31T16:29:21Z</updated>
    <published>2025-10-31T16:29:21Z</published>
    <title>Probing Gravity at Large Scales with kSZ-Reconstructed Velocities and
  CMB Lensing</title>
    <summary>  We present a new method for measuring the $E_G$ statistic that combines two
CMB secondaries -- the kinematic Sunyaev-Zeldovich (kSZ) effect and CMB lensing
-- for the first time to probe gravity on linear scales. The $E_G$ statistic is
a discriminating tool for modified gravity theories, which leave imprints in
lensing observables and peculiar velocities. Existing $E_G$ measurements rely
on redshift space distortions (RSD) to infer the velocity field. Here, we
employ kSZ velocity-reconstruction instead of RSD, a complementary technique
that constrains the largest-scale modes better than the galaxy survey it uses.
We construct a novel $\widehat{V}_G$ estimator that involves a ratio between
cross-correlations of a galaxy sample with a CMB convergence map and that with
a 3D kSZ-reconstructed velocity field. We forecast for current and upcoming CMB
maps from the Atacama Cosmology Telescope (ACT) and the Simons Observatory
(SO), respectively, in combination with three spectroscopic galaxy samples from
the Dark Energy Spectroscopic Instrument (DESI). We find cumulative detection
significances in the range $S/N \sim 20-55$, which can robustly test the
scale-independent $E_G$ prediction under general relativity (GR) at different
effective redshifts of the galaxy samples ($z\approx 0.73, 1.33, 1.84$). In
particular, the SO$\times$DESI LRG measurement would be able to distinguish
between GR and certain modified gravity models, including Hu-Sawicki $f(R)$ and
Chameleon theories, with high confidence. The proposed $\widehat{V}_G$
estimator opens up a new avenue for stress-testing gravity and the
$\Lambda$CDM+GR model at the largest observable scales.
</summary>
    <author>
      <name>Raagini Patki</name>
    </author>
    <author>
      <name>Nicholas Battaglia</name>
    </author>
    <author>
      <name>Rachel Bean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 Pages, 4 Figures, 4 Tables. To be submitted to Phys. Rev. D</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.27605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27583v1</id>
    <updated>2025-10-31T16:06:35Z</updated>
    <published>2025-10-31T16:06:35Z</published>
    <title>AMD MI300X GPU Performance Analysis</title>
    <summary>  The rapid growth of large language models (LLMs) has driven the need for
high-performance, scalable GPU hardware capable of efficiently serving models
with hundreds of billions of parameters. While NVIDIA GPUs have traditionally
dominated LLM deployments due to their mature CUDA software stack and state-of
the-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,
featuring high HBM capacity, matrix cores, and their proprietary interconnect.
In this paper, we present a comprehensive evaluation of the AMD MI300X GPUs
across key performance domains critical to LLM inference including compute
throughput, memory bandwidth, and interconnect communication.
</summary>
    <author>
      <name>Chandrish Ambati</name>
    </author>
    <author>
      <name>Trung Diep</name>
    </author>
    <link href="http://arxiv.org/abs/2510.27583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.27580v1</id>
    <updated>2025-10-31T16:02:00Z</updated>
    <published>2025-10-31T16:02:00Z</published>
    <title>Refining capture-recapture methods to estimate case counts in a finite
  population setting</title>
    <summary>  In this paper, we expand upon and refine a monitoring strategy proposed for
surveillance of diseases in finite, closed populations. This monitoring
strategy consists of augmenting an arbitrarily non-representative data stream
(such as a voluntary flu testing program) with a random sample (referred to as
an "anchor stream"). This design allows for the use of traditional
capture-recapture (CRC) estimators, as well as recently proposed anchor stream
estimators that more efficiently utilize the data. Here, we focus on a
particularly common situation in which the first data stream only records
positive test results, while the anchor stream documents both positives and
negatives. Due to the non-representative nature of the first data stream, along
with the fact that inference is being performed on a finite, closed population,
there are standard and non-standard finite population effects at play. Here, we
propose two methods of incorporating finite population corrections (FPCs) for
inference, along with an FPC-adjusted Bayesian credible interval. We compare
these approaches with existing methods through simulation and demonstrate that
the FPC adjustments can lead to considerable gains in precision. Finally, we
provide a real data example by applying these methods to estimating the breast
cancer recurrence count among Metro Atlanta-area patients in the Georgia Cancer
Registry-based Cancer Recurrence Information and Surveillance Program (CRISP)
database.
</summary>
    <author>
      <name>Michael Doerfler</name>
    </author>
    <author>
      <name>Wenhao Mao</name>
    </author>
    <author>
      <name>Lin Ge</name>
    </author>
    <author>
      <name>Yuzi Zhang</name>
    </author>
    <author>
      <name>Timothy L. Lash</name>
    </author>
    <author>
      <name>Kevin C. Ward</name>
    </author>
    <author>
      <name>Lance A. Waller</name>
    </author>
    <author>
      <name>Robert H. Lyles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.27580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.27580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
