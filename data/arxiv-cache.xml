<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-11T00:58:09Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-10T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">114627</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.08015v1</id>
    <updated>2025-06-09T17:59:59Z</updated>
    <published>2025-06-09T17:59:59Z</published>
    <title>4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular
  Videos</title>
    <summary>  We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene
reconstruction, trained entirely on real-world monocular posed videos. Using 4D
Gaussian as an inductive bias, 4DGT unifies static and dynamic components,
enabling the modeling of complex, time-varying environments with varying object
lifespans. We proposed a novel density control strategy in training, which
enables our 4DGT to handle longer space-time input and remain efficient
rendering at runtime. Our model processes 64 consecutive posed frames in a
rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike
optimization-based methods, 4DGT performs purely feed-forward inference,
reducing reconstruction time from hours to seconds and scaling effectively to
long video sequences. Trained only on large-scale monocular posed video
datasets, 4DGT can outperform prior Gaussian-based networks significantly in
real-world videos and achieve on-par accuracy with optimization-based methods
on cross-domain videos. Project page: https://4dgt.github.io
</summary>
    <author>
      <name>Zhen Xu</name>
    </author>
    <author>
      <name>Zhengqin Li</name>
    </author>
    <author>
      <name>Zhao Dong</name>
    </author>
    <author>
      <name>Xiaowei Zhou</name>
    </author>
    <author>
      <name>Richard Newcombe</name>
    </author>
    <author>
      <name>Zhaoyang Lv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://4dgt.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.08015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.08015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.08009v1</id>
    <updated>2025-06-09T17:59:55Z</updated>
    <published>2025-06-09T17:59:55Z</published>
    <title>Self Forcing: Bridging the Train-Test Gap in Autoregressive Video
  Diffusion</title>
    <summary>  We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/
</summary>
    <author>
      <name>Xun Huang</name>
    </author>
    <author>
      <name>Zhengqi Li</name>
    </author>
    <author>
      <name>Guande He</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website: http://self-forcing.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.08009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.08009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07999v1</id>
    <updated>2025-06-09T17:59:01Z</updated>
    <published>2025-06-09T17:59:01Z</published>
    <title>MADFormer: Mixed Autoregressive and Diffusion Transformers for
  Continuous Image Generation</title>
    <summary>  Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.
</summary>
    <author>
      <name>Junhao Chen</name>
    </author>
    <author>
      <name>Yulia Tsvetkov</name>
    </author>
    <author>
      <name>Xiaochuang Han</name>
    </author>
    <link href="http://arxiv.org/abs/2506.07999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07998v1</id>
    <updated>2025-06-09T17:58:36Z</updated>
    <published>2025-06-09T17:58:36Z</published>
    <title>Generative Modeling of Weights: Generalization or Memorization?</title>
    <summary>  Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.
</summary>
    <author>
      <name>Boya Zeng</name>
    </author>
    <author>
      <name>Yida Yin</name>
    </author>
    <author>
      <name>Zhiqiu Xu</name>
    </author>
    <author>
      <name>Zhuang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page at https://boyazeng.github.io/weight_memorization</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.07998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07978v1</id>
    <updated>2025-06-09T17:50:57Z</updated>
    <published>2025-06-09T17:50:57Z</published>
    <title>Not so dark, not so dense: an alternative explanation for the lensing
  subhalo in SDSSJ0946+1006</title>
    <summary>  Previous studies of the strong lens system SDSSJ0946+1006 have reported a
dark matter subhalo with an unusually high central density, potentially
challenging the standard cold dark matter (CDM) paradigm. However, these
analyses assumed the subhalo to be completely dark, neglecting the possibility
that it may host a faint galaxy. In this work, we revisit the lensing analysis
of SDSSJ0946+1006, explicitly modelling the subhalo as a luminous satellite.
Incorporating light from the perturber broadens the range of allowed subhalo
properties, revealing solutions with significantly lower central densities that
are consistent with CDM expectations. The inferred luminosity of the satellite
also aligns with predictions from hydrodynamical simulations. While
high-concentration subhaloes remain allowed, they are no longer statistically
preferred. The luminous subhalo model yields a better fit to the data, while
also offering a more plausible explanation that is in line with theoretical
expectations. We validate our methodology using mock data, demonstrating that
neglecting subhalo light can lead to inferred mass distributions that are
artificially compact.
</summary>
    <author>
      <name>Qiuhan He</name>
    </author>
    <author>
      <name>Andrew Robertson</name>
    </author>
    <author>
      <name>James W. Nightingale</name>
    </author>
    <author>
      <name>Aristeidis Amvrosiadis</name>
    </author>
    <author>
      <name>Shaun Cole</name>
    </author>
    <author>
      <name>Carlos S. Frenk</name>
    </author>
    <author>
      <name>Samuel C. Lange</name>
    </author>
    <author>
      <name>Shubo Li</name>
    </author>
    <author>
      <name>Ran Li</name>
    </author>
    <author>
      <name>Xiaoyue Cao</name>
    </author>
    <author>
      <name>Leo W. H. Fung</name>
    </author>
    <author>
      <name>Xianghao Ma</name>
    </author>
    <author>
      <name>Richard Massey</name>
    </author>
    <author>
      <name>Kaihao Wang</name>
    </author>
    <author>
      <name>Maximilian von Wietersheim-Kramsta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Comments Welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.07978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07971v1</id>
    <updated>2025-06-09T17:45:18Z</updated>
    <published>2025-06-09T17:45:18Z</published>
    <title>CyberV: Cybernetics for Test-time Scaling in Video Understanding</title>
    <summary>  Current Multimodal Large Language Models (MLLMs) may struggle with
understanding long or complex videos due to computational demands at test time,
lack of robustness, and limited accuracy, primarily stemming from their
feed-forward processing nature. These limitations could be more severe for
models with fewer parameters. To address these limitations, we propose a novel
framework inspired by cybernetic principles, redesigning video MLLMs as
adaptive systems capable of self-monitoring, self-correction, and dynamic
resource allocation during inference. Our approach, CyberV, introduces a
cybernetic loop consisting of an MLLM Inference System, a Sensor, and a
Controller. Specifically, the sensor monitors forward processes of the MLLM and
collects intermediate interpretations, such as attention drift, then the
controller determines when and how to trigger self-correction and generate
feedback to guide the next round. This test-time adaptive scaling framework
enhances frozen MLLMs without requiring retraining or additional components.
Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B
by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive
proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%
improvement, achieving performance even comparable to human experts.
Furthermore, our method demonstrates consistent gains on general-purpose
benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and
generalization capabilities in making MLLMs more robust and accurate for
dynamic video understanding. The code is released at
https://github.com/marinero4972/CyberV.
</summary>
    <author>
      <name>Jiahao Meng</name>
    </author>
    <author>
      <name>Shuyang Sun</name>
    </author>
    <author>
      <name>Yue Tan</name>
    </author>
    <author>
      <name>Lu Qi</name>
    </author>
    <author>
      <name>Yunhai Tong</name>
    </author>
    <author>
      <name>Xiangtai Li</name>
    </author>
    <author>
      <name>Longyin Wen</name>
    </author>
    <link href="http://arxiv.org/abs/2506.07971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07956v1</id>
    <updated>2025-06-09T17:26:14Z</updated>
    <published>2025-06-09T17:26:14Z</published>
    <title>Language Models over Canonical Byte-Pair Encodings</title>
    <summary>  Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.
</summary>
    <author>
      <name>Tim Vieira</name>
    </author>
    <author>
      <name>Tianyu Liu</name>
    </author>
    <author>
      <name>Clemente Pasti</name>
    </author>
    <author>
      <name>Yahya Emara</name>
    </author>
    <author>
      <name>Brian DuSell</name>
    </author>
    <author>
      <name>Benjamin LeBrun</name>
    </author>
    <author>
      <name>Mario Giulianelli</name>
    </author>
    <author>
      <name>Juan Luis Gastaldi</name>
    </author>
    <author>
      <name>Timothy J. O'Donnell</name>
    </author>
    <author>
      <name>Ryan Cotterell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.07956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07949v1</id>
    <updated>2025-06-09T17:14:41Z</updated>
    <published>2025-06-09T17:14:41Z</published>
    <title>Cost-Optimal Active AI Model Evaluation</title>
    <summary>  The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.
</summary>
    <author>
      <name>Anastasios N. Angelopoulos</name>
    </author>
    <author>
      <name>Jacob Eisenstein</name>
    </author>
    <author>
      <name>Jonathan Berant</name>
    </author>
    <author>
      <name>Alekh Agarwal</name>
    </author>
    <author>
      <name>Adam Fisch</name>
    </author>
    <link href="http://arxiv.org/abs/2506.07949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07948v1</id>
    <updated>2025-06-09T17:11:28Z</updated>
    <published>2025-06-09T17:11:28Z</published>
    <title>TokenBreak: Bypassing Text Classification Models Through Token
  Manipulation</title>
    <summary>  Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.
</summary>
    <author>
      <name>Kasimir Schulz</name>
    </author>
    <author>
      <name>Kenneth Yeung</name>
    </author>
    <author>
      <name>Kieran Evans</name>
    </author>
    <link href="http://arxiv.org/abs/2506.07948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.07947v1</id>
    <updated>2025-06-09T17:11:07Z</updated>
    <published>2025-06-09T17:11:07Z</published>
    <title>Statistical Hypothesis Testing for Auditing Robustness in Language
  Models</title>
    <summary>  Consider the problem of testing whether the outputs of a large language model
(LLM) system change under an arbitrary intervention, such as an input
perturbation or changing the model variant. We cannot simply compare two LLM
outputs since they might differ due to the stochastic nature of the system, nor
can we compare the entire output distribution due to computational
intractability. While existing methods for analyzing text-based outputs exist,
they focus on fundamentally different problems, such as measuring bias or
fairness. To this end, we introduce distribution-based perturbation analysis, a
framework that reformulates LLM perturbation analysis as a frequentist
hypothesis testing problem. We construct empirical null and alternative output
distributions within a low-dimensional semantic similarity space via Monte
Carlo sampling, enabling tractable inference without restrictive distributional
assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation
of arbitrary input perturbations on any black-box LLM, (iii) yields
interpretable p-values; (iv) supports multiple perturbations via controlled
error rates; and (v) provides scalar effect sizes. We demonstrate the
usefulness of the framework across multiple case studies, showing how we can
quantify response changes, measure true/false positive rates, and evaluate
alignment with reference models. Above all, we see this as a reliable
frequentist hypothesis testing framework for LLM auditing.
</summary>
    <author>
      <name>Paulius Rauba</name>
    </author>
    <author>
      <name>Qiyao Wei</name>
    </author>
    <author>
      <name>Mihaela van der Schaar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2412.00868</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Forty-second International Conference on Machine Learning. ICML
  2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2506.07947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.07947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
