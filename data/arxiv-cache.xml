<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-02-19T01:17:34Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-02-19T01:17:34Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>134372</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2602.15813v1</id>
    <title>FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy</title>
    <updated>2026-02-17T18:49:43Z</updated>
    <link href="https://arxiv.org/abs/2602.15813v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15813v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T18:49:43Z</published>
    <arxiv:comment>WACV 2026</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Haochen Zhang</name>
    </author>
    <author>
      <name>Nirav Savaliya</name>
    </author>
    <author>
      <name>Faizan Siddiqui</name>
    </author>
    <author>
      <name>Enna Sachdeva</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15811v1</id>
    <title>Task-Agnostic Continual Learning for Chest Radiograph Classification</title>
    <updated>2026-02-17T18:47:30Z</updated>
    <link href="https://arxiv.org/abs/2602.15811v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15811v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T18:47:30Z</published>
    <arxiv:comment>12 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Muthu Subash Kavitha</name>
    </author>
    <author>
      <name>Anas Zafar</name>
    </author>
    <author>
      <name>Amgad Muneer</name>
    </author>
    <author>
      <name>Jia Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15805v1</id>
    <title>Inviscid limit and an effective energy-enstrophy diffusion process</title>
    <updated>2026-02-17T18:43:16Z</updated>
    <link href="https://arxiv.org/abs/2602.15805v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15805v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this article we consider a stationary $N$-dimensional Galerkin-Navier-Stokes type evolution with Brownian forcing and random stirring (of arbitrarily small strength). We show that the stationary diffusion in an open two-dimensional cone constructed in a companion article, stands as the inviscid limit of the laws of the ``enstrophy-energy'' process of the $N$-dimensional diffusion process considered here, this regardless of the strength of the stirring. With the help of the quantitative condensation bounds of the companion article, we infer quantitative inviscid condensation bounds, which for suitable forcings show an attrition of all but the lowest modes in the inviscid limit.</summary>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T18:43:16Z</published>
    <arxiv:comment>37 pages, 1 figure</arxiv:comment>
    <arxiv:primary_category term="math.PR"/>
    <author>
      <name>Alain-Sol Sznitman</name>
    </author>
    <author>
      <name>Klaus Widmayer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15794v1</id>
    <title>Service Orchestration in the Computing Continuum: Structural Challenges and Vision</title>
    <updated>2026-02-17T18:34:07Z</updated>
    <link href="https://arxiv.org/abs/2602.15794v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15794v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T18:34:07Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Boris Sedlak</name>
    </author>
    <author>
      <name>Víctor Casamayor Pujol</name>
    </author>
    <author>
      <name>Ildefons Magrans de Abril</name>
    </author>
    <author>
      <name>Praveen Kumar Donta</name>
    </author>
    <author>
      <name>Adel N. Toosi</name>
    </author>
    <author>
      <name>Schahram Dustdar</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15787v1</id>
    <title>Energy budgets govern synaptic precision and its regulation during plasticity</title>
    <updated>2026-02-17T18:20:04Z</updated>
    <link href="https://arxiv.org/abs/2602.15787v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15787v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Synaptic transmission must balance the need for reliable signalling against the metabolic cost of achieving that reliability. How energetic constraints shape synaptic precision and its regulation during plasticity remains unclear. Here we develop an energy--constrained framework in which synapses minimise postsynaptic response variance subject to a fixed mean and an effective energy budget. Combinations of candidate physiological costs are used to estimate an energy cost for synaptic transmission; this cost is then inferred from quantal statistics. Analysing five published pre- and post-plasticity datasets, we find that observed synaptic mean--variance pairs cluster near a minimal-energy boundary, indicating that precision is limited by energetic availability. Model comparison identifies a dominant calcium pump-like cost paired with a smaller vesicle turnover-like cost, yielding a separable precision--energy relationship, $σ^{-2} \propto E^5$. We further show that plasticity systematically updates synaptic energy budgets according to the scale-free magnitude of mean change, enabling accurate prediction of post-plasticity variance from energy allocation alone. These results provide direct experimental support for the hypothesis that synaptic precision is governed by energy budgets, establishing energy allocation as a fundamental principle linking metabolic constraints, synaptic reliability, and plasticity.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T18:20:04Z</published>
    <arxiv:comment>39 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="q-bio.NC"/>
    <author>
      <name>James Malkin</name>
    </author>
    <author>
      <name>Cian O'Donnell</name>
    </author>
    <author>
      <name>Conor Houghton</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15785v1</id>
    <title>This human study did not involve human subjects: Validating LLM simulations as behavioral evidence</title>
    <updated>2026-02-17T18:18:38Z</updated>
    <link href="https://arxiv.org/abs/2602.15785v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15785v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T18:18:38Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Jessica Hullman</name>
    </author>
    <author>
      <name>David Broska</name>
    </author>
    <author>
      <name>Huaman Sun</name>
    </author>
    <author>
      <name>Aaron Shaw</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15776v1</id>
    <title>GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems</title>
    <updated>2026-02-17T18:05:48Z</updated>
    <link href="https://arxiv.org/abs/2602.15776v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15776v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T18:05:48Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <arxiv:journal_ref>ICLR-2026</arxiv:journal_ref>
    <author>
      <name>Yiqin Yang</name>
    </author>
    <author>
      <name>Xu Yang</name>
    </author>
    <author>
      <name>Yuhua Jiang</name>
    </author>
    <author>
      <name>Ni Mu</name>
    </author>
    <author>
      <name>Hao Hu</name>
    </author>
    <author>
      <name>Runpeng Xie</name>
    </author>
    <author>
      <name>Ziyou Zhang</name>
    </author>
    <author>
      <name>Siyuan Li</name>
    </author>
    <author>
      <name>Yuan-Hua Ni</name>
    </author>
    <author>
      <name>Qianchuan Zhao</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15763v1</id>
    <title>GLM-5: from Vibe Coding to Agentic Engineering</title>
    <updated>2026-02-17T17:50:56Z</updated>
    <link href="https://arxiv.org/abs/2602.15763v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15763v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T17:50:56Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>GLM-5 Team</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Aohan Zeng</name>
    </author>
    <author>
      <name>Xin Lv</name>
    </author>
    <author>
      <name>Zhenyu Hou</name>
    </author>
    <author>
      <name>Zhengxiao Du</name>
    </author>
    <author>
      <name>Qinkai Zheng</name>
    </author>
    <author>
      <name>Bin Chen</name>
    </author>
    <author>
      <name>Da Yin</name>
    </author>
    <author>
      <name>Chendi Ge</name>
    </author>
    <author>
      <name>Chengxing Xie</name>
    </author>
    <author>
      <name>Cunxiang Wang</name>
    </author>
    <author>
      <name>Gengzheng Pan</name>
    </author>
    <author>
      <name>Hao Zeng</name>
    </author>
    <author>
      <name>Haoke Zhang</name>
    </author>
    <author>
      <name>Haoran Wang</name>
    </author>
    <author>
      <name>Huilong Chen</name>
    </author>
    <author>
      <name>Jiajie Zhang</name>
    </author>
    <author>
      <name>Jian Jiao</name>
    </author>
    <author>
      <name>Jiaqi Guo</name>
    </author>
    <author>
      <name>Jingsen Wang</name>
    </author>
    <author>
      <name>Jingzhao Du</name>
    </author>
    <author>
      <name>Jinzhu Wu</name>
    </author>
    <author>
      <name>Kedong Wang</name>
    </author>
    <author>
      <name>Lei Li</name>
    </author>
    <author>
      <name>Lin Fan</name>
    </author>
    <author>
      <name>Lucen Zhong</name>
    </author>
    <author>
      <name>Mingdao Liu</name>
    </author>
    <author>
      <name>Mingming Zhao</name>
    </author>
    <author>
      <name>Pengfan Du</name>
    </author>
    <author>
      <name>Qian Dong</name>
    </author>
    <author>
      <name>Rui Lu</name>
    </author>
    <author>
      <name> Shuang-Li</name>
    </author>
    <author>
      <name>Shulin Cao</name>
    </author>
    <author>
      <name>Song Liu</name>
    </author>
    <author>
      <name>Ting Jiang</name>
    </author>
    <author>
      <name>Xiaodong Chen</name>
    </author>
    <author>
      <name>Xiaohan Zhang</name>
    </author>
    <author>
      <name>Xuancheng Huang</name>
    </author>
    <author>
      <name>Xuezhen Dong</name>
    </author>
    <author>
      <name>Yabo Xu</name>
    </author>
    <author>
      <name>Yao Wei</name>
    </author>
    <author>
      <name>Yifan An</name>
    </author>
    <author>
      <name>Yilin Niu</name>
    </author>
    <author>
      <name>Yitong Zhu</name>
    </author>
    <author>
      <name>Yuanhao Wen</name>
    </author>
    <author>
      <name>Yukuo Cen</name>
    </author>
    <author>
      <name>Yushi Bai</name>
    </author>
    <author>
      <name>Zhongpei Qiao</name>
    </author>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Zikang Wang</name>
    </author>
    <author>
      <name>Zilin Zhu</name>
    </author>
    <author>
      <name>Ziqiang Liu</name>
    </author>
    <author>
      <name>Zixuan Li</name>
    </author>
    <author>
      <name>Bojie Wang</name>
    </author>
    <author>
      <name>Bosi Wen</name>
    </author>
    <author>
      <name>Can Huang</name>
    </author>
    <author>
      <name>Changpeng Cai</name>
    </author>
    <author>
      <name>Chao Yu</name>
    </author>
    <author>
      <name>Chen Li</name>
    </author>
    <author>
      <name>Chen Li</name>
    </author>
    <author>
      <name>Chenghua Huang</name>
    </author>
    <author>
      <name>Chengwei Hu</name>
    </author>
    <author>
      <name>Chenhui Zhang</name>
    </author>
    <author>
      <name>Chenzheng Zhu</name>
    </author>
    <author>
      <name>Congfeng Yin</name>
    </author>
    <author>
      <name>Daoyan Lin</name>
    </author>
    <author>
      <name>Dayong Yang</name>
    </author>
    <author>
      <name>Di Wang</name>
    </author>
    <author>
      <name>Ding Ai</name>
    </author>
    <author>
      <name>Erle Zhu</name>
    </author>
    <author>
      <name>Fangzhou Yi</name>
    </author>
    <author>
      <name>Feiyu Chen</name>
    </author>
    <author>
      <name>Guohong Wen</name>
    </author>
    <author>
      <name>Hailong Sun</name>
    </author>
    <author>
      <name>Haisha Zhao</name>
    </author>
    <author>
      <name>Haiyi Hu</name>
    </author>
    <author>
      <name>Hanchen Zhang</name>
    </author>
    <author>
      <name>Hanrui Liu</name>
    </author>
    <author>
      <name>Hanyu Zhang</name>
    </author>
    <author>
      <name>Hao Peng</name>
    </author>
    <author>
      <name>Hao Tai</name>
    </author>
    <author>
      <name>Haobo Zhang</name>
    </author>
    <author>
      <name>He Liu</name>
    </author>
    <author>
      <name>Hongwei Wang</name>
    </author>
    <author>
      <name>Hongxi Yan</name>
    </author>
    <author>
      <name>Hongyu Ge</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <author>
      <name>Huanpeng Chu</name>
    </author>
    <author>
      <name>Jia'ni Zhao</name>
    </author>
    <author>
      <name>Jiachen Wang</name>
    </author>
    <author>
      <name>Jiajing Zhao</name>
    </author>
    <author>
      <name>Jiamin Ren</name>
    </author>
    <author>
      <name>Jiapeng Wang</name>
    </author>
    <author>
      <name>Jiaxin Zhang</name>
    </author>
    <author>
      <name>Jiayi Gui</name>
    </author>
    <author>
      <name>Jiayue Zhao</name>
    </author>
    <author>
      <name>Jijie Li</name>
    </author>
    <author>
      <name>Jing An</name>
    </author>
    <author>
      <name>Jing Li</name>
    </author>
    <author>
      <name>Jingwei Yuan</name>
    </author>
    <author>
      <name>Jinhua Du</name>
    </author>
    <author>
      <name>Jinxin Liu</name>
    </author>
    <author>
      <name>Junkai Zhi</name>
    </author>
    <author>
      <name>Junwen Duan</name>
    </author>
    <author>
      <name>Kaiyue Zhou</name>
    </author>
    <author>
      <name>Kangjian Wei</name>
    </author>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Keyun Luo</name>
    </author>
    <author>
      <name>Laiqiang Zhang</name>
    </author>
    <author>
      <name>Leigang Sha</name>
    </author>
    <author>
      <name>Liang Xu</name>
    </author>
    <author>
      <name>Lindong Wu</name>
    </author>
    <author>
      <name>Lintao Ding</name>
    </author>
    <author>
      <name>Lu Chen</name>
    </author>
    <author>
      <name>Minghao Li</name>
    </author>
    <author>
      <name>Nianyi Lin</name>
    </author>
    <author>
      <name>Pan Ta</name>
    </author>
    <author>
      <name>Qiang Zou</name>
    </author>
    <author>
      <name>Rongjun Song</name>
    </author>
    <author>
      <name>Ruiqi Yang</name>
    </author>
    <author>
      <name>Shangqing Tu</name>
    </author>
    <author>
      <name>Shangtong Yang</name>
    </author>
    <author>
      <name>Shaoxiang Wu</name>
    </author>
    <author>
      <name>Shengyan Zhang</name>
    </author>
    <author>
      <name>Shijie Li</name>
    </author>
    <author>
      <name>Shuang Li</name>
    </author>
    <author>
      <name>Shuyi Fan</name>
    </author>
    <author>
      <name>Wei Qin</name>
    </author>
    <author>
      <name>Wei Tian</name>
    </author>
    <author>
      <name>Weining Zhang</name>
    </author>
    <author>
      <name>Wenbo Yu</name>
    </author>
    <author>
      <name>Wenjie Liang</name>
    </author>
    <author>
      <name>Xiang Kuang</name>
    </author>
    <author>
      <name>Xiangmeng Cheng</name>
    </author>
    <author>
      <name>Xiangyang Li</name>
    </author>
    <author>
      <name>Xiaoquan Yan</name>
    </author>
    <author>
      <name>Xiaowei Hu</name>
    </author>
    <author>
      <name>Xiaoying Ling</name>
    </author>
    <author>
      <name>Xing Fan</name>
    </author>
    <author>
      <name>Xingye Xia</name>
    </author>
    <author>
      <name>Xinyuan Zhang</name>
    </author>
    <author>
      <name>Xinze Zhang</name>
    </author>
    <author>
      <name>Xirui Pan</name>
    </author>
    <author>
      <name>Xunkai Zhang</name>
    </author>
    <author>
      <name>Yandong Wu</name>
    </author>
    <author>
      <name>Yanfu Li</name>
    </author>
    <author>
      <name>Yidong Wang</name>
    </author>
    <author>
      <name>Yifan Zhu</name>
    </author>
    <author>
      <name>Yijun Tan</name>
    </author>
    <author>
      <name>Yilin Zhou</name>
    </author>
    <author>
      <name>Yiming Pan</name>
    </author>
    <author>
      <name>Ying Zhang</name>
    </author>
    <author>
      <name>Yinpei Su</name>
    </author>
    <author>
      <name>Yipeng Geng</name>
    </author>
    <author>
      <name>Yipeng Geng</name>
    </author>
    <author>
      <name>Yong Yan</name>
    </author>
    <author>
      <name>Yonglin Tan</name>
    </author>
    <author>
      <name>Yuean Bi</name>
    </author>
    <author>
      <name>Yuhan Shen</name>
    </author>
    <author>
      <name>Yuhao Yang</name>
    </author>
    <author>
      <name>Yujiang Li</name>
    </author>
    <author>
      <name>Yunan Liu</name>
    </author>
    <author>
      <name>Yunqing Wang</name>
    </author>
    <author>
      <name>Yuntao Li</name>
    </author>
    <author>
      <name>Yurong Wu</name>
    </author>
    <author>
      <name>Yutao Zhang</name>
    </author>
    <author>
      <name>Yuxi Duan</name>
    </author>
    <author>
      <name>Yuxuan Zhang</name>
    </author>
    <author>
      <name>Zezhen Liu</name>
    </author>
    <author>
      <name>Zhengtao Jiang</name>
    </author>
    <author>
      <name>Zhenhe Yan</name>
    </author>
    <author>
      <name>Zheyu Zhang</name>
    </author>
    <author>
      <name>Zhixiang Wei</name>
    </author>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Zhuoer Feng</name>
    </author>
    <author>
      <name>Zijun Yao</name>
    </author>
    <author>
      <name>Ziwei Chai</name>
    </author>
    <author>
      <name>Ziyuan Wang</name>
    </author>
    <author>
      <name>Zuzhou Zhang</name>
    </author>
    <author>
      <name>Bin Xu</name>
    </author>
    <author>
      <name>Minlie Huang</name>
    </author>
    <author>
      <name>Hongning Wang</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
    <author>
      <name>Yuxiao Dong</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15756v1</id>
    <title>A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference</title>
    <updated>2026-02-17T17:41:59Z</updated>
    <link href="https://arxiv.org/abs/2602.15756v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15756v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $δ$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T17:41:59Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Or Zamir</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2602.15749v1</id>
    <title>A Generative-First Neural Audio Autoencoder</title>
    <updated>2026-02-17T17:26:12Z</updated>
    <link href="https://arxiv.org/abs/2602.15749v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2602.15749v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural autoencoders underpin generative models. Practical, large-scale use of neural autoencoders for generative modeling necessitates fast encoding, low latent rates, and a single model across representations. Existing approaches are reconstruction-first: they incur high latent rates, slow encoding, and separate architectures for discrete vs. continuous latents and for different audio channel formats, hindering workflows from preprocessing to inference conditioning. We introduce a generative-first architecture for audio autoencoding that increases temporal downsampling from 2048x to 3360x and supports continuous and discrete representations and common audio channel formats in one model. By balancing compression, quality, and speed, it delivers 10x faster encoding, 1.6x lower rates, and eliminates channel-format-specific variants while maintaining competitive reconstruction quality. This enables applications previously constrained by processing costs: a 60-second mono signal compresses to 788 tokens, making generative modeling more tractable.</summary>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-02-17T17:26:12Z</published>
    <arxiv:comment>ICASSP 2026</arxiv:comment>
    <arxiv:primary_category term="cs.SD"/>
    <author>
      <name>Jonah Casebeer</name>
    </author>
    <author>
      <name>Ge Zhu</name>
    </author>
    <author>
      <name>Zhepei Wang</name>
    </author>
    <author>
      <name>Nicholas J. Bryan</name>
    </author>
  </entry>
</feed>
