<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-05T00:52:37Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-04T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">120246</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.03518v1</id>
    <updated>2025-09-03T17:59:45Z</updated>
    <published>2025-09-03T17:59:45Z</published>
    <title>Can LLMs Lie? Investigation beyond Hallucination</title>
    <summary>  Large language models (LLMs) have demonstrated impressive capabilities across
a variety of tasks, but their increasing autonomy in real-world applications
raises concerns about their trustworthiness. While hallucinations-unintentional
falsehoods-have been widely studied, the phenomenon of lying, where an LLM
knowingly generates falsehoods to achieve an ulterior objective, remains
underexplored. In this work, we systematically investigate the lying behavior
of LLMs, differentiating it from hallucinations and testing it in practical
scenarios. Through mechanistic interpretability techniques, we uncover the
neural mechanisms underlying deception, employing logit lens analysis, causal
interventions, and contrastive activation steering to identify and control
deceptive behavior. We study real-world lying scenarios and introduce
behavioral steering vectors that enable fine-grained manipulation of lying
tendencies. Further, we explore the trade-offs between lying and end-task
performance, establishing a Pareto frontier where dishonesty can enhance goal
optimization. Our findings contribute to the broader discourse on AI ethics,
shedding light on the risks and potential safeguards for deploying LLMs in
high-stakes environments. Code and more illustrations are available at
https://llm-liar.github.io/
</summary>
    <author>
      <name>Haoran Huan</name>
    </author>
    <author>
      <name>Mihir Prabhudesai</name>
    </author>
    <author>
      <name>Mengning Wu</name>
    </author>
    <author>
      <name>Shantanu Jaiswal</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website at https://llm-liar.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03516v1</id>
    <updated>2025-09-03T17:58:12Z</updated>
    <published>2025-09-03T17:58:12Z</published>
    <title>Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,
  but Not Direct the Play?</title>
    <summary>  Text-to-image (T2I) generation aims to synthesize images from textual
prompts, which jointly specify what must be shown and imply what can be
inferred, thereby corresponding to two core capabilities: composition and
reasoning. However, with the emerging advances of T2I models in reasoning
beyond composition, existing benchmarks reveal clear limitations in providing
comprehensive evaluations across and within these capabilities. Meanwhile,
these advances also enable models to handle more complex prompts, whereas
current benchmarks remain limited to low scene density and simplified
one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a
comprehensive and complex benchmark that evaluates both composition and
reasoning capabilities of T2I models. To ensure comprehensiveness, we structure
composition around scene graph elements (instance, attribute, and relation) and
reasoning around the philosophical framework of inference (deductive,
inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To
increase complexity, driven by the inherent complexities of real-world
scenarios, we curate each prompt with high compositional density for
composition and multi-step inference for reasoning. We also pair each prompt
with a checklist that specifies individual yes/no questions to assess each
intended element independently to facilitate fine-grained and reliable
evaluation. In statistics, our benchmark comprises 1,080 challenging prompts
and around 13,500 checklist questions. Experiments across 27 current T2I models
reveal that their composition capability still remains limited in complex
high-density scenarios, while the reasoning capability lags even further behind
as a critical bottleneck, with all models struggling to infer implicit elements
from prompts. Our project page: https://t2i-corebench.github.io/.
</summary>
    <author>
      <name>Ouxiang Li</name>
    </author>
    <author>
      <name>Yuan Wang</name>
    </author>
    <author>
      <name>Xinting Hu</name>
    </author>
    <author>
      <name>Huijuan Huang</name>
    </author>
    <author>
      <name>Rui Chen</name>
    </author>
    <author>
      <name>Jiarong Ou</name>
    </author>
    <author>
      <name>Xin Tao</name>
    </author>
    <author>
      <name>Pengfei Wan</name>
    </author>
    <author>
      <name>Fuli Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://t2i-corebench.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03512v1</id>
    <updated>2025-09-03T17:52:16Z</updated>
    <published>2025-09-03T17:52:16Z</published>
    <title>Bayesian Multivariate Sparse Functional PCA</title>
    <summary>  Functional Principal Components Analysis (FPCA) provides a parsimonious,
semi-parametric model for multivariate, sparsely-observed functional data.
Frequentist FPCA approaches estimate principal components (PCs) from the data,
then condition on these estimates in subsequent analyses. As an alternative, we
propose a fully Bayesian inferential framework for multivariate, sparse
functional data (MSFAST) which explicitly models the PCs and incorporates their
uncertainty. MSFAST builds upon the FAST approach to FPCA for univariate,
densely-observed functional data. Like FAST, MSFAST represents PCs using
orthonormal splines, samples the orthonormal spline coefficients using
parameter expansion, and enforces eigenvalue ordering during model fit. MSFAST
extends FAST to multivariate, sparsely-observed data by (1) standardizing each
functional covariate to mitigate poor posterior conditioning due to disparate
scales; (2) using a better-suited orthogonal spline basis; (3) parallelizing
likelihood calculations over covariates; (4) updating parameterizations and
priors for computational stability; (5) using a Procrustes-based posterior
alignment procedure; and (6) providing efficient prediction routines. We
evaluated MSFAST alongside existing implementations using simulations. MSFAST
produces uniquely valid inferences and accurate estimates, particularly for
smaller signals. MSFAST is motivated by and applied to a study of child growth,
with an accompanying vignette illustrating the implementation step-by-step.
</summary>
    <author>
      <name>Joseph Sartini</name>
    </author>
    <author>
      <name>Scott Zeger</name>
    </author>
    <author>
      <name>Ciprian Crainiceanu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 7 figures for main text. Appendix contains supplemental
  material</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03505v1</id>
    <updated>2025-09-03T17:39:08Z</updated>
    <published>2025-09-03T17:39:08Z</published>
    <title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist
  Intelligence</title>
    <summary>  We argue that progress toward general intelligence requires complementary
foundation models grounded in language, the physical world, and structured
data. This report presents LimiX, the first installment of our large
structured-data models (LDMs). LimiX treats structured data as a joint
distribution over variables and missingness, thus capable of addressing a wide
range of tabular tasks through query-based conditional prediction via a single
model. LimiX is pretrained using masked joint-distribution modeling with an
episodic, context-conditional objective, where the model predicts for query
subsets conditioned on dataset-specific contexts, supporting rapid,
training-free adaptation at inference. We evaluate LimiX across 10 large
structured-data benchmarks with broad regimes of sample size, feature
dimensionality, class number, categorical-to-numerical feature ratio,
missingness, and sample-to-feature ratios. With a single model and a unified
interface, LimiX consistently surpasses strong baselines including
gradient-boosting trees, deep tabular networks, recent tabular foundation
models, and automated ensembles, as shown in Figure 1 and Figure 2. The
superiority holds across a wide range of tasks, such as classification,
regression, missing value imputation, and data generation, often by substantial
margins, while avoiding task-specific architectures or bespoke training per
task. All LimiX models are publicly accessible under Apache 2.0.
</summary>
    <author>
      <name>Xingxuan Zhang</name>
    </author>
    <author>
      <name>Gang Ren</name>
    </author>
    <author>
      <name>Han Yu</name>
    </author>
    <author>
      <name>Hao Yuan</name>
    </author>
    <author>
      <name>Hui Wang</name>
    </author>
    <author>
      <name>Jiansheng Li</name>
    </author>
    <author>
      <name>Jiayun Wu</name>
    </author>
    <author>
      <name>Lang Mo</name>
    </author>
    <author>
      <name>Li Mao</name>
    </author>
    <author>
      <name>Mingchao Hao</name>
    </author>
    <author>
      <name>Ningbo Dai</name>
    </author>
    <author>
      <name>Renzhe Xu</name>
    </author>
    <author>
      <name>Shuyang Li</name>
    </author>
    <author>
      <name>Tianyang Zhang</name>
    </author>
    <author>
      <name>Yue He</name>
    </author>
    <author>
      <name>Yuanrui Wang</name>
    </author>
    <author>
      <name>Yunjia Zhang</name>
    </author>
    <author>
      <name>Zijing Xu</name>
    </author>
    <author>
      <name>Dongzhe Li</name>
    </author>
    <author>
      <name>Fang Gao</name>
    </author>
    <author>
      <name>Hao Zou</name>
    </author>
    <author>
      <name>Jiandong Liu</name>
    </author>
    <author>
      <name>Jiashuo Liu</name>
    </author>
    <author>
      <name>Jiawei Xu</name>
    </author>
    <author>
      <name>Kaijie Cheng</name>
    </author>
    <author>
      <name>Kehan Li</name>
    </author>
    <author>
      <name>Linjun Zhou</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Shaohua Fan</name>
    </author>
    <author>
      <name>Xiaoyu Lin</name>
    </author>
    <author>
      <name>Xinyan Han</name>
    </author>
    <author>
      <name>Xuanyue Li</name>
    </author>
    <author>
      <name>Yan Lu</name>
    </author>
    <author>
      <name>Yuan Xue</name>
    </author>
    <author>
      <name>Yuanyuan Jiang</name>
    </author>
    <author>
      <name>Zimu Wang</name>
    </author>
    <author>
      <name>Zhenlei Wang</name>
    </author>
    <author>
      <name>Peng Cui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03498v1</id>
    <updated>2025-09-03T17:29:50Z</updated>
    <published>2025-09-03T17:29:50Z</published>
    <title>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and
  Generation</title>
    <summary>  We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.
</summary>
    <author>
      <name>Han Li</name>
    </author>
    <author>
      <name>Xinyu Peng</name>
    </author>
    <author>
      <name>Yaoming Wang</name>
    </author>
    <author>
      <name>Zelin Peng</name>
    </author>
    <author>
      <name>Xin Chen</name>
    </author>
    <author>
      <name>Rongxiang Weng</name>
    </author>
    <author>
      <name>Jingang Wang</name>
    </author>
    <author>
      <name>Xunliang Cai</name>
    </author>
    <author>
      <name>Wenrui Dai</name>
    </author>
    <author>
      <name>Hongkai Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03494v1</id>
    <updated>2025-09-03T17:23:24Z</updated>
    <published>2025-09-03T17:23:24Z</published>
    <title>Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual
  Prompts for NR-IQA</title>
    <summary>  In this paper, we propose a novel parameter-efficient adaptation method for
No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized
in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models
(MLLMs), our approach trains only 600K parameters at most (&lt; 0.01% of the base
model), while keeping the underlying model fully frozen. During inference,
these visual prompts are combined with images via addition and processed by
mPLUG-Owl2 with the textual query "Rate the technical quality of the image."
Evaluations across distortion types (synthetic, realistic, AI-generated) on
KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against
full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on
KADID-10k. To our knowledge, this is the first work to leverage pixel-space
visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level
vision tasks. The source code is publicly available at https: // github. com/
yahya-ben/ mplug2-vp-for-nriqa .
</summary>
    <author>
      <name>Yahya Benmahane</name>
    </author>
    <author>
      <name>Mohammed El Hassouni</name>
    </author>
    <link href="http://arxiv.org/abs/2509.03494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03477v1</id>
    <updated>2025-09-03T16:56:27Z</updated>
    <published>2025-09-03T16:56:27Z</published>
    <title>Robult: Leveraging Redundancy and Modality Specific Features for Robust
  Multimodal Learning</title>
    <summary>  Addressing missing modalities and limited labeled data is crucial for
advancing robust multimodal learning. We propose Robult, a scalable framework
designed to mitigate these challenges by preserving modality-specific
information and leveraging redundancy through a novel information-theoretic
approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled
(PU) contrastive loss that maximizes task-relevant feature alignment while
effectively utilizing limited labeled data in semi-supervised settings, and (2)
a latent reconstruction loss that ensures unique modality-specific information
is retained. These strategies, embedded within a modular design, enhance
performance across various downstream tasks and ensure resilience to incomplete
modalities during inference. Experimental results across diverse datasets
validate that Robult achieves superior performance over existing approaches in
both semi-supervised learning and missing modality contexts. Furthermore, its
lightweight design promotes scalability and seamless integration with existing
architectures, making it suitable for real-world multimodal applications.
</summary>
    <author>
      <name>Duy A. Nguyen</name>
    </author>
    <author>
      <name>Abhi Kamboj</name>
    </author>
    <author>
      <name>Minh N. Do</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and presented at IJCAI 2025 in Montreal, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03458v1</id>
    <updated>2025-09-03T16:33:04Z</updated>
    <published>2025-09-03T16:33:04Z</published>
    <title>Comparison of Halo Model and Simulation Predictions for Projected-Field
  Kinematic Sunyaev-Zel'dovich Cross-Correlations</title>
    <summary>  The kinematic Sunyaev-Zel'dovich (kSZ) effect in the cosmic microwave
background (CMB) is a powerful probe of gas physics and large-scale structure
(LSS) in our universe. We consider the "projected-field" kSZ estimator, which
involves cross-correlating a foreground-cleaned, filtered, squared CMB
temperature map with an LSS tracer, and requires no individual tracer
redshifts. We compare $\verb|class_sz|$ halo model calculations of
projected-field kSZ cross-correlations with measurements of these signals from
the Websky numerical simulations. We cross-correlate halo density maps from
Websky with various CMB secondary signals. We first validate our halo model by
comparing its predictions for thermal SZ (tSZ) and patchy screening ($\tau$)
cross-correlations to measurements of these signals from Websky. We consider
three different halo redshift ranges in our comparisons. We also construct our
own kSZ, tSZ, and $\tau$ maps to validate the form of the relevant profiles.
Following the tSZ and $\tau$ validation, we compare projected-field kSZ
calculations between the halo model and the simulations. We use filters
constructed for $\textit{Planck}$ and the Simons Observatory (SO) to assess the
accuracy of the halo-model kSZ predictions for experiments of differing
sensitivity. Overall, we find good agreement, particularly at $\textit{Planck}$
sensitivity. However, we find an $\approx$ 20$\%$ difference between our halo
model and the simulations for SO, which significantly exceeds the predicted
error bars. We note that our halo model includes only the dominant expected
term in the projected-field kSZ signal; the magnitude of the difference between
our model and the simulations is consistent with previous predictions for terms
arising from other contractions in the theory calculation. These terms will
need to be included to obtain unbiased inference from upcoming projected-field
kSZ measurements.
</summary>
    <author>
      <name>Michael Jacob Rodriguez</name>
    </author>
    <author>
      <name>Aleksandra Kusiak</name>
    </author>
    <author>
      <name>Shivam Pandey</name>
    </author>
    <author>
      <name>J. Colin Hill</name>
    </author>
    <link href="http://arxiv.org/abs/2509.03458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03456v1</id>
    <updated>2025-09-03T16:25:45Z</updated>
    <published>2025-09-03T16:25:45Z</published>
    <title>Off-Policy Learning in Large Action Spaces: Optimization Matters More
  Than Estimation</title>
    <summary>  Off-policy evaluation (OPE) and off-policy learning (OPL) are foundational
for decision-making in offline contextual bandits. Recent advances in OPL
primarily optimize OPE estimators with improved statistical properties,
assuming that better estimators inherently yield superior policies. Although
theoretically justified, we argue this estimator-centric approach neglects a
critical practical obstacle: challenging optimization landscapes. In this
paper, we provide theoretical insights and extensive empirical evidence showing
that current OPL methods encounter severe optimization issues, particularly as
action spaces become large. We demonstrate that simpler weighted log-likelihood
objectives enjoy substantially better optimization properties and still recover
competitive, often superior, learned policies. Our findings emphasize the
necessity of explicitly addressing optimization considerations in the
development of OPL algorithms for large action spaces.
</summary>
    <author>
      <name>Imad Aouali</name>
    </author>
    <author>
      <name>Otmane Sakhi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Recsys '25, CONSEQUENCES: Causality, Counterfactuals &amp; Sequential
  Decision-Making Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.03438v1</id>
    <updated>2025-09-03T16:07:34Z</updated>
    <published>2025-09-03T16:07:34Z</published>
    <title>Non-Linear Counterfactual Aggregate Optimization</title>
    <summary>  We consider the problem of directly optimizing a non-linear function of an
outcome, where this outcome itself is the sum of many small contributions. The
non-linearity of the function means that the problem is not equivalent to the
maximization of the expectation of the individual contribution. By leveraging
the concentration properties of the sum of individual outcomes, we derive a
scalable descent algorithm that directly optimizes for our stated objective.
This allows for instance to maximize the probability of successful A/B test,
for which it can be wiser to target a success criterion, such as exceeding a
given uplift, rather than chasing the highest expected payoff.
</summary>
    <author>
      <name>Benjamin Heymann</name>
    </author>
    <author>
      <name>Otmane Sakhi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Recsys '25, CONSEQUENCES: Causality, Counterfactuals &amp; Sequential
  Decision-Making Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.03438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.03438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
