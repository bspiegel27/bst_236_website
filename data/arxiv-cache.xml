<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-02T00:52:01Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-01T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">122397</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.26646v1</id>
    <updated>2025-09-30T17:59:58Z</updated>
    <published>2025-09-30T17:59:58Z</published>
    <title>The Connection between Dusty Star-Forming Galaxies and the First Massive
  Quenched Galaxies</title>
    <summary>  High-redshift (z &gt; 2) massive quiescent (MQ) galaxies provide an opportunity
to probe the key physical processes driving the fuelling and quenching of star
formation in the early Universe. Observational evidence suggests a possible
evolutionary link between MQs and dusty star-forming galaxies (DSFGs; or
submillimetre galaxies), another extreme high-redshift population. However,
galaxy formation models have historically struggled to reproduce these
populations - especially simultaneously - limiting our understanding of their
formation and connection, particularly in light of recent JWST findings. In
previous work, we presented a re-calibrated version of the L-Galaxies
semi-analytic model that provides an improved match to observationally-inferred
number densities of both DSFG and MQ populations. In this work, we use this new
model to investigate the progenitors of MQs at z &gt; 2 and the physical
mechanisms that lead to their quenching. We find that most MQs at z &gt; 2 were
sub-millimetre-bright ($S_{870}$ &gt; 1 mJy) at some point in their cosmic past.
The stellar mass of MQs is strongly correlated with the maximum submillimetre
flux density attained over their history, and this relation appears to be
independent of redshift. However, only a minority of high-redshift DSFGs evolve
into MQs by z = 2. The key distinction between typical DSFGs and MQ progenitors
lies in their merger histories: MQ progenitors experience an early major merger
that triggers a brief, intense starburst and rapid black hole growth, depleting
their cold gas reservoirs. In our model, AGN feedback subsequently prevents
further gas cooling, resulting in quenching. In contrast, the broader DSFG
population remains sub-millimetre-bright, with star formation proceeding
primarily via secular processes, becoming quenched later.
</summary>
    <author>
      <name>Pablo Araya-Araya</name>
    </author>
    <author>
      <name>Rachel K. Cochrane</name>
    </author>
    <author>
      <name>Laerte Sodré Jr.</name>
    </author>
    <author>
      <name>Robert M. Yates</name>
    </author>
    <author>
      <name>Christopher C. Hayward</name>
    </author>
    <author>
      <name>Marcel P. van Daalen</name>
    </author>
    <author>
      <name>Marcelo C. Vicentin</name>
    </author>
    <author>
      <name>Bitten Gullberg</name>
    </author>
    <author>
      <name>Francesco Valentino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to A&amp;A, 15 pages, 11 Figures (including Appendix). Abstract
  shortened to meet ArXiv requirements. Comments are very welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.26646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26635v1</id>
    <updated>2025-09-30T17:59:12Z</updated>
    <published>2025-09-30T17:59:12Z</published>
    <title>A Tractable Family of Smooth Copulas with Rotational Dependence:
  Properties, Inference, and Application</title>
    <summary>  We introduce a new family of copula densities constructed from univariate
distributions on $[0,1]$. Although our construction is structurally simple, the
resulting family is versatile: it includes both smooth and irregular examples,
and reveals clear links between properties of the underlying univariate
distribution and the strength, direction, and form of multivariate dependence.
The framework brings with it a range of explicit mathematical properties,
including interpretable characterizations of dependence and transparent
descriptions of how rotational forms arise. We propose model selection and
inference methods in parametric and nonparametric settings, supported by
asymptotic theory that reduces multivariate estimation to well-studied
univariate problems. Simulation studies confirm the reliable recovery of
structural features, and an application involving neural connectivity data
illustrates how the family can yield a better fit than existing models.
</summary>
    <author>
      <name>Michaël Lalancette</name>
    </author>
    <author>
      <name>Robert Zimmerman</name>
    </author>
    <link href="http://arxiv.org/abs/2509.26635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H05 (Primary) 60E05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26634v1</id>
    <updated>2025-09-30T17:59:09Z</updated>
    <published>2025-09-30T17:59:09Z</published>
    <title>Scaling Spoken Language Models with Syllabic Speech Tokenization</title>
    <summary>  Spoken language models (SLMs) typically discretize speech into
high-frame-rate tokens extracted from SSL speech models. As the most successful
LMs are based on the Transformer architecture, processing these long token
streams with self-attention is expensive, as attention scales quadratically
with sequence length. A recent SSL work introduces acoustic tokenization of
speech at the syllable level, which is more interpretable and potentially more
scalable with significant compression in token lengths (4-5 Hz). Yet, their
value for spoken language modeling is not yet fully explored. We present the
first systematic study of syllabic tokenization for spoken language modeling,
evaluating models on a suite of SLU benchmarks while varying training data
scale. Syllabic tokens can match or surpass the previous high-frame rate tokens
while significantly cutting training and inference costs, achieving more than a
2x reduction in training time and a 5x reduction in FLOPs. Our findings
highlight syllable-level language modeling as a promising path to efficient
long-context spoken language models.
</summary>
    <author>
      <name>Nicholas Lee</name>
    </author>
    <author>
      <name>Cheol Jun Cho</name>
    </author>
    <author>
      <name>Alan W Black</name>
    </author>
    <author>
      <name>Gopala K. Anumanchipalli</name>
    </author>
    <link href="http://arxiv.org/abs/2509.26634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26631v1</id>
    <updated>2025-09-30T17:58:55Z</updated>
    <published>2025-09-30T17:58:55Z</published>
    <title>Learning Generalizable Shape Completion with SIM(3) Equivariance</title>
    <summary>  3D shape completion methods typically assume scans are pre-aligned to a
canonical frame. This leaks pose and scale cues that networks may exploit to
memorize absolute positions rather than inferring intrinsic geometry. When such
alignment is absent in real data, performance collapses. We argue that robust
generalization demands architectural equivariance to the similarity group,
SIM(3), so the model remains agnostic to pose and scale. Following this
principle, we introduce the first SIM(3)-equivariant shape completion network,
whose modular layers successively canonicalize features, reason over
similarity-invariant geometry, and restore the original frame. Under a
de-biased evaluation protocol that removes the hidden cues, our model
outperforms both equivariant and augmentation baselines on the PCN benchmark.
It also sets new cross-domain records on real driving and indoor scans,
lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$
on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol
still outperforms competitors under their biased settings. These results
establish full SIM(3) equivariance as an effective route to truly generalizable
shape completion. Project page: https://sime-completion.github.io.
</summary>
    <author>
      <name>Yuqing Wang</name>
    </author>
    <author>
      <name>Zhaiyu Chen</name>
    </author>
    <author>
      <name>Xiao Xiang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.26631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26626v1</id>
    <updated>2025-09-30T17:58:03Z</updated>
    <published>2025-09-30T17:58:03Z</published>
    <title>Recursive Self-Aggregation Unlocks Deep Thinking in Large Language
  Models</title>
    <summary>  Test-time scaling methods improve the capabilities of large language models
(LLMs) by increasing the amount of compute used during inference to make a
prediction. Inference-time compute can be scaled in parallel by choosing among
multiple independent solutions or sequentially through self-refinement. We
propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired
by evolutionary methods that combines the benefits of both parallel and
sequential scaling. Each step of RSA refines a population of candidate
reasoning chains through aggregation of subsets to yield a population of
improved solutions, which are then used as the candidate pool for the next
iteration. RSA exploits the rich information embedded in the reasoning chains
-- not just the final answers -- and enables bootstrapping from partially
correct intermediate steps within different chains of thought. Empirically, RSA
delivers substantial performance gains with increasing compute budgets across
diverse tasks, model families and sizes. Notably, RSA enables
Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning
models, including DeepSeek-R1 and o3-mini (high), while outperforming purely
parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning
Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the
model to combine solutions via a novel aggregation-aware reinforcement learning
approach yields significant performance gains. Code available at
https://github.com/HyperPotatoNeo/RSA.
</summary>
    <author>
      <name>Siddarth Venkatraman</name>
    </author>
    <author>
      <name>Vineet Jain</name>
    </author>
    <author>
      <name>Sarthak Mittal</name>
    </author>
    <author>
      <name>Vedant Shah</name>
    </author>
    <author>
      <name>Johan Obando-Ceron</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Brian R. Bartoldson</name>
    </author>
    <author>
      <name>Bhavya Kailkhura</name>
    </author>
    <author>
      <name>Guillaume Lajoie</name>
    </author>
    <author>
      <name>Glen Berseth</name>
    </author>
    <author>
      <name>Nikolay Malkin</name>
    </author>
    <author>
      <name>Moksh Jain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.26626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26616v1</id>
    <updated>2025-09-30T17:54:25Z</updated>
    <published>2025-09-30T17:54:25Z</published>
    <title>Black-box Context-free Grammar Inference for Readable &amp; Natural Grammars</title>
    <summary>  Black-box context-free grammar inference is crucial for program analysis,
reverse engineering, and security, yet existing tools such as Arvada, TreeVada,
and Kedavra struggle with scalability, readability, and accuracy on large,
complex languages. We present NatGI, a novel LLM-guided grammar inference
framework that extends TreeVada's parse tree recovery with three key
innovations: bracket-guided bubble exploration, LLM-driven bubble generation
and non-terminal labeling, and hierarchical delta debugging (HDD) for
systematic tree simplification. Bracket-guided exploration leverages syntactic
cues such as parentheses to propose well-structured grammar fragments, while
LLM guidance produces meaningful non-terminal names and selects more promising
merges. Finally, HDD incrementally reduces unnecessary rules, which makes the
grammars both compact and interpretable. In our experiments, we evaluate NatGI
on a comprehensive benchmark suite ranging from small languages to larger ones
such as lua, c, and mysql. Our results show that NatGI consistently outperforms
strong baselines in terms of F1 score. On average, NatGI achieves an F1 score
of 0.57, which is 25pp (percentage points) higher than the best-performing
baseline, TreeVada. In the case of interpretability, our generated grammars
perform significantly better than those produced by existing approaches.
Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules
with meaningful non-terminal names and compact structures that align more
closely with human intuition. As a result, developers and researchers can
achieve higher accuracy while still being able to easily inspect, verify, and
reason about the structure and semantics of the induced grammars.
</summary>
    <author>
      <name>Mohammad Rifat Arefin</name>
    </author>
    <author>
      <name>Shanto Rahman</name>
    </author>
    <author>
      <name>Christoph Csallner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.26616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q42, 68Q45 (Primary), 68T50 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.5; F.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26606v1</id>
    <updated>2025-09-30T17:50:22Z</updated>
    <published>2025-09-30T17:50:22Z</published>
    <title>Beyond Suboptimality: Resource-Rationality and Task Demands Shape the
  Complexity of Perceptual Representations</title>
    <summary>  Early theories of perception as probabilistic inference propose that
uncertainty about the interpretation of sensory input is represented as a
probability distribution over many interpretations -- a relatively complex
representation. However, critics argue that persistent demonstrations of
suboptimal perceptual decision-making indicate limits in representational
complexity. We contend that suboptimality arises not from genuine limits, but
participants' resource-rational adaptations to task demands. For example, when
tasks are solvable with minimal attention to stimuli, participants may neglect
information needed for complex representations, relying instead on simpler ones
that engender suboptimality. Across three experiments, we progressively reduced
the efficacy of resource-rational strategies on a carefully controlled decision
task. Model fits favored simple representations when resource-rational
strategies were effective, and favored complex representations when
ineffective, suggesting that perceptual representations can be simple or
complex depending on task demands. We conclude that resource-rationality is an
epistemic constraint for experimental design and essential to a complete theory
of perception.
</summary>
    <author>
      <name>Andrew Jun Lee</name>
    </author>
    <author>
      <name>Daniel Turek</name>
    </author>
    <author>
      <name>Omer Daglar Tanrikulu</name>
    </author>
    <link href="http://arxiv.org/abs/2509.26606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26593v1</id>
    <updated>2025-09-30T17:46:39Z</updated>
    <published>2025-09-30T17:46:39Z</published>
    <title>Exploring Large Language Model as an Interactive Sports Coach: Lessons
  from a Single-Subject Half Marathon Preparation</title>
    <summary>  Large language models (LLMs) are emerging as everyday assistants, but their
role as longitudinal virtual coaches is underexplored. This two-month single
subject case study documents LLM guided half marathon preparation
(July-September 2025). Using text based interactions and consumer app logs, the
LLM acted as planner, explainer, and occasional motivator. Performance improved
from sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec
per km, with gains in cadence, pace HR coupling, and efficiency index trends.
While causal attribution is limited without a control, outcomes demonstrate
safe, measurable progress. At the same time, gaps were evident, no realtime
sensor integration, text only feedback, motivation support that was user
initiated, and limited personalization or safety guardrails. We propose design
requirements for next generation systems, persistent athlete models with
explicit guardrails, multimodal on device sensing, audio, haptic, visual
feedback, proactive motivation scaffolds, and privacy-preserving
personalization. This study offers grounded evidence and a design agenda for
evolving LLMs from retrospective advisors to closed-loop coaching companions.
</summary>
    <author>
      <name>Kichang Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.26593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4, K.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26591v1</id>
    <updated>2025-09-30T17:45:29Z</updated>
    <published>2025-09-30T17:45:29Z</published>
    <title>The JWST EXCELS Survey: A spectroscopic investigation of the ionizing
  properties of star-forming galaxies at 1&lt;z&lt;8</title>
    <summary>  Charting the Epoch of Reionization demands robust assessments of what drives
the production of ionizing photons in high-redshift star-forming galaxies
(SFGs), and requires better predictive capabilities from current observations.
Using a sample of $N=159$ SFGs at $1&lt;z&lt;8$, observed with deep medium-resolution
spectroscopy from the JWST/NIRSpec EXCELS survey, we perform a statistical
analysis of their ionizing photon production efficiencies ($\xi_\rm{ion}$). We
consider $\xi_\rm{ion}$, measured with Balmer line measurements, in relation to
a number of key galaxy properties including; nebular emission line strengths
($W_\lambda(\rm{H\alpha})$ and $W_\lambda$( [OIII])), UV luminosity
($M_\rm{UV}$) and UV slope ($\beta_\rm{UV}$), as well as dust attenuation
($E(B-V)_\rm{neb}$) and redshift. Implementing a Bayesian linear regression
methodology, we fit $\xi_\rm{ion}$ against the principal observables while
fully marginalising over all measurement uncertainties, mitigating against the
impact of outliers and determining the intrinsic scatter. Significant relations
between $\xi_\rm{ion}$ and $ W_\lambda(\rm{H\alpha})$, $W_\lambda$([OIII]) and
$\beta_\rm{UV}$ are recovered. Moreover, the weak trends with $M_\rm{UV}$ and
redshift can be fully explained by the remaining property dependencies.
Expanding our analysis to multivariate regression, we determine that
$W_\lambda(\rm{H\alpha})$ or $W_\lambda$([OIII]), along with $\beta_\rm{UV}$
and $E(B-V)_\rm{neb}$, are the most important observables for accurately
predicting $\xi_\rm{ion,0}$. The latter identifies the most common outliers as
SFGs with relatively high $E(B-V)_\rm{neb}\gtrsim0.5$, possibly indicative of
obscured star-formation or strong differential attenuation. Combining these
properties enable $\xi_\rm{ion,0}$ to be inferred with an accuracy of
$\sim0.15\,$dex, with a population intrinsic scatter of
$\sigma_\rm{int}\sim0.035\,$dex.
</summary>
    <author>
      <name>R. Begley</name>
    </author>
    <author>
      <name>R. J. McLure</name>
    </author>
    <author>
      <name>F. Cullen</name>
    </author>
    <author>
      <name>A. C. Carnall</name>
    </author>
    <author>
      <name>T. M. Stanton</name>
    </author>
    <author>
      <name>D. Scholte</name>
    </author>
    <author>
      <name>D. J. McLeod</name>
    </author>
    <author>
      <name>J. S. Dunlop</name>
    </author>
    <author>
      <name>K. Z. Arellano-Córdova</name>
    </author>
    <author>
      <name>C. Bondestam</name>
    </author>
    <author>
      <name>C. T. Donnan</name>
    </author>
    <author>
      <name>M. L. Hamadouch</name>
    </author>
    <author>
      <name>A. E. Shapley</name>
    </author>
    <author>
      <name>S. Stevenson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 14 figures + 1 appendix. Submitted to MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.26591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26578v1</id>
    <updated>2025-09-30T17:38:45Z</updated>
    <published>2025-09-30T17:38:45Z</published>
    <title>Linking Process to Outcome: Conditional Reward Modeling for LLM
  Reasoning</title>
    <summary>  Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning capabilities of large language models (LLMs) by guiding their
step-by-step reasoning toward a final answer. However, existing PRMs either
treat each reasoning step in isolation, failing to capture inter-step
dependencies, or struggle to align process rewards with the final outcome.
Consequently, the reward signal fails to respect temporal causality in
sequential reasoning and faces ambiguous credit assignment. These limitations
make downstream models vulnerable to reward hacking and lead to suboptimal
performance. In this work, we propose Conditional Reward Modeling (CRM) that
frames LLM reasoning as a temporal process leading to a correct answer. The
reward of each reasoning step is not only conditioned on the preceding steps
but also explicitly linked to the final outcome of the reasoning trajectory. By
enforcing conditional probability rules, our design captures the causal
relationships among reasoning steps, with the link to the outcome allowing
precise attribution of each intermediate step, thereby resolving credit
assignment ambiguity. Further, through this consistent probabilistic modeling,
the rewards produced by CRM enable more reliable cross-sample comparison.
Experiments across Best-of-N sampling, beam search and reinforcement learning
demonstrate that CRM consistently outperforms existing reward models, offering
a principled framework for enhancing LLM reasoning. In particular, CRM is more
robust to reward hacking and delivers stable downstream improvements without
relying on verifiable rewards derived from ground truth.
</summary>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <author>
      <name>Ziwei Shan</name>
    </author>
    <author>
      <name>Kaitao Song</name>
    </author>
    <author>
      <name>Yexin Li</name>
    </author>
    <author>
      <name>Kan Ren</name>
    </author>
    <link href="http://arxiv.org/abs/2509.26578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
