<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-01T01:05:23Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-30T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">115928</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.22432v1</id>
    <updated>2025-06-27T17:59:01Z</updated>
    <published>2025-06-27T17:59:01Z</published>
    <title>Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy</title>
    <summary>  Recent advances in deep generative modeling have unlocked unprecedented
opportunities for video synthesis. In real-world applications, however, users
often seek tools to faithfully realize their creative editing intentions with
precise and consistent control. Despite the progress achieved by existing
methods, ensuring fine-grained alignment with user intentions remains an open
and challenging problem. In this work, we present Shape-for-Motion, a novel
framework that incorporates a 3D proxy for precise and consistent video
editing. Shape-for-Motion achieves this by converting the target object in the
input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be
performed directly on the proxy and then inferred back to the video frames. To
simplify the editing process, we design a novel Dual-Propagation Strategy that
allows users to perform edits on the 3D mesh of a single frame, and the edits
are then automatically propagated to the 3D meshes of the other frames. The 3D
meshes for different frames are further projected onto the 2D space to produce
the edited geometry and texture renderings, which serve as inputs to a
decoupled video diffusion model for generating edited results. Our framework
supports various precise and physically-consistent manipulations across the
video frames, including pose editing, rotation, scaling, translation, texture
modification, and object composition. Our approach marks a key step toward
high-quality, controllable video editing workflows. Extensive experiments
demonstrate the superiority and effectiveness of our approach. Project page:
https://shapeformotion.github.io/
</summary>
    <author>
      <name>Yuhao Liu</name>
    </author>
    <author>
      <name>Tengfei Wang</name>
    </author>
    <author>
      <name>Fang Liu</name>
    </author>
    <author>
      <name>Zhenwei Wang</name>
    </author>
    <author>
      <name>Rynson W. H. Lau</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22421v1</id>
    <updated>2025-06-27T17:46:13Z</updated>
    <published>2025-06-27T17:46:13Z</published>
    <title>Estimating causal distances with non-causal ones</title>
    <summary>  The adapted Wasserstein ($AW$) distance refines the classical Wasserstein
($W$) distance by incorporating the temporal structure of stochastic processes.
This makes the $AW$-distance well-suited as a robust distance for many dynamic
stochastic optimization problems where the classical $W$-distance fails.
However, estimating the $AW$-distance is a notably challenging task, compared
to the classical $W$-distance. In the present work, we build a sharp estimate
for the $AW$-distance in terms of the $W$-distance, for smooth measures. This
reduces estimating the $AW$-distance to estimating the $W$-distance, where many
well-established classical results can be leveraged. As an application, we
prove a fast convergence rate of the kernel-based empirical estimator under the
$AW$-distance, which approaches the Monte-Carlo rate ($n^{-1/2}$) in the regime
of highly regular densities. These results are accomplished by deriving a sharp
bi-Lipschitz estimate of the adapted total variation distance by the classical
total variation distance.
</summary>
    <author>
      <name>Beatrice Acciaio</name>
    </author>
    <author>
      <name>Songyan Hou</name>
    </author>
    <author>
      <name>Gudmund Pammer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.22421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22396v1</id>
    <updated>2025-06-27T17:10:32Z</updated>
    <published>2025-06-27T17:10:32Z</published>
    <title>QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,
  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization</title>
    <summary>  Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (&lt;=0.2).
</summary>
    <author>
      <name>Danush Khanna</name>
    </author>
    <author>
      <name>Aditya Kumar Guru</name>
    </author>
    <author>
      <name>Srivarshinee Sridhar</name>
    </author>
    <author>
      <name>Zidan Ahmed</name>
    </author>
    <author>
      <name>Rubhav Bahirwani</name>
    </author>
    <author>
      <name>Meetu Malhotra</name>
    </author>
    <author>
      <name>Vinija Jain</name>
    </author>
    <author>
      <name>Aman Chadha</name>
    </author>
    <author>
      <name>Amitava Das</name>
    </author>
    <author>
      <name>Kripabandhu Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Under submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.22396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22395v1</id>
    <updated>2025-06-27T17:09:44Z</updated>
    <published>2025-06-27T17:09:44Z</published>
    <title>Test-Time Consistency in Vision Language Models</title>
    <summary>  Vision-Language Models (VLMs) have achieved impressive performance across a
wide range of multimodal tasks, yet they often exhibit inconsistent behavior
when faced with semantically equivalent inputs, undermining their reliability
and robustness. Recent benchmarks, such as MM-R3, highlight that even
state-of-the-art VLMs can produce divergent predictions across semantically
equivalent inputs, despite maintaining high average accuracy. Prior work
addresses this issue by modifying model architectures or conducting large-scale
fine-tuning on curated datasets. In contrast, we propose a simple and effective
test-time consistency framework that enhances semantic consistency without
supervised re-training. Our method is entirely post-hoc, model-agnostic, and
applicable to any VLM with access to its weights. Given a single test point, we
enforce consistent predictions via two complementary objectives: (i) a
Cross-Entropy Agreement Loss that aligns predictive distributions across
semantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that
draws outputs toward a self-averaged consensus. Our method is plug-and-play and
leverages information from a single test input itself to improve consistency.
Experiments on the MM-R3 benchmark show that our framework yields substantial
gains in consistency across state-of-the-art models, establishing a new
direction for inference-time adaptation in multimodal learning.
</summary>
    <author>
      <name>Shih-Han Chou</name>
    </author>
    <author>
      <name>Shivam Chandhok</name>
    </author>
    <author>
      <name>James J. Little</name>
    </author>
    <author>
      <name>Leonid Sigal</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22385v1</id>
    <updated>2025-06-27T16:51:15Z</updated>
    <published>2025-06-27T16:51:15Z</published>
    <title>Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A
  Study on Defeasible Video Entailment</title>
    <summary>  Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.
</summary>
    <author>
      <name>Yue Zhang</name>
    </author>
    <author>
      <name>Jilei Sun</name>
    </author>
    <author>
      <name>Yunhui Guo</name>
    </author>
    <author>
      <name>Vibhav Gogate</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22376v1</id>
    <updated>2025-06-27T16:44:11Z</updated>
    <published>2025-06-27T16:44:11Z</published>
    <title>Probabilistic Optimality for Inference-time Scaling</title>
    <summary>  Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.
</summary>
    <author>
      <name>Youkang Wang</name>
    </author>
    <author>
      <name>Jian Wang</name>
    </author>
    <author>
      <name>Rubing Chen</name>
    </author>
    <author>
      <name>Xiao-Yong Wei</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22367v1</id>
    <updated>2025-06-27T16:29:25Z</updated>
    <published>2025-06-27T16:29:25Z</published>
    <title>Constraining the Stellar-to-Halo Mass Relation with Galaxy Clustering
  and Weak Lensing from DES Year 3 Data</title>
    <summary>  We develop a framework to study the relation between the stellar mass of a
galaxy and the total mass of its host dark matter halo using galaxy clustering
and galaxy-galaxy lensing measurements. We model a wide range of scales,
roughly from $\sim 100 \; {\rm kpc}$ to $\sim 100 \; {\rm Mpc}$, using a
theoretical framework based on the Halo Occupation Distribution and data from
Year 3 of the Dark Energy Survey (DES) dataset. The new advances of this work
include: 1) the generation and validation of a new stellar mass-selected galaxy
sample in the range of $\log M_\star/M_\odot \sim 9.6$ to $\sim 11.5$; 2) the
joint-modeling framework of galaxy clustering and galaxy-galaxy lensing that is
able to describe our stellar mass-selected sample deep into the 1-halo regime;
and 3) stellar-to-halo mass relation (SHMR) constraints from this dataset. In
general, our SHMR constraints agree well with existing literature with various
weak lensing measurements. We constrain the free parameters in the SHMR
functional form $\log M_\star (M_h) = \log(\epsilon M_1) + f\left[ \log\left(
M_h / M_1 \right) \right] - f(0)$, with $f(x) \equiv -\log(10^{\alpha x}+1) +
\delta [\log(1+\exp(x))]^\gamma / [1+\exp(10^{-x})]$, to be $\log M_1 =
11.559^{+0.334}_{-0.415}$, $\log \epsilon = -1.689^{+0.333}_{-0.220}$, $\alpha
= -1.637^{+0.107}_{-0.096}$, $\gamma = 0.588^{+0.265}_{-0.220}$ and $\delta =
4.227^{+2.223}_{-1.776}$. The inferred average satellite fraction is within
$\sim 5-35\%$ for our fiducial results and we do not see any clear trends with
redshift or stellar mass. Furthermore, we find that the inferred average galaxy
bias values follow the generally expected trends with stellar mass and
redshift. Our study is the first SHMR in DES in this mass range, and we expect
the stellar mass sample to be of general interest for other science cases.
</summary>
    <author>
      <name>G. Zacharegkas</name>
    </author>
    <author>
      <name>C. Chang</name>
    </author>
    <author>
      <name>J. Prat</name>
    </author>
    <author>
      <name>W. Hartley</name>
    </author>
    <author>
      <name>S. Mucesh</name>
    </author>
    <author>
      <name>A. Alarcon</name>
    </author>
    <author>
      <name>O. Alves</name>
    </author>
    <author>
      <name>A. Amon</name>
    </author>
    <author>
      <name>K. Bechtol</name>
    </author>
    <author>
      <name>M. R. Becker</name>
    </author>
    <author>
      <name>G. Bernstein</name>
    </author>
    <author>
      <name>J. Blazek</name>
    </author>
    <author>
      <name>A. Campos</name>
    </author>
    <author>
      <name>A. Carnero Rosell</name>
    </author>
    <author>
      <name>M. Carrasco Kind</name>
    </author>
    <author>
      <name>R. Cawthon</name>
    </author>
    <author>
      <name>R. Chen</name>
    </author>
    <author>
      <name>A. Choi</name>
    </author>
    <author>
      <name>J. Cordero</name>
    </author>
    <author>
      <name>C. Davis</name>
    </author>
    <author>
      <name>J. Derose</name>
    </author>
    <author>
      <name>H. Diehl</name>
    </author>
    <author>
      <name>S. Dodelson</name>
    </author>
    <author>
      <name>C. Doux</name>
    </author>
    <author>
      <name>A. Drlica-Wagner</name>
    </author>
    <author>
      <name>K. Eckert</name>
    </author>
    <author>
      <name>T. F. Eifler</name>
    </author>
    <author>
      <name>J. Elvin-Poole</name>
    </author>
    <author>
      <name>S. Everett</name>
    </author>
    <author>
      <name>X. Fang</name>
    </author>
    <author>
      <name>A. Ferte</name>
    </author>
    <author>
      <name>M. Gatti</name>
    </author>
    <author>
      <name>G. Giannini</name>
    </author>
    <author>
      <name>D. Gruen</name>
    </author>
    <author>
      <name>R. A. Gruendl</name>
    </author>
    <author>
      <name>I. Harrison</name>
    </author>
    <author>
      <name>H. Huang</name>
    </author>
    <author>
      <name>E. M. Huff</name>
    </author>
    <author>
      <name>M. Jarvis</name>
    </author>
    <author>
      <name>E. Krause</name>
    </author>
    <author>
      <name>N. Kuropatkin</name>
    </author>
    <author>
      <name>P. F. Leget</name>
    </author>
    <author>
      <name>N. Maccrann</name>
    </author>
    <author>
      <name>J. McCullough</name>
    </author>
    <author>
      <name>J. Myles</name>
    </author>
    <author>
      <name>A. N. Alsina</name>
    </author>
    <author>
      <name>S. Pandey</name>
    </author>
    <author>
      <name>M. Raveri</name>
    </author>
    <author>
      <name>R. P. Rollins</name>
    </author>
    <author>
      <name>A. Roodman</name>
    </author>
    <author>
      <name>A. J. Ross</name>
    </author>
    <author>
      <name>E. S. Rykoff</name>
    </author>
    <author>
      <name>C. Sanchez</name>
    </author>
    <author>
      <name>L. F. Secco</name>
    </author>
    <author>
      <name>I. Sevilla-Noarbe</name>
    </author>
    <author>
      <name>E. Sheldon</name>
    </author>
    <author>
      <name>T. Shin</name>
    </author>
    <author>
      <name>M. A. Troxel</name>
    </author>
    <author>
      <name>I. Tutusaus</name>
    </author>
    <author>
      <name>B. Yanny</name>
    </author>
    <author>
      <name>B. Yin</name>
    </author>
    <author>
      <name>Y. Zhang</name>
    </author>
    <author>
      <name>J. Zuntz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, main text ends at page 24, 6 appendices, 19 figures, 4
  tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.22367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22362v1</id>
    <updated>2025-06-27T16:23:07Z</updated>
    <published>2025-06-27T16:23:07Z</published>
    <title>DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding</title>
    <summary>  Token-based language modeling is a prominent approach for speech generation,
where tokens are obtained by quantizing features from self-supervised learning
(SSL) models and extracting codes from neural speech codecs, generally referred
to as semantic tokens and acoustic tokens. These tokens are often modeled
autoregressively, with the inference speed being constrained by the token rate.
In this work, we propose DiffSoundStream, a solution that improves the
efficiency of speech tokenization in non-streaming scenarios through two
techniques: (1) conditioning the neural codec on semantic tokens to minimize
redundancy between semantic and acoustic tokens, and (2) leveraging latent
diffusion models to synthesize high-quality waveforms from semantic and
coarse-level acoustic tokens. Experiments show that at 50 tokens per second,
DiffSoundStream achieves speech quality on par with a standard SoundStream
model operating at twice the token rate. Additionally, we achieve step-size
distillation using just four diffusion sampling steps with only a minor quality
loss.
</summary>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Yunpeng Li</name>
    </author>
    <author>
      <name>George Sung</name>
    </author>
    <author>
      <name>Shao-Fu Shih</name>
    </author>
    <author>
      <name>Craig Dooley</name>
    </author>
    <author>
      <name>Alessio Centazzo</name>
    </author>
    <author>
      <name>Ramanan Rajeswaran</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22334v1</id>
    <updated>2025-06-27T15:40:42Z</updated>
    <published>2025-06-27T15:40:42Z</published>
    <title>Linking climate and dengue in the Philippines using a two-stage Bayesian
  spatio-temporal model</title>
    <summary>  Dengue is an infectious disease which poses significant socioeconomic and
disease burden in many tropical and subtropical regions of the world. This work
aims to provide additional insight into the association between dengue and
climate in the Philippines. We employ a two-stage modelling framework: the
first stage fits climate models, while the second stage fits a health model
that uses the climate predictions from the first stage as inputs. We postulate
a Bayesian spatio-temporal model and use the integrated nested Laplace
approximation (INLA) approach for inference. To account for the uncertainty in
the climate models, we perform posterior sampling and then perform Bayesian
model averaging to compute the final posterior estimates of second-stage model
parameters. The results indicate that temperature is positively associated with
dengue, although extremely hot conditions tend to have a negative effect.
Moreover, the relationship between rainfall and dengue varies in space. In
areas with uniform amounts of rainfall all year round, rainfall is negatively
associated with dengue. In contrast, in regions with pronounced dry and wet
season, rainfall shows a positive association with dengue. Finally, there
remains unexplained structured variation in space and time after accounting for
the impact of climate variables and other covariates.
</summary>
    <author>
      <name>Stephen Jun Villejo</name>
    </author>
    <author>
      <name>Sara Martino</name>
    </author>
    <author>
      <name>Janine Illian</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22331v1</id>
    <updated>2025-06-27T15:39:48Z</updated>
    <published>2025-06-27T15:39:48Z</published>
    <title>Less Greedy Equivalence Search</title>
    <summary>  Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.
</summary>
    <author>
      <name>Adiba Ejaz</name>
    </author>
    <author>
      <name>Elias Bareinboim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 total pages. 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.22331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
