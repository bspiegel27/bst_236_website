<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-23T01:00:03Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-23T01:00:03Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>129124</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.17898v1</id>
    <title>Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally</title>
    <updated>2025-12-19T18:57:53Z</updated>
    <link href="https://arxiv.org/abs/2512.17898v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17898v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T18:57:53Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Robin Schimmelpfennig</name>
    </author>
    <author>
      <name>Mark Díaz</name>
    </author>
    <author>
      <name>Vinodkumar Prabhakaran</name>
    </author>
    <author>
      <name>Aida Davani</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17875v1</id>
    <title>Visually Prompted Benchmarks Are Surprisingly Fragile</title>
    <updated>2025-12-19T18:26:58Z</updated>
    <link href="https://arxiv.org/abs/2512.17875v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17875v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T18:26:58Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haiwen Feng</name>
    </author>
    <author>
      <name>Long Lian</name>
    </author>
    <author>
      <name>Lisa Dunlap</name>
    </author>
    <author>
      <name>Jiahao Shu</name>
    </author>
    <author>
      <name>XuDong Wang</name>
    </author>
    <author>
      <name>Renhao Wang</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Alane Suhr</name>
    </author>
    <author>
      <name>Angjoo Kanazawa</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17868v1</id>
    <title>Delayed Acceptance Slice Sampling</title>
    <updated>2025-12-19T18:17:19Z</updated>
    <link href="https://arxiv.org/abs/2512.17868v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17868v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Slice sampling is a well-established Markov chain Monte Carlo method for (approximate) sampling of target distributions which are only known up to a normalizing constant. The method is based on choosing a new state on a slice, i.e., a superlevel set of the given unnormalized target density (with respect to a reference measure). However, slice sampling algorithms usually require per step multiple evaluations of the target density, and thus can become computationally expensive. This is particularly the case for Bayesian inference with costly likelihoods. In this paper, we exploit deterministic approximations of the target density, which are relatively cheap to evaluate, and propose delayed acceptance versions of hybrid slice samplers. We show ergodicity of the resulting slice sampling methods, discuss the superiority of delayed acceptance (ideal) slice sampling over delayed acceptance Metropolis-Hastings algorithms, and illustrate the benefits of our novel approach in terms improved computational efficiency in several numerical experiments.</summary>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T18:17:19Z</published>
    <arxiv:comment>37 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="stat.CO"/>
    <author>
      <name>Kevin Bitterlich</name>
    </author>
    <author>
      <name>Daniel Rudolf</name>
    </author>
    <author>
      <name>Björn Sprungk</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17851v1</id>
    <title>InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</title>
    <updated>2025-12-19T17:52:43Z</updated>
    <link href="https://arxiv.org/abs/2512.17851v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17851v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T17:52:43Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Sarah Rastegar</name>
    </author>
    <author>
      <name>Violeta Chatalbasheva</name>
    </author>
    <author>
      <name>Sieger Falkena</name>
    </author>
    <author>
      <name>Anuj Singh</name>
    </author>
    <author>
      <name>Yanbo Wang</name>
    </author>
    <author>
      <name>Tejas Gokhale</name>
    </author>
    <author>
      <name>Hamid Palangi</name>
    </author>
    <author>
      <name>Hadi Jamali-Rad</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17846v1</id>
    <title>Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes</title>
    <updated>2025-12-19T17:49:13Z</updated>
    <link href="https://arxiv.org/abs/2512.17846v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17846v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.
  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.
  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\% success, strongly outperforming prior methods that peak at 68\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T17:49:13Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Carlos Vélez García</name>
    </author>
    <author>
      <name>Miguel Cazorla</name>
    </author>
    <author>
      <name>Jorge Pomares</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17841v1</id>
    <title>NeuRehab: A Reinforcement Learning and Spiking Neural Network-Based Rehab Automation Framework</title>
    <updated>2025-12-19T17:47:14Z</updated>
    <link href="https://arxiv.org/abs/2512.17841v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17841v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advancements in robotic rehabilitation therapy have provided modular exercise systems for post-stroke muscle recovery with basic control schemes. But these systems struggle to adapt to patients' complex and ever-changing behaviour, and to operate within mobile settings, such as heat and power. To aid this, we present NeuRehab: an end-to-end framework consisting of a training and inference pipeline with AI-based automation, co-designed with neuromorphic computing-based control systems that balance action performance, power consumption, and observed latency. The framework consists of 2 partitions. One is designated for the rehabilitation device based on ultra-low power spiking networks deployed on dedicated neuromorphic hardware. The other resides on stationary hardware that can accommodate computationally intensive hardware for fine-tuning on a per-patient basis. By maintaining a communication channel between both the modules and splitting the algorithm components, the power and latency requirements of the movable system have been optimised, while retaining the learning performance advantages of compute- and power-hungry hardware on the stationary machine. As part of the framework, we propose (a) the split machine learning processes for efficiency in architectural utilisation, and (b) task-specific temporal optimisations to lower edge-inference control latency. This paper evaluates the proposed methods on a reference stepper motor-based shoulder exercise. Overall, these methods offer comparable performance uplifts over the State-of-the-art for neuromorphic deployment, while achieving over 60% savings in both power and latency during inference compared to standard implementations.</summary>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T17:47:14Z</published>
    <arxiv:comment>18 pages, 22 figures, 3 tables, for submission to IOP Neuromorphic Computing and Engineering</arxiv:comment>
    <arxiv:primary_category term="cs.CE"/>
    <author>
      <name>Phani Pavan Kambhampati</name>
      <arxiv:affiliation>IIIT Bangalore</arxiv:affiliation>
    </author>
    <author>
      <name>Chainesh Gautam</name>
      <arxiv:affiliation>IIIT Bangalore</arxiv:affiliation>
    </author>
    <author>
      <name>Jagan Palaniswamy</name>
      <arxiv:affiliation>IIIT Bangalore</arxiv:affiliation>
    </author>
    <author>
      <name>Madhav Rao</name>
      <arxiv:affiliation>IIIT Bangalore</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17828v1</id>
    <title>Regge trajectories for UV completions of graviton scattering from polynomial boundedness</title>
    <updated>2025-12-19T17:34:47Z</updated>
    <link href="https://arxiv.org/abs/2512.17828v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17828v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study graviton scattering amplitudes. Assuming they are UV completed by a theory of weakly coupled massive higher spins, we demonstrate that the UV completion must possess infinitely many Regge trajectories, and thus they are forced to have a stringy spectrum. We extend and simplify a previous proof by some of us for open-string like states to the case of external gravitons. In the present new proof, we trace the need for infinitely many trajectories to the constraint of polynomial boundedness, ultimately tied to causality. We further present numerical results based on the stringy ansatz of Häring-Zhiboedov, which illustrates how single-trajectory-like solutions actually emerge as extremal solutions of numerical bootstrap. In our numerics, these trajectories curiously show up as numerically very large \textit{sister} trajectories. We provide solid evidence that the solutions are spurious as they appear to admit a divergent limit for infinite ansatz size.</summary>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T17:34:47Z</published>
    <arxiv:comment>37 pages, 13 figures</arxiv:comment>
    <arxiv:primary_category term="hep-th"/>
    <author>
      <name>Christopher Eckner</name>
    </author>
    <author>
      <name>Felipe Figueroa</name>
    </author>
    <author>
      <name>Simon Metayer</name>
    </author>
    <author>
      <name>Piotr Tourkine</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17787v1</id>
    <title>Selected topics on: 1) proposal of interpreting the Crab supernova with a GRB 2) progress in identifying the seven GRBs episodes 3) the role of Sagittarius A in identifying the dark matter component (the X fermion)</title>
    <updated>2025-12-19T16:58:14Z</updated>
    <link href="https://arxiv.org/abs/2512.17787v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17787v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As the fiftieth anniversary of our common effort in the field of relativistic astrophysics is approaching, we offer a new look to some of our acquired knowledge in a more complete view, which evidence previous unnoticed connections. They are gaining due prominence in reaching a more complete picture evidencing the main results.
  We outline the history of GRB observations along with a summary of the contributions made by our group to develop the BdHN interpreting model. We show the seven Episodes characterizing the most powerful BdHNe I occurred to date: GRB 190114C and GRB 220101A. New inferences for the explanation of the highest energy radiation in the TeV are presented.</summary>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T16:58:14Z</published>
    <arxiv:primary_category term="astro-ph.HE"/>
    <author>
      <name>R. Ruffini</name>
    </author>
    <author>
      <name>C. Sigismondi</name>
    </author>
    <author>
      <name>Y. Wang</name>
    </author>
    <author>
      <name>J. A. Rueda</name>
    </author>
    <author>
      <name>H. Quevedo</name>
    </author>
    <author>
      <name>S. Zhang</name>
    </author>
    <author>
      <name>Y. Aimuratov</name>
    </author>
    <author>
      <name>P. Chardonnet</name>
    </author>
    <author>
      <name>M. Della Valle</name>
    </author>
    <author>
      <name>C. L. Fryer</name>
    </author>
    <author>
      <name>T. Mirtorabi</name>
    </author>
    <author>
      <name>R. Moradi</name>
    </author>
    <author>
      <name>M. Prakapenia</name>
    </author>
    <author>
      <name>F. Rastegarnia</name>
    </author>
    <author>
      <name>S. -S. Xue</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17782v1</id>
    <title>UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover</title>
    <updated>2025-12-19T16:51:29Z</updated>
    <link href="https://arxiv.org/abs/2512.17782v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17782v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Satellite-derived Land Surface Temperature (LST) products are central to surface urban heat island (SUHI) monitoring due to their consistent grid-based coverage over large metropolitan regions. However, cloud contamination frequently obscures LST observations, limiting their usability for continuous SUHI analysis. Most existing LST reconstruction methods rely on multitemporal information or multisensor data fusion, requiring auxiliary observations that may be unavailable or unreliable under persistent cloud cover. Purely spatial gap-filling approaches offer an alternative, but traditional statistical methods degrade under large or spatially contiguous gaps, while many deep learning based spatial models deteriorate rapidly with increasing missingness.
  Recent advances in denoising diffusion based image inpainting models have demonstrated improved robustness under high missingness, motivating their adoption for spatial LST reconstruction. In this work, we introduce UrbanDIFF, a purely spatial denoising diffusion model for reconstructing cloud contaminated urban LST imagery. The model is conditioned on static urban structure information, including built-up surface data and a digital elevation model, and enforces strict consistency with revealed cloud free pixels through a supervised pixel guided refinement step during inference.
  UrbanDIFF is trained and evaluated using NASA MODIS Terra LST data from seven major United States metropolitan areas spanning 2002 to 2025. Experiments using synthetic cloud masks with 20 to 85 percent coverage show that UrbanDIFF consistently outperforms an interpolation baseline, particularly under dense cloud occlusion, achieving SSIM of 0.89, RMSE of 1.2 K, and R2 of 0.84 at 85 percent cloud coverage, while exhibiting slower performance degradation as cloud density increases.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T16:51:29Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Arya Chavoshi</name>
    </author>
    <author>
      <name>Hassan Dashtian</name>
    </author>
    <author>
      <name>Naveen Sudharsan</name>
    </author>
    <author>
      <name>Dev Niyogi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.17781v1</id>
    <title>LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence</title>
    <updated>2025-12-19T16:50:52Z</updated>
    <link href="https://arxiv.org/abs/2512.17781v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.17781v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-19T16:50:52Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yohanes Yudhi Adikusuma</name>
    </author>
    <author>
      <name>Qixing Huang</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
  </entry>
</feed>
