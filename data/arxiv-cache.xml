<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-27T00:55:45Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-26T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">113068</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.18149v1</id>
    <updated>2025-05-23T17:57:43Z</updated>
    <published>2025-05-23T17:57:43Z</published>
    <title>First Finish Search: Efficient Test-Time Scaling in Large Language
  Models</title>
    <summary>  Test-time scaling (TTS), which involves dynamic allocation of compute during
inference, offers a promising way to improve reasoning in large language
models. While existing TTS methods work well, they often rely on long decoding
paths or require a large number of samples to be generated, increasing the
token usage and inference latency. We observe the surprising fact that for
reasoning tasks, shorter traces are much more likely to be correct than longer
ones. Motivated by this, we introduce First Finish Search (FFS), a
training-free parallel decoding strategy that launches $n$ independent samples
and returns as soon as any one completes. We evaluate FFS alongside simple
decoding, beam search, majority voting, and budget forcing on four reasoning
models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and
across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With
DeepSeek-R1, FFS achieves $82.23\%$ accuracy on the AIME datasets, a $15\%$
improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's
o4-mini performance. Our theoretical analysis explains why stopping at the
shortest trace is likely to yield a correct answer and identifies the
conditions under which early stopping may be suboptimal. The elegance and
simplicity of FFS demonstrate that straightforward TTS strategies can perform
remarkably well, revealing the untapped potential of simple approaches at
inference time.
</summary>
    <author>
      <name>Aradhye Agarwal</name>
    </author>
    <author>
      <name>Ayan Sengupta</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18134v1</id>
    <updated>2025-05-23T17:43:27Z</updated>
    <published>2025-05-23T17:43:27Z</published>
    <title>VideoGameBench: Can Vision-Language Models complete popular video games?</title>
    <summary>  Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.
</summary>
    <author>
      <name>Alex L. Zhang</name>
    </author>
    <author>
      <name>Thomas L. Griffiths</name>
    </author>
    <author>
      <name>Karthik R. Narasimhan</name>
    </author>
    <author>
      <name>Ofir Press</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 33 pages including supplementary</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.18134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18120v1</id>
    <updated>2025-05-23T17:21:14Z</updated>
    <published>2025-05-23T17:21:14Z</published>
    <title>Bidirectional Knowledge Distillation for Enhancing Sequential
  Recommendation with Large Language Models</title>
    <summary>  Large language models (LLMs) have demonstrated exceptional performance in
understanding and generating semantic patterns, making them promising
candidates for sequential recommendation tasks. However, when combined with
conventional recommendation models (CRMs), LLMs often face challenges related
to high inference costs and static knowledge transfer methods. In this paper,
we propose a novel mutual distillation framework, LLMD4Rec, that fosters
dynamic and bidirectional knowledge exchange between LLM-centric and CRM-based
recommendation systems. Unlike traditional unidirectional distillation methods,
LLMD4Rec enables iterative optimization by alternately refining both models,
enhancing the semantic understanding of CRMs and enriching LLMs with
collaborative signals from user-item interactions. By leveraging sample-wise
adaptive weighting and aligning output distributions, our approach eliminates
the need for additional parameters while ensuring effective knowledge transfer.
Extensive experiments on real-world datasets demonstrate that LLMD4Rec
significantly improves recommendation accuracy across multiple benchmarks
without increasing inference costs. This method provides a scalable and
efficient solution for combining the strengths of both LLMs and CRMs in
sequential recommendation systems.
</summary>
    <author>
      <name>Jiongran Wu</name>
    </author>
    <author>
      <name>Jiahao Liu</name>
    </author>
    <author>
      <name>Dongsheng Li</name>
    </author>
    <author>
      <name>Guangping Zhang</name>
    </author>
    <author>
      <name>Mingzhe Han</name>
    </author>
    <author>
      <name>Hansu Gu</name>
    </author>
    <author>
      <name>Peng Zhang</name>
    </author>
    <author>
      <name>Li Shang</name>
    </author>
    <author>
      <name>Tun Lu</name>
    </author>
    <author>
      <name>Ning Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.18120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18118v1</id>
    <updated>2025-05-23T17:19:12Z</updated>
    <published>2025-05-23T17:19:12Z</published>
    <title>Scalable Policy Maximization Under Network Interference</title>
    <summary>  Many interventions, such as vaccines in clinical trials or coupons in online
marketplaces, must be assigned sequentially without full knowledge of their
effects. Multi-armed bandit algorithms have proven successful in such settings.
However, standard independence assumptions fail when the treatment status of
one individual impacts the outcomes of others, a phenomenon known as
interference. We study optimal-policy learning under interference on a dynamic
network. Existing approaches to this problem require repeated observations of
the same fixed network and struggle to scale in sample size beyond as few as
fifteen connected units -- both limit applications. We show that under common
assumptions on the structure of interference, rewards become linear. This
enables us to develop a scalable Thompson sampling algorithm that maximizes
policy impact when a new $n$-node network is observed each round. We prove a
Bayesian regret bound that is sublinear in $n$ and the number of rounds.
Simulation experiments show that our algorithm learns quickly and outperforms
existing methods. The results close a key scalability gap between causal
inference methods for interference and practical bandit algorithms, enabling
policy optimization in large-scale networked systems.
</summary>
    <author>
      <name>Aidan Gleich</name>
    </author>
    <author>
      <name>Eric Laber</name>
    </author>
    <author>
      <name>Alexander Volfovsky</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18092v1</id>
    <updated>2025-05-23T16:47:00Z</updated>
    <published>2025-05-23T16:47:00Z</published>
    <title>QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization</title>
    <summary>  This technical report presents QwenLong-CPRS, a context compression framework
designed for explicit long-context optimization, addressing prohibitive
computation overhead during the prefill stage and the "lost in the middle"
performance degradation of large language models (LLMs) during long sequence
processing. Implemented through a novel dynamic context optimization mechanism,
QwenLong-CPRS enables multi-granularity context compression guided by natural
language instructions, achieving both efficiency gains and improved
performance.
  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key
innovations: (1) Natural language-guided dynamic optimization, (2)
Bidirectional reasoning layers for enhanced boundary awareness, (3) Token
critic mechanisms with language modeling heads, and (4) Window-parallel
inference.
  Comprehensive evaluations across five benchmarks (4K-2M word contexts)
demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority
over other context management methods like RAG and sparse attention in both
accuracy and efficiency. (2) Architecture-agnostic integration with all
flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,
and Qwen2.5-max, achieves 21.59$\times$ context compression alongside
19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,
QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on
Ruler-128K and InfiniteBench, establishing new SOTA performance.
</summary>
    <author>
      <name>Weizhou Shen</name>
    </author>
    <author>
      <name>Chenliang Li</name>
    </author>
    <author>
      <name>Fanqi Wan</name>
    </author>
    <author>
      <name>Shengyi Liao</name>
    </author>
    <author>
      <name>Shaopeng Lai</name>
    </author>
    <author>
      <name>Bo Zhang</name>
    </author>
    <author>
      <name>Yingcheng Shi</name>
    </author>
    <author>
      <name>Yuning Wu</name>
    </author>
    <author>
      <name>Gang Fu</name>
    </author>
    <author>
      <name>Zhansheng Li</name>
    </author>
    <author>
      <name>Bin Yang</name>
    </author>
    <author>
      <name>Ji Zhang</name>
    </author>
    <author>
      <name>Fei Huang</name>
    </author>
    <author>
      <name>Jingren Zhou</name>
    </author>
    <author>
      <name>Ming Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18088v1</id>
    <updated>2025-05-23T16:45:14Z</updated>
    <published>2025-05-23T16:45:14Z</published>
    <title>Early-Exit Graph Neural Networks</title>
    <summary>  Early-exit mechanisms allow deep neural networks to halt inference as soon as
classification confidence is high enough, adaptively trading depth for
confidence, and thereby cutting latency and energy on easy inputs while
retaining full-depth accuracy for harder ones. Similarly, adding early exit
mechanisms to Graph Neural Networks (GNNs), the go-to models for
graph-structured data, allows for dynamic trading depth for confidence on
simple graphs while maintaining full-depth accuracy on harder and more complex
graphs to capture intricate relationships. Although early exits have proven
effective across various deep learning domains, their potential within GNNs in
scenarios that require deep architectures while resisting over-smoothing and
over-squashing remains largely unexplored. We unlock that potential by first
introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose
symmetry-based inductive biases mitigate these issues and yield stable
intermediate representations that can be useful to allow early exiting in GNNs.
Building on this backbone, we present Early-Exit Graph Neural Networks
(EEGNNs), which append confidence-aware exit heads that allow on-the-fly
termination of propagation based on each node or the entire graph. Experiments
show that EEGNNs preserve robust performance as depth grows and deliver
competitive accuracy on heterophilic and long-range benchmarks, matching
attention-based and asynchronous message-passing models while substantially
reducing computation and latency. We plan to release the code to reproduce our
experiments.
</summary>
    <author>
      <name>Andrea Giuseppe Di Francesco</name>
    </author>
    <author>
      <name>Maria Sofia Bucarelli</name>
    </author>
    <author>
      <name>Franco Maria Nardini</name>
    </author>
    <author>
      <name>Raffaele Perego</name>
    </author>
    <author>
      <name>Nicola Tonellotto</name>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.18088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18083v1</id>
    <updated>2025-05-23T16:41:08Z</updated>
    <published>2025-05-23T16:41:08Z</published>
    <title>What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?</title>
    <summary>  In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.
</summary>
    <author>
      <name>Quentin Clark</name>
    </author>
    <author>
      <name>Florian Shkurti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.18083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18081v1</id>
    <updated>2025-05-23T16:39:21Z</updated>
    <published>2025-05-23T16:39:21Z</published>
    <title>Backpropagation-Free Metropolis-Adjusted Langevin Algorithm</title>
    <summary>  Recent work on backpropagation-free learning has shown that it is possible to
use forward-mode automatic differentiation (AD) to perform optimization on
differentiable models. Forward-mode AD requires sampling a tangent vector for
each forward pass of a model. The result is the model evaluation with the
directional derivative along the tangent. In this paper, we illustrate how the
sampling of this tangent vector can be incorporated into the proposal mechanism
for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the
first to introduce a backpropagation-free gradient-based Markov chain Monte
Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free
position-specific preconditioned forward-mode MALA that leverages Hessian
information. Overall, we propose four new algorithms: Forward MALA; Line
Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward
MALA. We highlight the reduced computational cost of the forward-mode samplers
and show that forward-mode is competitive with the original MALA, while even
outperforming it depending on the probabilistic model. We include Bayesian
inference results on a range of probabilistic models, including hierarchical
distributions and Bayesian neural networks.
</summary>
    <author>
      <name>Adam D. Cobb</name>
    </author>
    <author>
      <name>Susmit Jha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 Pages, 8 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.18081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18077v1</id>
    <updated>2025-05-23T16:33:47Z</updated>
    <published>2025-05-23T16:33:47Z</published>
    <title>Bayesian Deep Learning for Discrete Choice</title>
    <summary>  Discrete choice models (DCMs) are used to analyze individual decision-making
in contexts such as transportation choices, political elections, and consumer
preferences. DCMs play a central role in applied econometrics by enabling
inference on key economic variables, such as marginal rates of substitution,
rather than focusing solely on predicting choices on new unlabeled data.
However, while traditional DCMs offer high interpretability and support for
point and interval estimation of economic quantities, these models often
underperform in predictive tasks compared to deep learning (DL) models. Despite
their predictive advantages, DL models remain largely underutilized in discrete
choice due to concerns about their lack of interpretability, unstable parameter
estimates, and the absence of established methods for uncertainty
quantification. Here, we introduce a deep learning model architecture
specifically designed to integrate with approximate Bayesian inference methods,
such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model
collapses to behaviorally informed hypotheses when data is limited, mitigating
overfitting and instability in underspecified settings while retaining the
flexibility to capture complex nonlinear relationships when sufficient data is
available. We demonstrate our approach using SGLD through a Monte Carlo
simulation study, evaluating both predictive metrics--such as out-of-sample
balanced accuracy--and inferential metrics--such as empirical coverage for
marginal rates of substitution interval estimates. Additionally, we present
results from two empirical case studies: one using revealed mode choice data in
NYC, and the other based on the widely used Swiss train choice stated
preference data.
</summary>
    <author>
      <name>Daniel F. Villarraga</name>
    </author>
    <author>
      <name>Ricardo A. Daziano</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18071v1</id>
    <updated>2025-05-23T16:16:46Z</updated>
    <published>2025-05-23T16:16:46Z</published>
    <title>Extended Inductive Reasoning for Personalized Preference Inference from
  Behavioral Signals</title>
    <summary>  Large language models (LLMs) have demonstrated significant success in complex
reasoning tasks such as math and coding. In contrast to these tasks where
deductive reasoning predominates, inductive reasoning\textemdash the ability to
derive general rules from incomplete evidence, remains underexplored. This
paper investigates extended inductive reasoning in LLMs through the lens of
personalized preference inference, a critical challenge in LLM alignment where
current approaches struggle to capture diverse user preferences. The task
demands strong inductive reasoning capabilities as user preferences are
typically embedded implicitly across various interaction forms, requiring
models to synthesize consistent preference patterns from scattered signals. We
propose \textsc{AlignXplore}, a model that leverages extended reasoning chains
to enable systematic preference inference from behavioral signals in users'
interaction histories. We develop \textsc{AlignXplore} by combining cold-start
training based on synthetic data with subsequent online reinforcement learning.
Through extensive experiments, we demonstrate that \textsc{AlignXplore}
achieves substantial improvements over the backbone model by an average of
11.05\% on in-domain and out-of-domain benchmarks, while maintaining strong
generalization ability across different input formats and downstream models.
Further analyses establish best practices for preference inference learning
through systematic comparison of reward modeling strategies, while revealing
the emergence of human-like inductive reasoning patterns during training.
</summary>
    <author>
      <name>Jia-Nan Li</name>
    </author>
    <author>
      <name>Jian Guan</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Rui Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
