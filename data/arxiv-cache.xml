<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-30T00:50:56Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">119891</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.21058v1</id>
    <updated>2025-08-28T17:57:55Z</updated>
    <published>2025-08-28T17:57:55Z</published>
    <title>Mixture of Contexts for Long Video Generation</title>
    <summary>  Long video generation is fundamentally a long context memory problem: models
must retain and retrieve salient events across a long range without collapsing
or drifting. However, scaling diffusion transformers to generate long-context
videos is fundamentally limited by the quadratic cost of self-attention, which
makes memory and computation intractable and difficult to optimize for long
sequences. We recast long-context video generation as an internal information
retrieval task and propose a simple, learnable sparse attention routing module,
Mixture of Contexts (MoC), as an effective long-term memory retrieval engine.
In MoC, each query dynamically selects a few informative chunks plus mandatory
anchors (caption, local windows) to attend to, with causal routing that
prevents loop closures. As we scale the data and gradually sparsify the
routing, the model allocates compute to salient history, preserving identities,
actions, and scenes over minutes of content. Efficiency follows as a byproduct
of retrieval (near-linear scaling), which enables practical training and
synthesis, and the emergence of memory and consistency at the scale of minutes.
</summary>
    <author>
      <name>Shengqu Cai</name>
    </author>
    <author>
      <name>Ceyuan Yang</name>
    </author>
    <author>
      <name>Lvmin Zhang</name>
    </author>
    <author>
      <name>Yuwei Guo</name>
    </author>
    <author>
      <name>Junfei Xiao</name>
    </author>
    <author>
      <name>Ziyan Yang</name>
    </author>
    <author>
      <name>Yinghao Xu</name>
    </author>
    <author>
      <name>Zhenheng Yang</name>
    </author>
    <author>
      <name>Alan Yuille</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <author>
      <name>Maneesh Agrawala</name>
    </author>
    <author>
      <name>Lu Jiang</name>
    </author>
    <author>
      <name>Gordon Wetzstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://primecai.github.io/moc/</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.21058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21046v1</id>
    <updated>2025-08-28T17:50:58Z</updated>
    <published>2025-08-28T17:50:58Z</published>
    <title>CogVLA: Cognition-Aligned Vision-Language-Action Model via
  Instruction-Driven Routing &amp; Sparsification</title>
    <summary>  Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.
</summary>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Renshan Zhang</name>
    </author>
    <author>
      <name>Rui Shao</name>
    </author>
    <author>
      <name>Jie He</name>
    </author>
    <author>
      <name>Liqiang Nie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 8 figures, Project Page:
  https://jiutian-vl.github.io/CogVLA-page</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.21046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21044v1</id>
    <updated>2025-08-28T17:50:03Z</updated>
    <published>2025-08-28T17:50:03Z</published>
    <title>MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for
  Efficient Video LLMs</title>
    <summary>  Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.
</summary>
    <author>
      <name>Junpeng Ma</name>
    </author>
    <author>
      <name>Qizhe Zhang</name>
    </author>
    <author>
      <name>Ming Lu</name>
    </author>
    <author>
      <name>Zhibin Wang</name>
    </author>
    <author>
      <name>Qiang Zhou</name>
    </author>
    <author>
      <name>Jun Song</name>
    </author>
    <author>
      <name>Shanghang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.21044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21032v1</id>
    <updated>2025-08-28T17:35:03Z</updated>
    <published>2025-08-28T17:35:03Z</published>
    <title>Reusing Computation in Text-to-Image Diffusion for Efficient Generation
  of Image Sets</title>
    <summary>  Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/
</summary>
    <author>
      <name>Dale Decatur</name>
    </author>
    <author>
      <name>Thibault Groueix</name>
    </author>
    <author>
      <name>Wang Yifan</name>
    </author>
    <author>
      <name>Rana Hanocka</name>
    </author>
    <author>
      <name>Vladimir Kim</name>
    </author>
    <author>
      <name>Matheus Gadelha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2025. Project page:
  https://ddecatur.github.io/hierarchical-diffusion/</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.21032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21025v1</id>
    <updated>2025-08-28T17:28:39Z</updated>
    <published>2025-08-28T17:28:39Z</published>
    <title>Pivotal inference for linear predictions in stationary processes</title>
    <summary>  In this paper we develop pivotal inference for the final (FPE) and relative
final prediction error (RFPE) of linear forecasts in stationary processes. Our
approach is based on a novel self-normalizing technique and avoids the
estimation of the asymptotic variances of the empirical autocovariances. We
provide pivotal confidence intervals for the (R)FPE, develop estimates for the
minimal order of a linear prediction that is required to obtain a prespecified
forecasting accuracy and also propose (pivotal) statistical tests for the
hypotheses that the (R)FPE exceeds a given threshold. Additionally, we provide
new (pivotal) inference tools for the partial autocorrelation, which do not
require the assumption of an autoregressive process.
</summary>
    <author>
      <name>Holger Dette</name>
    </author>
    <author>
      <name>Sebastian KÃ¼hnert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31, pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.21025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10, 62M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21016v1</id>
    <updated>2025-08-28T17:18:31Z</updated>
    <published>2025-08-28T17:18:31Z</published>
    <title>Inference-Time Alignment Control for Diffusion Models with Reinforcement
  Learning Guidance</title>
    <summary>  Denoising-based generative models, particularly diffusion and flow matching
algorithms, have achieved remarkable success. However, aligning their output
distributions with complex downstream objectives, such as human preferences,
compositional accuracy, or data compressibility, remains challenging. While
reinforcement learning (RL) fine-tuning methods, inspired by advances in RL
from human feedback (RLHF) for large language models, have been adapted to
these generative frameworks, current RL approaches are suboptimal for diffusion
models and offer limited flexibility in controlling alignment strength after
fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models
through the lens of stochastic differential equations and implicit reward
conditioning. We introduce Reinforcement Learning Guidance (RLG), an
inference-time method that adapts Classifier-Free Guidance (CFG) by combining
the outputs of the base and RL fine-tuned models via a geometric average. Our
theoretical analysis shows that RLG's guidance scale is mathematically
equivalent to adjusting the KL-regularization coefficient in standard RL
objectives, enabling dynamic control over the alignment-quality trade-off
without further training. Extensive experiments demonstrate that RLG
consistently improves the performance of RL fine-tuned models across various
architectures, RL algorithms, and downstream tasks, including human
preferences, compositional control, compressibility, and text rendering.
Furthermore, RLG supports both interpolation and extrapolation, thereby
offering unprecedented flexibility in controlling generative alignment. Our
approach provides a practical and theoretically sound solution for enhancing
and controlling diffusion model alignment at inference. The source code for RLG
is publicly available at the Github:
https://github.com/jinluo12345/Reinforcement-learning-guidance.
</summary>
    <author>
      <name>Luozhijie Jin</name>
    </author>
    <author>
      <name>Zijie Qiu</name>
    </author>
    <author>
      <name>Jie Liu</name>
    </author>
    <author>
      <name>Zijie Diao</name>
    </author>
    <author>
      <name>Lifeng Qiao</name>
    </author>
    <author>
      <name>Ning Ding</name>
    </author>
    <author>
      <name>Alex Lamb</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <link href="http://arxiv.org/abs/2508.21016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21010v1</id>
    <updated>2025-08-28T17:10:53Z</updated>
    <published>2025-08-28T17:10:53Z</published>
    <title>ChainReaction! Structured Approach with Causal Chains as Intermediate
  Representations for Improved and Explainable Causal Video Question Answering</title>
    <summary>  Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/
</summary>
    <author>
      <name>Paritosh Parmar</name>
    </author>
    <author>
      <name>Eric Peh</name>
    </author>
    <author>
      <name>Basura Fernando</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://paritoshparmar.github.io/chainreaction/</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.21010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21007v1</id>
    <updated>2025-08-28T17:09:05Z</updated>
    <published>2025-08-28T17:09:05Z</published>
    <title>Rapid Mismatch Estimation via Neural Network Informed Variational
  Inference</title>
    <summary>  With robots increasingly operating in human-centric environments, ensuring
soft and safe physical interactions, whether with humans, surroundings, or
other machines, is essential. While compliant hardware can facilitate such
interactions, this work focuses on impedance controllers that allow
torque-controlled robots to safely and passively respond to contact while
accurately executing tasks. From inverse dynamics to quadratic
programming-based controllers, the effectiveness of these methods relies on
accurate dynamics models of the robot and the object it manipulates. Any model
mismatch results in task failures and unsafe behaviors. Thus, we introduce
Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic,
probabilistic framework that estimates end-effector dynamics mismatches online,
without relying on external force-torque sensors. From the robot's
proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a
prior for a Variational Inference solver, which rapidly converges to the
unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator
driven by a state-of-the-art passive impedance controller, RME adapts to sudden
changes in mass and center of mass at the end-effector in $\sim400$ ms, in
static and dynamic settings. We demonstrate RME in a collaborative scenario
where a human attaches an unknown basket to the robot's end-effector and
dynamically adds/removes heavy items, showcasing fast and safe adaptation to
changing dynamics during physical interaction without any external sensory
system.
</summary>
    <author>
      <name>Mateusz Jaszczuk</name>
    </author>
    <author>
      <name>Nadia Figueroa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 9th Annual Conference on Robot Learning. Project Website
  - https://mateusz-jaszczuk.github.io/rme/</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.21007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.21006v1</id>
    <updated>2025-08-28T17:08:11Z</updated>
    <published>2025-08-28T17:08:11Z</published>
    <title>Practical indistinguishability in a gene regulatory network inference
  problem, a case study</title>
    <summary>  Computationally inferring mechanistic insights from typical biological data
is a challenging pursuit. Even the highest-quality experimental data come with
challenges. There are always sources of noise, a limit to how often we can
measure the system, and we can rarely measure all the relevant states that
participate in the underlying complexity. There are usually sources of
uncertainty in model development, which give rise to multiple competing model
structures. To underscore the need for further analysis of structural
uncertainty in modeling, we use a meta-analysis across six journals covering
mathematical biology and show that a huge number of models for biological
systems are developed each year, but model selection and comparison across
model structures appear to be less common. We walk through a case study
involving inference of regulatory network structure involved in a developmental
decision in the nematode, \textit{Pristonchus pacificus}. We use real
biological data and compare across 13,824 models--each corresponding to a
different regulatory network structure, to determine which regulatory features
are supported by the data across three experimental conditions. We find that
the best-fitting models for each experimental condition share a combination of
features and identify a regulatory network that is common across the model sets
for each condition. This model can describe the data across the experimental
conditions we considered and exhibits a high degree of positive regulation and
interconnectivity between the key regulators, \textit{eud-1}, $textit{sult-1},
and \textit{nhr-40}. While the biological results are specific to the molecular
biology of development in \textit{Pristonchus pacificus}, the general modeling
framework and underlying challenges we faced doing this analysis are widespread
across biology, chemistry, physics, and many other scientific disciplines.
</summary>
    <author>
      <name>Cody E. FitzGerald</name>
    </author>
    <author>
      <name>Shelley Reich</name>
    </author>
    <author>
      <name>Victor Agaba</name>
    </author>
    <author>
      <name>Arjun Mathur</name>
    </author>
    <author>
      <name>Michael S. Werner</name>
    </author>
    <author>
      <name>Niall M. Mangan</name>
    </author>
    <link href="http://arxiv.org/abs/2508.21006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.21006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.20998v1</id>
    <updated>2025-08-28T16:59:34Z</updated>
    <published>2025-08-28T16:59:34Z</published>
    <title>First-Order Viscous Relativistic Hydrodynamics on the Two-Sphere</title>
    <summary>  A few years ago, Bemfica, Disconzi, Noronha, and Kovtun (BDNK) formulated the
first causal, stable, strongly hyperbolic, and locally well-posed theory of
first-order viscous relativistic hydrodynamics. Since their inception, there
have been several numerical and analytic studies of the BDNK equations which
have revealed their promise in modeling relativistic flows when viscous,
first-order corrections to ideal hydrodynamics are important. In this paper, we
present numerical solutions to the BDNK equations for a $4$D conformal fluid in
Minkowski spacetime constrained to the surface of a geometric sphere. We
numerically solve the underlying equations of motion by use of finite
difference methods applied in cubed-sphere coordinates -- a multi-block grid
structure which regularly and continuously covers the surface of a sphere. We
present three test cases of our code: linearized fluid perturbations of
equilibrium states, a smooth, stationary initial Gaussian pulse of energy
density, and Kelvin-Helmholtz-unstable initial data. In the Gaussian test case
with sufficiently large entropy-normalized shear viscosity, the flow, though
initialized in equilibrium, dynamically diverges away from equilibrium and the
regime of validity of first-order hydrodynamics as very steep gradients form in
the solution, causing convergence to be lost in the numerical simulation. This
behavior persists at all grid resolutions we have considered, and also occurs
at much higher resolutions in simulations of planar-symmetric ($1+1$)D
conformal flows. These solutions provide numerical evidence that singularities
in solutions to the BDNK equations can form in finite time from smooth initial
data. The numerical methods we employ on the two-sphere can be readily extended
to include variations in the radial direction, allowing for full ($3+1$)D
simulations of the BDNK equations in astrophysical applications.
</summary>
    <author>
      <name>Lennox S. Keeble</name>
    </author>
    <author>
      <name>Frans Pretorius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 + 12 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.20998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.20998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
