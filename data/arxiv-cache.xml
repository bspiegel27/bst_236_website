<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-13T00:59:06Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-12T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">118717</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.08248v1</id>
    <updated>2025-08-11T17:58:24Z</updated>
    <published>2025-08-11T17:58:24Z</published>
    <title>StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation</title>
    <summary>  Current diffusion models for audio-driven avatar video generation struggle to
synthesize long videos with natural audio synchronization and identity
consistency. This paper presents StableAvatar, the first end-to-end video
diffusion transformer that synthesizes infinite-length high-quality videos
without post-processing. Conditioned on a reference image and audio,
StableAvatar integrates tailored training and inference modules to enable
infinite-length video generation. We observe that the main reason preventing
existing models from generating long videos lies in their audio modeling. They
typically rely on third-party off-the-shelf extractors to obtain audio
embeddings, which are then directly injected into the diffusion model via
cross-attention. Since current diffusion backbones lack any audio-related
priors, this approach causes severe latent distribution error accumulation
across video clips, leading the latent distribution of subsequent segments to
drift away from the optimal distribution gradually. To address this,
StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents
error accumulation via time-step-aware modulation. During inference, we propose
a novel Audio Native Guidance Mechanism to further enhance the audio
synchronization by leveraging the diffusion's own evolving joint audio-latent
prediction as a dynamic guidance signal. To enhance the smoothness of the
infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy
that fuses latent over time. Experiments on benchmarks show the effectiveness
of StableAvatar both qualitatively and quantitatively.
</summary>
    <author>
      <name>Shuyuan Tu</name>
    </author>
    <author>
      <name>Yueming Pan</name>
    </author>
    <author>
      <name>Yinming Huang</name>
    </author>
    <author>
      <name>Xintong Han</name>
    </author>
    <author>
      <name>Zhen Xing</name>
    </author>
    <author>
      <name>Qi Dai</name>
    </author>
    <author>
      <name>Chong Luo</name>
    </author>
    <author>
      <name>Zuxuan Wu</name>
    </author>
    <author>
      <name>Yu-Gang Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2508.08248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08219v1</id>
    <updated>2025-08-11T17:38:50Z</updated>
    <published>2025-08-11T17:38:50Z</published>
    <title>SAGOnline: Segment Any Gaussians Online</title>
    <summary>  3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit
3D scene representation, yet achieving efficient and consistent 3D segmentation
remains challenging. Current methods suffer from prohibitive computational
costs, limited 3D spatial reasoning, and an inability to track multiple objects
simultaneously. We present Segment Any Gaussians Online (SAGOnline), a
lightweight and zero-shot framework for real-time 3D segmentation in Gaussian
scenes that addresses these limitations through two key innovations: (1) a
decoupled strategy that integrates video foundation models (e.g., SAM2) for
view-consistent 2D mask propagation across synthesized views; and (2) a
GPU-accelerated 3D mask generation and Gaussian-level instance labeling
algorithm that assigns unique identifiers to 3D primitives, enabling lossless
multi-object tracking and segmentation across views. SAGOnline achieves
state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)
benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times
in inference speed (27 ms/frame). Qualitative results demonstrate robust
multi-object segmentation and tracking in complex scenes. Our contributions
include: (i) a lightweight and zero-shot framework for 3D segmentation in
Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling
simultaneous segmentation and tracking, and (iii) the effective adaptation of
2D video foundation models to the 3D domain. This work allows real-time
rendering and 3D scene understanding, paving the way for practical AR/VR and
robotic applications.
</summary>
    <author>
      <name>Wentao Sun</name>
    </author>
    <author>
      <name>Quanyun Wu</name>
    </author>
    <author>
      <name>Hanqing Xu</name>
    </author>
    <author>
      <name>Kyle Gao</name>
    </author>
    <author>
      <name>Zhengsen Xu</name>
    </author>
    <author>
      <name>Yiping Chen</name>
    </author>
    <author>
      <name>Dedong Zhang</name>
    </author>
    <author>
      <name>Lingfei Ma</name>
    </author>
    <author>
      <name>John S. Zelek</name>
    </author>
    <author>
      <name>Jonathan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.08219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08211v1</id>
    <updated>2025-08-11T17:33:18Z</updated>
    <published>2025-08-11T17:33:18Z</published>
    <title>SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling</title>
    <summary>  Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.
</summary>
    <author>
      <name>Zhuohao Yu</name>
    </author>
    <author>
      <name>Xingru Jiang</name>
    </author>
    <author>
      <name>Weizheng Gu</name>
    </author>
    <author>
      <name>Yidong Wang</name>
    </author>
    <author>
      <name>Shikun Zhang</name>
    </author>
    <author>
      <name>Wei Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 12 figures, code available:
  https://zhuohaoyu.github.io/SAEMark</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.08211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08204v1</id>
    <updated>2025-08-11T17:22:45Z</updated>
    <published>2025-08-11T17:22:45Z</published>
    <title>Human-Alignment and Calibration of Inference-Time Uncertainty in Large
  Language Models</title>
    <summary>  There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.
</summary>
    <author>
      <name>Kyle Moore</name>
    </author>
    <author>
      <name>Jesse Roberts</name>
    </author>
    <author>
      <name>Daryl Watson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.08204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08199v1</id>
    <updated>2025-08-11T17:17:20Z</updated>
    <published>2025-08-11T17:17:20Z</published>
    <title>Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating
  Room with Multimodal Large Language Model</title>
    <summary>  Precise spatial modeling in the operating room (OR) is foundational to many
clinical tasks, supporting intraoperative awareness, hazard avoidance, and
surgical decision-making. While existing approaches leverage large-scale
multimodal datasets for latent-space alignment to implicitly learn spatial
relationships, they overlook the 3D capabilities of MLLMs. However, this
approach raises two issues: (1) Operating rooms typically lack multiple video
and audio sensors, making multimodal 3D data difficult to obtain; (2) Training
solely on readily available 2D data fails to capture fine-grained details in
complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first
large vision-language model for 3D spatial reasoning in operating rooms using
only RGB modality to infer volumetric and semantic cues, enabling downstream
medical tasks with detailed and holistic spatial context. Spatial-ORMLLM
incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D
modality inputs with rich 3D spatial knowledge extracted by the estimation
algorithm and then feeds the combined features into the visual tower. By
employing a unified end-to-end MLLM framework, it combines powerful spatial
features with textual features to deliver robust 3D scene reasoning without any
additional expert annotations or sensor inputs. Experiments on multiple
benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves
state-of-the-art performance and generalizes robustly to previously unseen
surgical scenarios and downstream tasks.
</summary>
    <author>
      <name>Peiqi He</name>
    </author>
    <author>
      <name>Zhenhao Zhang</name>
    </author>
    <author>
      <name>Yixiang Zhang</name>
    </author>
    <author>
      <name>Xiongjun Zhao</name>
    </author>
    <author>
      <name>Shaoliang Peng</name>
    </author>
    <link href="http://arxiv.org/abs/2508.08199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08192v1</id>
    <updated>2025-08-11T17:11:26Z</updated>
    <published>2025-08-11T17:11:26Z</published>
    <title>Efficient Speculative Decoding for Llama at Scale: Challenges and
  Solutions</title>
    <summary>  Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.
</summary>
    <author>
      <name>Bangsheng Tang</name>
    </author>
    <author>
      <name>Carl Chengyan Fu</name>
    </author>
    <author>
      <name>Fei Kou</name>
    </author>
    <author>
      <name>Grigory Sizov</name>
    </author>
    <author>
      <name>Haoci Zhang</name>
    </author>
    <author>
      <name>Jason Park</name>
    </author>
    <author>
      <name>Jiawen Liu</name>
    </author>
    <author>
      <name>Jie You</name>
    </author>
    <author>
      <name>Qirui Yang</name>
    </author>
    <author>
      <name>Sachin Mehta</name>
    </author>
    <author>
      <name>Shengyong Cai</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <author>
      <name>Xingyu Liu</name>
    </author>
    <author>
      <name>Yunlu Li</name>
    </author>
    <author>
      <name>Yanjun Zhou</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Zhiwei Zhao</name>
    </author>
    <author>
      <name>Zixi Qi</name>
    </author>
    <author>
      <name>Adolfo Victoria</name>
    </author>
    <author>
      <name>Aya Ibrahim</name>
    </author>
    <author>
      <name>Bram Wasti</name>
    </author>
    <author>
      <name>Changkyu Kim</name>
    </author>
    <author>
      <name>Daniel Haziza</name>
    </author>
    <author>
      <name>Fei Sun</name>
    </author>
    <author>
      <name>Giancarlo Delfin</name>
    </author>
    <author>
      <name>Emily Guo</name>
    </author>
    <author>
      <name>Jialin Ouyang</name>
    </author>
    <author>
      <name>Jaewon Lee</name>
    </author>
    <author>
      <name>Jianyu Huang</name>
    </author>
    <author>
      <name>Jeremy Reizenstein</name>
    </author>
    <author>
      <name>Lu Fang</name>
    </author>
    <author>
      <name>Quinn Zhu</name>
    </author>
    <author>
      <name>Ria Verma</name>
    </author>
    <author>
      <name>Vlad Mihailescu</name>
    </author>
    <author>
      <name>Xingwen Guo</name>
    </author>
    <author>
      <name>Yan Cui</name>
    </author>
    <author>
      <name>Ye Hu</name>
    </author>
    <author>
      <name>Yejin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.08192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08186v1</id>
    <updated>2025-08-11T17:06:55Z</updated>
    <published>2025-08-11T17:06:55Z</published>
    <title>KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold
  Representation Learning</title>
    <summary>  Semantic segmentation of structural defects in civil infrastructure remains
challenging due to variable defect appearances, harsh imaging conditions, and
significant class imbalance. Current deep learning methods, despite their
effectiveness, typically require millions of parameters, rendering them
impractical for real-time inspection systems. We introduce KARMA
(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient
semantic segmentation framework that models complex defect patterns through
compositions of one-dimensional functions rather than conventional
convolutions. KARMA features three technical innovations: (1) a
parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging
low-rank factorization for KAN-based feature transformation; (2) an optimized
feature pyramid structure with separable convolutions for multi-scale defect
analysis; and (3) a static-dynamic prototype mechanism that enhances feature
representation for imbalanced classes. Extensive experiments on benchmark
infrastructure inspection datasets demonstrate that KARMA achieves competitive
or superior mean IoU performance compared to state-of-the-art approaches, while
using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).
Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for
real-time deployment, enabling practical automated infrastructure inspection
systems without compromising accuracy. The source code can be accessed at the
following URL: https://github.com/faeyelab/karma.
</summary>
    <author>
      <name>Md Meftahul Ferdaus</name>
    </author>
    <author>
      <name>Mahdi Abdelguerfi</name>
    </author>
    <author>
      <name>Elias Ioup</name>
    </author>
    <author>
      <name>Steven Sloan</name>
    </author>
    <author>
      <name>Kendall N. Niles</name>
    </author>
    <author>
      <name>Ken Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.08186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08178v2</id>
    <updated>2025-08-12T16:25:31Z</updated>
    <published>2025-08-11T16:59:14Z</published>
    <title>3D Human Mesh Estimation from Single View RGBD</title>
    <summary>  Despite significant progress in 3D human mesh estimation from RGB images;
RGBD cameras, offering additional depth data, remain underutilized. In this
paper, we present a method for accurate 3D human mesh estimation from a single
RGBD view, leveraging the affordability and widespread adoption of RGBD cameras
for real-world applications. A fully supervised approach for this problem,
requires a dataset with RGBD image and 3D mesh label pairs. However, collecting
such a dataset is costly and challenging, hence, existing datasets are small,
and limited in pose and shape diversity. To overcome this data scarcity, we
leverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D
meshes from the body models found in MoCap datasets, and create partial,
single-view versions of them by projection to a virtual camera. This simulates
the depth data provided by an RGBD camera from a single viewpoint. Then, we
train a masked autoencoder to complete the partial, single-view mesh. During
inference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',
matches the depth values coming from the sensor to vertices of a template human
mesh, which creates a partial, single-view mesh. We effectively recover parts
of the 3D human body mesh model that are not visible, resulting in a full body
mesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL
and CAPE datasets, respectively; outperforming existing methods that use
full-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE
dataset, outperforming a recently published RGB based method by 18.4 mm,
highlighting the usefulness of depth data. Code will be released.
</summary>
    <author>
      <name>Ozhan Suat</name>
    </author>
    <author>
      <name>Bedirhan Uguz</name>
    </author>
    <author>
      <name>Batuhan Karagoz</name>
    </author>
    <author>
      <name>Muhammed Can Keles</name>
    </author>
    <author>
      <name>Emre Akbas</name>
    </author>
    <link href="http://arxiv.org/abs/2508.08178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08165v1</id>
    <updated>2025-08-11T16:41:04Z</updated>
    <published>2025-08-11T16:41:04Z</published>
    <title>Integrating Task-Specific and Universal Adapters for Pre-Trained
  Model-based Class-Incremental Learning</title>
    <summary>  Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Existing pre-trained model-based CIL
methods often freeze the pre-trained network and adapt to incremental tasks
using additional lightweight modules such as adapters. However, incorrect
module selection during inference hurts performance, and task-specific modules
often overlook shared general knowledge, leading to errors on distinguishing
between similar classes across tasks. To address the aforementioned challenges,
we propose integrating Task-Specific and Universal Adapters (TUNA) in this
paper. Specifically, we train task-specific adapters to capture the most
crucial features relevant to their respective tasks and introduce an
entropy-based selection mechanism to choose the most suitable adapter.
Furthermore, we leverage an adapter fusion strategy to construct a universal
adapter, which encodes the most discriminative features shared across tasks. We
combine task-specific and universal adapter predictions to harness both
specialized and general knowledge during inference. Extensive experiments on
various benchmark datasets demonstrate the state-of-the-art performance of our
approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA
</summary>
    <author>
      <name>Yan Wang</name>
    </author>
    <author>
      <name>Da-Wei Zhou</name>
    </author>
    <author>
      <name>Han-Jia Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2025. Code is available at:
  https://github.com/LAMDA-CL/ICCV2025-TUNA</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.08165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.08140v1</id>
    <updated>2025-08-11T16:13:21Z</updated>
    <published>2025-08-11T16:13:21Z</published>
    <title>Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced
  Submodular Perspective</title>
    <summary>  Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.
</summary>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Zaifu Zhan</name>
    </author>
    <author>
      <name>Qixin Zhang</name>
    </author>
    <author>
      <name>Mingquan Lin</name>
    </author>
    <author>
      <name>Meijia Song</name>
    </author>
    <author>
      <name>Rui Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2508.08140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.08140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
