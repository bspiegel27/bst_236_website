<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-16T00:51:59Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-15T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">120809</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.10452v1</id>
    <updated>2025-09-12T17:59:09Z</updated>
    <published>2025-09-12T17:59:09Z</published>
    <title>WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained
  Speech Recognition Transformers</title>
    <summary>  Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.
</summary>
    <author>
      <name>Akshat Pandey</name>
    </author>
    <author>
      <name>Karun Kumar</name>
    </author>
    <author>
      <name>Raphael Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.10452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10420v1</id>
    <updated>2025-09-12T17:18:32Z</updated>
    <published>2025-09-12T17:18:32Z</published>
    <title>Testing the nature of compact objects in the lower mass gap using
  gravitational wave observations</title>
    <summary>  As the compact binary catalog continues to grow rapidly, developing and
refining tests to probe the nature of compact objects is essential for a
comprehensive understanding of both the observed data and the underlying
astrophysics of the binary population. We investigate the effectiveness of
spin-induced multipole moments (SIQM) and tidal deformability measurements in
distinguishing lower mass-gap black hole (BH) binaries from non-BH binaries
with different mass and spin configurations. We perform model-agnostic tests on
binary BH (BBH) simulations using full Bayesian inference, evaluating the
independent and joint measurability of SIQM and tidal parameters across the
parameter space. We extend the analysis to simulations of self-interacting
spinning boson stars, using synthetic signals that exhibit (a) both SIQM and
tidal effects and (b) each effect individually. For case (a), recovery is
performed using (i) a BBH model, (ii) a model incorporating both SIQM and tidal
effects, and (iii) models including either SIQM or tidal effects. For case (b),
we employ (i) a BBH model and (ii) models incorporating either SIQM or tidal
effects, consistent with the injection. Simulations employ TaylorF2 waveform
model and consider binaries in the low mass gap with varying spin magnitudes.
We find that employing an incorrect model to analyze the signal can lead to
biases in parameter inference. Notably, when analyzing a simulated binary boson
star-like signal with component masses $\rm{(4, 4) \, M_{\odot}}$ using a BBH
model, the system is incorrectly identified as having masses $\rm{(8, 2) \,
M_{\odot}}$. In contrast, using the correct recovery model that includes both
SIQM and tidal deformability effects successfully recovers the true masses,
highlighting the significance of waveform model accuracy in performing reliable
distinguishability tests for compact objects in the low-mass gap.
</summary>
    <author>
      <name>N. V. Krishnendu</name>
    </author>
    <author>
      <name>Frank Ohme</name>
    </author>
    <author>
      <name>K. G. Arun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8 Figures and 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.10420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10416v1</id>
    <updated>2025-09-12T17:13:18Z</updated>
    <published>2025-09-12T17:13:18Z</published>
    <title>TASC: Task-Aware Shared Control for Teleoperated Manipulation</title>
    <summary>  We present TASC, a Task-Aware Shared Control framework for teleoperated
manipulation that infers task-level user intent and provides assistance
throughout the task. To support everyday tasks without predefined knowledge,
TASC constructs an open-vocabulary interaction graph from visual input to
represent functional object relationships, and infers user intent accordingly.
A shared control policy then provides rotation assistance during both grasping
and object interaction, guided by spatial constraints predicted by a
vision-language model. Our method addresses two key challenges in
general-purpose, long-horizon shared control: (1) understanding and inferring
task-level user intent, and (2) generalizing assistance across diverse objects
and tasks. Experiments in both simulation and the real world demonstrate that
TASC improves task efficiency and reduces user input effort compared to prior
methods. To the best of our knowledge, this is the first shared control
framework that supports everyday manipulation tasks with zero-shot
generalization. The code that supports our experiments is publicly available at
https://github.com/fitz0401/tasc.
</summary>
    <author>
      <name>Ze Fu</name>
    </author>
    <author>
      <name>Pinhao Song</name>
    </author>
    <author>
      <name>Yutong Hu</name>
    </author>
    <author>
      <name>Renaud Detry</name>
    </author>
    <link href="http://arxiv.org/abs/2509.10416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10406v1</id>
    <updated>2025-09-12T16:58:17Z</updated>
    <published>2025-09-12T16:58:17Z</published>
    <title>Multipole Semantic Attention: A Fast Approximation of Softmax Attention
  for Pretraining</title>
    <summary>  We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.
</summary>
    <author>
      <name>Rupert Mitchell</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <link href="http://arxiv.org/abs/2509.10406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W25, 68T50 (primary) 68W40, 68T07 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10405v1</id>
    <updated>2025-09-12T16:54:56Z</updated>
    <published>2025-09-12T16:54:56Z</published>
    <title>Self-supervised Learning Of Visual Pose Estimation Without Pose Labels
  By Classifying LED States</title>
    <summary>  We introduce a model for monocular RGB relative pose estimation of a ground
robot that trains from scratch without pose labels nor prior knowledge about
the robot's shape or appearance. At training time, we assume: (i) a robot
fitted with multiple LEDs, whose states are independent and known at each
frame; (ii) knowledge of the approximate viewing direction of each LED; and
(iii) availability of a calibration image with a known target distance, to
address the ambiguity of monocular depth estimation. Training data is collected
by a pair of robots moving randomly without needing external infrastructure or
human supervision. Our model trains on the task of predicting from an image the
state of each LED on the robot. In doing so, it learns to predict the position
of the robot in the image, its distance, and its relative bearing. At inference
time, the state of the LEDs is unknown, can be arbitrary, and does not affect
the pose estimation performance. Quantitative experiments indicate that our
approach: is competitive with SoA approaches that require supervision from pose
labels or a CAD model of the robot; generalizes to different domains; and
handles multi-robot pose estimation.
</summary>
    <author>
      <name>Nicholas Carlotti</name>
    </author>
    <author>
      <name>Mirko Nava</name>
    </author>
    <author>
      <name>Alessandro Giusti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at CoRL 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.10405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10401v1</id>
    <updated>2025-09-12T16:51:15Z</updated>
    <published>2025-09-12T16:51:15Z</published>
    <title>Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure
  Attribution in Multi-Agent Systems</title>
    <summary>  Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&amp;When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.
</summary>
    <author>
      <name>Alva West</name>
    </author>
    <author>
      <name>Yixuan Weng</name>
    </author>
    <author>
      <name>Minjun Zhu</name>
    </author>
    <author>
      <name>Zhen Lin</name>
    </author>
    <author>
      <name>Yue Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2509.10401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10372v1</id>
    <updated>2025-09-12T16:05:27Z</updated>
    <published>2025-09-12T16:05:27Z</published>
    <title>MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging
  Bit-Slice-enabled Sparsity and Repetitiveness</title>
    <summary>  Large language models (LLMs) face significant inference latency due to
inefficiencies in GEMM operations, weight access, and KV cache access,
especially in real-time scenarios. This highlights the need for a versatile
compute-memory efficient accelerator. Unfortunately, existing Transformer
accelerators struggle to address both aspects simultaneously, as they focus on
value-level processing, missing fine-grained opportunities to optimize
computation and memory collaboratively. This paper introduces MCBP, a
bit-grained compute-memory efficient algorithm-hardware co-design that
leverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM
inference. MCBP features three key innovations: 1) BS-repetitiveness-enabled
computation reduction (BRCR), which eliminates redundant GEMM computations via
leveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state
coding (BSTC), which reduces weight access via exploiting significant sparsity
in high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),
which reduces KV cache access by leveraging early-termination-based bit-grained
prediction. These techniques, supported by custom accelerator designs,
effectively alleviate the burden in GEMM, weight access, and KV cache access.
Extensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up
and 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA
Transformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than
Spatten, FACT and SOFA, respectively.
</summary>
    <author>
      <name>Huizheng Wang</name>
    </author>
    <author>
      <name>Zichuan Wang</name>
    </author>
    <author>
      <name>Zhiheng Yue</name>
    </author>
    <author>
      <name>Yousheng Long</name>
    </author>
    <author>
      <name>Taiquan Wei</name>
    </author>
    <author>
      <name>Jianxun Yang</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Shaojun Wei</name>
    </author>
    <author>
      <name>Yang Hu</name>
    </author>
    <author>
      <name>Shouyi Yin</name>
    </author>
    <link href="http://arxiv.org/abs/2509.10372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10370v1</id>
    <updated>2025-09-12T16:03:48Z</updated>
    <published>2025-09-12T16:03:48Z</published>
    <title>The Language of Approval: Identifying the Drivers of Positive Feedback
  Online</title>
    <summary>  Positive feedback via likes and awards is central to online governance, yet
which attributes of users' posts elicit rewards -- and how these vary across
authors and communities -- remains unclear. To examine this, we combine
quasi-experimental causal inference with predictive modeling on 11M posts from
100 subreddits. We identify linguistic patterns and stylistic attributes
causally linked to rewards, controlling for author reputation, timing, and
community context. For example, overtly complicated language, tentative style,
and toxicity reduce rewards. We use our set of curated features to train models
that can detect highly-upvoted posts with high AUC. Our audit of community
guidelines highlights a ``policy-practice gap'' -- most rules focus primarily
on civility and formatting requirements, with little emphasis on the attributes
identified to drive positive feedback. These results inform the design of
community guidelines, support interfaces that teach users how to craft
desirable contributions, and moderation workflows that emphasize positive
reinforcement over purely punitive enforcement.
</summary>
    <author>
      <name>Agam Goyal</name>
    </author>
    <author>
      <name>Charlotte Lambert</name>
    </author>
    <author>
      <name>Eshwar Chandrasekharan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint: 21 pages, 7 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.10370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10356v1</id>
    <updated>2025-09-12T15:45:10Z</updated>
    <published>2025-09-12T15:45:10Z</published>
    <title>Constrained Variational Inference via Safe Particle Flow</title>
    <summary>  We propose a control barrier function (CBF) formulation for enforcing
equality and inequality constraints in variational inference. The key idea is
to define a barrier functional on the space of probability density functions
that encode the desired constraints imposed on the variational density. By
leveraging the Liouville equation, we establish a connection between the time
derivative of the variational density and the particle drift, which enables the
systematic construction of corresponding CBFs associated to the particle drift.
Enforcing these CBFs gives rise to the safe particle flow and ensures that the
variational density satisfies the original constraints imposed by the barrier
functional. This formulation provides a principled and computationally
tractable solution to constrained variational inference, with theoretical
guarantees of constraint satisfaction. The effectiveness of the method is
demonstrated through numerical simulations.
</summary>
    <author>
      <name>Yinzhuang Yi</name>
    </author>
    <author>
      <name>Jorge Cortés</name>
    </author>
    <author>
      <name>Nikolay Atanasov</name>
    </author>
    <link href="http://arxiv.org/abs/2509.10356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.10341v1</id>
    <updated>2025-09-12T15:24:41Z</updated>
    <published>2025-09-12T15:24:41Z</published>
    <title>GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT</title>
    <summary>  Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.
</summary>
    <author>
      <name>Botond Fazekas</name>
    </author>
    <author>
      <name>Thomas Pinetz</name>
    </author>
    <author>
      <name>Guilherme Aresta</name>
    </author>
    <author>
      <name>Taha Emre</name>
    </author>
    <author>
      <name>Hrvoje Bogunovic</name>
    </author>
    <link href="http://arxiv.org/abs/2509.10341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.10341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
