<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-25T01:01:56Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-24T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">112934</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.17011v1</id>
    <updated>2025-05-22T17:59:02Z</updated>
    <published>2025-05-22T17:59:02Z</published>
    <title>Learning Adaptive and Temporally Causal Video Tokenization in a 1D
  Latent Space</title>
    <summary>  We propose AdapTok, an adaptive temporal causal video tokenizer that can
flexibly allocate tokens for different frames based on video content. AdapTok
is equipped with a block-wise masking strategy that randomly drops tail tokens
of each block during training, and a block causal scorer to predict the
reconstruction quality of video frames using different numbers of tokens.
During inference, an adaptive token allocation strategy based on integer linear
programming is further proposed to adjust token usage given predicted scores.
Such design allows for sample-wise, content-aware, and temporally dynamic token
allocation under a controllable overall budget. Extensive experiments for video
reconstruction and generation on UCF-101 and Kinetics-600 demonstrate the
effectiveness of our approach. Without additional image data, AdapTok
consistently improves reconstruction quality and generation performance under
different token budgets, allowing for more scalable and token-efficient
generative video modeling.
</summary>
    <author>
      <name>Yan Li</name>
    </author>
    <author>
      <name>Changyao Tian</name>
    </author>
    <author>
      <name>Renqiu Xia</name>
    </author>
    <author>
      <name>Ning Liao</name>
    </author>
    <author>
      <name>Weiwei Guo</name>
    </author>
    <author>
      <name>Junchi Yan</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <author>
      <name>Jifeng Dai</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Xue Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/VisionXLab/AdapTok</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.17011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.17011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.17004v1</id>
    <updated>2025-05-22T17:58:12Z</updated>
    <published>2025-05-22T17:58:12Z</published>
    <title>Guided Diffusion Sampling on Function Spaces with Applications to PDEs</title>
    <summary>  We propose a general framework for conditional sampling in PDE-based inverse
problems, targeting the recovery of whole solutions from extremely sparse or
noisy measurements. This is accomplished by a function-space diffusion model
and plug-and-play guidance for conditioning. Our method first trains an
unconditional discretization-agnostic denoising model using neural operator
architectures. At inference, we refine the samples to satisfy sparse
observation data via a gradient-based guidance mechanism. Through rigorous
mathematical analysis, we extend Tweedie's formula to infinite-dimensional
Hilbert spaces, providing the theoretical foundation for our posterior sampling
approach. Our method (FunDPS) accurately captures posterior distributions in
function spaces under minimal supervision and severe data scarcity. Across five
PDE tasks with only 3% observation, our method achieves an average 32% accuracy
improvement over state-of-the-art fixed-resolution diffusion baselines while
reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning
ensures strong cross-resolution generalizability. To the best of our knowledge,
this is the first diffusion-based framework to operate independently of
discretization, offering a practical and flexible solution for forward and
inverse problems in the context of PDEs. Code is available at
https://github.com/neuraloperator/FunDPS
</summary>
    <author>
      <name>Jiachen Yao</name>
    </author>
    <author>
      <name>Abbas Mammadov</name>
    </author>
    <author>
      <name>Julius Berner</name>
    </author>
    <author>
      <name>Gavin Kerrigan</name>
    </author>
    <author>
      <name>Jong Chul Ye</name>
    </author>
    <author>
      <name>Kamyar Azizzadenesheli</name>
    </author>
    <author>
      <name>Anima Anandkumar</name>
    </author>
    <link href="http://arxiv.org/abs/2505.17004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.17004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16990v1</id>
    <updated>2025-05-22T17:55:04Z</updated>
    <published>2025-05-22T17:55:04Z</published>
    <title>Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel
  Decoding</title>
    <summary>  In this work, we propose Dimple, the first Discrete Diffusion Multimodal
Large Language Model (DMLLM). We observe that training with a purely discrete
diffusion approach leads to significant training instability, suboptimal
performance, and severe length bias issues. To address these challenges, we
design a novel training paradigm that combines an initial autoregressive phase
with a subsequent diffusion phase. This approach yields the Dimple-7B model,
trained on the same dataset and using a similar training pipeline as
LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,
demonstrating that DMLLM can achieve performance comparable to that of
autoregressive models. To improve inference efficiency, we propose a decoding
strategy termed confident decoding, which dynamically adjusts the number of
tokens generated at each step, significantly reducing the number of generation
iterations. In autoregressive models, the number of forward iterations during
generation equals the response length. With confident decoding, however, the
number of iterations needed by Dimple is even only $\frac{\text{response
length}}{3}$. We also re-implement the prefilling technique in autoregressive
models and demonstrate that it does not significantly impact performance on
most benchmark evaluations, while offering a speedup of 1.5x to 7x.
Additionally, we explore Dimple's capability to precisely control its response
using structure priors. These priors enable structured responses in a manner
distinct from instruction-based or chain-of-thought prompting, and allow
fine-grained control over response format and length, which is difficult to
achieve in autoregressive models. Overall, this work validates the feasibility
and advantages of DMLLM and enhances its inference efficiency and
controllability. Code and models are available at
https://github.com/yu-rp/Dimple.
</summary>
    <author>
      <name>Runpeng Yu</name>
    </author>
    <author>
      <name>Xinyin Ma</name>
    </author>
    <author>
      <name>Xinchao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.16990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16982v1</id>
    <updated>2025-05-22T17:52:59Z</updated>
    <published>2025-05-22T17:52:59Z</published>
    <title>Beyond Correlation: Towards Causal Large Language Model Agents in
  Biomedicine</title>
    <summary>  Large Language Models (LLMs) show promise in biomedicine but lack true causal
understanding, relying instead on correlations. This paper envisions causal LLM
agents that integrate multimodal data (text, images, genomics, etc.) and
perform intervention-based reasoning to infer cause-and-effect. Addressing this
requires overcoming key challenges: designing safe, controllable agentic
frameworks; developing rigorous benchmarks for causal evaluation; integrating
heterogeneous data sources; and synergistically combining LLMs with structured
knowledge (KGs) and formal causal inference tools. Such agents could unlock
transformative opportunities, including accelerating drug discovery through
automated hypothesis generation and simulation, enabling personalized medicine
through patient-specific causal models. This research agenda aims to foster
interdisciplinary efforts, bridging causal concepts and foundation models to
develop reliable AI partners for biomedical progress.
</summary>
    <author>
      <name>Adib Bazgir</name>
    </author>
    <author>
      <name>Amir Habibdoust Lafmajani</name>
    </author>
    <author>
      <name>Yuwen Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.16982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16978v1</id>
    <updated>2025-05-22T17:52:31Z</updated>
    <published>2025-05-22T17:52:31Z</published>
    <title>HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar
  Generation</title>
    <summary>  Grammar plays a critical role in natural language processing and text/code
generation by enabling the definition of syntax, the creation of parsers, and
guiding structured outputs. Although large language models (LLMs) demonstrate
impressive capabilities across domains, their ability to infer and generate
grammars has not yet been thoroughly explored. In this paper, we aim to study
and improve the ability of LLMs for few-shot grammar generation, where grammars
are inferred from sets of a small number of positive and negative examples and
generated in Backus-Naur Form. To explore this, we introduced a novel dataset
comprising 540 structured grammar generation challenges, devised 6 metrics, and
evaluated 8 various LLMs against it. Our findings reveal that existing LLMs
perform sub-optimally in grammar generation. To address this, we propose an
LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar
generation. HyGenar achieves substantial improvements in both the syntactic and
semantic correctness of generated grammars across LLMs.
</summary>
    <author>
      <name>Weizhi Tang</name>
    </author>
    <author>
      <name>Yixuan Li</name>
    </author>
    <author>
      <name>Chris Sypherd</name>
    </author>
    <author>
      <name>Elizabeth Polgreen</name>
    </author>
    <author>
      <name>Vaishak Belle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2025 Findings. Code available at
  https://github.com/RutaTang/HyGenar</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.16978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16974v1</id>
    <updated>2025-05-22T17:51:48Z</updated>
    <published>2025-05-22T17:51:48Z</published>
    <title>OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step
  Visual Reasoning</title>
    <summary>  Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its
capacity to generalize segmentation beyond predefined categories. However,
existing methods typically predict segmentation masks with simple forward
inference, lacking explicit reasoning and interpretability. This makes it
challenging for OVS model to distinguish similar categories in open-world
settings due to the lack of contextual understanding and discriminative visual
cues. To address this limitation, we propose a step-by-step visual reasoning
framework for open-vocabulary segmentation, named OpenSeg-R. The proposed
OpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical
visual reasoning before segmentation. Specifically, we generate both generic
and image-specific reasoning for each image, forming structured triplets that
explain the visual reason for objects in a coarse-to-fine manner. Based on
these reasoning steps, we can compose detailed description prompts, and feed
them to the segmentor to produce more accurate segmentation masks. To the best
of our knowledge, OpenSeg-R is the first framework to introduce explicit
step-by-step visual reasoning into OVS. Experimental results demonstrate that
OpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary
semantic segmentation across five benchmark datasets. Moreover, it achieves
consistent gains across all metrics on open-vocabulary panoptic segmentation.
Qualitative results further highlight the effectiveness of our reasoning-guided
framework in improving both segmentation precision and interpretability. Our
code is publicly available at https://github.com/Hanzy1996/OpenSeg-R.
</summary>
    <author>
      <name>Zongyan Han</name>
    </author>
    <author>
      <name>Jiale Cao</name>
    </author>
    <author>
      <name>Shuo Chen</name>
    </author>
    <author>
      <name>Tong Wang</name>
    </author>
    <author>
      <name>Jorma Laaksonen</name>
    </author>
    <author>
      <name>Rao Muhammad Anwer</name>
    </author>
    <link href="http://arxiv.org/abs/2505.16974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16971v1</id>
    <updated>2025-05-22T17:50:52Z</updated>
    <published>2025-05-22T17:50:52Z</published>
    <title>UniPhy: Learning a Unified Constitutive Model for Inverse Physics
  Simulation</title>
    <summary>  We propose UniPhy, a common latent-conditioned neural constitutive model that
can encode the physical properties of diverse materials. At inference UniPhy
allows `inverse simulation' i.e. inferring material properties by optimizing
the scene-specific latent to match the available observations via
differentiable simulation. In contrast to existing methods that treat such
inference as system identification, UniPhy does not rely on user-specified
material type information. Compared to prior neural constitutive modeling
approaches which learn instance specific networks, the shared training across
materials improves both, robustness and accuracy of the estimates. We train
UniPhy using simulated trajectories across diverse geometries and materials --
elastic, plasticine, sand, and fluids (Newtonian &amp; non-Newtonian). At
inference, given an object with unknown material properties, UniPhy can infer
the material properties via latent optimization to match the motion
observations, and can then allow re-simulating the object under diverse
scenarios. We compare UniPhy against prior inverse simulation methods, and show
that the inference from UniPhy enables more accurate replay and re-simulation
under novel conditions.
</summary>
    <author>
      <name>Himangi Mittal</name>
    </author>
    <author>
      <name>Peiye Zhuang</name>
    </author>
    <author>
      <name>Hsin-Ying Lee</name>
    </author>
    <author>
      <name>Shubham Tulsiani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.16971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16943v1</id>
    <updated>2025-05-22T17:30:58Z</updated>
    <published>2025-05-22T17:30:58Z</published>
    <title>Sliding Friction of Hard Sliders on Rubber: Theory and Experiment</title>
    <summary>  We present a study of sliding friction for rigid triangular steel sliders on
soft rubber substrates under both lubricated and dry conditions. For rubber
surfaces lubricated with a thin film of silicone oil, the measured sliding
friction at room temperature agrees well with theoretical predictions obtained
from a viscoelastic model originally developed for rolling friction. On the
lubricated surface, the sliding friction is primarily due to bulk viscoelastic
energy dissipation in the rubber. The model, which includes strain-dependent
softening of the rubber modulus, accurately predicts the experimental friction
curves. At lower temperatures ($T = -20^\circ {\rm C}$ and $-40^\circ {\rm
C}$), the measured friction exceeds the theoretical prediction. We attribute
this increase to penetration of the lubricant film by surface asperities,
leading to a larger adhesive contribution. For dry surfaces, the adhesive
contribution becomes dominant. By subtracting the viscoelastic component
inferred from the lubricated case, we estimate the interfacial frictional shear
stress. This shear stress increases approximately linearly with the logarithm
of the sliding speed, consistent with stress-augmented thermal activation
mechanisms.
</summary>
    <author>
      <name>R. Xu</name>
    </author>
    <author>
      <name>B. N. J Persson</name>
    </author>
    <link href="http://arxiv.org/abs/2505.16943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16942v1</id>
    <updated>2025-05-22T17:30:38Z</updated>
    <published>2025-05-22T17:30:38Z</published>
    <title>Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical
  Flow Estimation</title>
    <summary>  Recent optical flow estimation methods often employ local cost sampling from
a dense all-pairs correlation volume. This results in quadratic computational
and memory complexity in the number of pixels. Although an alternative
memory-efficient implementation with on-demand cost computation exists, this is
slower in practice and therefore prior methods typically process images at
reduced resolutions, missing fine-grained details.
  To address this, we propose a more efficient implementation of the all-pairs
correlation volume sampling, still matching the exact mathematical operator as
defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while
maintaining low memory usage, and performs on par with the default
implementation with up to 95% lower memory usage. As cost sampling makes up a
significant portion of the overall runtime, this can translate to up to 50%
savings for the total end-to-end model inference in memory-constrained
environments. Our evaluation of existing methods includes an 8K
ultra-high-resolution dataset and an additional inference-time modification of
the recent SEA-RAFT method. With this, we achieve state-of-the-art results at
high resolutions both in accuracy and efficiency.
</summary>
    <author>
      <name>Karlis Martins Briedis</name>
    </author>
    <author>
      <name>Markus Gross</name>
    </author>
    <author>
      <name>Christopher Schroers</name>
    </author>
    <link href="http://arxiv.org/abs/2505.16942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16936v1</id>
    <updated>2025-05-22T17:26:23Z</updated>
    <published>2025-05-22T17:26:23Z</published>
    <title>SPAR: Self-supervised Placement-Aware Representation Learning for
  Multi-Node IoT Systems</title>
    <summary>  This work develops the underpinnings of self-supervised placement-aware
representation learning given spatially-distributed (multi-view and multimodal)
sensor observations, motivated by the need to represent external environmental
state in multi-sensor IoT systems in a manner that correctly distills spatial
phenomena from the distributed multi-vantage observations. The objective of
sensing in IoT systems is, in general, to collectively represent an externally
observed environment given multiple vantage points from which sensory
observations occur. Pretraining of models that help interpret sensor data must
therefore encode the relation between signals observed by sensors and the
observers' vantage points in order to attain a representation that encodes the
observed spatial phenomena in a manner informed by the specific placement of
the measuring instruments, while allowing arbitrary placement. The work
significantly advances self-supervised model pretraining from IoT signals
beyond current solutions that often overlook the distinctive spatial nature of
IoT data. Our framework explicitly learns the dependencies between measurements
and geometric observer layouts and structural characteristics, guided by a core
design principle: the duality between signals and observer positions. We
further provide theoretical analyses from the perspectives of information
theory and occlusion-invariant representation learning to offer insight into
the rationale behind our design. Experiments on three real-world
datasets--covering vehicle monitoring, human activity recognition, and
earthquake localization--demonstrate the superior generalizability and
robustness of our method across diverse modalities, sensor placements,
application-level inference tasks, and spatial scales.
</summary>
    <author>
      <name>Yizhuo Chen</name>
    </author>
    <author>
      <name>Tianchen Wang</name>
    </author>
    <author>
      <name>You Lyu</name>
    </author>
    <author>
      <name>Yanlan Hu</name>
    </author>
    <author>
      <name>Jinyang Li</name>
    </author>
    <author>
      <name>Tomoyoshi Kimura</name>
    </author>
    <author>
      <name>Hongjue Zhao</name>
    </author>
    <author>
      <name>Yigong Hu</name>
    </author>
    <author>
      <name>Denizhan Kara</name>
    </author>
    <author>
      <name>Tarek Abdelzaher</name>
    </author>
    <link href="http://arxiv.org/abs/2505.16936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
