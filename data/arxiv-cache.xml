<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-16T01:01:17Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-15T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">116917</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.10543v1</id>
    <updated>2025-07-14T17:59:08Z</updated>
    <published>2025-07-14T17:59:08Z</published>
    <title>MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation</title>
    <summary>  In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the MeanFlow Identity, our policy avoids any
additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://mp1-2254.github.io/.
</summary>
    <author>
      <name>Juyi Sheng</name>
    </author>
    <author>
      <name>Ziyi Wang</name>
    </author>
    <author>
      <name>Peiming Li</name>
    </author>
    <author>
      <name>Mengyuan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2507.10543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10530v1</id>
    <updated>2025-07-14T17:54:47Z</updated>
    <published>2025-07-14T17:54:47Z</published>
    <title>Accurate generation of chemical reaction transition states by
  conditional flow matching</title>
    <summary>  Transition state (TS) structures define the critical geometries and energy
barriers underlying chemical reactivity, yet their fleeting nature renders them
experimentally elusive and drives the reliance on costly, high-throughput
density functional theory (DFT) calculations. Here, we introduce TS-GEN, a
conditional flow-matching generative model that maps samples from a simple
Gaussian prior directly to transition-state saddle-point geometries in a
single, deterministic pass. By embedding both reactant and product
conformations as conditioning information, TS-GEN learns to transport latent
noise to true TS structures via an optimal-transport path, effectively
replacing the iterative optimization common in nudged-elastic band or
string-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a
root-mean-square deviation of $0.004\ \rm{\mathring{A}}$ (vs. $0.103\
\rm{\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error
of $1.019\ {\rm kcal/mol}$ (vs. $2.864\ {\rm kcal/mol}$), while requiring only
$0.06\ {\rm s}$ GPU time per inference. Over 87% of generated TSs meet
chemical-accuracy criteria ($&lt;1.58\ {\rm kcal/mol}$ error), substantially
outpacing existing methods. TS-GEN also exhibits strong transferability to
out-of-distribution reactions from a larger database. By uniting sub-angstrom
precision, sub-second speed, and broad applicability, TS-GEN will be highly
useful for high-throughput exploration of complex reaction networks, paving the
way to the exploration of novel chemical reaction mechanisms.
</summary>
    <author>
      <name>Ping Tuo</name>
    </author>
    <author>
      <name>Jiale Chen</name>
    </author>
    <author>
      <name>Ju Li</name>
    </author>
    <link href="http://arxiv.org/abs/2507.10530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10527v1</id>
    <updated>2025-07-14T17:50:03Z</updated>
    <published>2025-07-14T17:50:03Z</published>
    <title>Multi-epoch spectro-photometric characterization of the minimoon 2024
  PT$_5$ in the visible and near-infrared</title>
    <summary>  2024 PT$_5$ is a tiny ($D\leq10$ m) near-Earth asteroid (NEA) discovered in
August 2024. 2024 PT$_5$ was gravitationally bound to the Earth-Moon system
from September to November 2024 and classified as a minimoon. Several quick
response observations suggest the lunar ejecta origin of 2024 PT$_5$, while
rotation state and albedo, essential properties to investigate its origin, are
not well constrained. We performed visible to near-infrared multicolor
photometry of 2024 PT$_5$ from data taken using the TriColor CMOS Camera and
Spectrograph (TriCCS) on the Seimei 3.8 m telescope during 2025 January 4-10.
The Seimei/TriCCS observations of 2024 PT$_5$ cover phase angles from 14 deg to
27 deg, and were obtained in the $g$, $r$, $i$, and $z$ bands in the Pan-STARRS
system. In addition, we analyzed $Y$, $J$, $H$, and $K$ photometry taken with
the Multi-Object Spectrograph for Infrared Exploration (MOSFIRE) on the Keck I
10-m telescope taken on 2025 January 16-17. Our lightcurves show brightness
variations over time periods of several tens of minutes. We infer that 2024
PT$_5$ is in a tumbling state and has a lightcurve amplitude of about 0.3 mag.
Visible and near-infrared color indices of 2024 PT$_5$, $g-r=0.567\pm0.044$,
$r-i=0.155\pm0.009$, $r-z=0.147\pm0.066$, $Y-J=0.557\pm0.046$,
$J-H=0.672\pm0.078$, and $H-Ks=0.148\pm0.098$, indicate that 2024 PT$_5$ is an
S-complex asteroid, largely consistent with previous observations. Using the
$H$-$G$ model, we derived an absolute magnitude $H_{V,HG}$ of $27.72\pm0.09$
and a slope parameter $G_V$ of $0.223\pm0.073$ in V-band. A geometric albedo of
2024 PT$_5$ is derived to be $0.26\pm0.07$ from the slope of its photometric
phase curve. This albedo value is typical of the S- and Q-type NEAs. The color
properties of 2024 PT$_5$ derived from our observations match rock samples
taken from the lunar surface, which agrees with previous studies.
</summary>
    <author>
      <name>Jin Beniyama</name>
    </author>
    <author>
      <name>Bryce T. Bolin</name>
    </author>
    <author>
      <name>Alexey V. Sergeyev</name>
    </author>
    <author>
      <name>Marco Delbo</name>
    </author>
    <author>
      <name>Laura-May Abron</name>
    </author>
    <author>
      <name>Matthew Belyakov</name>
    </author>
    <author>
      <name>Tomohiko Sekiguchi</name>
    </author>
    <author>
      <name>Seiko Takagi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Astronomy &amp; Astrophysics (A&amp;A). Abstract
  shortened for arXiv. Any comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.10527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10517v1</id>
    <updated>2025-07-14T17:42:57Z</updated>
    <published>2025-07-14T17:42:57Z</published>
    <title>Closure of superstatistics</title>
    <summary>  Plasmas and other systems with long-range interactions are commonly found in
non-equilibrium steady states that are outside traditional Boltzmann-Gibbs
statistics, but can be described using generalized statistical mechanics
frameworks such as superstatistics, where steady states are treated as
superpositions of canonical ensembles under a temperature distribution. In this
work we solve the problem of inferring the possible steady states of a
composite system $AB$ where subsystem $A$ is described by superstatistics and
$E_{AB} = E_A + E_B$. Our result establishes a closure property of
superstatistics, namely that $A$ is described by superstatistics if and only if
$AB$ and $B$ are also superstatistical with the same temperature distribution.
Some consequences of this result are discussed, such as the impossibility of
local thermal equilibrium (LTE) for additive subsystems in non-canonical steady
states.
</summary>
    <author>
      <name>Sergio Davis</name>
    </author>
    <link href="http://arxiv.org/abs/2507.10517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10511v2</id>
    <updated>2025-07-15T08:59:45Z</updated>
    <published>2025-07-14T17:37:56Z</published>
    <title>Constructing Confidence Intervals for Infinite-Dimensional Functional
  Prameters by Highly Adaptive Lasso</title>
    <summary>  Estimating the conditional mean function is a central task in statistical
learning. In this paper, we consider estimation and inference for a
nonparametric class of real-valued c\`adl\`ag functions with bounded sectional
variation (Gill et al., 1995), using the Highly Adaptive Lasso (HAL) (van der
Laan, 2015; Benkeser and van der Laan, 2016; van der Laan, 2023), a flexible
empirical risk minimizer over linear combinations of tensor products of zero-
or higher-order spline basis functions under an L1 norm constraint. Building on
recent theoretical advances in asymptotic normality and uniform convergence
rates for higher-order spline HAL estimators (van der Laan, 2023), this work
focuses on constructing robust confidence intervals for HAL-based conditional
mean estimators. To address regularization bias, we propose a targeted HAL with
a debiasing step to remove bias for the conditional mean, and also consider a
relaxed HAL estimator to reduce bias. We also introduce both global and local
undersmoothing strategies to adaptively select the working model, reducing bias
relative to variance. Combined with delta-method-based variance estimation, we
construct confidence intervals for conditional means based on HAL. Through
simulations, we evaluate combinations of estimation and model selection
strategies, showing that our methods substantially reduce bias and yield
confidence intervals with coverage rates close to nominal levels across
scenarios. We also provide recommendations for different estimation objectives
and illustrate the generality of our framework by applying it to estimate
conditional average treatment effect (CATE) functions, highlighting how
HAL-based inference extends to other infinite-dimensional, non-pathwise
differentiable parameters.
</summary>
    <author>
      <name>Wenxin Zhang</name>
    </author>
    <author>
      <name>Junming Shi</name>
    </author>
    <author>
      <name>Alan Hubbard</name>
    </author>
    <author>
      <name>Mark van der Laan</name>
    </author>
    <link href="http://arxiv.org/abs/2507.10511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10510v1</id>
    <updated>2025-07-14T17:34:49Z</updated>
    <published>2025-07-14T17:34:49Z</published>
    <title>Chat with AI: The Surprising Turn of Real-time Video Communication from
  Human to AI</title>
    <summary>  AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.
</summary>
    <author>
      <name>Jiangkai Wu</name>
    </author>
    <author>
      <name>Zhiyuan Ren</name>
    </author>
    <author>
      <name>Liming Liu</name>
    </author>
    <author>
      <name>Xinggong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2507.10510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10494v1</id>
    <updated>2025-07-14T17:18:07Z</updated>
    <published>2025-07-14T17:18:07Z</published>
    <title>Split Happens: Combating Advanced Threats with Split Learning and
  Function Secret Sharing</title>
    <summary>  Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.
</summary>
    <author>
      <name>Tanveer Khan</name>
    </author>
    <author>
      <name>Mindaugas Budzys</name>
    </author>
    <author>
      <name>Antonis Michalas</name>
    </author>
    <link href="http://arxiv.org/abs/2507.10494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10485v1</id>
    <updated>2025-07-14T17:04:05Z</updated>
    <published>2025-07-14T17:04:05Z</published>
    <title>Overcoming catastrophic forgetting in neural networks</title>
    <summary>  Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.
</summary>
    <author>
      <name>Brandon Shuen Yi Loke</name>
    </author>
    <author>
      <name>Filippo Quadri</name>
    </author>
    <author>
      <name>Gabriel Vivanco</name>
    </author>
    <author>
      <name>Maximilian Casagrande</name>
    </author>
    <author>
      <name>Sa√∫l Fenollosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, EE-411 Fundamentals of inference and learning
  course project</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.10485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10473v1</id>
    <updated>2025-07-14T16:54:57Z</updated>
    <published>2025-07-14T16:54:57Z</published>
    <title>GT-Loc: Unifying When and Where in Images Through a Joint Embedding
  Space</title>
    <summary>  Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.
</summary>
    <author>
      <name>David G. Shatwell</name>
    </author>
    <author>
      <name>Ishan Rajendrakumar Dave</name>
    </author>
    <author>
      <name>Sirnam Swetha</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICCV2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.10473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10465v1</id>
    <updated>2025-07-14T16:45:00Z</updated>
    <published>2025-07-14T16:45:00Z</published>
    <title>Flexible Modeling of Multivariate Skewed and Heavy-Tailed Data via a
  Non-Central Skew t Distribution: Application to Tumor Shape Data</title>
    <summary>  We propose a flexible formulation of the multivariate non-central skew t
(NCST) distribution, defined by scaling skew-normal random vectors with
independent chi-squared variables. This construction extends the classical
multivariate t family by allowing both asymmetry and non-centrality, which
provides an alternative to existing skew t models that often rely on
restrictive assumptions for tractability. We derive key theoretical properties
of the NCST distribution, which includes its moment structure, affine
transformation behavior, and the distribution of quadratic forms. Due to the
lack of a closed-form density, we implement a Monte Carlo likelihood
approximation to enable maximum likelihood estimation and evaluate its
performance through simulation studies. To demonstrate practical utility, we
apply the NCST model to breast cancer diagnostic data, modeling multiple
features of tumor shape. The NCST model achieves a superior fit based on
information criteria and visual diagnostics, particularly in the presence of
skewness and heavy tails compared to standard alternatives, including the
multivariate normal, skew normal, and Azzalini's skew $t$ distribution. Our
findings suggest that the NCST distribution offers a useful and interpretable
choice for modeling complex multivariate data, which highlights promising
directions for future development in likelihood inference, Bayesian
computation, and applications involving asymmetry and non-Gaussian dependence.
</summary>
    <author>
      <name>Abeer M. Hasan</name>
    </author>
    <author>
      <name>Ying-Ju Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.10465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
