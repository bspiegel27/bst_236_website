<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-18T00:58:13Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-17T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">115179</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.13759v1</id>
    <updated>2025-06-16T17:59:08Z</updated>
    <published>2025-06-16T17:59:08Z</published>
    <title>Discrete Diffusion in Large Language and Multimodal Models: A Survey</title>
    <summary>  In this work, we provide a systematic survey of Discrete Diffusion Language
Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).
Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,
parallel decoding paradigm using full attention and a denoising-based
generation strategy. This paradigm naturally enables parallel generation,
fine-grained output controllability, and dynamic, response-aware perception.
These capabilities are previously difficult to achieve with AR models.
Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as
a large number of open-source academic d(M)LLMs, have demonstrated performance
comparable to their autoregressive counterparts, while achieving up to 10x
acceleration in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven
by progress in two domains. The first is the development of autoregressive LLMs
and MLLMs, which has accumulated vast amounts of data, benchmarks, and
foundational infrastructure for training and inference. The second contributing
domain is the evolution of the mathematical models underlying discrete
diffusion. Together, these advancements have catalyzed a surge in dLLMs and
dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM
and dMLLM domains. We trace the historical development of dLLMs and dMLLMs,
formalize the underlying mathematical frameworks, and categorize representative
models. We further analyze key techniques for training and inference, and
summarize emerging applications across language, vision-language, and
biological domains. We conclude by discussing future directions for research
and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey
</summary>
    <author>
      <name>Runpeng Yu</name>
    </author>
    <author>
      <name>Qi Li</name>
    </author>
    <author>
      <name>Xinchao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.13759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13756v1</id>
    <updated>2025-06-16T17:58:29Z</updated>
    <published>2025-06-16T17:58:29Z</published>
    <title>UltraZoom: Generating Gigapixel Images from Regular Photos</title>
    <summary>  We present UltraZoom, a system for generating gigapixel-resolution images of
objects from casually captured inputs, such as handheld phone photos. Given a
full-shot image (global, low-detail) and one or more close-ups (local,
high-detail), UltraZoom upscales the full image to match the fine detail and
scale of the close-up examples. To achieve this, we construct a per-instance
paired dataset from the close-ups and adapt a pretrained generative model to
learn object-specific low-to-high resolution mappings. At inference, we apply
the model in a sliding window fashion over the full image. Constructing these
pairs is non-trivial: it requires registering the close-ups within the full
image for scale estimation and degradation alignment. We introduce a simple,
robust method for getting registration on arbitrary materials in casual,
in-the-wild captures. Together, these components form a system that enables
seamless pan and zoom across the entire object, producing consistent,
photorealistic gigapixel imagery from minimal input.
</summary>
    <author>
      <name>Jingwei Ma</name>
    </author>
    <author>
      <name>Vivek Jayaram</name>
    </author>
    <author>
      <name>Brian Curless</name>
    </author>
    <author>
      <name>Ira Kemelmacher-Shlizerman</name>
    </author>
    <author>
      <name>Steven M. Seitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://ultra-zoom.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13755v1</id>
    <updated>2025-06-16T17:58:09Z</updated>
    <published>2025-06-16T17:58:09Z</published>
    <title>MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with
  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering</title>
    <summary>  This paper introduces MARCO (Multi-Agent Reinforcement learning with
Conformal Optimization), a novel hardware-aware framework for efficient neural
architecture search (NAS) targeting resource-constrained edge devices. By
significantly reducing search time and maintaining accuracy under strict
hardware constraints, MARCO bridges the gap between automated DNN design and
CAD for edge AI deployment. MARCO's core technical contribution lies in its
unique combination of multi-agent reinforcement learning (MARL) with Conformal
Prediction (CP) to accelerate the hardware/software co-design process for
deploying deep neural networks. Unlike conventional once-for-all (OFA) supernet
approaches that require extensive pretraining, MARCO decomposes the NAS task
into a hardware configuration agent (HCA) and a Quantization Agent (QA). The
HCA optimizes high-level design parameters, while the QA determines per-layer
bit-widths under strict memory and latency budgets using a shared reward signal
within a centralized-critic, decentralized-execution (CTDE) paradigm. A key
innovation is the integration of a calibrated CP surrogate model that provides
statistical guarantees (with a user-defined miscoverage rate) to prune
unpromising candidate architectures before incurring the high costs of partial
training or hardware simulation. This early filtering drastically reduces the
search space while ensuring that high-quality designs are retained with a high
probability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100
demonstrate that MARCO achieves a 3-4x reduction in total search time compared
to an OFA baseline while maintaining near-baseline accuracy (within 0.3%).
Furthermore, MARCO also reduces inference latency. Validation on a MAX78000
evaluation board confirms that simulator trends hold in practice, with
simulator estimates deviating from measured values by less than 5%.
</summary>
    <author>
      <name>Arya Fayyazi</name>
    </author>
    <author>
      <name>Mehdi Kamal</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <link href="http://arxiv.org/abs/2506.13755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13754v2</id>
    <updated>2025-06-17T02:15:17Z</updated>
    <published>2025-06-16T17:58:00Z</published>
    <title>VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion
  Models</title>
    <summary>  We present a unified framework for solving partial differential equations
(PDEs) using video-inpainting diffusion transformer models. Unlike existing
methods that devise specialized strategies for either forward or inverse
problems under full or partial observation, our approach unifies these tasks
under a single, flexible generative framework. Specifically, we recast
PDE-solving as a generalized inpainting problem, e.g., treating forward
prediction as inferring missing spatiotemporal information of future states
from initial conditions. To this end, we design a transformer-based
architecture that conditions on arbitrary patterns of known data to infer
missing values across time and space. Our method proposes pixel-space video
diffusion models for fine-grained, high-fidelity inpainting and conditioning,
while enhancing computational efficiency through hierarchical modeling.
Extensive experiments show that our video inpainting-based diffusion model
offers an accurate and versatile solution across a wide range of PDEs and
problem setups, outperforming state-of-the-art baselines.
</summary>
    <author>
      <name>Edward Li</name>
    </author>
    <author>
      <name>Zichen Wang</name>
    </author>
    <author>
      <name>Jiahe Huang</name>
    </author>
    <author>
      <name>Jeong Joon Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://videopde.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13754v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13754v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13752v1</id>
    <updated>2025-06-16T17:57:05Z</updated>
    <published>2025-06-16T17:57:05Z</published>
    <title>Steering LLM Thinking with Budget Guidance</title>
    <summary>  Recent deep-thinking large language models often reason extensively to
improve performance, but such lengthy reasoning is not always desirable, as it
incurs excessive inference costs with disproportionate performance gains.
Controlling reasoning length without sacrificing performance is therefore
important, but remains challenging, especially under tight thinking budgets. We
propose budget guidance, a simple yet effective method for steering the
reasoning process of LLMs toward a target budget without requiring any LLM
fine-tuning. Our approach introduces a lightweight predictor that models a
Gamma distribution over the remaining thinking length during next-token
generation. This signal is then used to guide generation in a soft, token-level
manner, ensuring that the overall reasoning trace adheres to the specified
thinking budget. Budget guidance enables natural control of the thinking
length, along with significant token efficiency improvements over baseline
methods on challenging math benchmarks. For instance, it achieves up to a 26%
accuracy gain on the MATH-500 benchmark under tight budgets compared to
baseline methods, while maintaining competitive accuracy with only 63% of the
thinking tokens used by the full-thinking model. Budget guidance also
generalizes to broader task domains and exhibits emergent capabilities, such as
estimating question difficulty. The source code is available at:
https://github.com/UMass-Embodied-AGI/BudgetGuidance.
</summary>
    <author>
      <name>Junyan Li</name>
    </author>
    <author>
      <name>Wenshuo Zhao</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Chuang Gan</name>
    </author>
    <link href="http://arxiv.org/abs/2506.13752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13740v1</id>
    <updated>2025-06-16T17:50:23Z</updated>
    <published>2025-06-16T17:50:23Z</published>
    <title>Kolmogorov-Arnold Network for Gene Regulatory Network Inference</title>
    <summary>  Gene regulation is central to understanding cellular processes and
development, potentially leading to the discovery of new treatments for
diseases and personalized medicine. Inferring gene regulatory networks (GRNs)
from single-cell RNA sequencing (scRNA-seq) data presents significant
challenges due to its high dimensionality and complexity. Existing tree-based
models, such as GENIE3 and GRNBOOST2, demonstrated scalability and
explainability in GRN inference, but they cannot distinguish regulation types
nor effectively capture continuous cellular dynamics. In this paper, we
introduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN)
with explainable AI to infer GRNs from scRNA-seq data. By modeling gene
expression as differentiable functions matching the smooth nature of cellular
dynamics, scKAN can accurately and precisely detect activation and inhibition
regulations through explainable AI and geometric tools. We conducted extensive
experiments on the BEELINE benchmark, and scKAN surpasses and improves the
leading signed GRN inference models ranging from 5.40\% to 28.37\% in AUROC and
from 1.97\% to 40.45\% in AUPRC. These results highlight the potential of scKAN
in capturing the underlying biological processes in gene regulation without
prior knowledge of the graph structure.
</summary>
    <author>
      <name>Tsz Pan Tong</name>
    </author>
    <author>
      <name>Aoran Wang</name>
    </author>
    <author>
      <name>George Panagopoulos</name>
    </author>
    <author>
      <name>Jun Pang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 14 figures, accepted in CMSB 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13733v1</id>
    <updated>2025-06-16T17:41:36Z</updated>
    <published>2025-06-16T17:41:36Z</published>
    <title>Robust Recursive Fusion of Multiresolution Multispectral Images with
  Location-Aware Neural Networks</title>
    <summary>  Multiresolution image fusion is a key problem for real-time satellite imaging
and plays a central role in detecting and monitoring natural phenomena such as
floods. It aims to solve the trade-off between temporal and spatial resolution
in remote sensing instruments. Although several algorithms have been proposed
for this problem, the presence of outliers such as clouds downgrades their
performance. Moreover, strategies that integrate robustness, recursive
operation and learned models are missing. In this paper, a robust recursive
image fusion framework leveraging location-aware neural networks (NN) to model
the image dynamics is proposed. Outliers are modeled by representing the
probability of contamination of a given pixel and band. A NN model trained on a
small dataset provides accurate predictions of the stochastic image time
evolution, which improves both the accuracy and robustness of the method. A
recursive solution is proposed to estimate the high-resolution images using a
Bayesian variational inference framework. Experiments fusing images from the
Landsat 8 and MODIS instruments show that the proposed approach is
significantly more robust against cloud cover, without losing performance when
no clouds are present.
</summary>
    <author>
      <name>Haoqing Li</name>
    </author>
    <author>
      <name>Ricardo Borsoi</name>
    </author>
    <author>
      <name>Tales Imbiriba</name>
    </author>
    <author>
      <name>Pau Closas</name>
    </author>
    <link href="http://arxiv.org/abs/2506.13733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13727v1</id>
    <updated>2025-06-16T17:38:36Z</updated>
    <published>2025-06-16T17:38:36Z</published>
    <title>Attribution-guided Pruning for Compression, Circuit Discovery, and
  Targeted Correction in LLMs</title>
    <summary>  Large Language Models (LLMs) are central to many contemporary AI
applications, yet their extensive parameter counts pose significant challenges
for deployment in memory- and compute-constrained environments. Recent works in
eXplainable AI (XAI), particularly on attribution methods, suggest that
interpretability can also enable model compression by identifying and removing
components irrelevant to inference. In this paper, we leverage Layer-wise
Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs.
While LRP has shown promise in structured pruning for vision models, we extend
it to unstructured pruning in LLMs and demonstrate that it can substantially
reduce model size with minimal performance loss. Our method is especially
effective in extracting task-relevant subgraphs -- so-called ``circuits'' --
which can represent core functions (e.g., indirect object identification).
Building on this, we introduce a technique for model correction, by selectively
removing circuits responsible for spurious behaviors (e.g., toxic outputs). All
in all, we gather these techniques as a uniform holistic framework and showcase
its effectiveness and limitations through extensive experiments for
compression, circuit discovery and model correction on Llama and OPT models,
highlighting its potential for improving both model efficiency and safety. Our
code is publicly available at https://github.com/erfanhatefi/SparC3.
</summary>
    <author>
      <name>Sayed Mohammad Vakilzadeh Hatefi</name>
    </author>
    <author>
      <name>Maximilian Dreyer</name>
    </author>
    <author>
      <name>Reduan Achtibat</name>
    </author>
    <author>
      <name>Patrick Kahardipraja</name>
    </author>
    <author>
      <name>Thomas Wiegand</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <author>
      <name>Sebastian Lapuschkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress (10 pages manuscript, 3 pages references, 12 pages
  appendix)</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13725v1</id>
    <updated>2025-06-16T17:31:16Z</updated>
    <published>2025-06-16T17:31:16Z</published>
    <title>CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit
  Decoding</title>
    <summary>  In recent years, Vision-Language-Action (VLA) models have become a vital
research direction in robotics due to their impressive multimodal understanding
and generalization capabilities. Despite the progress, their practical
deployment is severely constrained by inference speed bottlenecks, particularly
in high-frequency and dexterous manipulation tasks. While recent studies have
explored Jacobi decoding as a more efficient alternative to traditional
autoregressive decoding, its practical benefits are marginal due to the lengthy
iterations. To address it, we introduce consistency distillation training to
predict multiple correct action tokens in each iteration, thereby achieving
acceleration. Besides, we design mixed-label supervision to mitigate the error
accumulation during distillation. Although distillation brings acceptable
speedup, we identify that certain inefficient iterations remain a critical
bottleneck. To tackle this, we propose an early-exit decoding strategy that
moderately relaxes convergence conditions, which further improves average
inference efficiency. Experimental results show that the proposed method
achieves more than 4 times inference acceleration across different baselines
while maintaining high task success rates in both simulated and real-world
robot tasks. These experiments validate that our approach provides an efficient
and general paradigm for accelerating multimodal decision-making in robotics.
Our project page is available at https://irpn-eai.github.io/CEED-VLA/.
</summary>
    <author>
      <name>Wenxuan Song</name>
    </author>
    <author>
      <name>Jiayi Chen</name>
    </author>
    <author>
      <name>Pengxiang Ding</name>
    </author>
    <author>
      <name>Yuxin Huang</name>
    </author>
    <author>
      <name>Han Zhao</name>
    </author>
    <author>
      <name>Donglin Wang</name>
    </author>
    <author>
      <name>Haoang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13716v1</id>
    <updated>2025-06-16T17:24:23Z</updated>
    <published>2025-06-16T17:24:23Z</published>
    <title>Mass models of galaxy clusters from a non-parametric weak-lensing
  reconstruction</title>
    <summary>  We study the CLASH sample of galaxy clusters using a new deprojection method
for weak gravitational lensing observations. This method is non-parametric,
allowing us to infer mass profiles, or equivalently circular velocities,
without having to assume a specific halo profile. While this method assumes
spherical symmetry, we show that, on average, triaxiality is unlikely to
significantly affect our results. We use this method to study the total mass
profiles of the CLASH clusters, as well as the relation between their total and
baryonic components: (1) We find that the implied circular velocities are
consistent with being approximately flat at large radii, akin to the rotation
curves of galaxies. (2) We infer radially resolved baryonic mass fractions,
finding that these vary significantly from cluster to cluster and depend
strongly on the details of the X-ray gas mass profiles. Since the gas mass
profiles are poorly constrained at large radii, it is unclear whether the CLASH
clusters reach the cosmic baryon fraction expected in $\Lambda$CDM. (3) The
non-parametric masses are consistent with the stellar mass--halo mass relation
expected in $\Lambda$CDM; fitting parametric NFW halos to the non-parametric
mass profiles gives results in overall agreement with the expected
mass-concentration relation, though the concentrations are relatively poorly
constrained. (4) Galaxy clusters systematically deviate from the Baryonic
Tully-Fisher Relation (BTFR) and the Radial Acceleration Relation (RAR) defined
by galaxies, but the magnitude of the offset depends strongly on the gas mass
extrapolation at large radii. Contrary to some previous results based on
hydrostatic equilibrium, we find that galaxy clusters may fall on the same BTFR
and RAR as galaxies if one adds a suitable positive baryonic mass component.
</summary>
    <author>
      <name>Tobias Mistele</name>
    </author>
    <author>
      <name>Federico Lelli</name>
    </author>
    <author>
      <name>Stacy McGaugh</name>
    </author>
    <author>
      <name>James Schombert</name>
    </author>
    <author>
      <name>Benoit Famaey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 15 figures, plus appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
