<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-17T00:56:41Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-17T00:56:41Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>128738</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.13692v1</id>
    <title>Quantum oracles give an advantage for identifying classical counterfactuals</title>
    <updated>2025-12-15T18:59:58Z</updated>
    <link href="https://arxiv.org/abs/2512.13692v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13692v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We show that quantum oracles provide an advantage over classical oracles for answering classical counterfactual questions in causal models, or equivalently, for identifying unknown causal parameters such as distributions over functional dependences. In structural causal models with discrete classical variables, observational data and even ideal interventions generally fail to answer all counterfactual questions, since different causal parameters can reproduce the same observational and interventional data while disagreeing on counterfactuals. Using a simple binary example, we demonstrate that if the classical variables of interest are encoded in quantum systems and the causal dependence among them is encoded in a quantum oracle, coherently querying the oracle enables the identification of all causal parameters -- hence all classical counterfactuals. We generalize this to arbitrary finite cardinalities and prove that coherent probing 1) allows the identification of all two-way joint counterfactuals p(Y_x=y, Y_{x'}=y'), which is not possible with any number of queries to a classical oracle, and 2) provides tighter bounds on higher-order multi-way counterfactuals than with a classical oracle. This work can also be viewed as an extension to traditional quantum oracle problems such as Deutsch--Jozsa to identifying more causal parameters beyond just, e.g., whether a function is constant or balanced. Finally, we raise the question of whether this quantum advantage relies on uniquely non-classical features like contextuality. We provide some evidence against this by showing that in the binary case, oracles in some classically-explainable theories like Spekkens' toy theory also give rise to a counterfactual identifiability advantage over strictly classical oracles.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:58Z</published>
    <arxiv:comment>5+4 pages. Comments welcome!</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Ciarán M. Gilligan-Lee</name>
    </author>
    <author>
      <name>Yìlè Yīng</name>
    </author>
    <author>
      <name>Jonathan Richens</name>
    </author>
    <author>
      <name>David Schmid</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13683v1</id>
    <title>I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</title>
    <updated>2025-12-15T18:59:13Z</updated>
    <link href="https://arxiv.org/abs/2512.13683v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13683v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:13Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Lu Ling</name>
    </author>
    <author>
      <name>Yunhao Ge</name>
    </author>
    <author>
      <name>Yichen Sheng</name>
    </author>
    <author>
      <name>Aniket Bera</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13680v1</id>
    <title>LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</title>
    <updated>2025-12-15T18:59:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13680v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13680v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:04Z</published>
    <arxiv:comment>16 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tianye Ding</name>
    </author>
    <author>
      <name>Yiming Xie</name>
    </author>
    <author>
      <name>Yiqing Liang</name>
    </author>
    <author>
      <name>Moitreya Chatterjee</name>
    </author>
    <author>
      <name>Pedro Miraldo</name>
    </author>
    <author>
      <name>Huaizu Jiang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13636v1</id>
    <title>MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</title>
    <updated>2025-12-15T18:31:32Z</updated>
    <link href="https://arxiv.org/abs/2512.13636v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13636v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:31:32Z</published>
    <arxiv:comment>16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haoyu Fu</name>
    </author>
    <author>
      <name>Diankun Zhang</name>
    </author>
    <author>
      <name>Zongchuang Zhao</name>
    </author>
    <author>
      <name>Jianfeng Cui</name>
    </author>
    <author>
      <name>Hongwei Xie</name>
    </author>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Guang Chen</name>
    </author>
    <author>
      <name>Dingkang Liang</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13635v1</id>
    <title>SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning</title>
    <updated>2025-12-15T18:30:40Z</updated>
    <link href="https://arxiv.org/abs/2512.13635v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13635v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:30:40Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Junchao Zhu</name>
    </author>
    <author>
      <name>Ruining Deng</name>
    </author>
    <author>
      <name>Junlin Guo</name>
    </author>
    <author>
      <name>Tianyuan Yao</name>
    </author>
    <author>
      <name>Chongyu Qu</name>
    </author>
    <author>
      <name>Juming Xiong</name>
    </author>
    <author>
      <name>Siqi Lu</name>
    </author>
    <author>
      <name>Zhengyi Lu</name>
    </author>
    <author>
      <name>Yanfan Zhu</name>
    </author>
    <author>
      <name>Marilyn Lionts</name>
    </author>
    <author>
      <name>Yuechen Yang</name>
    </author>
    <author>
      <name>Yalin Zheng</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Shilin Zhao</name>
    </author>
    <author>
      <name>Haichun Yang</name>
    </author>
    <author>
      <name>Yuankai Huo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13633v1</id>
    <title>A post-inflationary kinetic axion</title>
    <updated>2025-12-15T18:29:01Z</updated>
    <link href="https://arxiv.org/abs/2512.13633v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13633v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a novel realization of axion kinetic misalignment, triggered by a Hubble-induced phase transition during a post-inflationary stiff (kination) era. A negative Ricci scalar flips the sign of a non-minimally coupled mass term for a non-minimally coupled complex field $Φ$, driving its radial mode to large amplitudes via a tachyonic instability. At large $|Φ|$, higher-dimensional $U(1)$-breaking operators become relevant and impart a kick in the angular direction, generating a conserved $U(1)$ charge that sustains rotation as the symmetry is approximately restored. Because phases randomize across causally disconnected regions, multiple domains with distinct charges form. The subsequent axion potential converts the domain charges into an axion abundance, yielding dark matter even when the net global charge vanishes. We analyze the dynamics through a linear, domain-averaged treatment and identify two thermal histories: (i) Ricci reheating via saxion decays to Higgs bosons; (ii) external reheating with efficient damping of saxion energy by Higgs/fermion scatterings. The mechanism populates regions underabundant in standard misalignment, which are accessible to next generation axion searches.</summary>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:29:01Z</published>
    <arxiv:comment>28 pages + appendix</arxiv:comment>
    <arxiv:primary_category term="hep-ph"/>
    <author>
      <name>Enrico Morgante</name>
    </author>
    <author>
      <name>Riccardo Natale</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13629v1</id>
    <title>A comparative overview of win ratio and joint frailty models for recurrent event endpoints with applications in oncology and cardiology</title>
    <updated>2025-12-15T18:26:18Z</updated>
    <link href="https://arxiv.org/abs/2512.13629v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13629v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Composite endpoints that combine recurrent non-fatal events with a terminal event are increasingly used in randomized clinical trials, yet conventional time-to-first event analyses may obscure clinically relevant information. We compared two statistical frameworks tailored to such endpoints: the joint frailty model (JFM) and the last-event assisted recurrent-event win ratio (LWR). The JFM specifies proportional hazards for the recurrent and terminal events linked through a shared frailty, yielding covariate-adjusted, component-specific hazard ratios that account for informative recurrences and dependence with death. The LWR is a nonparametric, prioritized pairwise comparison that incorporates all observed events over follow-up and summarizes a population-level benefit of treatment while respecting a pre-specified hierarchy between death and recurrences. We first assessed the performance of the methods using simulations that varied both the gamma-frailty variance and the event rates. Next, we investigated these two frameworks using practical clinical applications, to assess the performance of the methods and to estimate the sample size required to achieve adequate power. These two approaches delivered complementary insights. The JFM provided component-specific estimates, while the LWR led to a summary measure of treatment effect with direction. Power was systematically improved with JFM, which thus appeared as the most reliable approach for inference and sample size estimation. Methodological extensions of the LWR to appropriately handle censoring and to formalize causal estimands remain a promising direction for future research.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:26:18Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Adrien Orué</name>
    </author>
    <author>
      <name>Derek Dinart</name>
    </author>
    <author>
      <name>Laurent Billot</name>
    </author>
    <author>
      <name>Carine Bellera</name>
    </author>
    <author>
      <name>Virginie Rondeau</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13607v1</id>
    <title>Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</title>
    <updated>2025-12-15T18:02:35Z</updated>
    <link href="https://arxiv.org/abs/2512.13607v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13607v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:02:35Z</published>
    <arxiv:comment>We publicly release the Nemotron-Cascade models and the full collection of training data at: https://huggingface.co/collections/nvidia/nemotron-cascade</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Boxin Wang</name>
    </author>
    <author>
      <name>Chankyu Lee</name>
    </author>
    <author>
      <name>Nayeon Lee</name>
    </author>
    <author>
      <name>Sheng-Chieh Lin</name>
    </author>
    <author>
      <name>Wenliang Dai</name>
    </author>
    <author>
      <name>Yang Chen</name>
    </author>
    <author>
      <name>Yangyi Chen</name>
    </author>
    <author>
      <name>Zhuolin Yang</name>
    </author>
    <author>
      <name>Zihan Liu</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <author>
      <name>Wei Ping</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13604v1</id>
    <title>LongVie 2: Multimodal Controllable Ultra-Long Video World Model</title>
    <updated>2025-12-15T17:59:58Z</updated>
    <link href="https://arxiv.org/abs/2512.13604v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13604v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:59:58Z</published>
    <arxiv:comment>Project Page: https://vchitect.github.io/LongVie2-project/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jianxiong Gao</name>
    </author>
    <author>
      <name>Zhaoxi Chen</name>
    </author>
    <author>
      <name>Xian Liu</name>
    </author>
    <author>
      <name>Junhao Zhuang</name>
    </author>
    <author>
      <name>Chengming Xu</name>
    </author>
    <author>
      <name>Jianfeng Feng</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Yanwei Fu</name>
    </author>
    <author>
      <name>Chenyang Si</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13592v1</id>
    <title>Image Diffusion Preview with Consistency Solver</title>
    <updated>2025-12-15T17:47:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13592v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13592v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:47:49Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Fu-Yun Wang</name>
    </author>
    <author>
      <name>Hao Zhou</name>
    </author>
    <author>
      <name>Liangzhe Yuan</name>
    </author>
    <author>
      <name>Sanghyun Woo</name>
    </author>
    <author>
      <name>Boqing Gong</name>
    </author>
    <author>
      <name>Bohyung Han</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <author>
      <name>Han Zhang</name>
    </author>
    <author>
      <name>Yukun Zhu</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
    <author>
      <name>Long Zhao</name>
    </author>
  </entry>
</feed>
