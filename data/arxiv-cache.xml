<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-19T01:00:24Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-18T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">123898</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.14981v1</id>
    <updated>2025-10-16T17:59:59Z</updated>
    <published>2025-10-16T17:59:59Z</published>
    <title>Coupled Diffusion Sampling for Training-Free Multi-View Image Editing</title>
    <summary>  We present an inference-time diffusion sampling method to perform multi-view
consistent image editing using pre-trained 2D image editing models. These
models can independently produce high-quality edits for each image in a set of
multi-view images of a 3D scene or object, but they do not maintain consistency
across views. Existing approaches typically address this by optimizing over
explicit 3D representations, but they suffer from a lengthy optimization
process and instability under sparse view settings. We propose an implicit 3D
regularization approach by constraining the generated 2D image sequences to
adhere to a pre-trained multi-view image distribution. This is achieved through
coupled diffusion sampling, a simple diffusion sampling technique that
concurrently samples two trajectories from both a multi-view image distribution
and a 2D edited image distribution, using a coupling term to enforce the
multi-view consistency among the generated images. We validate the
effectiveness and generality of this framework on three distinct multi-view
image editing tasks, demonstrating its applicability across various model
architectures and highlighting its potential as a general solution for
multi-view consistent editing.
</summary>
    <author>
      <name>Hadi Alzayer</name>
    </author>
    <author>
      <name>Yunzhi Zhang</name>
    </author>
    <author>
      <name>Chen Geng</name>
    </author>
    <author>
      <name>Jia-Bin Huang</name>
    </author>
    <author>
      <name>Jiajun Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://coupled-diffusion.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14976v1</id>
    <updated>2025-10-16T17:59:56Z</updated>
    <published>2025-10-16T17:59:56Z</published>
    <title>Ponimator: Unfolding Interactive Pose for Versatile Human-human
  Interaction Animation</title>
    <summary>  Close-proximity human-human interactive poses convey rich contextual
information about interaction dynamics. Given such poses, humans can
intuitively infer the context and anticipate possible past and future dynamics,
drawing on strong priors of human behavior. Inspired by this observation, we
propose Ponimator, a simple framework anchored on proximal interactive poses
for versatile interaction animation. Our training data consists of
close-contact two-person poses and their surrounding temporal context from
motion-capture interaction datasets. Leveraging interactive pose priors,
Ponimator employs two conditional diffusion models: (1) a pose animator that
uses the temporal prior to generate dynamic motion sequences from interactive
poses, and (2) a pose generator that applies the spatial prior to synthesize
interactive poses from a single pose, text, or both when interactive poses are
unavailable. Collectively, Ponimator supports diverse tasks, including
image-based interaction animation, reaction animation, and text-to-interaction
synthesis, facilitating the transfer of interaction knowledge from high-quality
mocap data to open-world scenarios. Empirical experiments across diverse
datasets and applications demonstrate the universality of the pose prior and
the effectiveness and robustness of our framework.
</summary>
    <author>
      <name>Shaowei Liu</name>
    </author>
    <author>
      <name>Chuan Guo</name>
    </author>
    <author>
      <name>Bing Zhou</name>
    </author>
    <author>
      <name>Jian Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2025. Project page:
  https://stevenlsw.github.io/ponimator/</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14970v1</id>
    <updated>2025-10-16T17:59:38Z</updated>
    <published>2025-10-16T17:59:38Z</published>
    <title>Biology-informed neural networks learn nonlinear representations from
  omics data to improve genomic prediction and interpretability</title>
    <summary>  We extend biologically-informed neural networks (BINNs) for genomic
prediction (GP) and selection (GS) in crops by integrating thousands of
single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior
biological knowledge. Traditional genotype-to-phenotype (G2P) models depend
heavily on direct mappings that achieve only modest accuracy, forcing breeders
to conduct large, costly field trials to maintain or marginally improve genetic
gain. Models that incorporate intermediate molecular phenotypes such as gene
expression can achieve higher predictive fit, but they remain impractical for
GS since such data are unavailable at deployment or design time. BINNs overcome
this limitation by encoding pathway-level inductive biases and leveraging
multi-omics data only during training, while using genotype data alone during
inference. Applied to maize gene-expression and multi-environment field-trial
data, BINN improves rank-correlation accuracy by up to 56% within and across
subpopulations under sparse-data conditions and nonlinearly identifies genes
that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic
metabolomics benchmark, BINN reduces prediction error by 75% relative to
conventional neural nets and correctly identifies the most important nonlinear
pathway. Importantly, both cases show highly sensitive BINN latent variables
correlate with the experimental quantities they represent, despite not being
trained on them. This suggests BINNs learn biologically-relevant
representations, nonlinear or linear, from genotype to phenotype. Together,
BINNs establish a framework that leverages intermediate domain information to
improve genomic prediction accuracy and reveal nonlinear biological
relationships that can guide genomic selection, candidate gene selection,
pathway enrichment, and gene-editing prioritization.
</summary>
    <author>
      <name>Katiana Kontolati</name>
    </author>
    <author>
      <name>Rini Jasmine Gladstone</name>
    </author>
    <author>
      <name>Ian Davis</name>
    </author>
    <author>
      <name>Ethan Pickering</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14961v1</id>
    <updated>2025-10-16T17:59:07Z</updated>
    <published>2025-10-16T17:59:07Z</published>
    <title>Efficient Parallel Samplers for Recurrent-Depth Models and Their
  Connection to Diffusion Language Models</title>
    <summary>  Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.
</summary>
    <author>
      <name>Jonas Geiping</name>
    </author>
    <author>
      <name>Xinyu Yang</name>
    </author>
    <author>
      <name>Guinan Su</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code can be found at https://github.com/seal-rg/recurrent-pretraining</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14954v1</id>
    <updated>2025-10-16T17:57:53Z</updated>
    <published>2025-10-16T17:57:53Z</published>
    <title>OmniMotion: Multimodal Motion Generation with Continuous Masked
  Autoregression</title>
    <summary>  Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.
</summary>
    <author>
      <name>Zhe Li</name>
    </author>
    <author>
      <name>Weihao Yuan</name>
    </author>
    <author>
      <name>Weichao Shen</name>
    </author>
    <author>
      <name>Siyu Zhu</name>
    </author>
    <author>
      <name>Zilong Dong</name>
    </author>
    <author>
      <name>Chang Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2510.14954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14952v1</id>
    <updated>2025-10-16T17:57:47Z</updated>
    <published>2025-10-16T17:57:47Z</published>
    <title>From Language to Locomotion: Retargeting-free Humanoid Control via
  Motion Latent Guidance</title>
    <summary>  Natural language offers a natural interface for humanoid robots, but existing
language-guided humanoid locomotion pipelines remain cumbersome and unreliable.
They typically decode human motion, retarget it to robot morphology, and then
track it with a physics-based controller. However, this multi-stage process is
prone to cumulative errors, introduces high latency, and yields weak coupling
between semantics and control. These limitations call for a more direct pathway
from language to action, one that eliminates fragile intermediate stages.
Therefore, we present RoboGhost, a retargeting-free framework that directly
conditions humanoid policies on language-grounded motion latents. By bypassing
explicit motion decoding and retargeting, RoboGhost enables a diffusion-based
policy to denoise executable actions directly from noise, preserving semantic
intent and supporting fast, reactive control. A hybrid causal
transformer-diffusion motion generator further ensures long-horizon consistency
while maintaining stability and diversity, yielding rich latent representations
for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost
substantially reduces deployment latency, improves success rates and tracking
accuracy, and produces smooth, semantically aligned locomotion on real
humanoids. Beyond text, the framework naturally extends to other modalities
such as images, audio, and music, providing a general foundation for
vision-language-action humanoid systems.
</summary>
    <author>
      <name>Zhe Li</name>
    </author>
    <author>
      <name>Cheng Chi</name>
    </author>
    <author>
      <name>Yangyang Wei</name>
    </author>
    <author>
      <name>Boan Zhu</name>
    </author>
    <author>
      <name>Yibo Peng</name>
    </author>
    <author>
      <name>Tao Huang</name>
    </author>
    <author>
      <name>Pengwei Wang</name>
    </author>
    <author>
      <name>Zhongyuan Wang</name>
    </author>
    <author>
      <name>Shanghang Zhang</name>
    </author>
    <author>
      <name>Chang Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2510.14952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14950v1</id>
    <updated>2025-10-16T17:57:06Z</updated>
    <published>2025-10-16T17:57:06Z</published>
    <title>A formative measurement validation methodology for survey questionnaires</title>
    <summary>  Model misspecification of formative indicators remains a widely documented
issue across academic literature, yet scholars lack a clear consensus on
pragmatic, prescriptive approaches to manage this gap. This ambiguity forces
researchers to rely on psychometric frameworks primarily intended for
reflective models, and thus risks misleading findings. This article introduces
a Multi-Step Validation Methodology Framework specifically designed for
formative constructs in survey-based research. The proposed framework is
grounded in an exhaustive literature review and integrates essential pilot
diagnostics through descriptive statistics and multicollinearity checks. The
methodology provides researchers with the necessary theoretical and structural
clarity to finally justify and adhere to appropriate validation techniques that
accurately account for the causal nature of the constructs while ensuring high
psychometric and statistical integrity.
</summary>
    <author>
      <name>Mark Dominique Dalipe Muñoz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14946v1</id>
    <updated>2025-10-16T17:55:56Z</updated>
    <published>2025-10-16T17:55:56Z</published>
    <title>EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge
  Devices</title>
    <summary>  Deployment of efficient and accurate Deep Learning models has long been a
challenge in autonomous navigation, particularly for real-time applications on
resource-constrained edge devices. Edge devices are limited in computing power
and memory, making model efficiency and compression essential. In this work, we
propose EdgeNavMamba, a reinforcement learning-based framework for
goal-directed navigation using an efficient Mamba object detection model. To
train and evaluate the detector, we introduce a custom shape detection dataset
collected in diverse indoor settings, reflecting visual cues common in
real-world navigation. The object detector serves as a pre-processing module,
extracting bounding boxes (BBOX) from visual input, which are then passed to an
RL policy to control goal-oriented navigation. Experimental results show that
the student model achieved a reduction of 67% in size, and up to 73% in energy
per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,
while keeping the same performance as the teacher model. EdgeNavMamba also
maintains high detection accuracy in MiniWorld and IsaacLab simulators while
reducing parameters by 31% compared to the baseline. In the MiniWorld
simulator, the navigation policy achieves over 90% success across environments
of varying complexity.
</summary>
    <author>
      <name>Romina Aalishah</name>
    </author>
    <author>
      <name>Mozhgan Navardi</name>
    </author>
    <author>
      <name>Tinoosh Mohsenin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 11th IEEE International Conference on Edge Computing and Scalable
  Cloud (IEEE EdgeCom 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14943v1</id>
    <updated>2025-10-16T17:55:11Z</updated>
    <published>2025-10-16T17:55:11Z</published>
    <title>LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</title>
    <summary>  Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a core paradigm for enhancing the reasoning capabilities of Large Language
Models (LLMs). To address the lack of verification signals at test time, prior
studies incorporate the training of model's self-verification capability into
the standard RLVR process, thereby unifying reasoning and verification
capabilities within a single LLM. However, previous practice requires the LLM
to sequentially generate solutions and self-verifications using two separate
prompt templates, which significantly reduces efficiency. In this work, we
theoretically reveal that the closed-form solution to the RL objective of
self-verification can be reduced to a remarkably simple form: the true
reasoning reward of a solution is equal to its last-token self-rewarding score,
which is computed as the difference between the policy model's next-token
log-probability assigned to any pre-specified token at the solution's last
token and a pre-calculated constant, scaled by the KL coefficient. Based on
this insight, we propose LaSeR (Reinforcement Learning with Last-Token
Self-Rewarding), an algorithm that simply augments the original RLVR loss with
a MSE loss that aligns the last-token self-rewarding scores with verifier-based
reasoning rewards, jointly optimizing the reasoning and self-rewarding
capabilities of LLMs. The optimized self-rewarding scores can be utilized in
both training and testing to enhance model performance. Notably, our algorithm
derives these scores from the predicted next-token probability distribution of
the last token immediately after generation, incurring only the minimal extra
cost of one additional token inference. Experiments show that our method not
only improves the model's reasoning performance but also equips it with
remarkable self-rewarding capability, thereby boosting its inference-time
scaling performance.
</summary>
    <author>
      <name>Wenkai Yang</name>
    </author>
    <author>
      <name>Weijie Liu</name>
    </author>
    <author>
      <name>Ruobing Xie</name>
    </author>
    <author>
      <name>Yiju Guo</name>
    </author>
    <author>
      <name>Lulu Wu</name>
    </author>
    <author>
      <name>Saiyong Yang</name>
    </author>
    <author>
      <name>Yankai Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress. Github repo: https://github.com/RUCBM/LaSeR</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14942v1</id>
    <updated>2025-10-16T17:54:07Z</updated>
    <published>2025-10-16T17:54:07Z</published>
    <title>GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for
  Step-Level Reasoning</title>
    <summary>  Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.
</summary>
    <author>
      <name>Yao Zhang</name>
    </author>
    <author>
      <name>Yu Wu</name>
    </author>
    <author>
      <name>Haowei Zhang</name>
    </author>
    <author>
      <name>Weiguo Li</name>
    </author>
    <author>
      <name>Haokun Chen</name>
    </author>
    <author>
      <name>Jingpei Wu</name>
    </author>
    <author>
      <name>Guohao Li</name>
    </author>
    <author>
      <name>Zhen Han</name>
    </author>
    <author>
      <name>Volker Tresp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
