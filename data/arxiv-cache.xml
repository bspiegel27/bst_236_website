<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-18T00:51:39Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-17T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">121075</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.13318v1</id>
    <updated>2025-09-16T17:59:31Z</updated>
    <published>2025-09-16T17:59:31Z</published>
    <title>How Theory-Informed Priors Affect DESI Evidence for Evolving Dark Energy</title>
    <summary>  Recent measurements of baryon acoustic oscillations (BAO) from the Dark
Energy Spectroscopic Instrument (DESI) have been interpreted to suggest that
dark energy may be evolving. In this work, we examine how prior choices affect
such conclusions. Specifically, we study the biases introduced by the customary
use of uniform priors on the Chevallier-Polarski-Linder (CPL) parameters, $w_0$
and $w_a$, when assessing evidence for evolving dark energy. To do so, we
construct theory-informed priors on $(w_0, w_a)$ using a normalizing flow (NF),
trained on two representative quintessence models, which learns the
distribution of these parameters conditional on the underlying $\Lambda$CDM
parameters. In the combined $\textit{Planck}$ CMB + DESI BAO analysis we find
that the apparent tension with a cosmological constant in the CPL framework can
be reduced from $\sim 3.1\sigma$ to $\sim 1.3\sigma$ once theory-informed
priors are applied, rendering the result effectively consistent with
$\Lambda$CDM. For completeness, we also analyze combinations that include Type
Ia supernova data, showing similar shifts toward the $\Lambda$CDM limit. Taken
together, the observed sensitivity to prior choices in these analyses arises
because uniform priors - often mischaracterized as "uninformative" - can
actually bias inferences toward unphysical parameter regions. Consequently, our
results underscore the importance of adopting physically motivated priors to
ensure robust cosmological inferences, especially when evaluating new
hypotheses with only marginal statistical support. Lastly, our NF-based
framework achieves these results by post-processing existing MCMC chains,
requiring $\approx 1$ hour of additional CPU compute time on top of the base
analysis - a dramatic speedup over direct model sampling that highlights the
scalability of this approach for testing diverse theoretical models.
</summary>
    <author>
      <name>Michael W. Toomey</name>
    </author>
    <author>
      <name>Gabriele Montefalcone</name>
    </author>
    <author>
      <name>Evan McDonough</name>
    </author>
    <author>
      <name>Katherine Freese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13317v1</id>
    <updated>2025-09-16T17:59:06Z</updated>
    <published>2025-09-16T17:59:06Z</published>
    <title>3D Aware Region Prompted Vision Language Model</title>
    <summary>  We present Spatial Region 3D (SR-3D) aware vision-language model that
connects single-view 2D images and multi-view 3D data through a shared visual
token space. SR-3D supports flexible region prompting, allowing users to
annotate regions with bounding boxes, segmentation masks on any frame, or
directly in 3D, without the need for exhaustive multi-frame labeling. We
achieve this by enriching 2D visual features with 3D positional embeddings,
which allows the 3D model to draw upon strong 2D priors for more accurate
spatial reasoning across frames, even when objects of interest do not co-occur
within the same view. Extensive experiments on both general 2D vision language
and specialized 3D spatial benchmarks demonstrate that SR-3D achieves
state-of-the-art performance, underscoring its effectiveness for unifying 2D
and 3D representation space on scene understanding. Moreover, we observe
applicability to in-the-wild videos without sensory 3D inputs or ground-truth
3D annotations, where SR-3D accurately infers spatial relationships and metric
measurements.
</summary>
    <author>
      <name>An-Chieh Cheng</name>
    </author>
    <author>
      <name>Yang Fu</name>
    </author>
    <author>
      <name>Yukang Chen</name>
    </author>
    <author>
      <name>Zhijian Liu</name>
    </author>
    <author>
      <name>Xiaolong Li</name>
    </author>
    <author>
      <name>Subhashree Radhakrishnan</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <author>
      <name>Pavlo Molchanov</name>
    </author>
    <author>
      <name>Hongxu Yin</name>
    </author>
    <author>
      <name>Xiaolong Wang</name>
    </author>
    <author>
      <name>Sifei Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Website: https://www.anjiecheng.me/sr3d</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13307v1</id>
    <updated>2025-09-16T17:57:05Z</updated>
    <published>2025-09-16T17:57:05Z</published>
    <title>High-Dimensional Bayesian Model Comparison in Cosmology with
  GPU-accelerated Nested Sampling and Neural Emulators</title>
    <summary>  We demonstrate a GPU-accelerated nested sampling framework for efficient
high-dimensional Bayesian inference in cosmology. Using JAX-based neural
emulators and likelihoods for cosmic microwave background and cosmic shear
analyses, our approach provides parameter constraints and direct calculation of
Bayesian evidence. In the 39 dimensional $\Lambda$CDM vs $w_0w_a$ shear
analysis, we produce Bayes Factors and a robust error bar in just 2 days on a
single A100 GPU, without loss of accuracy. Where CPU-based nested sampling can
now be outpaced by methods relying on MCMC sampling and decoupled evidence
estimation, we demonstrate that with GPU acceleration nested sampling offers
the necessary speed-up to put it on equal computational footing with these
methods, especially where reliable model comparison is paramount. We put
forward both nested and gradient-based sampling as useful tools for the modern
cosmologist, where cutting-edge inference pipelines can yield orders of
magnitude improvements in computation time.
</summary>
    <author>
      <name>Toby Lovick</name>
    </author>
    <author>
      <name>David Yallup</name>
    </author>
    <author>
      <name>Davide Piras</name>
    </author>
    <author>
      <name>Alessio Spurio Mancini</name>
    </author>
    <author>
      <name>Will Handley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13293v1</id>
    <updated>2025-09-16T17:44:58Z</updated>
    <published>2025-09-16T17:44:58Z</published>
    <title>Inferring Soil Drydown Behaviour with Adaptive Bayesian Online
  Changepoint Analysis</title>
    <summary>  Continuous soil-moisture measurements provide a direct lens on subsurface
hydrological processes, notably the post-rainfall "drydown" phase. Because
these records consist of distinct, segment-specific behaviours whose forms and
scales vary over time, realistic inference demands a model that captures
piecewise dynamics while accommodating parameters that are unknown a priori.
Building on Bayesian Online Changepoint Detection (BOCPD), we introduce two
complementary extensions: a particle-filter variant that substitutes exact
marginalisation with sequential Monte Carlo to enable real-time inference when
critical parameters cannot be integrated out analytically, and an
online-gradient variant that embeds stochastic gradient updates within BOCPD to
learn application-relevant parameters on the fly without prohibitive
computational cost. After validating both algorithms on synthetic data that
replicate the temporal structure of field observations-detailing hyperparameter
choices, priors, and cost-saving strategies-we apply them to soil-moisture
series from experimental sites in Austria and the United States, quantifying
site-specific drydown rates and demonstrating the advantages of our adaptive
framework over static models.
</summary>
    <author>
      <name>Mengyi Gong</name>
    </author>
    <author>
      <name>Christopher Nemeth</name>
    </author>
    <author>
      <name>Rebecca Killick</name>
    </author>
    <author>
      <name>Peter Strauss</name>
    </author>
    <author>
      <name>John Quinton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages of main manuscript and 3 pages if supplemental document</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13272v1</id>
    <updated>2025-09-16T17:29:32Z</updated>
    <published>2025-09-16T17:29:32Z</published>
    <title>Ionization and temperature measurements in warm dense copper using x-ray
  absorption spectroscopy</title>
    <summary>  We detail experimental results inferring ionization and temperature for warm
dense copper plasmas at several times solid density (15 to 25 g/cm$^3$) and
temperatures of 10 to 21 eV. Experiments performed at the OMEGA Laser Facility
generate uniform warm dense matter conditions via symmetric shock compression
of a buried copper layer. The plasma is probed with a laser-generated x-ray
source to collect the K-shell x-ray absorption spectrum. Fitting bound-bound
absorption contributions from constituent charge states of copper provides an
estimated $\overline{Z}$ of approximately 4 to 7 for these warm dense copper
plasmas. We find that these partially ionized plasmas have K-edge shifts of 12
to 30 eV and bound-bound resonance 1s$\rightarrow$3p absorption shifts of 4 to
26 eV with respect to the cold K-edge. This study provides necessary
experimental data to improve ionization and opacity models in the warm dense
matter regime.
</summary>
    <author>
      <name>T. Cordova</name>
    </author>
    <author>
      <name>E. V. Marley</name>
    </author>
    <author>
      <name>D. A. Chin</name>
    </author>
    <author>
      <name>R. A. London</name>
    </author>
    <author>
      <name>H. A. Scott</name>
    </author>
    <author>
      <name>T. Döppner</name>
    </author>
    <author>
      <name>F. N. Beg</name>
    </author>
    <author>
      <name>F. Coppari</name>
    </author>
    <author>
      <name>M. Millot</name>
    </author>
    <author>
      <name>J. Emig</name>
    </author>
    <author>
      <name>S. B. Hansen</name>
    </author>
    <author>
      <name>P. M. Nilson</name>
    </author>
    <author>
      <name>P. Sterne</name>
    </author>
    <author>
      <name>M. J. MacDonald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 7 Figures, In submission for Physical Review Research</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13267v1</id>
    <updated>2025-09-16T17:24:35Z</updated>
    <published>2025-09-16T17:24:35Z</published>
    <title>Learning Discrete Bayesian Networks with Hierarchical Dirichlet
  Shrinkage</title>
    <summary>  Discrete Bayesian networks (DBNs) provide a broadly useful framework for
modeling dependence structures in multivariate categorical data. There is a
vast literature on methods for inferring conditional probabilities and
graphical structure in DBNs, but data sparsity and parametric assumptions are
major practical issues. In this article, we detail a comprehensive Bayesian
framework for learning DBNs. First, we propose a hierarchical prior for the
conditional probabilities that enables complicated interactions between parent
variables and stability in sparse regimes. We give a novel Markov chain Monte
Carlo (MCMC) algorithm utilizing parallel Langevin proposals to generate exact
posterior samples, avoiding the pitfalls of variational approximations.
Moreover, we verify that the full conditional distribution of the concentration
parameters is log-concave under mild conditions, facilitating efficient
sampling. We then propose two methods for learning network structures,
including parent sets, Markov blankets, and DAGs, from categorical data. The
first cycles through individual edges each MCMC iteration, whereas the second
updates the entire structure as a single step. We evaluate the accuracy, power,
and MCMC performance of our methods on several simulation studies. Finally, we
apply our methodology to uncover prognostic network structure from primary
breast cancer samples.
</summary>
    <author>
      <name>Alexander Dombowsky</name>
    </author>
    <author>
      <name>David B. Dunson</name>
    </author>
    <link href="http://arxiv.org/abs/2509.13267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13255v1</id>
    <updated>2025-09-16T17:12:23Z</updated>
    <published>2025-09-16T17:12:23Z</published>
    <title>ResidualViT for Efficient Temporally Dense Video Encoding</title>
    <summary>  Several video understanding tasks, such as natural language temporal video
grounding, temporal activity localization, and audio description generation,
require "temporally dense" reasoning over frames sampled at high temporal
resolution. However, computing frame-level features for these tasks is
computationally expensive given the temporal resolution requirements. In this
paper, we make three contributions to reduce the cost of computing features for
temporally dense tasks. First, we introduce a vision transformer (ViT)
architecture, dubbed ResidualViT, that leverages the large temporal redundancy
in videos to efficiently compute temporally dense frame-level features. Our
architecture incorporates (i) learnable residual connections that ensure
temporal consistency across consecutive frames and (ii) a token reduction
module that enhances processing speed by selectively discarding temporally
redundant information while reusing weights of a pretrained foundation model.
Second, we propose a lightweight distillation strategy to approximate the
frame-level features of the original foundation model. Finally, we evaluate our
approach across four tasks and five datasets, in both zero-shot and fully
supervised settings, demonstrating significant reductions in computational cost
(up to 60%) and improvements in inference speed (up to 2.5x faster), all while
closely approximating the accuracy of the original foundation model.
</summary>
    <author>
      <name>Mattia Soldan</name>
    </author>
    <author>
      <name>Fabian Caba Heilbron</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <author>
      <name>Josef Sivic</name>
    </author>
    <author>
      <name>Bryan Russell</name>
    </author>
    <link href="http://arxiv.org/abs/2509.13255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13244v1</id>
    <updated>2025-09-16T16:54:35Z</updated>
    <published>2025-09-16T16:54:35Z</published>
    <title>Evaluating LLM Alignment on Personality Inference from Real-World
  Interview Data</title>
    <summary>  Large Language Models (LLMs) are increasingly deployed in roles requiring
nuanced psychological understanding, such as emotional support agents,
counselors, and decision-making assistants. However, their ability to interpret
human personality traits, a critical aspect of such applications, remains
unexplored, particularly in ecologically valid conversational settings. While
prior work has simulated LLM "personas" using discrete Big Five labels on
social media data, the alignment of LLMs with continuous, ground-truth
personality assessments derived from natural interactions is largely
unexamined. To address this gap, we introduce a novel benchmark comprising
semi-structured interview transcripts paired with validated continuous Big Five
trait scores. Using this dataset, we systematically evaluate LLM performance
across three paradigms: (1) zero-shot and chain-of-thought prompting with
GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA
architectures, and (3) regression using static embeddings from pretrained BERT
and OpenAI's text-embedding-3-small. Our results reveal that all Pearson
correlations between model predictions and ground-truth personality traits
remain below 0.26, highlighting the limited alignment of current LLMs with
validated psychological constructs. Chain-of-thought prompting offers minimal
gains over zero-shot, suggesting that personality inference relies more on
latent semantic representation than explicit reasoning. These findings
underscore the challenges of aligning LLMs with complex human attributes and
motivate future work on trait-specific prompting, context-aware modeling, and
alignment-oriented fine-tuning.
</summary>
    <author>
      <name>Jianfeng Zhu</name>
    </author>
    <author>
      <name>Julina Maharjan</name>
    </author>
    <author>
      <name>Xinyu Li</name>
    </author>
    <author>
      <name>Karin G. Coifman</name>
    </author>
    <author>
      <name>Ruoming Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13237v1</id>
    <updated>2025-09-16T16:44:26Z</updated>
    <published>2025-09-16T16:44:26Z</published>
    <title>Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise
  Behaviors</title>
    <summary>  Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.
</summary>
    <author>
      <name>Aniket Didolkar</name>
    </author>
    <author>
      <name>Nicolas Ballas</name>
    </author>
    <author>
      <name>Sanjeev Arora</name>
    </author>
    <author>
      <name>Anirudh Goyal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 Figures, 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.13201v1</id>
    <updated>2025-09-16T16:06:33Z</updated>
    <published>2025-09-16T16:06:33Z</published>
    <title>Scaling Up Throughput-oriented LLM Inference Applications on
  Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management</title>
    <summary>  The widespread growth in LLM developments increasingly demands more
computational power from clusters than what they can supply. Traditional LLM
applications inherently require huge static resource allocations, which force
users to either wait in a long job queue and accept progress delay, or buy
expensive hardware to fulfill their needs and exacerbate the demand-supply
problem. However, not all LLM applications are latency-sensitive and can
instead be executed in a throughput-oriented way. This throughput orientation
allows a dynamic allocation that opportunistically pools available resources
over time, avoiding both the long queue and expensive GPU purchases.
Effectively utilizing opportunistic resources brings numerous challenges
nevertheless. Our solution, pervasive context management, exploits the common
computational context in LLM applications and provides mechanisms and policies
that allow seamless context reuse on opportunistic resources. Our evaluation
shows an LLM application with pervasive context management on opportunistic
resources reduces its execution time by 98.1%.
</summary>
    <author>
      <name>Thanh Son Phung</name>
    </author>
    <author>
      <name>Douglas Thain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.13201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.13201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
