<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-15T00:50:49Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-14T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">108774</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.10636v1</id>
    <updated>2025-03-13T17:59:56Z</updated>
    <published>2025-03-13T17:59:56Z</published>
    <title>The Curse of Conditions: Analyzing and Improving Optimal Transport for
  Conditional Flow-Based Generation</title>
    <summary>  Minibatch optimal transport coupling straightens paths in unconditional flow
matching. This leads to computationally less demanding inference as fewer
integration steps and less complex numerical solvers can be employed when
numerically solving an ordinary differential equation at test time. However, in
the conditional setting, minibatch optimal transport falls short. This is
because the default optimal transport mapping disregards conditions, resulting
in a conditionally skewed prior distribution during training. In contrast, at
test time, we have no access to the skewed prior, and instead sample from the
full, unbiased prior distribution. This gap between training and testing leads
to a subpar performance. To bridge this gap, we propose conditional optimal
transport C^2OT that adds a conditional weighting term in the cost matrix when
computing the optimal transport assignment. Experiments demonstrate that this
simple fix works with both discrete and continuous conditions in
8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method
performs better overall compared to the existing baselines across different
function evaluation budgets. Code is available at
https://hkchengrex.github.io/C2OT
</summary>
    <author>
      <name>Ho Kei Cheng</name>
    </author>
    <author>
      <name>Alexander Schwing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://hkchengrex.github.io/C2OT</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10637v1</id>
    <updated>2025-03-13T17:59:56Z</updated>
    <published>2025-03-13T17:59:56Z</published>
    <title>Distilling Diversity and Control in Diffusion Models</title>
    <summary>  Distilled diffusion models suffer from a critical limitation: reduced sample
diversity compared to their base counterparts. In this work, we uncover that
despite this diversity loss, distilled models retain the fundamental concept
representations of base models. We demonstrate control distillation - where
control mechanisms like Concept Sliders and LoRAs trained on base models can be
seamlessly transferred to distilled models and vice-versa, effectively
distilling control without any retraining. This preservation of
representational structure prompted our investigation into the mechanisms of
diversity collapse during distillation. To understand how distillation affects
diversity, we introduce Diffusion Target (DT) Visualization, an analysis and
debugging tool that reveals how models predict final outputs at intermediate
steps. Through DT-Visualization, we identify generation artifacts,
inconsistencies, and demonstrate that initial diffusion timesteps
disproportionately determine output diversity, while later steps primarily
refine details. Based on these insights, we introduce diversity distillation -
a hybrid inference approach that strategically employs the base model for only
the first critical timestep before transitioning to the efficient distilled
model. Our experiments demonstrate that this simple modification not only
restores the diversity capabilities from base to distilled models but
surprisingly exceeds it, while maintaining nearly the computational efficiency
of distilled inference, all without requiring additional training or model
modifications. Our code and data are available at
https://distillation.baulab.info
</summary>
    <author>
      <name>Rohit Gandikota</name>
    </author>
    <author>
      <name>David Bau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://distillation.baulab.info</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10630v1</id>
    <updated>2025-03-13T17:59:48Z</updated>
    <published>2025-03-13T17:59:48Z</published>
    <title>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</title>
    <summary>  In this paper, we propose a general framework for universal zero-shot
goal-oriented navigation. Existing zero-shot methods build inference framework
upon large language models (LLM) for specific tasks, which differs a lot in
overall pipeline and fails to generalize across different types of goal.
Towards the aim of universal zero-shot navigation, we propose a uniform graph
representation to unify different goals, including object category, instance
image and text description. We also convert the observation of agent into an
online maintained scene graph. With this consistent scene and goal
representation, we preserve most structural information compared with pure text
and are able to leverage LLM for explicit graph-based reasoning. Specifically,
we conduct graph matching between the scene graph and goal graph at each time
instant and propose different strategies to generate long-term goal of
exploration according to different matching states. The agent first iteratively
searches subgraph of goal when zero-matched. With partial matching, the agent
then utilizes coordinate projection and anchor pair alignment to infer the goal
location. Finally scene graph correction and goal verification are applied for
perfect matching. We also present a blacklist mechanism to enable robust switch
between stages. Extensive experiments on several benchmarks show that our
UniGoal achieves state-of-the-art zero-shot performance on three studied
navigation tasks with a single model, even outperforming task-specific
zero-shot methods and supervised universal methods.
</summary>
    <author>
      <name>Hang Yin</name>
    </author>
    <author>
      <name>Xiuwei Xu</name>
    </author>
    <author>
      <name>Lingqing Zhao</name>
    </author>
    <author>
      <name>Ziwei Wang</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Jiwen Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10625v1</id>
    <updated>2025-03-13T17:59:21Z</updated>
    <published>2025-03-13T17:59:21Z</published>
    <title>LHM: Large Animatable Human Reconstruction Model from a Single Image in
  Seconds</title>
    <summary>  Animatable 3D human reconstruction from a single image is a challenging
problem due to the ambiguity in decoupling geometry, appearance, and
deformation. Recent advances in 3D human reconstruction mainly focus on static
human modeling, and the reliance of using synthetic 3D scans for training
limits their generalization ability. Conversely, optimization-based video
methods achieve higher fidelity but demand controlled capture conditions and
computationally intensive refinement processes. Motivated by the emergence of
large reconstruction models for efficient static reconstruction, we propose LHM
(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars
represented as 3D Gaussian splatting in a feed-forward pass. Our model
leverages a multimodal transformer architecture to effectively encode the human
body positional features and image features with attention mechanism, enabling
detailed preservation of clothing geometry and texture. To further boost the
face identity preservation and fine detail recovery, we propose a head feature
pyramid encoding scheme to aggregate multi-scale features of the head regions.
Extensive experiments demonstrate that our LHM generates plausible animatable
human in seconds without post-processing for face and hands, outperforming
existing methods in both reconstruction accuracy and generalization ability.
</summary>
    <author>
      <name>Lingteng Qiu</name>
    </author>
    <author>
      <name>Xiaodong Gu</name>
    </author>
    <author>
      <name>Peihao Li</name>
    </author>
    <author>
      <name>Qi Zuo</name>
    </author>
    <author>
      <name>Weichao Shen</name>
    </author>
    <author>
      <name>Junfei Zhang</name>
    </author>
    <author>
      <name>Kejie Qiu</name>
    </author>
    <author>
      <name>Weihao Yuan</name>
    </author>
    <author>
      <name>Guanying Chen</name>
    </author>
    <author>
      <name>Zilong Dong</name>
    </author>
    <author>
      <name>Liefeng Bo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://lingtengqiu.github.io/LHM/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10621v1</id>
    <updated>2025-03-13T17:59:01Z</updated>
    <published>2025-03-13T17:59:01Z</published>
    <title>DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model
  for Driving Scenario Understanding</title>
    <summary>  While large multimodal models (LMMs) have demonstrated strong performance
across various Visual Question Answering (VQA) tasks, certain challenges
require complex multi-step reasoning to reach accurate answers. One
particularly challenging task is autonomous driving, which demands thorough
cognitive processing before decisions can be made. In this domain, a sequential
and interpretive understanding of visual cues is essential for effective
perception, prediction, and planning. Nevertheless, common VQA benchmarks often
focus on the accuracy of the final answer while overlooking the reasoning
process that enables the generation of accurate responses. Moreover, existing
methods lack a comprehensive framework for evaluating step-by-step reasoning in
realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new
dataset and benchmark specifically designed to advance step-wise visual
reasoning for autonomous driving. Our benchmark features over 18k VQA examples
in the training set and more than 4k in the test set, covering diverse
questions on perception, prediction, and planning, each enriched with
step-by-step reasoning to ensure logical inference in autonomous driving
scenarios. We further introduce a large multimodal model that is fine-tuned on
our reasoning dataset, demonstrating robust performance in complex driving
scenarios. In addition, we benchmark various open-source and closed-source
methods on our proposed dataset, systematically comparing their reasoning
capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in
final answer accuracy, along with a 3.62% improvement in reasoning score over
the previous best open-source model. Our framework, dataset, and model are
available at https://github.com/ayesha-ishaq/DriveLMM-o1.
</summary>
    <author>
      <name>Ayesha Ishaq</name>
    </author>
    <author>
      <name>Jean Lahoud</name>
    </author>
    <author>
      <name>Ketan More</name>
    </author>
    <author>
      <name>Omkar Thawakar</name>
    </author>
    <author>
      <name>Ritesh Thawkar</name>
    </author>
    <author>
      <name>Dinura Dissanayake</name>
    </author>
    <author>
      <name>Noor Ahsan</name>
    </author>
    <author>
      <name>Yuhao Li</name>
    </author>
    <author>
      <name>Fahad Shahbaz Khan</name>
    </author>
    <author>
      <name>Hisham Cholakkal</name>
    </author>
    <author>
      <name>Ivan Laptev</name>
    </author>
    <author>
      <name>Rao Muhammad Anwer</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, 3 tables, github:
  https://github.com/ayesha-ishaq/DriveLMM-o1</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10616v1</id>
    <updated>2025-03-13T17:56:10Z</updated>
    <published>2025-03-13T17:56:10Z</published>
    <title>OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with
  Transformer</title>
    <summary>  Open-vocabulary multiple object tracking aims to generalize trackers to
unseen categories during training, enabling their application across a variety
of real-world scenarios. However, the existing open-vocabulary tracker is
constrained by its framework structure, isolated frame-level perception, and
insufficient modal interactions, which hinder its performance in
open-vocabulary classification and tracking. In this paper, we propose OVTR
(End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the
first end-to-end open-vocabulary tracker that models motion, appearance, and
category simultaneously. To achieve stable classification and continuous
tracking, we design the CIP (Category Information Propagation) strategy, which
establishes multiple high-level category information priors for subsequent
frames. Additionally, we introduce a dual-branch structure for generalization
capability and deep multimodal interaction, and incorporate protective
strategies in the decoder to enhance performance. Experimental results show
that our method surpasses previous trackers on the open-vocabulary MOT
benchmark while also achieving faster inference speeds and significantly
reducing preprocessing requirements. Moreover, the experiment transferring the
model to another dataset demonstrates its strong adaptability. Models and code
are released at https://github.com/jinyanglii/OVTR.
</summary>
    <author>
      <name>Jinyang Li</name>
    </author>
    <author>
      <name>En Yu</name>
    </author>
    <author>
      <name>Sijia Chen</name>
    </author>
    <author>
      <name>Wenbing Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICLR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10614v1</id>
    <updated>2025-03-13T17:55:58Z</updated>
    <published>2025-03-13T17:55:58Z</published>
    <title>ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style
  Transfer</title>
    <summary>  Style transfer involves transferring the style from a reference image to the
content of a target image. Recent advancements in LoRA-based (Low-Rank
Adaptation) methods have shown promise in effectively capturing the style of a
single image. However, these approaches still face significant challenges such
as content inconsistency, style misalignment, and content leakage. In this
paper, we comprehensively analyze the limitations of the standard diffusion
parameterization, which learns to predict noise, in the context of style
transfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method
that enhances both content and style consistency by optimizing the LoRA weights
to predict the original image rather than noise. We also propose a two-step
training strategy that decouples the learning of content and style from the
reference image. To effectively capture both the global structure and local
details of the content image, we introduce a stepwise loss transition strategy.
Additionally, we present an inference guidance method that enables continuous
control over content and style strengths during inference. Through both
qualitative and quantitative evaluations, our method demonstrates significant
improvements in content and style consistency while effectively reducing
content leakage.
</summary>
    <author>
      <name>Bolin Chen</name>
    </author>
    <author>
      <name>Baoquan Zhao</name>
    </author>
    <author>
      <name>Haoran Xie</name>
    </author>
    <author>
      <name>Yi Cai</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Xudong Mao</name>
    </author>
    <link href="http://arxiv.org/abs/2503.10614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10608v1</id>
    <updated>2025-03-13T17:52:43Z</updated>
    <published>2025-03-13T17:52:43Z</published>
    <title>Hierarchical Bayesian inference for uncertainty quantification of
  thermal grease rheology</title>
    <summary>  Rheologically complex soft solids such as thermal greases consist of filler
particles within a polymer matrix. These materials find applications in
improving the conformity of solid-solid contacts and enhancing heat transfer.
Complex soft solids exhibit a transient non-Newtonian rheological response,
including thixotropy and viscoelasticity. Previously, stress relaxation and
buildup in sheared commercial thermal greases were successfully captured using
a nonlinear elasto-visco-plastic (NEVP) model and a thixo-elasto-visco-plastic
(TEVP). However, the previous model calibration methods ignored parameter
uncertainty, providing only single values of the rheological parameters, and
did not quantitatively address the chosen model's identifiability from the data
or credibility of the calibration. We address these limitations via
hierarchical Bayesian inference, accounting for uncertainties arising from
epistemic and aleatoric sources. Importantly, the hierarchical approach allows
us to assimilate experiments measuring the stress responses at various startup
shear rates by allowing the models' parameters to vary across different shear
rates. Then, a global distribution and the associated uncertainty are obtained
by pooling. We also propagate uncertainties to the transient shear stress
response predicted by the models. Overall, we demonstrate that the chosen NEVP
and NEVP models are identifiable from rheometric startup data. However, for the
TEVP model, the uncertainty of the parameters is lower (narrower distributions)
when higher shear rates are used for inference.
</summary>
    <author>
      <name>Pranay P. Nagrani</name>
    </author>
    <author>
      <name>Akshay J. Thomas</name>
    </author>
    <author>
      <name>Amy M. Marconnet</name>
    </author>
    <author>
      <name>Ivan C. Christov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10602v1</id>
    <updated>2025-03-13T17:46:06Z</updated>
    <published>2025-03-13T17:46:06Z</published>
    <title>TruthPrInt: Mitigating LVLM Object Hallucination Via Latent
  Truthful-Guided Pre-Intervention</title>
    <summary>  Object Hallucination (OH) has been acknowledged as one of the major
trustworthy challenges in Large Vision-Language Models (LVLMs). Recent
advancements in Large Language Models (LLMs) indicate that internal states,
such as hidden states, encode the "overall truthfulness" of generated
responses. However, it remains under-explored how internal states in LVLMs
function and whether they could serve as "per-token" hallucination indicators,
which is essential for mitigating OH. In this paper, we first conduct an
in-depth exploration of LVLM internal states in relation to OH issues and
discover that (1) LVLM internal states are high-specificity per-token
indicators of hallucination behaviors. Moreover, (2) different LVLMs encode
universal patterns of hallucinations in common latent subspaces, indicating
that there exist "generic truthful directions" shared by various LVLMs. Based
on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)
that first learns the truthful direction of LVLM decoding and then applies
truthful-guided inference-time intervention during LVLM decoding. We further
propose ComnHallu to enhance both cross-LVLM and cross-data hallucination
detection transferability by constructing and aligning hallucination latent
subspaces. We evaluate TruthPrInt in extensive experimental settings, including
in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.
Experimental results indicate that TruthPrInt significantly outperforms
state-of-the-art methods. Codes will be available at
https://github.com/jinhaoduan/TruthPrInt.
</summary>
    <author>
      <name>Jinhao Duan</name>
    </author>
    <author>
      <name>Fei Kong</name>
    </author>
    <author>
      <name>Hao Cheng</name>
    </author>
    <author>
      <name>James Diffenderfer</name>
    </author>
    <author>
      <name>Bhavya Kailkhura</name>
    </author>
    <author>
      <name>Lichao Sun</name>
    </author>
    <author>
      <name>Xiaofeng Zhu</name>
    </author>
    <author>
      <name>Xiaoshuang Shi</name>
    </author>
    <author>
      <name>Kaidi Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures, the first two authors contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10591v1</id>
    <updated>2025-03-13T17:40:28Z</updated>
    <published>2025-03-13T17:40:28Z</published>
    <title>Analysis and sample-size determination for $2^K$ audit experiments with
  binary response and application to identification of effect of racial
  discrimination on access to justice</title>
    <summary>  Social scientists have increasingly turned to audit experiments to
investigate discrimination in the market for jobs, loans, housing and other
opportunities. In a typical audit experiment, researchers assign ``signals''
(the treatment) to subjects at random and compare success rates across
treatment conditions. In the recent past there has been increased interest in
using randomized multifactor designs for audit experiments, popularly called
factorial experiments, in which combinations of multiple signals are assigned
to subjects. Although social scientists have manipulated multiple factors like
race, gender and income, the analyses have been mostly exploratory in nature.
In this paper we lay out a comprehensive methodology for design and analysis of
$2^K$ factorial designs with binary response using model-free,
randomization-based Neymanian inference and demonstrate its application by
analyzing the audit experiment reported in Libgober (2020). Specifically, we
integrate and extend several sections of the randomization-based,
finite-population literature for binary outcomes, including sample size and
power calculations, and non-linear factorial estimators, extending results.
</summary>
    <author>
      <name>Nicole Pashley</name>
    </author>
    <author>
      <name>Brian Libgober</name>
    </author>
    <author>
      <name>Tirthankar Dasgupta</name>
    </author>
    <link href="http://arxiv.org/abs/2503.10591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
