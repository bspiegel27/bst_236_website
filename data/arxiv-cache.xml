<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-07-14T01:03:26Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-13T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">116692</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.07996v1</id>
    <updated>2025-07-10T17:59:53Z</updated>
    <published>2025-07-10T17:59:53Z</published>
    <title>Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs</title>
    <summary>  Can a pretrained neural network adapt its architecture to different inputs
without any finetuning? Do we need all layers for simple tasks, and are they
adequate for challenging tasks? We found that the layers of a pretrained large
language model (LLM) can be manipulated as separate modules to build a better
and even shallower model customized for each test sample. In particular, each
layer from the pretrained model can be skipped/pruned or repeated multiple
times as recurrent neural networks (RNN), and stacked with others in arbitrary
orders, yielding a chain-of-layers (CoLa) per sample. This compositional space
greatly expands the scope of existing works on looped/recurrent pretrained
modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree
Search (MCTS) protocol to explore and identify the optimal CoLa for each sample
from math and commonsense reasoning benchmarks. Compared to a static model of a
fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same
layer(s) (slow thinking), and combining both, offering more flexible, dynamic
architectures for different inputs. We conduct an extensive analysis of the
MCTS-optimized CoLa, which leads to two key findings: (1) For &gt;75% of samples
with correct predictions by the original LLM, we can find shorter CoLa,
suggesting a large space for improving inference efficiency; (2) For &gt;60% of
samples with originally incorrect predictions, we can identify CoLa achieving
correct predictions, suggesting a large space of performance enhancement. Our
results highlight the shortcomings of using a fixed architecture of pre-trained
LLMs for inference on different samples and pave the way to unlock the
generalization power of test-time depth adaptation.
</summary>
    <author>
      <name>Ziyue Li</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Tianyi Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07992v1</id>
    <updated>2025-07-10T17:59:12Z</updated>
    <published>2025-07-10T17:59:12Z</published>
    <title>Correlations and quantum circuits with dynamical causal order</title>
    <summary>  Requiring that the causal structure between different parties is well-defined
imposes constraints on the correlations they can establish, which define
so-called causal correlations. Some of these are known to have a "dynamical"
causal order in the sense that their causal structure is not fixed a priori but
is instead established on the fly, with for instance the causal order between
future parties depending on some choice of action of parties in the past. Here
we identify a new way that the causal order between the parties can be
dynamical: with at least four parties, there can be some dynamical order which
can nevertheless not be influenced by the actions of past parties. This leads
us to introduce an intermediate class of correlations with what we call
non-influenceable causal order, in between the set of correlations with static
(non-dynamical) causal order and the set of general causal correlations. We
then define analogous classes of quantum processes, considering recently
introduced classes of quantum circuits with classical or quantum control of
causal order - the latter being the largest class within the process matrix
formalism known to have a clear interpretation in terms of coherent
superpositions of causal orders. This allows us to formalise precisely in which
sense certain quantum processes can have both indefinite and dynamical causal
order.
</summary>
    <author>
      <name>RaphaÃ«l Mothe</name>
    </author>
    <author>
      <name>Alastair A. Abbott</name>
    </author>
    <author>
      <name>Cyril Branciard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages + 51 pages of appendices, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07981v1</id>
    <updated>2025-07-10T17:55:05Z</updated>
    <published>2025-07-10T17:55:05Z</published>
    <title>Why is Your Language Model a Poor Implicit Reward Model?</title>
    <summary>  Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.
</summary>
    <author>
      <name>Noam Razin</name>
    </author>
    <author>
      <name>Yong Lin</name>
    </author>
    <author>
      <name>Jiarui Yao</name>
    </author>
    <author>
      <name>Sanjeev Arora</name>
    </author>
    <link href="http://arxiv.org/abs/2507.07981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07967v1</id>
    <updated>2025-07-10T17:47:40Z</updated>
    <published>2025-07-10T17:47:40Z</published>
    <title>Synthesizing Sun-as-a-star flare spectra from high-resolution solar
  observations</title>
    <summary>  Spatially resolved observations of the Sun and the astronomical sample size
of stellar bodies are the respective key strengths of solar and stellar
observations. However, the large difference in object brightness between the
Sun and other stars has led to distinctly different instrumentation and
methodologies between the two fields. We produce and analyze synthetic
full-disk spectra derived from 19 small area field-of-view optical observations
of solar flares acquired by the Swedish 1-m Solar Telescope (SST) between 2011
and 2024. These are used to investigate what can and cannot be inferred about
physical processes on the Sun from Sun-as-a-star observations. The recently
released Numerical Empirical Sun-as-a-Star Integrator (NESSI) code provides
synthetic full-disk integrated spectral line emission based on smaller
field-of-view input, accounting for center-to-limb variations and differential
rotation. We use this code to generate pseudo-Sun-as-a-star spectra from the
SST observations. ...
</summary>
    <author>
      <name>M. De Wilde</name>
    </author>
    <author>
      <name>A. G. M. Pietrow</name>
    </author>
    <author>
      <name>M. K. Druett</name>
    </author>
    <author>
      <name>A. Pastor Yabar</name>
    </author>
    <author>
      <name>J. Koza</name>
    </author>
    <author>
      <name>I. Kontogiannis</name>
    </author>
    <author>
      <name>O. Andriienko</name>
    </author>
    <author>
      <name>A. Berlicki</name>
    </author>
    <author>
      <name>A. R. Brunvoll</name>
    </author>
    <author>
      <name>J. de la Cruz RodrÃ­guez</name>
    </author>
    <author>
      <name>J. T. Faber</name>
    </author>
    <author>
      <name>R. Joshi</name>
    </author>
    <author>
      <name>D. Kuridze</name>
    </author>
    <author>
      <name>D. NÃ³brega-Siverio</name>
    </author>
    <author>
      <name>L. H. M. Rouppe van der Voort</name>
    </author>
    <author>
      <name>J. RybÃ¡k</name>
    </author>
    <author>
      <name>E. Scullion</name>
    </author>
    <author>
      <name>A. M. Silva</name>
    </author>
    <author>
      <name>Z. Vashalomidze</name>
    </author>
    <author>
      <name>A. Vicente ArÃ©valo</name>
    </author>
    <author>
      <name>A. WiÅniewska</name>
    </author>
    <author>
      <name>R. Yadav</name>
    </author>
    <author>
      <name>T. V. Zaqarashvili</name>
    </author>
    <author>
      <name>J. Zbinden</name>
    </author>
    <author>
      <name>E. S. Ãyre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in A&amp;A. 22 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07964v1</id>
    <updated>2025-07-10T17:44:27Z</updated>
    <published>2025-07-10T17:44:27Z</published>
    <title>Gravitational lensing rarely produces high-mass outliers to the compact
  binary population</title>
    <summary>  All gravitational-wave signals are inevitably gravitationally lensed by
intervening matter as they propagate through the Universe. When a
gravitational-wave signal is magnified, it appears to have originated from a
closer, more massive system. Thus, high-mass outliers to the gravitational-wave
source population are often proposed as natural candidates for strongly lensed
events. However, when using a data-driven method for identifying population
outliers, we find that high-mass outliers are not necessarily strongly lensed,
nor will the majority of strongly-lensed signals appear as high-mass outliers.
This is both because statistical fluctuations produce a larger effect on
observed binary parameters than does lensing magnification, and because
lensing-induced outliers must originate from intrinsically high-mass sources,
which are rare. Thus, the appearance of a single lensing-induced outlier
implies the existence of many other lensed events within the catalog. We
additionally show that it is possible to constrain the strong lensing optical
depth, which is a fundamental quantity of our Universe, with the detection or
absence of high-mass outliers. However, constraints using the latest
gravitational-wave catalog are weak$\unicode{x2014}$we obtain an upper limit on
the optical depth of sources at redshift $1$ magnified by a factor of $5$ or
more of $\tau(\mu\geq5,z=1)\leq 0.035 \unicode{x2014}$and future observing runs
will not make an outlier-based method competitive with other probes of the
optical depth. Future work will investigate the ability of the full inferred
population of compact binaries to inform the distribution of lenses in the
Universe, opening a unique opportunity to access the high-redshift Universe and
constrain cosmic structures.
</summary>
    <author>
      <name>Amanda Farah</name>
    </author>
    <author>
      <name>Jose MarÃ­a Ezquiaga</name>
    </author>
    <author>
      <name>Maya Fishbach</name>
    </author>
    <author>
      <name>Daniel Holz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures. Code release:
  https://github.com/afarah18/GW-lensing-outliers-public</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07954v1</id>
    <updated>2025-07-10T17:39:03Z</updated>
    <published>2025-07-10T17:39:03Z</published>
    <title>Input Conditioned Layer Dropping in Speech Foundation Models</title>
    <summary>  Curating foundation speech models for edge and IoT settings, where
computational resources vary over time, requires dynamic architectures
featuring adaptable reduction strategies. One emerging approach is layer
dropping ($\mathcal{LD}$) which skips fraction of the layers of a backbone
network during inference to reduce the computational load. This allows
transforming static models into dynamic ones. However, existing approaches
exhibit limitations either in the mode of selecting layers or by significantly
modifying the neural architecture. To this end, we propose input-driven
$\mathcal{LD}$ that employs the network's input features and a lightweight
layer selecting network to determine the optimum combination of processing
layers. Extensive experimentation on 4 speech and audio public benchmarks,
using two different pre-trained foundation models, demonstrates the
effectiveness of our approach, thoroughly outperforming random dropping and
producing on-par (or better) results to early exit.
</summary>
    <author>
      <name>Abdul Hannan</name>
    </author>
    <author>
      <name>Daniele Falavigna</name>
    </author>
    <author>
      <name>Alessio Brutti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE MLSP 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07949v1</id>
    <updated>2025-07-10T17:33:52Z</updated>
    <published>2025-07-10T17:33:52Z</published>
    <title>TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient
  Human Activity Recognition on Edge Devices</title>
    <summary>  Human Activity Recognition (HAR) on resource-constrained wearable devices
demands inference models that harmonize accuracy with computational efficiency.
This paper introduces TinierHAR, an ultra-lightweight deep learning
architecture that synergizes residual depthwise separable convolutions, gated
recurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency
without compromising performance. Evaluated across 14 public HAR datasets,
TinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.
DeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the
averaged F1-scores. Beyond quantitative gains, this work provides the first
systematic ablation study dissecting the contributions of spatial-temporal
components across proposed TinierHAR, prior SOTA TinyHAR, and the classical
DeepConvLSTM, offering actionable insights for designing efficient HAR systems.
We finally discussed the findings and suggested principled design guidelines
for future efficient HAR. To catalyze edge-HAR research, we open-source all
materials in this work for future
benchmarking\footnote{https://github.com/zhaxidele/TinierHAR}
</summary>
    <author>
      <name>Sizhen Bian</name>
    </author>
    <author>
      <name>Mengxi Liu</name>
    </author>
    <author>
      <name>Vitor Fortes Rey</name>
    </author>
    <author>
      <name>Daniel Geissler</name>
    </author>
    <author>
      <name>Paul Lukowicz</name>
    </author>
    <link href="http://arxiv.org/abs/2507.07949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07948v1</id>
    <updated>2025-07-10T17:33:28Z</updated>
    <published>2025-07-10T17:33:28Z</published>
    <title>Physics-Informed Gaussian Process Inference of Liquid Structure from
  Scattering Data</title>
    <summary>  We present a nonparametric Bayesian framework to infer radial distribution
functions from experimental scattering measurements with uncertainty
quantification using non-stationary Gaussian processes. The Gaussian process
prior mean and kernel functions are designed to mitigate well-known numerical
challenges with the Fourier transform, including discrete measurement binning
and detector windowing, while encoding fundamental yet minimal physical
knowledge of liquid structure. We demonstrate uncertainty propagation of the
Gaussian process posterior to unmeasured quantities of interest. The
methodology is applied to liquid argon and water as a proof of principle. The
full implementation is available on GitHub at
https://github.com/hoepfnergroup/LiquidStructureGP-Sullivan.
</summary>
    <author>
      <name>Harry W. Sullivan</name>
    </author>
    <author>
      <name>Brennon L. Shanks</name>
    </author>
    <author>
      <name>Matej Cervenka</name>
    </author>
    <author>
      <name>Michael P. Hoepfner</name>
    </author>
    <link href="http://arxiv.org/abs/2507.07948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07941v1</id>
    <updated>2025-07-10T17:27:04Z</updated>
    <published>2025-07-10T17:27:04Z</published>
    <title>Late Fusion Multi-task Learning for Semiparametric Inference with
  Nuisance Parameters</title>
    <summary>  In the age of large and heterogeneous datasets, the integration of
information from diverse sources is essential to improve parameter estimation.
Multi-task learning offers a powerful approach by enabling simultaneous
learning across related tasks. In this work, we introduce a late fusion
framework for multi-task learning with semiparametric models that involve
infinite-dimensional nuisance parameters, focusing on applications such as
heterogeneous treatment effect estimation across multiple data sources,
including electronic health records from different hospitals or clinical trial
data. Our framework is two-step: first, initial double machine-learning
estimators are obtained through individual task learning; second, these
estimators are adaptively aggregated to exploit task similarities while
remaining robust to task-specific differences. In particular, the framework
avoids individual level data sharing, preserving privacy. Additionally, we
propose a novel multi-task learning method for nuisance parameter estimation,
which further enhances parameter estimation when nuisance parameters exhibit
similarity across tasks. We establish theoretical guarantees for the method,
demonstrating faster convergence rates compared to individual task learning
when tasks share similar parametric components. Extensive simulations and real
data applications complement the theoretical findings of our work while
highlight the effectiveness of our framework even in moderate sample sizes.
</summary>
    <author>
      <name>Sohom Bhattacharya</name>
    </author>
    <author>
      <name>Yongzhuo Chen</name>
    </author>
    <author>
      <name>Muxuan Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07932v1</id>
    <updated>2025-07-10T17:10:51Z</updated>
    <published>2025-07-10T17:10:51Z</published>
    <title>KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based
  Auto-Scaling</title>
    <summary>  Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.
</summary>
    <author>
      <name>Guilin Zhang</name>
    </author>
    <author>
      <name>Wulan Guo</name>
    </author>
    <author>
      <name>Ziqi Tan</name>
    </author>
    <author>
      <name>Qiang Guan</name>
    </author>
    <author>
      <name>Hailong Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
