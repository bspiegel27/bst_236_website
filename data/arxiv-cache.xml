<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-01-13T00:58:45Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-01-13T00:58:45Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>130513</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2601.06022v1</id>
    <title>AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs</title>
    <updated>2026-01-09T18:58:22Z</updated>
    <link href="https://arxiv.org/abs/2601.06022v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.06022v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T18:58:22Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Chengming Cui</name>
    </author>
    <author>
      <name>Tianxin Wei</name>
    </author>
    <author>
      <name>Ziyi Chen</name>
    </author>
    <author>
      <name>Ruizhong Qiu</name>
    </author>
    <author>
      <name>Zhichen Zeng</name>
    </author>
    <author>
      <name>Zhining Liu</name>
    </author>
    <author>
      <name>Xuying Ning</name>
    </author>
    <author>
      <name>Duo Zhou</name>
    </author>
    <author>
      <name>Jingrui He</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.06006v1</id>
    <title>Discriminative-Generative Target Speaker Extraction with Decoder-Only Language Models</title>
    <updated>2026-01-09T18:41:12Z</updated>
    <link href="https://arxiv.org/abs/2601.06006v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.06006v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Target speaker extraction (TSE) aims to recover the speech signal of a desired speaker from a mixed audio recording, given a short enrollment utterance. Most existing TSE approaches are based on discriminative modeling paradigms. Although effective at suppressing interfering speakers, these methods often struggle to produce speech with high perceptual quality and naturalness. To address this limitation, we first propose LauraTSE, a generative TSE model built upon an auto-regressive decoder-only language model. However, purely generative approaches may suffer from hallucinations, content drift, and limited controllability, which may undermine their reliability in complex acoustic scenarios. To overcome these challenges, we further introduce a discriminative-generative TSE framework. In this framework, a discriminative front-end is employed to robustly extract the target speaker's speech, yielding stable and controllable intermediate representations. A generative back-end then operates in the neural audio codec representation space to reconstruct fine-grained speech details and enhance perceptual quality. This two-stage design effectively combines the robustness and controllability of discriminative models with the superior naturalness and quality enhancement capabilities of generative models. Moreover, we systematically investigate collaborative training strategies for the proposed framework, including freezing or fine-tuning the front-end, incorporating an auxiliary SI-SDR loss, and exploring both auto-regressive and non-auto-regressive inference mechanisms. Experimental results demonstrate that the proposed framework achieves a more favorable trade-off among speech quality, intelligibility, and speaker consistency.</summary>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T18:41:12Z</published>
    <arxiv:comment>16 pages,6 figures</arxiv:comment>
    <arxiv:primary_category term="eess.AS"/>
    <author>
      <name>Bang Zeng</name>
    </author>
    <author>
      <name>Beilong Tang</name>
    </author>
    <author>
      <name>Wang Xiang</name>
    </author>
    <author>
      <name>Ming Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05999v1</id>
    <title>Constraining Hamiltonians from chiral effective field theory with neutron-star data</title>
    <updated>2026-01-09T18:33:25Z</updated>
    <link href="https://arxiv.org/abs/2601.05999v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05999v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multi-messenger observations of neutron stars (NSs) and their mergers have placed strong constraints on the dense-matter equation of state (EOS). The EOS, in turn, depends on microscopic nuclear interactions that are described by nuclear Hamiltonians. These Hamiltonians are commonly derived within chiral effective field theory (EFT). Ideally, multi-messenger observations of NSs could be used to directly inform our understanding of EFT interactions, but such a direct inference necessitates millions of model evaluations. This is computationally prohibitive because each evaluation requires us to calculate the EOS from a Hamiltonian by solving the quantum many-body problem with methods such as auxiliary-field diffusion Monte Carlo (AFDMC), which provides very accurate and precise solutions but at a significant computational cost. Additionally, we need to solve the stellar structure equations for each EOS which further slows down each model evaluation by a few seconds. In this work, we combine emulators for AFDMC calculations of neutron matter, built using parametric matrix models, and for the stellar structure equations, built using multilayer perceptron neural networks, with the \texttt{PyCBC} data-analysis framework to enable a direct inference of coupling constants in an EFT Hamiltonian using multi-messenger observations of NSs. We find that astrophysical data can provide informative constraints on two-nucleon couplings despite the high densities probed in NS interiors.</summary>
    <category term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T18:33:25Z</published>
    <arxiv:comment>8 pages, 4 figures, contains supplemental material. Comments welcome</arxiv:comment>
    <arxiv:primary_category term="nucl-th"/>
    <author>
      <name>Cassandra L. Armstrong</name>
    </author>
    <author>
      <name>Brendan T. Reed</name>
    </author>
    <author>
      <name>Tate Plohr</name>
    </author>
    <author>
      <name>Henrik Rose</name>
    </author>
    <author>
      <name>Soumi De</name>
    </author>
    <author>
      <name>Rahul Somasundaram</name>
    </author>
    <author>
      <name>Ingo Tews</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05987v1</id>
    <title>The Causal Effect of First-Time Academic Failure on University Dropout: Evidence from a Regression Discontinuity Design</title>
    <updated>2026-01-09T18:08:15Z</updated>
    <link href="https://arxiv.org/abs/2601.05987v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05987v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>University dropout remains a persistent challenge in higher education systems, yet causal evidence on the mechanisms triggering early disengagement is limited. This study estimates the causal effect of first-time academic failure on subsequent university attrition. Exploiting a sharp institutional grading threshold on a 0-10 scale, we implement a regression discontinuity design (RDD) comparing students who narrowly fail to those who narrowly pass their first attempt. Using longitudinal administrative data spanning multiple cohorts and degree programmes, we estimate local average treatment effects (LATE) for students at the margin of success and examine dropout outcomes within 12 and 24 months following the initial evaluation. Contrary to conventional assumptions, the results indicate that marginal first-time failure is associated with a lower probability of subsequent dropout relative to marginal passing at both horizons. A comprehensive battery of robustness checks - including donut RDD specifications, placebo cutoffs, and formal density tests - supports the validity of the identification strategy. These findings suggest that early academic failure may function as a salient signal that prompts behavioural adjustment or reorientation, while marginal passing may sustain a state of "fragile persistence". The study provides causal evidence on the non-linear effects of early academic performance and highlights the importance of carefully designed institutional responses at critical evaluation thresholds.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T18:08:15Z</published>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>H. R. Paz</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05981v1</id>
    <title>Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation</title>
    <updated>2026-01-09T18:00:49Z</updated>
    <link href="https://arxiv.org/abs/2601.05981v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05981v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T18:00:49Z</published>
    <arxiv:comment>Accepted by ieee transactions on Medical Imaging</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yinsong Wang</name>
    </author>
    <author>
      <name>Xinzhe Luo</name>
    </author>
    <author>
      <name>Siyi Du</name>
    </author>
    <author>
      <name>Chen Qin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05975v1</id>
    <title>DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management</title>
    <updated>2026-01-09T17:47:32Z</updated>
    <link href="https://arxiv.org/abs/2601.05975v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05975v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous "ragged filtration" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s "CTA (Commodity Trading Advisor) Winter" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.</summary>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T17:47:32Z</published>
    <arxiv:primary_category term="q-fin.TR"/>
    <author>
      <name>Kieran Wood</name>
    </author>
    <author>
      <name>Stephen J. Roberts</name>
    </author>
    <author>
      <name>Stefan Zohren</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05967v1</id>
    <title>The Linear Point Standard Ruler with DESI DR1 and DR2 Data</title>
    <updated>2026-01-09T17:39:54Z</updated>
    <link href="https://arxiv.org/abs/2601.05967v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05967v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The linear point, a purely geometric feature in the monopole of the two-point correlation function, has been proposed as an alternative standard ruler. Compared to the peak in the correlation function, it is more robust to late-time nonlinear effects at the percent level. In light of improved simulations and high quality data, we revisit the robustness of the linear point and use it as an alternative to template-based fitting approaches typically used in BAO analyses. We present the linear point measurements on galaxy samples from the first and second data releases (DR1 and DR2) of the DESI survey. We convert the linear point into a dimensionless parameter $α_{iso,LP}$, defined as the ratio of the linear point in the fiducial cosmology and the observed value, analogous to the isotropic BAO scaling parameter $α_{iso}$ used in previous BAO measurements. Using the 2nd generation of AbacusSummit mock catalogs, we find that linear point measurements are more precise when calculated in the post-reconstruction regime with 15-60% smaller uncertainties than those pre-reconstruction. We find a systematic shift in the linear point measurements compared against the isotropic BAO measurements in mocks; we attribute this to the isotropic damping parameter responsible for smearing the linear point in the nonlinear regime. We propose a sample-dependent correction that mitigates the impact of late-time nonlinear effects. While this introduces a cosmology dependence in an otherwise model-independent measurement, this is necessary given the sub-percent precision dictated by current cosmological surveys. Comparing $α_{iso,LP}$ with isotropic BAO measurements made on the DESI DR1 and DR2 galaxy samples, we find excellent agreement after applying this correction, particularly post-reconstruction. We discuss future scope regarding cosmological inference with linear point measurements.</summary>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T17:39:54Z</published>
    <arxiv:comment>43 pages, 17 figures, 7 tables</arxiv:comment>
    <arxiv:primary_category term="astro-ph.CO"/>
    <author>
      <name>N. Uberoi</name>
    </author>
    <author>
      <name>F. Nikakhtar</name>
    </author>
    <author>
      <name>N. Padmanabhan</name>
    </author>
    <author>
      <name>R. K. Sheth</name>
    </author>
    <author>
      <name>J. Aguilar</name>
    </author>
    <author>
      <name>S. Ahlen</name>
    </author>
    <author>
      <name>D. Bianchi</name>
    </author>
    <author>
      <name>D. Brooks</name>
    </author>
    <author>
      <name>F. J. Castander</name>
    </author>
    <author>
      <name>T. Claybaugh</name>
    </author>
    <author>
      <name>A. Cuceu</name>
    </author>
    <author>
      <name>A. de la Macorra</name>
    </author>
    <author>
      <name>A. Dey</name>
    </author>
    <author>
      <name>B. Dey</name>
    </author>
    <author>
      <name>P. Doel</name>
    </author>
    <author>
      <name>J. E. Forero-Romero</name>
    </author>
    <author>
      <name>E. Gaztañaga</name>
    </author>
    <author>
      <name>S. Gontcho A Gontcho</name>
    </author>
    <author>
      <name>G. Gutierrez</name>
    </author>
    <author>
      <name>K. Honscheid</name>
    </author>
    <author>
      <name>C. Howlett</name>
    </author>
    <author>
      <name>M. Ishak</name>
    </author>
    <author>
      <name>R. Joyce</name>
    </author>
    <author>
      <name>D. Kirkby</name>
    </author>
    <author>
      <name>T. Kisner</name>
    </author>
    <author>
      <name>O. Lahav</name>
    </author>
    <author>
      <name>C. Lamman</name>
    </author>
    <author>
      <name>M. Landriau</name>
    </author>
    <author>
      <name>L. Le Guillou</name>
    </author>
    <author>
      <name>M. Manera</name>
    </author>
    <author>
      <name>P. Martini</name>
    </author>
    <author>
      <name>A. Meisner</name>
    </author>
    <author>
      <name>R. Miquel</name>
    </author>
    <author>
      <name>S. Nadathur</name>
    </author>
    <author>
      <name>W. J. Percival</name>
    </author>
    <author>
      <name>C. Poppett</name>
    </author>
    <author>
      <name>F. Prada</name>
    </author>
    <author>
      <name>I. Pérez-Ràfols</name>
    </author>
    <author>
      <name>G. Rossi</name>
    </author>
    <author>
      <name>L. Samushia</name>
    </author>
    <author>
      <name>E. Sanchez</name>
    </author>
    <author>
      <name>D. Schlegel</name>
    </author>
    <author>
      <name>M. Schubnell</name>
    </author>
    <author>
      <name>J. Silber</name>
    </author>
    <author>
      <name>D. Sprayberry</name>
    </author>
    <author>
      <name>G. Tarlé</name>
    </author>
    <author>
      <name>B. A. Weaver</name>
    </author>
    <author>
      <name>H. Zou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05966v1</id>
    <title>VideoAR: Autoregressive Video Generation via Next-Frame &amp; Scale Prediction</title>
    <updated>2026-01-09T17:34:59Z</updated>
    <link href="https://arxiv.org/abs/2601.05966v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05966v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T17:34:59Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Longbin Ji</name>
    </author>
    <author>
      <name>Xiaoxiong Liu</name>
    </author>
    <author>
      <name>Junyuan Shang</name>
    </author>
    <author>
      <name>Shuohuan Wang</name>
    </author>
    <author>
      <name>Yu Sun</name>
    </author>
    <author>
      <name>Hua Wu</name>
    </author>
    <author>
      <name>Haifeng Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05964v1</id>
    <title>Negative binomial models for development triangles of counts</title>
    <updated>2026-01-09T17:34:16Z</updated>
    <link href="https://arxiv.org/abs/2601.05964v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05964v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prediction of outstanding claims has been done via nonparametric models (chain ladder), semiparametric models (overdispersed poisson) or fully parametric models. In this paper, we propose models based on negative binomial distributions for the prediction of outstanding number of claims, which are particularly useful to account for overdispersion. We first assume independence of random variables and introduce appropriate notation. Later, we generalise the model to account for dependence across development years. In both cases, the marginal distributions are negative binomials. We study the properties of the models and carry out bayesian inference. We illustrate the performance of the models with simulated and real datasets.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T17:34:16Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Luis E. Nieto-Barajas</name>
    </author>
    <author>
      <name>Rodrigo S. Targino</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.05960v1</id>
    <title>Distilling Feedback into Memory-as-a-Tool</title>
    <updated>2026-01-09T17:26:52Z</updated>
    <link href="https://arxiv.org/abs/2601.05960v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.05960v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-09T17:26:52Z</published>
    <arxiv:comment>Code: https://github.com/vicgalle/feedback-memory-as-a-tool Data: https://huggingface.co/datasets/vicgalle/rubric-feedback-bench</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Víctor Gallego</name>
    </author>
  </entry>
</feed>
