<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-29T01:05:47Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-29T01:05:47Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>129489</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.21338v1</id>
    <title>HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</title>
    <updated>2025-12-24T18:59:58Z</updated>
    <link href="https://arxiv.org/abs/2512.21338v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21338v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T18:59:58Z</published>
    <arxiv:comment>Project Page: http://haonanqiu.com/projects/HiStream.html</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haonan Qiu</name>
    </author>
    <author>
      <name>Shikun Liu</name>
    </author>
    <author>
      <name>Zijian Zhou</name>
    </author>
    <author>
      <name>Zhaochong An</name>
    </author>
    <author>
      <name>Weiming Ren</name>
    </author>
    <author>
      <name>Zhiheng Liu</name>
    </author>
    <author>
      <name>Jonas Schult</name>
    </author>
    <author>
      <name>Sen He</name>
    </author>
    <author>
      <name>Shoufa Chen</name>
    </author>
    <author>
      <name>Yuren Cong</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
    <author>
      <name>Juan-Manuel Perez-Rua</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21335v1</id>
    <title>Autonomous Uncertainty Quantification for Computational Point-of-care Sensors</title>
    <updated>2025-12-24T18:59:47Z</updated>
    <link href="https://arxiv.org/abs/2512.21335v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21335v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities. These systems can utilize neural network-based algorithms to accurately infer a diagnosis from the signals generated by rapid diagnostic tests or sensors. However, neural network-based diagnostic models are subject to hallucinations and can produce erroneous predictions, posing a risk of misdiagnosis and inaccurate clinical decisions. To address this challenge, here we present an autonomous uncertainty quantification technique developed for POC diagnostics. As our testbed, we used a paper-based, computational vertical flow assay (xVFA) platform developed for rapid POC diagnosis of Lyme disease, the most prevalent tick-borne disease globally. The xVFA platform integrates a disposable paper-based assay, a handheld optical reader and a neural network-based inference algorithm, providing rapid and cost-effective Lyme disease diagnostics in under 20 min using only 20 uL of patient serum. By incorporating a Monte Carlo dropout (MCDO)-based uncertainty quantification approach into the diagnostics pipeline, we identified and excluded erroneous predictions with high uncertainty, significantly improving the sensitivity and reliability of the xVFA in an autonomous manner, without access to the ground truth diagnostic information of patients. Blinded testing using new patient samples demonstrated an increase in diagnostic sensitivity from 88.2% to 95.7%, indicating the effectiveness of MCDO-based uncertainty quantification in enhancing the robustness of neural network-driven computational POC sensing systems.</summary>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T18:59:47Z</published>
    <arxiv:comment>18 Pages, 5 Figures</arxiv:comment>
    <arxiv:primary_category term="physics.med-ph"/>
    <author>
      <name>Artem Goncharov</name>
    </author>
    <author>
      <name>Rajesh Ghosh</name>
    </author>
    <author>
      <name>Hyou-Arm Joung</name>
    </author>
    <author>
      <name>Dino Di Carlo</name>
    </author>
    <author>
      <name>Aydogan Ozcan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21333v1</id>
    <title>Fast SAM2 with Text-Driven Token Pruning</title>
    <updated>2025-12-24T18:59:05Z</updated>
    <link href="https://arxiv.org/abs/2512.21333v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21333v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T18:59:05Z</published>
    <arxiv:comment>28 pages, 9 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Avilasha Mandal</name>
    </author>
    <author>
      <name>Chaoning Zhang</name>
    </author>
    <author>
      <name>Fachrina Dewi Puspitasari</name>
    </author>
    <author>
      <name>Xudong Wang</name>
    </author>
    <author>
      <name>Jiaquan Zhang</name>
    </author>
    <author>
      <name>Caiyan Qin</name>
    </author>
    <author>
      <name>Guoqing Wang</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Heng Tao Shen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21332v1</id>
    <title>C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling</title>
    <updated>2025-12-24T18:59:01Z</updated>
    <link href="https://arxiv.org/abs/2512.21332v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21332v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T18:59:01Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jin Qin</name>
    </author>
    <author>
      <name>Zihan Liao</name>
    </author>
    <author>
      <name>Ziyin Zhang</name>
    </author>
    <author>
      <name>Hang Yu</name>
    </author>
    <author>
      <name>Peng Di</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21311v1</id>
    <title>Learning to Solve PDEs on Neural Shape Representations</title>
    <updated>2025-12-24T18:14:02Z</updated>
    <link href="https://arxiv.org/abs/2512.21311v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21311v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T18:14:02Z</published>
    <arxiv:comment>Article webpage link: https://welschinger.github.io/Learning-to-Solve-PDEs-on-Neural-Shape-Representations/</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Lilian Welschinger</name>
    </author>
    <author>
      <name>Yilin Liu</name>
    </author>
    <author>
      <name>Zican Wang</name>
    </author>
    <author>
      <name>Niloy Mitra</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21284v1</id>
    <title>Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential</title>
    <updated>2025-12-24T17:05:09Z</updated>
    <link href="https://arxiv.org/abs/2512.21284v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21284v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T17:05:09Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shihao Zou</name>
    </author>
    <author>
      <name>Jingjing Li</name>
    </author>
    <author>
      <name>Wei Ji</name>
    </author>
    <author>
      <name>Jincai Huang</name>
    </author>
    <author>
      <name>Kai Wang</name>
    </author>
    <author>
      <name>Guo Dan</name>
    </author>
    <author>
      <name>Weixin Si</name>
    </author>
    <author>
      <name>Yi Pan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21280v1</id>
    <title>SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance</title>
    <updated>2025-12-24T16:59:04Z</updated>
    <link href="https://arxiv.org/abs/2512.21280v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21280v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T16:59:04Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Divij Dudeja</name>
    </author>
    <author>
      <name>Mayukha Pal</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21276v1</id>
    <title>GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</title>
    <updated>2025-12-24T16:46:04Z</updated>
    <link href="https://arxiv.org/abs/2512.21276v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21276v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T16:46:04Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Snehal Singh Tomar</name>
    </author>
    <author>
      <name>Alexandros Graikos</name>
    </author>
    <author>
      <name>Arjun Krishna</name>
    </author>
    <author>
      <name>Dimitris Samaras</name>
    </author>
    <author>
      <name>Klaus Mueller</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21264v1</id>
    <title>AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI</title>
    <updated>2025-12-24T16:16:09Z</updated>
    <link href="https://arxiv.org/abs/2512.21264v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21264v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T16:16:09Z</published>
    <arxiv:comment>15 pages, 8 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Changwei Wu</name>
    </author>
    <author>
      <name>Yifei Chen</name>
    </author>
    <author>
      <name>Yuxin Du</name>
    </author>
    <author>
      <name>Mingxuan Liu</name>
    </author>
    <author>
      <name>Jinying Zong</name>
    </author>
    <author>
      <name>Beining Wu</name>
    </author>
    <author>
      <name>Jie Dong</name>
    </author>
    <author>
      <name>Feiwei Qin</name>
    </author>
    <author>
      <name>Yunkang Cao</name>
    </author>
    <author>
      <name>Qiyuan Tian</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.21257v1</id>
    <title>ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling</title>
    <updated>2025-12-24T16:06:20Z</updated>
    <link href="https://arxiv.org/abs/2512.21257v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.21257v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics and cross-domain behavioral patterns that Large Language Models have learned from vast corpora.
  To address these challenges, we introduce ReaSeq, a reasoning-enhanced framework that leverages world knowledge in Large Language Models to address both limitations through explicit and implicit reasoning. Specifically, ReaSeq employs explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into semantically enriched item representations, and latent reasoning via Diffusion Large Language Models to infer plausible beyond-log behaviors. Deployed on Taobao's ranking system serving hundreds of millions of users, ReaSeq achieves substantial gains: &gt;6.0% in IPV and CTR, &gt;2.9% in Orders, and &gt;2.5% in GMV, validating the effectiveness of world-knowledge-enhanced reasoning over purely log-driven approaches.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T16:06:20Z</published>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Chuan Wang</name>
    </author>
    <author>
      <name>Gaoming Yang</name>
    </author>
    <author>
      <name>Han Wu</name>
    </author>
    <author>
      <name>Jiakai Tang</name>
    </author>
    <author>
      <name>Jiahao Yu</name>
    </author>
    <author>
      <name>Jian Wu</name>
    </author>
    <author>
      <name>Jianwu Hu</name>
    </author>
    <author>
      <name>Junjun Zheng</name>
    </author>
    <author>
      <name>Shuwen Xiao</name>
    </author>
    <author>
      <name>Yeqiu Yang</name>
    </author>
    <author>
      <name>Yuning Jiang</name>
    </author>
    <author>
      <name>Ahjol Nurlanbek</name>
    </author>
    <author>
      <name>Binbin Cao</name>
    </author>
    <author>
      <name>Bo Zheng</name>
    </author>
    <author>
      <name>Fangmei Zhu</name>
    </author>
    <author>
      <name>Gaoming Zhou</name>
    </author>
    <author>
      <name>Huimin Yi</name>
    </author>
    <author>
      <name>Huiping Chu</name>
    </author>
    <author>
      <name>Jin Huang</name>
    </author>
    <author>
      <name>Jinzhe Shan</name>
    </author>
    <author>
      <name>Kenan Cui</name>
    </author>
    <author>
      <name>Longbin Li</name>
    </author>
    <author>
      <name>Silu Zhou</name>
    </author>
    <author>
      <name>Wen Chen</name>
    </author>
    <author>
      <name>Xia Ming</name>
    </author>
    <author>
      <name>Xiang Gao</name>
    </author>
    <author>
      <name>Xin Yao</name>
    </author>
    <author>
      <name>Xingyu Wen</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Yiwen Hu</name>
    </author>
    <author>
      <name>Yulin Wang</name>
    </author>
    <author>
      <name>Ziheng Bao</name>
    </author>
    <author>
      <name>Zongyuan Wu</name>
    </author>
  </entry>
</feed>
