<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-11T00:50:20Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-10T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">123279</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.08575v1</id>
    <updated>2025-10-09T17:59:59Z</updated>
    <published>2025-10-09T17:59:59Z</published>
    <title>ReSplat: Learning Recurrent Gaussian Splats</title>
    <summary>  While feed-forward Gaussian splatting models provide computational efficiency
and effectively handle sparse input settings, their performance is
fundamentally limited by the reliance on a single forward pass during
inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients. Our key insight is that the Gaussian splatting rendering error
serves as a rich feedback signal, guiding the recurrent network to learn
effective Gaussian updates. This feedback signal naturally adapts to unseen
data distributions at test time, enabling robust generalization. To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a $16 \times$ subsampled space, producing $16 \times$ fewer
Gaussians than previous per-pixel Gaussian models. This substantially reduces
computational overhead and allows for efficient Gaussian updates. Extensive
experiments across varying of input views (2, 8, 16), resolutions ($256 \times
256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed. Our project
page is at https://haofeixu.github.io/resplat/.
</summary>
    <author>
      <name>Haofei Xu</name>
    </author>
    <author>
      <name>Daniel Barath</name>
    </author>
    <author>
      <name>Andreas Geiger</name>
    </author>
    <author>
      <name>Marc Pollefeys</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://haofeixu.github.io/resplat/</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.08575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08569v1</id>
    <updated>2025-10-09T17:59:55Z</updated>
    <published>2025-10-09T17:59:55Z</published>
    <title>ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive
  Evaluation</title>
    <summary>  Benchmarks are central to measuring the capabilities of large language models
and guiding model development, yet widespread data leakage from pretraining
corpora undermines their validity. Models can match memorized content rather
than demonstrate true generalization, which inflates scores, distorts
cross-model comparisons, and misrepresents progress. We introduce ArenaBencher,
a model-agnostic framework for automatic benchmark evolution that updates test
cases while preserving comparability. Given an existing benchmark and a diverse
pool of models to be evaluated, ArenaBencher infers the core ability of each
test case, generates candidate question-answer pairs that preserve the original
objective, verifies correctness and intent with an LLM as a judge, and
aggregates feedback from multiple models to select candidates that expose
shared weaknesses. The process runs iteratively with in-context demonstrations
that steer generation toward more challenging and diagnostic cases. We apply
ArenaBencher to math problem solving, commonsense reasoning, and safety domains
and show that it produces verified, diverse, and fair updates that uncover new
failure modes, increase difficulty while preserving test objective alignment,
and improve model separability. The framework provides a scalable path to
continuously evolve benchmarks in step with the rapid progress of foundation
models.
</summary>
    <author>
      <name>Qin Liu</name>
    </author>
    <author>
      <name>Jacob Dineen</name>
    </author>
    <author>
      <name>Yuxi Huang</name>
    </author>
    <author>
      <name>Sheng Zhang</name>
    </author>
    <author>
      <name>Hoifung Poon</name>
    </author>
    <author>
      <name>Ben Zhou</name>
    </author>
    <author>
      <name>Muhao Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.08569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08562v1</id>
    <updated>2025-10-09T17:59:36Z</updated>
    <published>2025-10-09T17:59:36Z</published>
    <title>ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous
  Driving</title>
    <summary>  End-to-end autonomous driving (E2EAD) systems, which learn to predict future
trajectories directly from sensor data, are fundamentally challenged by the
inherent spatio-temporal imbalance of trajectory data. This imbalance creates a
significant optimization burden, causing models to learn spurious correlations
instead of causal inference, while also prioritizing uncertain, distant
predictions, thereby compromising immediate safety. To address these issues, we
propose ResAD, a novel Normalized Residual Trajectory Modeling framework.
Instead of predicting the future trajectory directly, our approach reframes the
learning task to predict the residual deviation from a deterministic inertial
reference. The inertial reference serves as a counterfactual, forcing the model
to move beyond simple pattern recognition and instead identify the underlying
causal factors (e.g., traffic rules, obstacles) that necessitate deviations
from a default, inertially-guided path. To deal with the optimization imbalance
caused by uncertain, long-term horizons, ResAD further incorporates Point-wise
Normalization of the predicted residual. It re-weights the optimization
objective, preventing large-magnitude errors associated with distant, uncertain
waypoints from dominating the learning signal. Extensive experiments validate
the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a
state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two
denoising steps, demonstrating that our approach significantly simplifies the
learning task and improves model performance. The code will be released to
facilitate further research.
</summary>
    <author>
      <name>Zhiyu Zheng</name>
    </author>
    <author>
      <name>Shaoyu Chen</name>
    </author>
    <author>
      <name>Haoran Yin</name>
    </author>
    <author>
      <name>Xinbang Zhang</name>
    </author>
    <author>
      <name>Jialv Zou</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
    <author>
      <name>Qian Zhang</name>
    </author>
    <author>
      <name>Lefei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.08562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08555v1</id>
    <updated>2025-10-09T17:58:59Z</updated>
    <published>2025-10-09T17:58:59Z</published>
    <title>VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal
  Patches via In-Context Conditioning</title>
    <summary>  We introduce the task of arbitrary spatio-temporal video completion, where a
video is generated from arbitrary, user-specified patches placed at any spatial
location and timestamp, akin to painting on a video canvas. This flexible
formulation naturally unifies many existing controllable video generation
tasks--including first-frame image-to-video, inpainting, extension, and
interpolation--under a single, cohesive paradigm. Realizing this vision,
however, faces a fundamental obstacle in modern latent video diffusion models:
the temporal ambiguity introduced by causal VAEs, where multiple pixel frames
are compressed into a single latent representation, making precise frame-level
conditioning structurally difficult. We address this challenge with
VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)
paradigm to this fine-grained control task with zero new parameters. We propose
a hybrid conditioning strategy that decouples spatial and temporal control:
spatial placement is handled via zero-padding, while temporal alignment is
achieved through Temporal RoPE Interpolation, which assigns each condition a
continuous fractional position within the latent sequence. This resolves the
VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen
backbone. To evaluate this new capability, we develop VideoCanvasBench, the
first benchmark for arbitrary spatio-temporal video completion, covering both
intra-scene fidelity and inter-scene creativity. Experiments demonstrate that
VideoCanvas significantly outperforms existing conditioning paradigms,
establishing a new state of the art in flexible and unified video generation.
</summary>
    <author>
      <name>Minghong Cai</name>
    </author>
    <author>
      <name>Qiulin Wang</name>
    </author>
    <author>
      <name>Zongli Ye</name>
    </author>
    <author>
      <name>Wenze Liu</name>
    </author>
    <author>
      <name>Quande Liu</name>
    </author>
    <author>
      <name>Weicai Ye</name>
    </author>
    <author>
      <name>Xintao Wang</name>
    </author>
    <author>
      <name>Pengfei Wan</name>
    </author>
    <author>
      <name>Kun Gai</name>
    </author>
    <author>
      <name>Xiangyu Yue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://onevfall.github.io/project_page/videocanvas</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.08555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08553v1</id>
    <updated>2025-10-09T17:58:01Z</updated>
    <published>2025-10-09T17:58:01Z</published>
    <title>Dream to Recall: Imagination-Guided Experience Retrieval for
  Memory-Persistent Vision-and-Language Navigation</title>
    <summary>  Vision-and-Language Navigation (VLN) requires agents to follow natural
language instructions through environments, with memory-persistent variants
demanding progressive improvement through accumulated experience. Existing
approaches for memory-persistent VLN face critical limitations: they lack
effective memory access mechanisms, instead relying on entire memory
incorporation or fixed-horizon lookup, and predominantly store only
environmental observations while neglecting navigation behavioral patterns that
encode valuable decision-making strategies. We present Memoir, which employs
imagination as a retrieval mechanism grounded by explicit memory: a world model
imagines future navigation states as queries to selectively retrieve relevant
environmental observations and behavioral histories. The approach comprises: 1)
a language-conditioned world model that imagines future states serving dual
purposes: encoding experiences for storage and generating retrieval queries; 2)
Hybrid Viewpoint-Level Memory that anchors both observations and behavioral
patterns to viewpoints, enabling hybrid retrieval; and 3) an
experience-augmented navigation model that integrates retrieved knowledge
through specialized encoders. Extensive evaluation across diverse
memory-persistent VLN benchmarks with 10 distinctive testing scenarios
demonstrates Memoir's effectiveness: significant improvements across all
scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent
baseline, accompanied by 8.3x training speedup and 74% inference memory
reduction. The results validate that predictive retrieval of both environmental
and behavioral memories enables more effective navigation, with analysis
indicating substantial headroom (73.3% vs 93.4% upper bound) for this
imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.
</summary>
    <author>
      <name>Yunzhe Xu</name>
    </author>
    <author>
      <name>Yiyuan Pan</name>
    </author>
    <author>
      <name>Zhe Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, 13 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.08553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08551v1</id>
    <updated>2025-10-09T17:57:38Z</updated>
    <published>2025-10-09T17:57:38Z</published>
    <title>ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D
  Reconstruction with Structured Scene Representation</title>
    <summary>  On-the-fly 3D reconstruction from monocular image sequences is a
long-standing challenge in computer vision, critical for applications such as
real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:
per-scene optimization yields high fidelity but is computationally expensive,
whereas feed-forward foundation models enable real-time inference but struggle
with accuracy and robustness. In this work, we propose ARTDECO, a unified
framework that combines the efficiency of feed-forward models with the
reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose
estimation and point prediction, coupled with a Gaussian decoder that
transforms multi-scale features into structured 3D Gaussians. To sustain both
fidelity and efficiency at scale, we design a hierarchical Gaussian
representation with a LoD-aware rendering strategy, which improves rendering
fidelity while reducing redundancy. Experiments on eight diverse indoor and
outdoor benchmarks show that ARTDECO delivers interactive performance
comparable to SLAM, robustness similar to feed-forward systems, and
reconstruction quality close to per-scene optimization, providing a practical
path toward on-the-fly digitization of real-world environments with both
accurate geometry and high visual fidelity. Explore more demos on our project
page: https://city-super.github.io/artdeco/.
</summary>
    <author>
      <name>Guanghao Li</name>
    </author>
    <author>
      <name>Kerui Ren</name>
    </author>
    <author>
      <name>Linning Xu</name>
    </author>
    <author>
      <name>Zhewen Zheng</name>
    </author>
    <author>
      <name>Changjian Jiang</name>
    </author>
    <author>
      <name>Xin Gao</name>
    </author>
    <author>
      <name>Bo Dai</name>
    </author>
    <author>
      <name>Jian Pu</name>
    </author>
    <author>
      <name>Mulin Yu</name>
    </author>
    <author>
      <name>Jiangmiao Pang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.08551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08544v1</id>
    <updated>2025-10-09T17:55:08Z</updated>
    <published>2025-10-09T17:55:08Z</published>
    <title>SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM
  Inference</title>
    <summary>  Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.
</summary>
    <author>
      <name>Hengrui Zhang</name>
    </author>
    <author>
      <name>Pratyush Patel</name>
    </author>
    <author>
      <name>August Ning</name>
    </author>
    <author>
      <name>David Wentzlaff</name>
    </author>
    <link href="http://arxiv.org/abs/2510.08544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08527v1</id>
    <updated>2025-10-09T17:50:22Z</updated>
    <published>2025-10-09T17:50:22Z</published>
    <title>FlexTraj: Image-to-Video Generation with Flexible Point Trajectory
  Control</title>
    <summary>  We present FlexTraj, a framework for image-to-video generation with flexible
point trajectory control. FlexTraj introduces a unified point-based motion
representation that encodes each point with a segmentation ID, a temporally
consistent trajectory ID, and an optional color channel for appearance cues,
enabling both dense and sparse trajectory control. Instead of injecting
trajectory conditions into the video generator through token concatenation or
ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that
achieves faster convergence, stronger controllability, and more efficient
inference, while maintaining robustness under unaligned conditions. To train
such a unified point trajectory-controlled video generator, FlexTraj adopts an
annealing training strategy that gradually reduces reliance on complete
supervision and aligned condition. Experimental results demonstrate that
FlexTraj enables multi-granularity, alignment-agnostic trajectory control for
video generation, supporting various applications such as motion cloning,
drag-based image-to-video, motion interpolation, camera redirection, flexible
action control and mesh animations.
</summary>
    <author>
      <name>Zhiyuan Zhang</name>
    </author>
    <author>
      <name>Can Wang</name>
    </author>
    <author>
      <name>Dongdong Chen</name>
    </author>
    <author>
      <name>Jing Liao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://bestzzhang.github.io/FlexTraj</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.08527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08525v1</id>
    <updated>2025-10-09T17:50:00Z</updated>
    <published>2025-10-09T17:50:00Z</published>
    <title>Which Heads Matter for Reasoning? RL-Guided KV Cache Compression</title>
    <summary>  Reasoning large language models exhibit complex reasoning behaviors through
the extended chain-of-thought generation, creating unprecedented Key-Value (KV)
cache overhead during the decoding phase. Existing KV cache compression methods
underperform on reasoning models: token-dropping methods break reasoning
integrity by discarding critical information, while head-reallocating methods
mistakenly compress reasoning-critical heads since they are designed for
retrieval tasks, resulting in significant performance degradation as
compression rates increase. We hypothesize that KV heads exhibit functional
heterogeneity in reasoning models-some heads are critical for chain-of-thought
consistency while others are compressible. To validate and exploit this
insight, we propose RLKV, a novel reasoning-critical head identification
framework, which uses reinforcement learning to directly optimize the
relationship between each head's cache usage and reasoning quality. As RLKV
produces rewards from actual generated samples during training, it naturally
identifies heads relevant to reasoning behaviors. We then allocate full KV
cache to these heads while applying compressed constant KV cache to others for
efficient inference. Our experiments reveal that only a small fraction of
attention heads is essential for reasoning, enabling our KV compression
approach to outperform baseline methods while achieving 20-50% cache reduction
with near lossless performance compared to uncompressed results.
</summary>
    <author>
      <name>Wenjie Du</name>
    </author>
    <author>
      <name>Li Jiang</name>
    </author>
    <author>
      <name>Keda Tao</name>
    </author>
    <author>
      <name>Xue Liu</name>
    </author>
    <author>
      <name>Huan Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.08525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08507v1</id>
    <updated>2025-10-09T17:42:04Z</updated>
    <published>2025-10-09T17:42:04Z</published>
    <title>The quantum communication power of indefinite causal order</title>
    <summary>  Quantum theory is in principle compatible with scenarios where physical
processes take place in an indefinite causal order, a possibility that was
shown to yield advantages in several information processing tasks. However,
advantages in communication, the most basic form of information processing,
have so far remained controversial and hard to prove. Here we develop a
framework that can be used to rigorously assess the role of causal order in a
scenario where communication links are built by assembling multiple quantum
devices. In this setting, we establish a clear-cut advantage of indefinite
order in the one-shot transmission of classical messages. On the other hand, we
also show that the advantage is not generic to all communication tasks.
Notably, we find that indefinite order does not offer any advantage over shared
entanglement in the asymptotic scenario where a large number of uses of the
same communication device is employed. Overall, our results unveil non-trivial
relations between communication, causal order, entanglement, and no-signaling
resources in quantum mechanics.
</summary>
    <author>
      <name>Xuanqiang Zhao</name>
    </author>
    <author>
      <name>Benchi Zhao</name>
    </author>
    <author>
      <name>Giulio Chiribella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.08507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
