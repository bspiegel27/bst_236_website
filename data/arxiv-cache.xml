<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-09T00:56:04Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">111858</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.04613v1</id>
    <updated>2025-05-07T17:56:19Z</updated>
    <published>2025-05-07T17:56:19Z</published>
    <title>From Two Sample Testing to Singular Gaussian Discrimination</title>
    <summary>  We establish that testing for the equality of two probability measures on a
general separable and compact metric space is equivalent to testing for the
singularity between two corresponding Gaussian measures on a suitable
Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via
the notion of kernel mean and covariance embedding of a probability measure.
Discerning two singular Gaussians is fundamentally simpler from an
information-theoretic perspective than non-parametric two-sample testing,
particularly in high-dimensional settings. Our proof leverages the
Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert
spaces, and shows that discrepancies between distributions are heavily
magnified through their corresponding Gaussian embeddings: at a population
level, distinct probability measures lead to essentially separated Gaussian
embeddings. This appears to be a new instance of the blessing of dimensionality
that can be harnessed for the design of efficient inference tools in great
generality.
</summary>
    <author>
      <name>Leonardo V. Santoro</name>
    </author>
    <author>
      <name>Kartik G. Waghmare</name>
    </author>
    <author>
      <name>Victor M. Panaretos</name>
    </author>
    <link href="http://arxiv.org/abs/2505.04613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G10, 46E22, 60G15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04611v2</id>
    <updated>2025-05-08T11:56:07Z</updated>
    <published>2025-05-07T17:55:32Z</published>
    <title>Particle Gibbs without the Gibbs bit</title>
    <summary>  Exact parameter and trajectory inference in state-space models is typically
achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or
particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly
proposes a new trajectory and parameter, and accepts or rejects both at once.
PGibbs instead alternates between sampling from the trajectory, using an
algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter
in a Hastings-within-Gibbs fashion. While particle independent Metropolis
Hastings (PIMH), the parameter-free version of PMMH, is known to be
statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter
and the state trajectory are very correlated. This has made PMMH the method of
choice for many practitioners, despite theory and experiments favouring CSMC
over PIMH for the parameter-free problem. In this article, we describe a
formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing
over the trajectory distribution in a fashion similar to PMMH. This is achieved
by considering the implementation of a CSMC algortihm for the state-space model
integrated over the joint distribution of the current parameter and the
parameter proposal. We illustrate the benefits of method on a simple example
known to be challenging for PMMH.
</summary>
    <author>
      <name>Adrien Corenflos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Feedback most welcome. 12 pages, 1 figure. Difference with previous
  version: fixed a couple of typos + longer simulations to remove noise in the
  figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04611v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04611v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04608v1</id>
    <updated>2025-05-07T17:53:47Z</updated>
    <published>2025-05-07T17:53:47Z</published>
    <title>WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via
  Weighted-Conformal Martingales</title>
    <summary>  Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria,'' such as data shifts that violate
certain exchangeability assumptions, or do not allow for online adaptation in
response to shifts. In this paper, we expand the scope of these monitoring
methods by proposing a weighted generalization of conformal test martingales
(WCTMs), which lay a theoretical foundation for online monitoring for any
unexpected changepoints in the data distribution while controlling
false-alarms. For practical applications, we propose specific WCTM algorithms
that accommodate online adaptation to mild covariate shifts (in the marginal
input distribution) while raising alarms in response to more severe shifts,
such as concept shifts (in the conditional label distribution) or extreme
(out-of-support) covariate shifts that cannot be easily adapted to. On
real-world datasets, we demonstrate improved performance relative to
state-of-the-art baselines.
</summary>
    <author>
      <name>Drew Prinster</name>
    </author>
    <author>
      <name>Xing Han</name>
    </author>
    <author>
      <name>Anqi Liu</name>
    </author>
    <author>
      <name>Suchi Saria</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in The International Conference on Machine Learning
  (ICML), 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04603v1</id>
    <updated>2025-05-07T17:50:14Z</updated>
    <published>2025-05-07T17:50:14Z</published>
    <title>Likelihood-Free Adaptive Bayesian Inference via Nonparametric
  Distribution Matching</title>
    <summary>  When the likelihood is analytically unavailable and computationally
intractable, approximate Bayesian computation (ABC) has emerged as a widely
used methodology for approximate posterior inference; however, it suffers from
severe computational inefficiency in high-dimensional settings or under diffuse
priors. To overcome these limitations, we propose Adaptive Bayesian Inference
(ABI), a framework that bypasses traditional data-space discrepancies and
instead compares distributions directly in posterior space through
nonparametric distribution matching. By leveraging a novel Marginally-augmented
Sliced Wasserstein (MSW) distance on posterior measures and exploiting its
quantile representation, ABI transforms the challenging problem of measuring
divergence between posterior distributions into a tractable sequence of
one-dimensional conditional quantile regression tasks. Moreover, we introduce a
new adaptive rejection sampling scheme that iteratively refines the posterior
approximation by updating the proposal distribution via generative density
estimation. Theoretically, we establish parametric convergence rates for the
trimmed MSW distance and prove that the ABI posterior converges to the true
posterior as the tolerance threshold vanishes. Through extensive empirical
evaluation, we demonstrate that ABI significantly outperforms data-based
Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free
simulators, especially in high-dimensional or dependent observation regimes.
</summary>
    <author>
      <name>Wenhui Sophia Lu</name>
    </author>
    <author>
      <name>Wing Hung Wong</name>
    </author>
    <link href="http://arxiv.org/abs/2505.04603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04602v1</id>
    <updated>2025-05-07T17:48:56Z</updated>
    <published>2025-05-07T17:48:56Z</published>
    <title>Extracting local velocity from cosmic dipole using simulations</title>
    <summary>  Our velocity with respect to the cosmic frame of rest leads to a dipole in
the number count distribution of galaxies. The dipole depends on the source
spectrum, which is usually assumed to be a power law, $S(\nu) \propto
\nu^{-\alpha}$ and on the flux dependence of the number density of sources. The
latter is also generally assumed to be a power law, parametrised with exponent
$x$. The velocity can be extracted from the observed dipole once the two
parameters $x$ and $\alpha$ are known. The standard procedure uses the mean
value of $\alpha$ across the entire sample, and the parameter $x$ is inferred
by fitting the cumulative number count, $\frac{dN}{d\Omega}(&gt;S_*) \propto
S_*^{-x}$, near the flux limit $S_*$ of the survey. Here, we introduce a
simulation procedure to extract the velocity which directly uses the $\alpha$
values of each source rather than their mean and does not rely on the
functional form of the cumulative number count near the flux limit. We apply
this to the quasar sample in CatWISE2020 data and find that the final results
differ from the standard procedure by approximately one sigma.
</summary>
    <author>
      <name>Mohit Panwar</name>
    </author>
    <author>
      <name>Akash Gandhi</name>
    </author>
    <author>
      <name>Pankaj Jain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04586v1</id>
    <updated>2025-05-07T17:27:51Z</updated>
    <published>2025-05-07T17:27:51Z</published>
    <title>Active Sampling for MRI-based Sequential Decision Making</title>
    <summary>  Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling
</summary>
    <author>
      <name>Yuning Du</name>
    </author>
    <author>
      <name>Jingshuai Liu</name>
    </author>
    <author>
      <name>Rohan Dharmakumar</name>
    </author>
    <author>
      <name>Sotirios A. Tsaftaris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04583v1</id>
    <updated>2025-05-07T17:21:45Z</updated>
    <published>2025-05-07T17:21:45Z</published>
    <title>Modeling Personalized Difficulty of Rehabilitation Exercises Using
  Causal Trees</title>
    <summary>  Rehabilitation robots are often used in game-like interactions for
rehabilitation to increase a person's motivation to complete rehabilitation
exercises. By adjusting exercise difficulty for a specific user throughout the
exercise interaction, robots can maximize both the user's rehabilitation
outcomes and the their motivation throughout the exercise. Previous approaches
have assumed exercises have generic difficulty values that apply to all users
equally, however, we identified that stroke survivors have varied and unique
perceptions of exercise difficulty. For example, some stroke survivors found
reaching vertically more difficult than reaching farther but lower while others
found reaching farther more challenging than reaching vertically. In this
paper, we formulate a causal tree-based method to calculate exercise difficulty
based on the user's performance. We find that this approach accurately models
exercise difficulty and provides a readily interpretable model of why that
exercise is difficult for both users and caretakers.
</summary>
    <author>
      <name>Nathaniel Dennler</name>
    </author>
    <author>
      <name>Zhonghao Shi</name>
    </author>
    <author>
      <name>Uksang Yoo</name>
    </author>
    <author>
      <name>Stefanos Nikolaidis</name>
    </author>
    <author>
      <name>Maja MatariÄ‡</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE/RAS-EMBS International Conference on Rehabilitation
  Robotics (ICORR 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04575v1</id>
    <updated>2025-05-07T17:12:15Z</updated>
    <published>2025-05-07T17:12:15Z</published>
    <title>Componential Prompt-Knowledge Alignment for Domain Incremental Learning</title>
    <summary>  Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt
</summary>
    <author>
      <name>Kunlun Xu</name>
    </author>
    <author>
      <name>Xu Zou</name>
    </author>
    <author>
      <name>Gang Hua</name>
    </author>
    <author>
      <name>Jiahuan Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accpted by ICML2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04568v1</id>
    <updated>2025-05-07T17:03:22Z</updated>
    <published>2025-05-07T17:03:22Z</published>
    <title>Conformal Survival Bands for Risk Screening under Right-Censoring</title>
    <summary>  We propose a method to quantify uncertainty around individual survival
distribution estimates using right-censored data, compatible with any survival
model. Unlike classical confidence intervals, the survival bands produced by
this method offer predictive rather than population-level inference, making
them useful for personalized risk screening. For example, in a low-risk
screening scenario, they can be applied to flag patients whose survival band at
12 months lies entirely above 50\%, while ensuring that at least half of
flagged individuals will survive past that time on average. Our approach builds
on recent advances in conformal inference and integrates ideas from inverse
probability of censoring weighting and multiple testing with false discovery
rate control. We provide asymptotic guarantees and show promising performance
in finite samples with both simulated and real data.
</summary>
    <author>
      <name>Matteo Sesia</name>
    </author>
    <author>
      <name>Vladimir Svetnik</name>
    </author>
    <link href="http://arxiv.org/abs/2505.04568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04565v1</id>
    <updated>2025-05-07T16:57:51Z</updated>
    <published>2025-05-07T16:57:51Z</published>
    <title>Hierarchical Task Decomposition for Execution Monitoring and Error
  Recovery: Understanding the Rationale Behind Task Demonstrations</title>
    <summary>  Multi-step manipulation tasks where robots interact with their environment
and must apply process forces based on the perceived situation remain
challenging to learn and prone to execution errors. Accurately simulating these
tasks is also difficult. Hence, it is crucial for robust task performance to
learn how to coordinate end-effector pose and applied force, monitor execution,
and react to deviations. To address these challenges, we propose a learning
approach that directly infers both low- and high-level task representations
from user demonstrations on the real system. We developed an unsupervised task
segmentation algorithm that combines intention recognition and feature
clustering to infer the skills of a task. We leverage the inferred
characteristic features of each skill in a novel unsupervised anomaly detection
approach to identify deviations from the intended task execution. Together,
these components form a comprehensive framework capable of incrementally
learning task decisions and new behaviors as new situations arise. Compared to
state-of-the-art learning techniques, our approach significantly reduces the
required amount of training data and computational complexity while efficiently
learning complex in-contact behaviors and recovery strategies. Our proposed
task segmentation and anomaly detection approaches outperform state-of-the-art
methods on force-based tasks evaluated on two different robotic systems.
</summary>
    <author>
      <name>Christoph Willibald</name>
    </author>
    <author>
      <name>Dongheui Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper has been accepted for publication by the International
  Journal of Robotics Research (IJRR), 26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
