<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-06-17T00:58:02Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-06-16T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">115012</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.12015v1</id>
    <updated>2025-06-13T17:59:58Z</updated>
    <published>2025-06-13T17:59:58Z</published>
    <title>EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction</title>
    <summary>  Open-source foundation models have seen rapid adoption and development,
enabling powerful general-purpose capabilities across diverse domains. However,
fine-tuning large foundation models for domain-specific or personalized tasks
remains prohibitively expensive for most users due to the significant memory
overhead beyond that of inference. We introduce EMLoC, an Emulator-based
Memory-efficient fine-tuning framework with LoRA Correction, which enables
model fine-tuning within the same memory budget required for inference. EMLoC
constructs a task-specific light-weight emulator using activation-aware
singular value decomposition (SVD) on a small downstream calibration set.
Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle
the misalignment between the original model and the compressed emulator, we
propose a novel compensation algorithm to correct the fine-tuned LoRA module,
which thus can be merged into the original model for inference. EMLoC supports
flexible compression ratios and standard training pipelines, making it
adaptable to a wide range of applications. Extensive experiments demonstrate
that EMLoC outperforms other baselines across multiple datasets and modalities.
Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a
single 24GB consumer GPU-bringing efficient and practical model adaptation to
individual users.
</summary>
    <author>
      <name>Hsi-Che Lin</name>
    </author>
    <author>
      <name>Yu-Chu Yu</name>
    </author>
    <author>
      <name>Kai-Po Chang</name>
    </author>
    <author>
      <name>Yu-Chiang Frank Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review. Project page: https://hsi-che-lin.github.io/EMLoC/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.12015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.12015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.12004v1</id>
    <updated>2025-06-13T17:56:04Z</updated>
    <published>2025-06-13T17:56:04Z</published>
    <title>Bayesian and frequentist perspectives agree on dynamical dark energy</title>
    <summary>  Baryon acoustic oscillation data from the Dark Energy Spectroscopic
Instrument (DESI) show evidence of a deviation from a cosmological constant
$\Lambda$ within a Bayesian analysis. In this work, we validate that
frequentist constraints from profile likelihoods on the
Chevallier-Polarski-Linder parameters $w_0$, $w_a$ are in excellent agreement
with the Bayesian constraints when combining with Planck cosmic microwave
background, Planck and Atacama Cosmology Telescope lensing, and either
Pantheon+ or Dark Energy Survey Y5 supernova data. Further, we assess which
datasets drive these constraints by considering the contributions to the
$\chi^2$ from the individual datasets. For profile likelihoods of the matter
fraction $\Omega_\mathrm{m}$, such an investigation shows internal
inconsistencies when assuming $\Lambda$, which are resolved when assuming a
$w_0w_a$ dark-energy model. We infer the equations of state $w(z)$ at the pivot
redshifts, supporting previous interpretations that current data appears to be
more sensitive to the derivative of $w(z)$ rather than a mean offset from
$\Lambda$. Thus our frequentist analysis corroborates previous findings on
dynamical DE.
</summary>
    <author>
      <name>Laura Herold</name>
    </author>
    <author>
      <name>Tanvi Karwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.12004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.12004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11994v1</id>
    <updated>2025-06-13T17:49:25Z</updated>
    <published>2025-06-13T17:49:25Z</published>
    <title>Spectral Estimation with Free Decompression</title>
    <summary>  Computing eigenvalues of very large matrices is a critical task in many
machine learning applications, including the evaluation of log-determinants,
the trace of matrix functions, and other important metrics. As datasets
continue to grow in scale, the corresponding covariance and kernel matrices
become increasingly large, often reaching magnitudes that make their direct
formation impractical or impossible. Existing techniques typically rely on
matrix-vector products, which can provide efficient approximations, if the
matrix spectrum behaves well. However, in settings like distributed learning,
or when the matrix is defined only indirectly, access to the full data set can
be restricted to only very small sub-matrices of the original matrix. In these
cases, the matrix of nominal interest is not even available as an implicit
operator, meaning that even matrix-vector products may not be available. In
such settings, the matrix is "impalpable," in the sense that we have access to
only masked snapshots of it. We draw on principles from free probability theory
to introduce a novel method of "free decompression" to estimate the spectrum of
such matrices. Our method can be used to extrapolate from the empirical
spectral densities of small submatrices to infer the eigenspectrum of extremely
large (impalpable) matrices (that we cannot form or even evaluate with full
matrix-vector products). We demonstrate the effectiveness of this approach
through a series of examples, comparing its performance against known limiting
distributions from random matrix theory in synthetic settings, as well as
applying it to submatrices of real-world datasets, matching them with their
full empirical eigenspectra.
</summary>
    <author>
      <name>Siavash Ameli</name>
    </author>
    <author>
      <name>Chris van der Heide</name>
    </author>
    <author>
      <name>Liam Hodgkinson</name>
    </author>
    <author>
      <name>Michael W. Mahoney</name>
    </author>
    <link href="http://arxiv.org/abs/2506.11994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11991v1</id>
    <updated>2025-06-13T17:47:43Z</updated>
    <published>2025-06-13T17:47:43Z</published>
    <title>VGR: Visual Grounded Reasoning</title>
    <summary>  In the field of multimodal chain-of-thought (CoT) reasoning, existing
approaches predominantly rely on reasoning on pure language space, which
inherently suffers from language bias and is largely confined to math or
science domains. This narrow focus limits their ability to handle complex
visual reasoning tasks that demand comprehensive understanding of image
details. To address these limitations, this paper introduces VGR, a novel
reasoning multimodal large language model (MLLM) with enhanced fine-grained
visual perception capabilities. Unlike traditional MLLMs that answer the
question or reasoning solely on the language space, our VGR first detects
relevant regions that may help to solve problems, and then provides precise
answers based on replayed image regions. To achieve this, we conduct a
large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed
vision grounding and language deduction. The inference pipeline of VGR allows
the model to choose bounding boxes for visual reference and a replay stage is
introduced to integrates the corresponding regions into the reasoning process,
enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline
show that VGR achieves superior performance on multi-modal benchmarks requiring
comprehensive image detail understanding. Compared to the baseline, VGR uses
only 30\% of the image token count while delivering scores of +4.1 on MMStar,
+7.1 on AI2D, and a +12.9 improvement on ChartQA.
</summary>
    <author>
      <name>Jiacong Wang</name>
    </author>
    <author>
      <name>Zijiang Kang</name>
    </author>
    <author>
      <name>Haochen Wang</name>
    </author>
    <author>
      <name>Haiyong Jiang</name>
    </author>
    <author>
      <name>Jiawen Li</name>
    </author>
    <author>
      <name>Bohong Wu</name>
    </author>
    <author>
      <name>Ya Wang</name>
    </author>
    <author>
      <name>Jiao Ran</name>
    </author>
    <author>
      <name>Xiao Liang</name>
    </author>
    <author>
      <name>Chao Feng</name>
    </author>
    <author>
      <name>Jun Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.11991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11989v1</id>
    <updated>2025-06-13T17:46:14Z</updated>
    <published>2025-06-13T17:46:14Z</published>
    <title>Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal</title>
    <summary>  Test-time scaling offers a promising way to improve the reasoning performance
of vision-language large models (VLLMs) without additional training. In this
paper, we explore a simple but effective approach for applying test-time
scaling to radiology report generation. Specifically, we introduce a
lightweight Thought Graph Traversal (TGT) framework that guides the model to
reason through organ-specific findings in a medically coherent order. This
framework integrates structured medical priors into the prompt, enabling deeper
and more logical analysis with no changes to the underlying model. To further
enhance reasoning depth, we apply a reasoning budget forcing strategy that
adjusts the model's inference depth at test time by dynamically extending its
generation process. This simple yet powerful combination allows a frozen
radiology VLLM to self-correct and generate more accurate, consistent chest
X-ray reports. Our method outperforms baseline prompting approaches on standard
benchmarks, and also reveals dataset biases through traceable reasoning paths.
Code and prompts are open-sourced for reproducibility at
https://github.com/glerium/Thought-Graph-Traversal.
</summary>
    <author>
      <name>Yue Yao</name>
    </author>
    <author>
      <name>Zelin Wen</name>
    </author>
    <author>
      <name>Yan Tong</name>
    </author>
    <author>
      <name>Xinyu Tian</name>
    </author>
    <author>
      <name>Xuqing Li</name>
    </author>
    <author>
      <name>Xiao Ma</name>
    </author>
    <author>
      <name>Dongliang Xu</name>
    </author>
    <author>
      <name>Tom Gedeon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2404.11209 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.11989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11949v1</id>
    <updated>2025-06-13T16:58:31Z</updated>
    <published>2025-06-13T16:58:31Z</published>
    <title>Modeling Complex Life Systems: Bayesian Inference for Weibull Failure
  Times Using Adaptive MCMC</title>
    <summary>  This research develops a Bayesian framework for analyzing failure times using
the Weibull distribution, addressing challenges in prior selection due to the
lack of conjugate priors and multi-dimensional sufficient statistics. We
propose an adaptive semi-parametric MCMC algorithm for lifetime data analysis,
employing a hierarchical Bayesian model and the No-U-Turn Sampler (NUTS) in
STAN. Twenty-four combinations of prior distributions are evaluated, with a
noninformative LogNormal hyper-prior ensuring flexibility. A simulation study
of seventy-two datasets with varying structures compares MCMC and classical
methods, identifying optimal priors for Bayesian regularization. The approach
effectively handles the Increasing Hazard Rate (IHR) and Decreasing Hazard Rate
(DHR) scenarios. Finally, we demonstrate the algorithm's utility by predicting
the remaining lifetime of prostate cancer patients, showcasing its practical
application. This work advances Bayesian methodologies for modeling complex
life systems and testing processes.
</summary>
    <author>
      <name>Tobias Oketch</name>
    </author>
    <author>
      <name>Mohammad Sepehrifar</name>
    </author>
    <link href="http://arxiv.org/abs/2506.11949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11948v1</id>
    <updated>2025-06-13T16:58:20Z</updated>
    <published>2025-06-13T16:58:20Z</published>
    <title>SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</title>
    <summary>  Offline Imitation Learning (IL) methods such as Behavior Cloning are
effective at acquiring complex robotic manipulation skills. However, existing
IL-trained policies are confined to executing the task at the same speed as
shown in demonstration data. This limits the task throughput of a robotic
system, a critical requirement for applications such as industrial automation.
In this paper, we introduce and formalize the novel problem of enabling
faster-than-demonstration execution of visuomotor policies and identify
fundamental challenges in robot dynamics and state-action distribution shifts.
We instantiate the key insights as SAIL (Speed Adaptation for Imitation
Learning), a full-stack system integrating four tightly-connected components:
(1) a consistency-preserving action inference algorithm for smooth motion at
high speed, (2) high-fidelity tracking of controller-invariant motion targets,
(3) adaptive speed modulation that dynamically adjusts execution speed based on
motion complexity, and (4) action scheduling to handle real-world system
latencies. Experiments on 12 tasks across simulation and two real, distinct
robot platforms show that SAIL achieves up to a 4x speedup over demonstration
speed in simulation and up to 3.2x speedup in the real world. Additional detail
is available at https://nadunranawaka1.github.io/sail-policy
</summary>
    <author>
      <name>Nadun Ranawaka Arachchige</name>
    </author>
    <author>
      <name>Zhenyang Chen</name>
    </author>
    <author>
      <name>Wonsuhk Jung</name>
    </author>
    <author>
      <name>Woo Chul Shin</name>
    </author>
    <author>
      <name>Rohan Bansal</name>
    </author>
    <author>
      <name>Pierre Barroso</name>
    </author>
    <author>
      <name>Yu Hang He</name>
    </author>
    <author>
      <name>Yingyang Celine Lin</name>
    </author>
    <author>
      <name>Benjamin Joffe</name>
    </author>
    <author>
      <name>Shreyas Kousik</name>
    </author>
    <author>
      <name>Danfei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.11948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11936v1</id>
    <updated>2025-06-13T16:38:51Z</updated>
    <published>2025-06-13T16:38:51Z</published>
    <title>Bubble Dynamics Transformer: Microrheology at Ultra-High Strain Rates</title>
    <summary>  Laser-induced inertial cavitation (LIC)-where microscale vapor bubbles
nucleate due to a focused high-energy pulsed laser and then violently collapse
under surrounding high local pressures-offers a unique opportunity to
investigate soft biological material mechanics at extremely high strain rates
(&gt;1000 1/s). Traditional rheological tools are often limited in these regimes
by loading speed, resolution, or invasiveness. Here we introduce novel machine
learning (ML) based microrheological frameworks that leverage LIC to
characterize the viscoelastic properties of biological materials at ultra-high
strain rates. We utilize ultra-high-speed imaging to capture time-resolved
bubble radius dynamics during LIC events in various soft viscoelastic
materials. These bubble radius versus time measurements are then analyzed using
a newly developed Bubble Dynamics Transformer (BDT), a neural network trained
on physics-based simulation data. The BDT accurately infers material
viscoelastic parameters, eliminating the need for iterative fitting or complex
inversion processes. This enables fast, accurate, and non-contact
characterization of soft materials under extreme loading conditions, with
significant implications for biomedical applications and materials science.
</summary>
    <author>
      <name>Lehu Bu</name>
    </author>
    <author>
      <name>Zhaohan Yu</name>
    </author>
    <author>
      <name>Shaoting Lin</name>
    </author>
    <author>
      <name>Jan N. Fuhg</name>
    </author>
    <author>
      <name>Jin Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.11936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11925v1</id>
    <updated>2025-06-13T16:24:28Z</updated>
    <published>2025-06-13T16:24:28Z</published>
    <title>Real-World Deployment of a Lane Change Prediction Architecture Based on
  Knowledge Graph Embeddings and Bayesian Inference</title>
    <summary>  Research on lane change prediction has gained a lot of momentum in the last
couple of years. However, most research is confined to simulation or results
obtained from datasets, leaving a gap between algorithmic advances and on-road
deployment. This work closes that gap by demonstrating, on real hardware, a
lane-change prediction system based on Knowledge Graph Embeddings (KGEs) and
Bayesian inference. Moreover, the ego-vehicle employs a longitudinal braking
action to ensure the safety of both itself and the surrounding vehicles. Our
architecture consists of two modules: (i) a perception module that senses the
environment, derives input numerical features, and converts them into
linguistic categories; and communicates them to the prediction module; (ii) a
pretrained prediction module that executes a KGE and Bayesian inference model
to anticipate the target vehicle's maneuver and transforms the prediction into
longitudinal braking action. Real-world hardware experimental validation
demonstrates that our prediction system anticipates the target vehicle's lane
change three to four seconds in advance, providing the ego vehicle sufficient
time to react and allowing the target vehicle to make the lane change safely.
</summary>
    <author>
      <name>M. Manzour</name>
    </author>
    <author>
      <name>Catherine M. Elias</name>
    </author>
    <author>
      <name>Omar M. Shehata</name>
    </author>
    <author>
      <name>R. Izquierdo</name>
    </author>
    <author>
      <name>M. A. Sotelo</name>
    </author>
    <link href="http://arxiv.org/abs/2506.11925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.11924v1</id>
    <updated>2025-06-13T16:19:00Z</updated>
    <published>2025-06-13T16:19:00Z</published>
    <title>Aligned Novel View Image and Geometry Synthesis via Cross-modal
  Attention Instillation</title>
    <summary>  We introduce a diffusion-based framework that performs aligned novel view
image and geometry generation via a warping-and-inpainting methodology. Unlike
prior methods that require dense posed images or pose-embedded generative
models limited to in-domain views, our method leverages off-the-shelf geometry
predictors to predict partial geometries viewed from reference images, and
formulates novel-view synthesis as an inpainting task for both image and
geometry. To ensure accurate alignment between generated images and geometry,
we propose cross-modal attention distillation, where attention maps from the
image diffusion branch are injected into a parallel geometry diffusion branch
during both training and inference. This multi-task approach achieves
synergistic effects, facilitating geometrically robust image synthesis as well
as well-defined geometry prediction. We further introduce proximity-based mesh
conditioning to integrate depth and normal cues, interpolating between point
cloud and filtering erroneously predicted geometry from influencing the
generation process. Empirically, our method achieves high-fidelity
extrapolative view synthesis on both image and geometry across a range of
unseen scenes, delivers competitive reconstruction quality under interpolation
settings, and produces geometrically aligned colored point clouds for
comprehensive 3D completion. Project page is available at
https://cvlab-kaist.github.io/MoAI.
</summary>
    <author>
      <name>Min-Seop Kwak</name>
    </author>
    <author>
      <name>Junho Kim</name>
    </author>
    <author>
      <name>Sangdoo Yun</name>
    </author>
    <author>
      <name>Dongyoon Han</name>
    </author>
    <author>
      <name>Taekyoung Kim</name>
    </author>
    <author>
      <name>Seungryong Kim</name>
    </author>
    <author>
      <name>Jin-Hwa Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2506.11924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.11924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
