<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-01T01:09:59Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-07-31T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">117887</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2507.22887v1</id>
    <updated>2025-07-30T17:59:46Z</updated>
    <published>2025-07-30T17:59:46Z</published>
    <title>Where to show Demos in Your Prompt: A Positional Bias of In-Context
  Learning</title>
    <summary>  In-context learning (ICL) is a critical emerging capability of large language
models (LLMs), enabling few-shot learning during inference by including a few
demonstrations (demos) in the prompt. However, it has been found that ICL's
performance can be sensitive to the choices of demos and their order. This
paper investigates an unexplored new positional bias of ICL for the first time:
we observe that the predictions and accuracy can drift drastically when the
positions of demos, the system prompt, and the user message in LLM input are
varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We
design a systematic evaluation pipeline to study this type of positional bias
across classification, question answering, summarization, and reasoning tasks.
We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify
net gains and output volatility induced by changes in the demos' position.
Extensive experiments on ten LLMs from four open-source model families (QWEN,
LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their
accuracy and predictions: placing demos at the start of the prompt yields the
most stable and accurate outputs with gains of up to +6 points. In contrast,
placing demos at the end of the user message flips over 30\% of predictions
without improving correctness on QA tasks. Smaller models are most affected by
this sensitivity, though even large models remain marginally affected on
complex tasks.
</summary>
    <author>
      <name>Kwesi Cobbina</name>
    </author>
    <author>
      <name>Tianyi Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22869v1</id>
    <updated>2025-07-30T17:44:07Z</updated>
    <published>2025-07-30T17:44:07Z</published>
    <title>Inference on Common Trends in a Cointegrated Nonlinear SVAR</title>
    <summary>  We consider the problem of performing inference on the number of common
stochastic trends when data is generated by a cointegrated CKSVAR (a
two-regime, piecewise-linear SVAR; Mavroeidis, 2021), using a modified version
of the Breitung (2002) multivariate variance ratio test that is robust to the
presence of nonlinear cointegration (of a known form). To derive the
asymptotics of our test statistic, we prove a fundamental LLN-type result for a
class of stable but nonstationary autoregressive processes, using a novel dual
linear process approximation. We show that our modified test yields correct
inferences regarding the number of common trends in such a system, whereas the
unmodified test tends to infer a higher number of common trends than are
actually present, when cointegrating relations are nonlinear.
</summary>
    <author>
      <name>James A. Duffy</name>
    </author>
    <author>
      <name>Xiyu Jiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ii + 38 pp</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.22869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10 (Primary), 91B84, 62E20, 60G65 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22867v1</id>
    <updated>2025-07-30T17:42:13Z</updated>
    <published>2025-07-30T17:42:13Z</published>
    <title>Hawkes Processes with Variable Length Memory: Existence, Inference and
  Application to Neuronal Activity</title>
    <summary>  Motivated by applications in neuroscience, where the memory of a neuron may
reset upon firing, we introduce a new class of nonlinear Hawkes processes with
variable length memory. Multivariate Hawkes processes are past-dependant point
processes originally introduced tomodel excitation effects, later extended to a
nonlinear framework to account for the opposite effect, known as inhibition.
Our model generalises classical Hawkes processes, with or without inhibition,
focusing on the situation where the probability of an event occurring within a
given subprocess depends solely on the history since its last event. Our main
contributions are to prove existence of such processes, and to derive a
workable likelihood maximisation method, capable of identifying both classical
and variable memory dynamics. We demonstrate the effectiveness of our approach
both on synthetic data, and on a neuronal activity dataset.
</summary>
    <author>
      <name>Sacha Quayle</name>
    </author>
    <author>
      <name>Anna Bonnet</name>
    </author>
    <author>
      <name>Maxime Sangnier</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22863v1</id>
    <updated>2025-07-30T17:38:49Z</updated>
    <published>2025-07-30T17:38:49Z</published>
    <title>Formation of over-massive black holes in high-redshift disk galaxies via
  globular cluster accretion</title>
    <summary>  Recent observations with the James Webb Space Telescope (JWST) have suggested
the existence of over-massive black holes (OMBHs) in high-redshift galaxies. In
this paper, we propose a new mechanism for the formation of OMBHs, based on the
accretion of globular clusters (GCs) in compact disk galaxies. We derive the
conditions under which OMBHs can form, focusing on key parameters such as halo
mass, redshift, and halo spin parameter. Our results show that at redshift $z =
10$, a halo with mass $10^{11}~M_{\odot}$ and a spin parameter of $0.02$ can
form a black hole of $1.4 \times 10^{8}~M_{\odot}$ through GC migration and
accretion via tidal disruption events (TDEs). The resulting black
hole-to-stellar mass ratio can reach $\sim 0.1$, corresponding to the fraction
of GC mass accreted onto the black hole. This mechanism thus provides a
plausible explanation for the OMBHs observed by JWST. Furthermore, by combining
our model with halo mass functions and the distribution of spin parameters, we
construct black hole mass functions that successfully reproduce the number
densities of massive BH candidates at $z \sim 5$ inferred from JWST
observations, and UHZ1 and GHZ9 at $z \sim 10$.
</summary>
    <author>
      <name>Hidenobu Yajima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.22863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22853v1</id>
    <updated>2025-07-30T17:24:05Z</updated>
    <published>2025-07-30T17:24:05Z</published>
    <title>Repair-R1: Better Test Before Repair</title>
    <summary>  APR (Automated Program Repair) aims to automatically locate program defects,
generate patches and validate the repairs. Existing techniques for APR are
often combined with LLMs (Large Language Models), which leverages the
code-related knowledge of LLMs to improve repair effectiveness. Current
LLM-based APR methods typically utilize test cases only during the inference
stage, adopting an iterative approach that performs repair first and validates
it through test execution afterward. This conventional paradigm neglects two
important aspects: the potential contribution of test cases in the training
phase, and the possibility of leveraging testing prior to repair. To address
this, we propose Repair-R1, which introduces test cases into the model's
training phase and shifts test generation to precede repair. The model is
required to first generate discriminative test cases that can distinguish
defective behaviors, and then perform repair based on these tests. This enables
the model to better locate defects and understand the underlying causes of
defects, thereby improving repair effectiveness. We implement Repair-R1 with
three different backbone models, using RL (reinforcement learning) to
co-optimize test generation and bug repair. Experimental results on four widely
adopted benchmarks demonstrate the superiority of Repair-R1. Specially,
compared to vanilla models, Repair-R1 improves repair success rate by 2.68\% to
48.29\%, test generation success rate by 16.38\% to 53.28\%, and test coverage
by 0.78\% to 53.96\%. We publish the code and weights at
https://github.com/Tomsawyerhu/APR-RL and
https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.
</summary>
    <author>
      <name>Haichuan Hu</name>
    </author>
    <author>
      <name>Xiaochen Xie</name>
    </author>
    <author>
      <name>Quanjun Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22825v1</id>
    <updated>2025-07-30T16:40:46Z</updated>
    <published>2025-07-30T16:40:46Z</published>
    <title>DepR: Depth Guided Single-view Scene Reconstruction with Instance-level
  Diffusion</title>
    <summary>  We propose DepR, a depth-guided single-view scene reconstruction framework
that integrates instance-level diffusion within a compositional paradigm.
Instead of reconstructing the entire scene holistically, DepR generates
individual objects and subsequently composes them into a coherent 3D layout.
Unlike previous methods that use depth solely for object layout estimation
during inference and therefore fail to fully exploit its rich geometric
information, DepR leverages depth throughout both training and inference.
Specifically, we introduce depth-guided conditioning to effectively encode
shape priors into diffusion models. During inference, depth further guides DDIM
sampling and layout optimization, enhancing alignment between the
reconstruction and the input image. Despite being trained on limited synthetic
data, DepR achieves state-of-the-art performance and demonstrates strong
generalization in single-view scene reconstruction, as shown through
evaluations on both synthetic and real-world datasets.
</summary>
    <author>
      <name>Qingcheng Zhao</name>
    </author>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <author>
      <name>Haiyang Xu</name>
    </author>
    <author>
      <name>Zeyuan Chen</name>
    </author>
    <author>
      <name>Jianwen Xie</name>
    </author>
    <author>
      <name>Yuan Gao</name>
    </author>
    <author>
      <name>Zhuowen Tu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.22825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22817v1</id>
    <updated>2025-07-30T16:32:47Z</updated>
    <published>2025-07-30T16:32:47Z</published>
    <title>Wall Shear Stress Estimation in Abdominal Aortic Aneurysms: Towards
  Generalisable Neural Surrogate Models</title>
    <summary>  Abdominal aortic aneurysms (AAAs) are pathologic dilatations of the abdominal
aorta posing a high fatality risk upon rupture. Studying AAA progression and
rupture risk often involves in-silico blood flow modelling with computational
fluid dynamics (CFD) and extraction of hemodynamic factors like time-averaged
wall shear stress (TAWSS) or oscillatory shear index (OSI). However, CFD
simulations are known to be computationally demanding. Hence, in recent years,
geometric deep learning methods, operating directly on 3D shapes, have been
proposed as compelling surrogates, estimating hemodynamic parameters in just a
few seconds. In this work, we propose a geometric deep learning approach to
estimating hemodynamics in AAA patients, and study its generalisability to
common factors of real-world variation. We propose an E(3)-equivariant deep
learning model utilising novel robust geometrical descriptors and projective
geometric algebra. Our model is trained to estimate transient WSS using a
dataset of CT scans of 100 AAA patients, from which lumen geometries are
extracted and reference CFD simulations with varying boundary conditions are
obtained. Results show that the model generalizes well within the distribution,
as well as to the external test set. Moreover, the model can accurately
estimate hemodynamics across geometry remodelling and changes in boundary
conditions. Furthermore, we find that a trained model can be applied to
different artery tree topologies, where new and unseen branches are added
during inference. Finally, we find that the model is to a large extent agnostic
to mesh resolution. These results show the accuracy and generalisation of the
proposed model, and highlight its potential to contribute to hemodynamic
parameter estimation in clinical practice.
</summary>
    <author>
      <name>Patryk Rygiel</name>
    </author>
    <author>
      <name>Julian Suk</name>
    </author>
    <author>
      <name>Christoph Brune</name>
    </author>
    <author>
      <name>Kak Khee Yeung</name>
    </author>
    <author>
      <name>Jelmer M. Wolterink</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22816v1</id>
    <updated>2025-07-30T16:32:33Z</updated>
    <published>2025-07-30T16:32:33Z</published>
    <title>Kan Approximations of the Persistent Homology Transform</title>
    <summary>  The persistent homology transform (PHT) of a subset $M \subset \mathbb{R}^d$
is a map $\text{PHT}(M):\mathbb{S}^{d-1} \to \mathbf{Dgm}$ from the unit sphere
to the space of persistence diagrams. This map assigns to each direction $v\in
\mathbb{S}^{d-1}$ the persistent homology of the filtration of $M$ in direction
$v$. In practice, one can only sample the map $\text{PHT}(M)$ at a finite set
of directions $A \subset \mathbb{S}^{d-1}$. This suggests two natural
questions: (1) Can we interpolate the PHT from this finite sample of directions
to the entire sphere? If so, (2) can we prove that the resulting interpolation
is close to the true PHT? In this paper we show that if we can sample the PHT
at the module level, where we have information about how homology from each
direction interacts, a ready-made interpolation theory due to Bubenik, de
Silva, and Nanda using Kan extensions can answer both of these questions in the
affirmative. A close inspection of those techniques shows that we can infer the
PHT from a finite sample of heights from each direction as well. Our paper
presents the first known results for approximating the PHT from finite
directional and scalar data.
</summary>
    <author>
      <name>Shreya Arya</name>
    </author>
    <author>
      <name>Justin Curry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 4 figures. Dedicated to our beloved graduate and post-doc
  mentor, Sayan Mukherjee (1971--2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.22816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22805v1</id>
    <updated>2025-07-30T16:15:22Z</updated>
    <published>2025-07-30T16:15:22Z</published>
    <title>MoCHA: Advanced Vision-Language Reasoning with MoE Connector and
  Hierarchical Group Attention</title>
    <summary>  Vision large language models (VLLMs) are focusing primarily on handling
complex and fine-grained visual information by incorporating advanced vision
encoders and scaling up visual models. However, these approaches face high
training and inference costs, as well as challenges in extracting visual
details, effectively bridging across modalities. In this work, we propose a
novel visual framework, MoCHA, to address these issues. Our framework
integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to
extract complementary visual features and is equipped with a sparse Mixture of
Experts Connectors (MoECs) module to dynamically select experts tailored to
different visual dimensions. To mitigate redundant or insufficient use of the
visual information encoded by the MoECs module, we further design a
Hierarchical Group Attention (HGA) with intra- and inter-group operations and
an adaptive gating strategy for encoded visual features. We train MoCHA on two
mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance
across various benchmarks. Notably, MoCHA outperforms state-of-the-art
open-weight models on various tasks. For example, compared to CuMo
(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate
hallucination by showing improvements of 3.25% in POPE and to follow visual
instructions by raising 153 points on MME. Finally, ablation studies further
confirm the effectiveness and robustness of the proposed MoECs and HGA in
improving the overall performance of MoCHA.
</summary>
    <author>
      <name>Yuqi Pang</name>
    </author>
    <author>
      <name>Bowen Yang</name>
    </author>
    <author>
      <name>Yun Cao</name>
    </author>
    <author>
      <name>Fan Rong</name>
    </author>
    <author>
      <name>Xiaoyu Li</name>
    </author>
    <author>
      <name>Chen He</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.22800v1</id>
    <updated>2025-07-30T16:03:21Z</updated>
    <published>2025-07-30T16:03:21Z</published>
    <title>The Multi-Agent Fault Localization System Based on Monte Carlo Tree
  Search Approach</title>
    <summary>  In real-world scenarios, due to the highly decoupled and flexible nature of
microservices, it poses greater challenges to system reliability. The more
frequent occurrence of incidents has created a demand for Root Cause
Analysis(RCA) methods that enable rapid identification and recovery of
incidents. Large language model (LLM) provides a new path for quickly locating
and recovering from incidents by leveraging their powerful generalization
ability combined with expert experience. Current LLM for RCA frameworks are
based on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM
and the propagation nature of anomalies often lead to incorrect localization
results. Moreover, the massive amount of anomalous information generated in
large, complex systems presents a huge challenge for the context window length
of LLMs. To address these challenges, we propose KnowledgeMind, an innovative
LLM multi-agent system based on Monte Carlo Tree Search and a knowledge base
reward mechanism for standardized service-by-service reasoning. Compared to
State-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration
approach significantly reduces the burden on the maximum context window length,
requiring only one-tenth of its size. Additionally, by incorporating a
rule-based real-time reward mechanism, our method effectively mitigates
hallucinations during the inference process. Compared to the SOTA LLM for RCA
framework, our method achieves a 49.29% to 128.35% improvement in root cause
localization accuracy.
</summary>
    <author>
      <name>Rui Ren</name>
    </author>
    <link href="http://arxiv.org/abs/2507.22800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.22800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
