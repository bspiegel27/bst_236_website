<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-17T00:53:48Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-16T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">110702</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.11455v1</id>
    <updated>2025-04-15T17:59:46Z</updated>
    <published>2025-04-15T17:59:46Z</published>
    <title>SimpleAR: Pushing the Frontier of Autoregressive Visual Generation
  through Pretraining, SFT, and RL</title>
    <summary>  This work presents SimpleAR, a vanilla autoregressive visual generation
framework without complex architecure modifications. Through careful
exploration of training and inference optimization, we demonstrate that: 1)
with only 0.5B parameters, our model can generate 1024x1024 resolution images
with high fidelity, and achieve competitive results on challenging
text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both
supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)
training could lead to significant improvements on generation aesthectics and
prompt alignment; and 3) when optimized with inference acceleraton techniques
like vLLM, the time for SimpleAR to generate an 1024x1024 image could be
reduced to around 14 seconds. By sharing these findings and open-sourcing the
code, we hope to reveal the potential of autoregressive visual generation and
encourage more participation in this research field. Code is available at
https://github.com/wdrink/SimpleAR.
</summary>
    <author>
      <name>Junke Wang</name>
    </author>
    <author>
      <name>Zhi Tian</name>
    </author>
    <author>
      <name>Xun Wang</name>
    </author>
    <author>
      <name>Xinyu Zhang</name>
    </author>
    <author>
      <name>Weilin Huang</name>
    </author>
    <author>
      <name>Zuxuan Wu</name>
    </author>
    <author>
      <name>Yu-Gang Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report, work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11451v1</id>
    <updated>2025-04-15T17:58:16Z</updated>
    <published>2025-04-15T17:58:16Z</published>
    <title>PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond</title>
    <summary>  We propose PartField, a feedforward approach for learning part-based 3D
features, which captures the general concept of parts and their hierarchy
without relying on predefined templates or text-based names, and can be applied
to open-world 3D shapes across various modalities. PartField requires only a 3D
feedforward pass at inference time, significantly improving runtime and
robustness compared to prior approaches. Our model is trained by distilling 2D
and 3D part proposals from a mix of labeled datasets and image segmentations on
large unsupervised datasets, via a contrastive learning formulation. It
produces a continuous feature field which can be clustered to yield a
hierarchical part decomposition. Comparisons show that PartField is up to 20%
more accurate and often orders of magnitude faster than other recent
class-agnostic part-segmentation methods. Beyond single-shape part
decomposition, consistency in the learned field emerges across shapes, enabling
tasks such as co-segmentation and correspondence, which we demonstrate in
several applications of these general-purpose, hierarchical, and consistent 3D
feature fields. Check our Webpage!
https://research.nvidia.com/labs/toronto-ai/partfield-release/
</summary>
    <author>
      <name>Minghua Liu</name>
    </author>
    <author>
      <name>Mikaela Angelina Uy</name>
    </author>
    <author>
      <name>Donglai Xiang</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <author>
      <name>Nicholas Sharp</name>
    </author>
    <author>
      <name>Jun Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://research.nvidia.com/labs/toronto-ai/partfield-release/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11446v1</id>
    <updated>2025-04-15T17:56:24Z</updated>
    <published>2025-04-15T17:56:24Z</published>
    <title>eXplainable AI for data driven control: an inverse optimal control
  approach</title>
    <summary>  Understanding the behavior of black-box data-driven controllers is a key
challenge in modern control design. In this work, we propose an eXplainable AI
(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local
explanations for the behavior of a controller operating around a given region.
Specifically, we extract the weights assigned to tracking errors and control
effort in the implicit cost function that a black-box controller is optimizing,
offering a more transparent and interpretable representation of the
controller's underlying objectives. This approach presents connections with
well-established XAI techniques, such as Local Interpretable Model-agnostic
Explanations (LIME) since it is still based on a local approximation of the
control policy. However, rather being limited to a standard sensitivity
analysis, the explanation provided by our method relies on the solution of an
inverse Linear Quadratic (LQ) problem, offering a structured and more
control-relevant perspective. Numerical examples demonstrate that the inferred
cost function consistently provides a deeper understanding of the controller's
decision-making process, shedding light on otherwise counterintuitive or
unexpected phenomena.
</summary>
    <author>
      <name>Federico Porcari</name>
    </author>
    <author>
      <name>Donatello Materassi</name>
    </author>
    <author>
      <name>Simone Formentin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to CDC 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11423v1</id>
    <updated>2025-04-15T17:37:50Z</updated>
    <published>2025-04-15T17:37:50Z</published>
    <title>ADT: Tuning Diffusion Models with Adversarial Supervision</title>
    <summary>  Diffusion models have achieved outstanding image generation by reversing a
forward noising process to approximate true data distributions. During
training, these models predict diffusion scores from noised versions of true
samples in a single forward pass, while inference requires iterative denoising
starting from white noise. This training-inference divergences hinder the
alignment between inference and training data distributions, due to potential
prediction biases and cumulative error accumulation. To address this problem,
we propose an intuitive but effective fine-tuning framework, called Adversarial
Diffusion Tuning (ADT), by stimulating the inference process during
optimization and aligning the final outputs with training data by adversarial
supervision. Specifically, to achieve robust adversarial training, ADT features
a siamese-network discriminator with a fixed pre-trained backbone and
lightweight trainable parameters, incorporates an image-to-image sampling
strategy to smooth discriminative difficulties, and preserves the original
diffusion loss to prevent discriminator hacking. In addition, we carefully
constrain the backward-flowing path for back-propagating gradients along the
inference path without incurring memory overload or gradient explosion.
Finally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),
demonstrate that ADT significantly improves both distribution alignment and
image quality.
</summary>
    <author>
      <name>Dazhong Shen</name>
    </author>
    <author>
      <name>Guanglu Song</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Bingqi Ma</name>
    </author>
    <author>
      <name>Lujundong Li</name>
    </author>
    <author>
      <name>Dongzhi Jiang</name>
    </author>
    <author>
      <name>Zhuofan Zong</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.11423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11419v1</id>
    <updated>2025-04-15T17:35:13Z</updated>
    <published>2025-04-15T17:35:13Z</published>
    <title>Embodied World Models Emerge from Navigational Task in Open-Ended
  Environments</title>
    <summary>  Understanding how artificial systems can develop spatial awareness and
reasoning has long been a challenge in AI research. Traditional models often
rely on passive observation, but embodied cognition theory suggests that deeper
understanding emerges from active interaction with the environment. This study
investigates whether neural networks can autonomously internalize spatial
concepts through interaction, focusing on planar navigation tasks. Using Gated
Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we
show that agents can learn to encode spatial properties like direction,
distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)
to model the agent-environment interaction as a closed dynamical system,
revealing stable limit cycles that correspond to optimal navigation strategies.
Ridge Representation allows us to map navigation paths into a fixed-dimensional
behavioral space, enabling comparison with neural states. Canonical Correlation
Analysis (CCA) confirms strong alignment between these representations,
suggesting that the agent's neural states actively encode spatial knowledge.
Intervention experiments further show that specific neural dimensions are
causally linked to navigation performance. This work provides an approach to
bridging the gap between action and perception in AI, offering new insights
into building adaptive, interpretable models that can generalize across complex
environments. The causal validation of neural representations also opens new
avenues for understanding and controlling the internal mechanisms of AI
systems, pushing the boundaries of how machines learn and reason in dynamic,
real-world scenarios.
</summary>
    <author>
      <name>Li Jin</name>
    </author>
    <author>
      <name>Liu Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Research on explainable meta-reinforcement learning AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11409v1</id>
    <updated>2025-04-15T17:26:29Z</updated>
    <published>2025-04-15T17:26:29Z</published>
    <title>Efficient Hybrid Language Model Compression through Group-Aware SSM
  Pruning</title>
    <summary>  Hybrid LLM architectures that combine Attention and State Space Models (SSMs)
achieve state-of-the-art accuracy and runtime performance. Recent work has
demonstrated that applying compression and distillation to Attention-only
models yields smaller, more accurate models at a fraction of the training cost.
In this work, we explore the effectiveness of compressing Hybrid architectures.
We introduce a novel group-aware pruning strategy that preserves the structural
integrity of SSM blocks and their sequence modeling capabilities. Furthermore,
we demonstrate the necessity of such SSM pruning to achieve improved accuracy
and inference speed compared to traditional approaches. Our compression recipe
combines SSM, FFN, embedding dimension, and layer pruning, followed by
knowledge distillation-based retraining, similar to the MINITRON technique.
Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B
parameters with up to 40x fewer training tokens. The resulting model surpasses
the accuracy of similarly-sized models while achieving 2x faster inference,
significantly advancing the Pareto frontier.
</summary>
    <author>
      <name>Ali Taghibakhshi</name>
    </author>
    <author>
      <name>Sharath Turuvekere Sreenivas</name>
    </author>
    <author>
      <name>Saurav Muralidharan</name>
    </author>
    <author>
      <name>Marcin Chochowski</name>
    </author>
    <author>
      <name>Yashaswi Karnati</name>
    </author>
    <author>
      <name>Raviraj Joshi</name>
    </author>
    <author>
      <name>Ameya Sunil Mahabaleshwarkar</name>
    </author>
    <author>
      <name>Zijia Chen</name>
    </author>
    <author>
      <name>Yoshi Suhara</name>
    </author>
    <author>
      <name>Oluwatobi Olabiyi</name>
    </author>
    <author>
      <name>Daniel Korzekwa</name>
    </author>
    <author>
      <name>Mostofa Patwary</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <author>
      <name>Ashwath Aithal</name>
    </author>
    <author>
      <name>Nima Tajbakhsh</name>
    </author>
    <author>
      <name>Pavlo Molchanov</name>
    </author>
    <link href="http://arxiv.org/abs/2504.11409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11389v1</id>
    <updated>2025-04-15T16:58:15Z</updated>
    <published>2025-04-15T16:58:15Z</published>
    <title>VideoPanda: Video Panoramic Diffusion with Multi-view Attention</title>
    <summary>  High resolution panoramic video content is paramount for immersive
experiences in Virtual Reality, but is non-trivial to collect as it requires
specialized equipment and intricate camera setups. In this work, we introduce
VideoPanda, a novel approach for synthesizing 360$^\circ$ videos conditioned on
text or single-view video data. VideoPanda leverages multi-view attention
layers to augment a video diffusion model, enabling it to generate consistent
multi-view videos that can be combined into immersive panoramic content.
VideoPanda is trained jointly using two conditions: text-only and single-view
video, and supports autoregressive generation of long-videos. To overcome the
computational burden of multi-view video generation, we randomly subsample the
duration and camera views used during training and show that the model is able
to gracefully generalize to generating more frames during inference. Extensive
evaluations on both real-world and synthetic video datasets demonstrate that
VideoPanda generates more realistic and coherent 360$^\circ$ panoramas across
all input conditions compared to existing methods. Visit the project website at
https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.
</summary>
    <author>
      <name>Kevin Xie</name>
    </author>
    <author>
      <name>Amirmojtaba Sabour</name>
    </author>
    <author>
      <name>Jiahui Huang</name>
    </author>
    <author>
      <name>Despoina Paschalidou</name>
    </author>
    <author>
      <name>Greg Klar</name>
    </author>
    <author>
      <name>Umar Iqbal</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <author>
      <name>Xiaohui Zeng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website at
  https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11364v1</id>
    <updated>2025-04-15T16:30:02Z</updated>
    <published>2025-04-15T16:30:02Z</published>
    <title>Teaching Large Language Models to Reason through Learning and Forgetting</title>
    <summary>  Leveraging inference-time search in large language models has proven
effective in further enhancing a trained model's capability to solve complex
mathematical and reasoning problems. However, this approach significantly
increases computational costs and inference time, as the model must generate
and evaluate multiple candidate solutions to identify a viable reasoning path.
To address this, we propose an effective approach that integrates search
capabilities directly into the model by fine-tuning it using both successful
(learning) and failed reasoning paths (forgetting) derived from diverse search
methods. While fine-tuning the model with these data might seem
straightforward, we identify a critical issue: the model's search capability
tends to degrade rapidly if fine-tuning is performed naively. We show that this
degradation can be substantially mitigated by employing a smaller learning
rate. Extensive experiments on the challenging Game-of-24 and Countdown
mathematical reasoning benchmarks show that our approach not only outperforms
both standard fine-tuning and inference-time search baselines but also
significantly reduces inference time by 180$\times$.
</summary>
    <author>
      <name>Tianwei Ni</name>
    </author>
    <author>
      <name>Allen Nie</name>
    </author>
    <author>
      <name>Sapana Chaudhary</name>
    </author>
    <author>
      <name>Yao Liu</name>
    </author>
    <author>
      <name>Huzefa Rangwala</name>
    </author>
    <author>
      <name>Rasool Fakoor</name>
    </author>
    <link href="http://arxiv.org/abs/2504.11364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11336v1</id>
    <updated>2025-04-15T16:09:06Z</updated>
    <published>2025-04-15T16:09:06Z</published>
    <title>Looking beyond the next token</title>
    <summary>  The structure of causal language model training assumes that each token can
be accurately predicted from the previous context. This contrasts with humans'
natural writing and reasoning process, where goals are typically known before
the exact argument or phrasings. While this mismatch has been well studied in
the literature, the working assumption has been that architectural changes are
needed to address this mismatch. We argue that rearranging and processing the
training data sequences can allow models to more accurately imitate the true
data-generating process, and does not require any other changes to the
architecture or training infrastructure. We demonstrate that this technique,
Trelawney, and the inference algorithms derived from it allow us to improve
performance on several key benchmarks that span planning, algorithmic
reasoning, and story generation tasks. Finally, our method naturally enables
the generation of long-term goals at no additional cost. We investigate how
using the model's goal-generation capability can further improve planning and
reasoning. Additionally, we believe Trelawney could potentially open doors to
new capabilities beyond the current language modeling paradigm.
</summary>
    <author>
      <name>Abitha Thankaraj</name>
    </author>
    <author>
      <name>Yiding Jiang</name>
    </author>
    <author>
      <name>J. Zico Kolter</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <link href="http://arxiv.org/abs/2504.11336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11330v1</id>
    <updated>2025-04-15T16:04:02Z</updated>
    <published>2025-04-15T16:04:02Z</published>
    <title>Decorrelation in Complex Wave Scattering</title>
    <summary>  Phenomena involving multiple scattering, despite having attracted
considerable attention in physics for decades, continue to generate unexpected
and counterintuitive behaviours prompting further studies. For optical
scattering, the memory effect well predicts fourth order statistics, i.e. the
intensity correlation, as long as the scattering strength and depth are within
certain bounds. The memory effect has found a wide range of applications, where
its limitations also become apparent: for example, in imaging through turbid
media, decorrelation due to multiscattering in thick samples has been shown to
restrict the field of view. However, to our knowledge, no comprehensive
mechanism exists to date that can account for decorrelation precisely. In this
paper, we quantify how the scatterer's own statistics determine such
limitations. We show that the ensemble statistics of the backscattered field
may be decomposed into two terms: one expresses surface scattering, where
statistical distributions of multiscale structure features may be inferred from
our previous works; while the second term originates from the underlying
scattering volume and is diffusive. The new framework agrees well with
experiments, including the prediction of a new quasipower law for fluctuations
induced by the single realization.
</summary>
    <author>
      <name>Qihang Zhang</name>
    </author>
    <author>
      <name>Haoyu Yue</name>
    </author>
    <author>
      <name>Ninghe Liu</name>
    </author>
    <author>
      <name>Danlin Xu</name>
    </author>
    <author>
      <name>Renjie Zhou</name>
    </author>
    <author>
      <name>Liangcai Cao</name>
    </author>
    <author>
      <name>George Barbastathis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
