<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-12T01:00:29Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-12T01:00:30Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>128356</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.09929v1</id>
    <title>Closing the Train-Test Gap in World Models for Gradient-Based Planning</title>
    <updated>2025-12-10T18:59:45Z</updated>
    <link href="https://arxiv.org/abs/2512.09929v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09929v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:59:45Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Arjun Parthasarathy</name>
    </author>
    <author>
      <name>Nimit Kalra</name>
    </author>
    <author>
      <name>Rohun Agrawal</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Oumayma Bounou</name>
    </author>
    <author>
      <name>Pavel Izmailov</name>
    </author>
    <author>
      <name>Micah Goldblum</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09928v1</id>
    <title>HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</title>
    <updated>2025-12-10T18:59:32Z</updated>
    <link href="https://arxiv.org/abs/2512.09928v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09928v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:59:32Z</published>
    <arxiv:comment>Project page: https://hifvla.github.io Github: https://github.com/OpenHelix-Team/HiF-VLA</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Minghui Lin</name>
    </author>
    <author>
      <name>Pengxiang Ding</name>
    </author>
    <author>
      <name>Shu Wang</name>
    </author>
    <author>
      <name>Zifeng Zhuang</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Xinyang Tong</name>
    </author>
    <author>
      <name>Wenxuan Song</name>
    </author>
    <author>
      <name>Shangke Lyu</name>
    </author>
    <author>
      <name>Siteng Huang</name>
    </author>
    <author>
      <name>Donglin Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09927v1</id>
    <title>Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models</title>
    <updated>2025-12-10T18:59:24Z</updated>
    <link href="https://arxiv.org/abs/2512.09927v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09927v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:59:24Z</published>
    <arxiv:comment>8 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Yifan Ye</name>
    </author>
    <author>
      <name>Jiaqi Ma</name>
    </author>
    <author>
      <name>Jun Cen</name>
    </author>
    <author>
      <name>Zhihe Lu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09924v1</id>
    <title>ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning</title>
    <updated>2025-12-10T18:57:09Z</updated>
    <link href="https://arxiv.org/abs/2512.09924v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09924v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:57:09Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xinyu Liu</name>
    </author>
    <author>
      <name>Hangjie Yuan</name>
    </author>
    <author>
      <name>Yujie Wei</name>
    </author>
    <author>
      <name>Jiazheng Xing</name>
    </author>
    <author>
      <name>Yujin Han</name>
    </author>
    <author>
      <name>Jiahao Pan</name>
    </author>
    <author>
      <name>Yanbiao Ma</name>
    </author>
    <author>
      <name>Chi-Min Chan</name>
    </author>
    <author>
      <name>Kang Zhao</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Wenhan Luo</name>
    </author>
    <author>
      <name>Yike Guo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09922v1</id>
    <title>Self-calibration of weak lensing cosmic shear biases</title>
    <updated>2025-12-10T18:56:31Z</updated>
    <link href="https://arxiv.org/abs/2512.09922v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09922v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In order to reach the required performance of Stage-III and IV weak lensing surveys, cosmic shear measurements have to rely on external simulations to calibrate residual biases. Over the years, several techniques have been developed to mitigate the impact of residual biases prior to calibration, including the inference of shear responses on images to correct multiplicative biases, and the empirical correction of additive biases. We introduce a novel methodology that generalises upon the state-of-the-art approaches by inferring multiplicative and additive biases jointly from parameterised distributions of measured ellipticities, crucially without relying on external simulations and independently from cosmology. Shear biases are marginalised over the unknown hyper-parameters in the modelling, hence mitigating the impact of degeneracies. We apply the technique to a representative problem and show the performance of the estimation, even in the presence of noise. The method has a high potential for applicability to the calibration of weak lensing cosmic shear in current and future lensing surveys.</summary>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:56:31Z</published>
    <arxiv:comment>6 pages, 2 figures</arxiv:comment>
    <arxiv:primary_category term="astro-ph.CO"/>
    <author>
      <name>G. Congedo</name>
    </author>
    <author>
      <name>A. N. Taylor</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09920v1</id>
    <title>LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating</title>
    <updated>2025-12-10T18:54:30Z</updated>
    <link href="https://arxiv.org/abs/2512.09920v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09920v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:54:30Z</published>
    <arxiv:comment>8 pages</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Junting Chen</name>
    </author>
    <author>
      <name>Yunchuan Li</name>
    </author>
    <author>
      <name>Panfeng Jiang</name>
    </author>
    <author>
      <name>Jiacheng Du</name>
    </author>
    <author>
      <name>Zixuan Chen</name>
    </author>
    <author>
      <name>Chenrui Tie</name>
    </author>
    <author>
      <name>Jiajun Deng</name>
    </author>
    <author>
      <name>Lin Shao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09918v1</id>
    <title>Multiplicative Renormalization in Causal Perturbation Theory</title>
    <updated>2025-12-10T18:53:06Z</updated>
    <link href="https://arxiv.org/abs/2512.09918v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09918v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We construct multiplicative renormalization for the Epstein--Glaser renormalization scheme in perturbative Algebraic Quantum Field Theory: To this end, we fully combine the Connes--Kreimer renormalization framework with the Epstein--Glaser renormalization scheme. In particular, in addition to the already established position-space renormalization Hopf algebra, we also construct the renormalized Feynman rules and the counterterm map via an algebraic Birkhoff decomposition. This includes a discussion about the appropriate target algebra of regularized distributions and the renormalization scheme as a Rota--Baxter operator thereon. In particular, we show that the Hadamard singular part satisfies the Rota--Baxter property and thus relate factorization in Epstein--Glaser with multiplicativity in Connes--Kreimer. Next, we define $Z$-factors as the images of the counterterm map under the corresponding combinatorial Green's functions. This allows us to define the multiplicatively renormalized Lagrange density, for which we show that the corresponding Feynman rules are regular. Finally, we exemplify the developed theory by working out the specific case of $Ï†^3_6$-theory.</summary>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:53:06Z</published>
    <arxiv:comment>44 pages, 26 figures, article</arxiv:comment>
    <arxiv:primary_category term="math-ph"/>
    <author>
      <name>Jonah Epstein</name>
    </author>
    <author>
      <name>Arne Hofmann</name>
    </author>
    <author>
      <name>David Prinz</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09897v1</id>
    <title>SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments</title>
    <updated>2025-12-10T18:26:14Z</updated>
    <link href="https://arxiv.org/abs/2512.09897v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09897v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:26:14Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Haoye Lu</name>
    </author>
    <author>
      <name>Pavan Seshadri</name>
    </author>
    <author>
      <name>Kaheer Suleman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09893v1</id>
    <title>A Speculative GLRT-Backed Approach for Adversarial Resilience on Deep Learning-Based Array Processing</title>
    <updated>2025-12-10T18:19:44Z</updated>
    <link href="https://arxiv.org/abs/2512.09893v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09893v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Classical array processing methods such as the generalized likelihood ratio test (GLRT) provide statistically grounded solutions for signal detection and direction-of-arrival (DoA) estimation, but their high computational cost limits their use in low-latency settings. Deep learning (DL) has recently emerged as an efficient alternative, offering fast inference for array processing tasks. However, DL models lack statistical guarantees and, moreover, are highly susceptible to adversarial perturbations, raising fundamental concerns about their reliability in adversarial wireless environments. To address these challenges, we propose an adversarially resilient speculative array processing framework that consists of a low-latency DL classifier backed by a theoretically-grounded GLRT validator, where DL is used for fast speculative inference and later confirmed with the GLRT. We show that second order statistics of the received array, which the GLRT operates on, are spatially invariant to L-p bounded adversarial perturbations, providing adversarial robustness and theoretically-grounded validation of DL predictions. Empirical evaluations under multiple L-p bounds, perturbation designs, and perturbation magnitudes corroborate our theoretical findings, demonstrating the superior performance of our proposed framework in comparison to multiple state-of-the-art baselines.</summary>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T18:19:44Z</published>
    <arxiv:comment>12 pages, 3 figures, 2 tables</arxiv:comment>
    <arxiv:primary_category term="eess.SP"/>
    <author>
      <name>Nian-Cin Wang</name>
    </author>
    <author>
      <name>Rajeev Sahay</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.09870v1</id>
    <title>Tomographic characterization of non-Hermitian Hamiltonians in reciprocal space</title>
    <updated>2025-12-10T17:57:27Z</updated>
    <link href="https://arxiv.org/abs/2512.09870v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.09870v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Non-Hermitian Hamiltonians enrich quantum physics by extending conventional phase diagrams, enabling novel topological phenomena, and realizing exceptional points with potential applications in quantum sensing. Here, we present an experimental photonic platform capable of simulating a non-unitary quantum walk generated by a peculiar type of non-Hermitian Hamiltonian, largely unexplored in the literature. The novelty of this platform lies in its direct access to the reciprocal space, which enables us to scan the quasi-momentum across the entire Brillouin zone and thus achieve a precise tomographic reconstruction of the underlying non-Hermitian Hamiltonian, indicated by the comparison between theoretical predictions and experimental measurements. From the inferred Hamiltonian, it is possible to retrieve complex-valued band structures, resolve exceptional points in momentum space, and detect the associated parity-time symmetry breaking through eigenvector coalescence. Our results, presented entirely in quasi-momentum space, represent a substantial shift in perspective in the study of non-Hermitian phenomena.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-10T17:57:27Z</published>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Francesco Di Colandrea</name>
    </author>
    <author>
      <name>Fabrizio Pavan</name>
    </author>
    <author>
      <name>Sarvesh Bansal</name>
    </author>
    <author>
      <name>Paola Savarese</name>
    </author>
    <author>
      <name>Grazia Di Bello</name>
    </author>
    <author>
      <name>Giulio De Filippis</name>
    </author>
    <author>
      <name>Carmine Antonio Perroni</name>
    </author>
    <author>
      <name>Donato Farina</name>
    </author>
    <author>
      <name>Filippo Cardano</name>
    </author>
  </entry>
</feed>
