<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-02-28T00:50:37Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-02-27T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">107786</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.19405v1</id>
    <updated>2025-02-26T18:53:31Z</updated>
    <published>2025-02-26T18:53:31Z</published>
    <title>Verde: Verification via Refereed Delegation for Machine Learning
  Programs</title>
    <summary>  Machine learning programs, such as those performing inference, fine-tuning,
and training of LLMs, are commonly delegated to untrusted compute providers. To
provide correctness guarantees for the client, we propose adapting the
cryptographic notion of refereed delegation to the machine learning setting.
This approach enables a computationally limited client to delegate a program to
multiple untrusted compute providers, with a guarantee of obtaining the correct
result if at least one of them is honest. Refereed delegation of ML programs
poses two technical hurdles: (1) an arbitration protocol to resolve disputes
when compute providers disagree on the output, and (2) the ability to bitwise
reproduce ML programs across different hardware setups, For (1), we design
Verde, a dispute arbitration protocol that efficiently handles the large scale
and graph-based computational model of modern ML programs. For (2), we build
RepOps (Reproducible Operators), a library that eliminates hardware
"non-determinism" by controlling the order of floating point operations
performed on all hardware. Our implementation shows that refereed delegation
achieves both strong guarantees for clients and practical overheads for compute
providers.
</summary>
    <author>
      <name>Arasu Arun</name>
    </author>
    <author>
      <name>Adam St. Arnaud</name>
    </author>
    <author>
      <name>Alexey Titov</name>
    </author>
    <author>
      <name>Brian Wilcox</name>
    </author>
    <author>
      <name>Viktor Kolobaric</name>
    </author>
    <author>
      <name>Marc Brinkmann</name>
    </author>
    <author>
      <name>Oguzhan Ersoy</name>
    </author>
    <author>
      <name>Ben Fielding</name>
    </author>
    <author>
      <name>Joseph Bonneau</name>
    </author>
    <link href="http://arxiv.org/abs/2502.19405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19399v1</id>
    <updated>2025-02-26T18:47:53Z</updated>
    <published>2025-02-26T18:47:53Z</published>
    <title>DROID: Discrete-Time Simulation for Ring-Oscillator-Based Ising Design</title>
    <summary>  Many combinatorial problems can be mapped to Ising machines, i.e., networks
of coupled oscillators that settle to a minimum-energy ground state, from which
the problem solution is inferred. This work proposes DROID, a novel
event-driven method for simulating the evolution of a CMOS Ising machine to its
ground state. The approach is accurate under general delay-phase relations that
include the effects of the transistor nonlinearities and is computationally
efficient. On a realistic-size all-to-all coupled ring oscillator array, DROID
is nearly four orders of magnitude faster than a traditional HSPICE simulation
in predicting the evolution of a coupled oscillator system and is demonstrated
to attain a similar distribution of solutions as the hardware.
</summary>
    <author>
      <name>Abhimanyu Kumar</name>
    </author>
    <author>
      <name>Ramprasath S.</name>
    </author>
    <author>
      <name>Chris H. Kim</name>
    </author>
    <author>
      <name>Ulya R. Karpuzcu</name>
    </author>
    <author>
      <name>Sachin S. Sapatnekar</name>
    </author>
    <link href="http://arxiv.org/abs/2502.19399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19384v1</id>
    <updated>2025-02-26T18:30:20Z</updated>
    <published>2025-02-26T18:30:20Z</published>
    <title>Towards a robust approach to infer causality in molecular systems
  satisfying detailed balance</title>
    <summary>  The ability to distinguish between correlation and causation of variables in
molecular systems remains an interesting and open area of investigation. In
this work, we probe causality in a molecular system using two independent
computational methods that infer the causal direction through the language of
information transfer. Specifically, we demonstrate that a molecular dynamics
simulation involving a single Tryptophan in liquid water displays asymmetric
information transfer between specific collective variables, such as solute and
solvent coordinates. Analyzing a discrete Markov-state and Langevin dynamics on
a 2D free energy surface, we show that the same kind of asymmetries can emerge
even in extremely simple systems, undergoing equilibrium and time-reversible
dynamics. We use these model systems to rationalize the unidirectional
information transfer in the molecular system in terms of asymmetries in the
underlying free energy landscape and/or relaxation dynamics of the relevant
coordinates. Finally, we propose a computational experiment that allows one to
decide if an asymmetric information transfer between two variables corresponds
to a genuine causal link.
</summary>
    <author>
      <name>Vittorio Del Tatto</name>
    </author>
    <author>
      <name>Debarshi Banerjee</name>
    </author>
    <author>
      <name>Ali Hassanali</name>
    </author>
    <author>
      <name>Alessandro Laio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19353v1</id>
    <updated>2025-02-26T17:53:27Z</updated>
    <published>2025-02-26T17:53:27Z</published>
    <title>An emulation-based model for the projected correlation function</title>
    <summary>  Data from the ongoing \textit{Euclid} survey will map out billions of
galaxies in the Universe, covering more than a third of the sky. This data will
provide a wealth of information about the large-scale structure (LSS) of the
Universe and will have a significant impact on cosmology in the coming years.
In this paper, we introduce an emulator-based halo model approach to forward
model the relationship between cosmological parameters and the projected
galaxy-galaxy two-point correlation function (2PCF). Utilizing the large
\textsc{AbacusSummit} simulation suite, we emulate the 2PCF by generating
mock-galaxy catalogues within the Halo Occupation Distribution (HOD) framework.
Our emulator is designed to predict the 2PCF over scales $0.1 \leq r /
(h^{-1}\text{Mpc}) \leq 105$, from which we derive the projected correlation
function, independent of redshift space distortions. We demonstrate that the
emulator accurately predicts the projected correlation function over scales
$0.5 \leq r_\perp/(h^{-1}\text{Mpc}) \leq 40$, given a set of cosmological and
HOD parameters. This model is then employed in a parameter inference analysis,
showcasing its ability to constrain cosmological parameters. Our findings
indicate that while the projected correlation function places weak constraints
on several cosmological parameters due to its intrinsic lack of information,
additional clustering statistics are necessary to better probe the underlying
cosmology. Despite the simplified covariance matrix used in the likelihood
model, the posterior distributions of several cosmological parameters remain
broad, underscoring the need for a more comprehensive approach.
</summary>
    <author>
      <name>Vetle A. Vikenes</name>
    </author>
    <author>
      <name>Cheng-Zong Ruan</name>
    </author>
    <author>
      <name>David F. Mota</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19328v1</id>
    <updated>2025-02-26T17:19:12Z</updated>
    <published>2025-02-26T17:19:12Z</published>
    <title>Agentic Reward Modeling: Integrating Human Preferences with Verifiable
  Correctness Signals for Reliable Reward Systems</title>
    <summary>  Reward models (RMs) are crucial for the training and inference-time scaling
up of large language models (LLMs). However, existing reward models primarily
focus on human preferences, neglecting verifiable correctness signals which
have shown strong potential in training LLMs. In this paper, we propose agentic
reward modeling, a reward system that combines reward models with verifiable
correctness signals from different aspects to provide reliable rewards. We
empirically implement a reward agent, named RewardAgent, that combines human
preference rewards with two verifiable signals: factuality and instruction
following, to provide more reliable rewards. We conduct comprehensive
experiments on existing reward model benchmarks and inference time best-of-n
searches on real-world downstream tasks. RewardAgent significantly outperforms
vanilla reward models, demonstrating its effectiveness. We further construct
training preference pairs using RewardAgent and train an LLM with the DPO
objective, achieving superior performance on various NLP benchmarks compared to
conventional reward models. Our codes are publicly released to facilitate
further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).
</summary>
    <author>
      <name>Hao Peng</name>
    </author>
    <author>
      <name>Yunjia Qi</name>
    </author>
    <author>
      <name>Xiaozhi Wang</name>
    </author>
    <author>
      <name>Zijun Yao</name>
    </author>
    <author>
      <name>Bin Xu</name>
    </author>
    <author>
      <name>Lei Hou</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19298v1</id>
    <updated>2025-02-26T16:56:19Z</updated>
    <published>2025-02-26T16:56:19Z</published>
    <title>Agent-centric Information Access</title>
    <summary>  As large language models (LLMs) become more specialized, we envision a future
where millions of expert LLMs exist, each trained on proprietary data and
excelling in specific domains. In such a system, answering a query requires
selecting a small subset of relevant models, querying them efficiently, and
synthesizing their responses. This paper introduces a framework for
agent-centric information access, where LLMs function as knowledge agents that
are dynamically ranked and queried based on their demonstrated expertise.
Unlike traditional document retrieval, this approach requires inferring
expertise on the fly, rather than relying on static metadata or predefined
model descriptions. This shift introduces several challenges, including
efficient expert selection, cost-effective querying, response aggregation
across multiple models, and robustness against adversarial manipulation. To
address these issues, we propose a scalable evaluation framework that leverages
retrieval-augmented generation and clustering techniques to construct and
assess thousands of specialized models, with the potential to scale toward
millions.
</summary>
    <author>
      <name>Evangelos Kanoulas</name>
    </author>
    <author>
      <name>Panagiotis Eustratiadis</name>
    </author>
    <author>
      <name>Yongkang Li</name>
    </author>
    <author>
      <name>Yougang Lyu</name>
    </author>
    <author>
      <name>Vaishali Pal</name>
    </author>
    <author>
      <name>Gabrielle Poerwawinata</name>
    </author>
    <author>
      <name>Jingfen Qiao</name>
    </author>
    <author>
      <name>Zihan Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2502.19298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19295v1</id>
    <updated>2025-02-26T16:52:31Z</updated>
    <published>2025-02-26T16:52:31Z</published>
    <title>Complex LLM Planning via Automated Heuristics Discovery</title>
    <summary>  We consider enhancing large language models (LLMs) for complex planning
tasks. While existing methods allow LLMs to explore intermediate steps to make
plans, they either depend on unreliable self-verification or external verifiers
to evaluate these steps, which demand significant data and computations. Here,
we propose automated heuristics discovery (AutoHD), a novel approach that
enables LLMs to explicitly generate heuristic functions to guide inference-time
search, allowing accurate evaluation of intermediate states. These heuristic
functions are further refined through a heuristic evolution process, improving
their robustness and effectiveness. Our proposed method requires no additional
model training or fine-tuning, and the explicit definition of heuristic
functions generated by the LLMs provides interpretability and insights into the
reasoning process. Extensive experiments across diverse benchmarks demonstrate
significant gains over multiple baselines, including nearly twice the accuracy
on some datasets, establishing our approach as a reliable and interpretable
solution for complex planning tasks.
</summary>
    <author>
      <name>Hongyi Ling</name>
    </author>
    <author>
      <name>Shubham Parashar</name>
    </author>
    <author>
      <name>Sambhav Khurana</name>
    </author>
    <author>
      <name>Blake Olson</name>
    </author>
    <author>
      <name>Anwesha Basu</name>
    </author>
    <author>
      <name>Gaurangi Sinha</name>
    </author>
    <author>
      <name>Zhengzhong Tu</name>
    </author>
    <author>
      <name>James Caverlee</name>
    </author>
    <author>
      <name>Shuiwang Ji</name>
    </author>
    <link href="http://arxiv.org/abs/2502.19295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19285v1</id>
    <updated>2025-02-26T16:45:09Z</updated>
    <published>2025-02-26T16:45:09Z</published>
    <title>On the Importance of Text Preprocessing for Multimodal Representation
  Learning and Pathology Report Generation</title>
    <summary>  Vision-language models in pathology enable multimodal case retrieval and
automated report generation. Many of the models developed so far, however, have
been trained on pathology reports that include information which cannot be
inferred from paired whole slide images (e.g., patient history), potentially
leading to hallucinated sentences in generated reports. To this end, we
investigate how the selection of information from pathology reports for
vision-language modeling affects the quality of the multimodal representations
and generated reports. More concretely, we compare a model trained on full
reports against a model trained on preprocessed reports that only include
sentences describing the cell and tissue appearances based on the H&amp;E-stained
slides. For the experiments, we built upon the BLIP-2 framework and used a
cutaneous melanocytic lesion dataset of 42,433 H&amp;E-stained whole slide images
and 19,636 corresponding pathology reports. Model performance was assessed
using image-to-text and text-to-image retrieval, as well as qualitative
evaluation of the generated reports by an expert pathologist. Our results
demonstrate that text preprocessing prevents hallucination in report
generation. Despite the improvement in the quality of the generated reports,
training the vision-language model on full reports showed better cross-modal
retrieval performance.
</summary>
    <author>
      <name>Ruben T. Lucassen</name>
    </author>
    <author>
      <name>Tijn van de Luijtgaarden</name>
    </author>
    <author>
      <name>Sander P. J. Moonemans</name>
    </author>
    <author>
      <name>Gerben E. Breimer</name>
    </author>
    <author>
      <name>Willeke A. M. Blokx</name>
    </author>
    <author>
      <name>Mitko Veta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19261v1</id>
    <updated>2025-02-26T16:06:36Z</updated>
    <published>2025-02-26T16:06:36Z</published>
    <title>Drop-Upcycling: Training Sparse Mixture of Experts with Partial
  Re-initialization</title>
    <summary>  The Mixture of Experts (MoE) architecture reduces the training and inference
cost significantly compared to a dense model of equivalent capacity. Upcycling
is an approach that initializes and trains an MoE model using a pre-trained
dense model. While upcycling leads to initial performance gains, the training
progresses slower than when trained from scratch, leading to suboptimal
performance in the long term. We propose Drop-Upcycling - a method that
effectively addresses this problem. Drop-Upcycling combines two seemingly
contradictory approaches: utilizing the knowledge of pre-trained dense models
while statistically re-initializing some parts of the weights. This approach
strategically promotes expert specialization, significantly enhancing the MoE
model's efficiency in knowledge acquisition. Extensive large-scale experiments
demonstrate that Drop-Upcycling significantly outperforms previous MoE
construction methods in the long term, specifically when training on hundreds
of billions of tokens or more. As a result, our MoE model with 5.9B active
parameters achieves comparable performance to a 13B dense model in the same
model family, while requiring approximately 1/4 of the training FLOPs. All
experimental resources, including source code, training data, model checkpoints
and logs, are publicly available to promote reproducibility and future research
on MoE.
</summary>
    <author>
      <name>Taishi Nakamura</name>
    </author>
    <author>
      <name>Takuya Akiba</name>
    </author>
    <author>
      <name>Kazuki Fujii</name>
    </author>
    <author>
      <name>Yusuke Oda</name>
    </author>
    <author>
      <name>Rio Yokota</name>
    </author>
    <author>
      <name>Jun Suzuki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 13th International Conference on Learning
  Representations (ICLR 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19253v1</id>
    <updated>2025-02-26T15:58:35Z</updated>
    <published>2025-02-26T15:58:35Z</published>
    <title>U-Net 3+ for Anomalous Diffusion Analysis enhanced with Mixture
  Estimates (U-AnD-ME) in particle-tracking data</title>
    <summary>  Biophysical processes within living systems rely on encounters and
interactions between molecules in complex environments such as cells. They are
often described by anomalous diffusion transport. Recent advances in
single-molecule microscopy and particle-tracking techniques have yielded an
abundance of data in the form of videos and trajectories that contain critical
information about these biologically significant processes. However, standard
approaches for characterizing anomalous diffusion from these measurements often
struggle in cases of practical interest, e.g. due to short, noisy trajectories.
Fully exploiting this data therefore requires the development of advanced
analysis methods -- a core goal at the heart of the recent international
Anomalous Diffusion Challenges. Here, we introduce a novel machine-learning
framework, U-net 3+ for Anomalous Diffusion analysis enhanced with Mixture
Estimates (U-AnD-ME), that applies a U-Net 3+ based neural network alongside
Gaussian mixture models to enable highly accurate characterisation of
single-particle tracking data. In the 2024 Anomalous Diffusion Challenge,
U-AnD-ME outperformed all other participating methods for the analysis of
two-dimensional anomalous diffusion trajectories at both single-trajectory and
ensemble levels. Using a large dataset inspired by the Challenge, we further
characterize the performance of U-AnD-ME in segmenting trajectories and
inferring anomalous diffusion properties.
</summary>
    <author>
      <name>Solomon Asghar</name>
    </author>
    <author>
      <name>Ran Ni</name>
    </author>
    <author>
      <name>Giorgio Volpe</name>
    </author>
    <link href="http://arxiv.org/abs/2502.19253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
