<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-30T00:56:39Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">113562</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.22663v1</id>
    <updated>2025-05-28T17:59:57Z</updated>
    <published>2025-05-28T17:59:57Z</published>
    <title>Training Free Stylized Abstraction</title>
    <summary>  Stylized abstraction synthesizes visually exaggerated yet semantically
faithful representations of subjects, balancing recognizability with perceptual
distortion. Unlike image-to-image translation, which prioritizes structural
fidelity, stylized abstraction demands selective retention of identity cues
while embracing stylistic divergence, especially challenging for
out-of-distribution individuals. We propose a training-free framework that
generates stylized abstractions from a single image using inference-time
scaling in vision-language models (VLLMs) to extract identity-relevant
features, and a novel cross-domain rectified flow inversion strategy that
reconstructs structure based on style-dependent priors. Our method adapts
structural restoration dynamically through style-aware temporal scheduling,
enabling high-fidelity reconstructions that honor both subject and style. It
supports multi-round abstraction-aware generation without fine-tuning. To
evaluate this task, we introduce StyleBench, a GPT-based human-aligned metric
suited for abstract styles where pixel-level similarity fails. Experiments
across diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong
generalization to unseen identities and styles in a fully open-source setup.
</summary>
    <author>
      <name>Aimon Rahman</name>
    </author>
    <author>
      <name>Kartik Narayan</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://kartik-3004.github.io/TF-SA/</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.22663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22662v1</id>
    <updated>2025-05-28T17:59:53Z</updated>
    <published>2025-05-28T17:59:53Z</published>
    <title>AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models</title>
    <summary>  The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special &lt;EASY&gt;
token. We then use &lt;EASY&gt; token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.
</summary>
    <author>
      <name>Feng Luo</name>
    </author>
    <author>
      <name>Yu-Neng Chuang</name>
    </author>
    <author>
      <name>Guanchu Wang</name>
    </author>
    <author>
      <name>Hoang Anh Duy Le</name>
    </author>
    <author>
      <name>Shaochen Zhong</name>
    </author>
    <author>
      <name>Hongyi Liu</name>
    </author>
    <author>
      <name>Jiayi Yuan</name>
    </author>
    <author>
      <name>Yang Sui</name>
    </author>
    <author>
      <name>Vladimir Braverman</name>
    </author>
    <author>
      <name>Vipin Chaudhary</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.22662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22659v1</id>
    <updated>2025-05-28T17:59:29Z</updated>
    <published>2025-05-28T17:59:29Z</published>
    <title>Network Generating Processes With Self Exciting Arrival Times</title>
    <summary>  In this paper, we propose a novel modeling framework for time-evolving
networks allowing for long-term dependence in network features that update in
continuous time. Dynamic network growth is functionally parameterized via the
conditional intensity of a marked point process. This characterization enables
flexible modeling of both the time of updates and the network updates
themselves, dependent on the entire left-continuous sample path. We propose a
path-dependent nonlinear marked Hawkes process as an expressive platform for
modeling such data; its dynamic mark space embeds the time-evolving network. We
establish stability conditions, demonstrate simulation and subsequent feasible
likelihood-based inference through numerical study, and present an application
to conference attendee social network data. The resulting methodology serves as
a general framework that can be readily adapted to a wide range of network
topologies and point process model specifications.
</summary>
    <author>
      <name>Duncan A Clark</name>
    </author>
    <author>
      <name>Conor J. Kresin</name>
    </author>
    <author>
      <name>Charlotte M. Jones-Todd</name>
    </author>
    <link href="http://arxiv.org/abs/2505.22659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22654v1</id>
    <updated>2025-05-28T17:59:08Z</updated>
    <published>2025-05-28T17:59:08Z</published>
    <title>VScan: Rethinking Visual Token Reduction for Efficient Large
  Vision-Language Models</title>
    <summary>  Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.
</summary>
    <author>
      <name>Ce Zhang</name>
    </author>
    <author>
      <name>Kaixin Ma</name>
    </author>
    <author>
      <name>Tianqing Fang</name>
    </author>
    <author>
      <name>Wenhao Yu</name>
    </author>
    <author>
      <name>Hongming Zhang</name>
    </author>
    <author>
      <name>Zhisong Zhang</name>
    </author>
    <author>
      <name>Yaqi Xie</name>
    </author>
    <author>
      <name>Katia Sycara</name>
    </author>
    <author>
      <name>Haitao Mi</name>
    </author>
    <author>
      <name>Dong Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.22654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22650v1</id>
    <updated>2025-05-28T17:57:29Z</updated>
    <published>2025-05-28T17:57:29Z</published>
    <title>On Learning Verifiers for Chain-of-Thought Reasoning</title>
    <summary>  Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.
</summary>
    <author>
      <name>Maria-Florina Balcan</name>
    </author>
    <author>
      <name>Avrim Blum</name>
    </author>
    <author>
      <name>Zhiyuan Li</name>
    </author>
    <author>
      <name>Dravyansh Sharma</name>
    </author>
    <link href="http://arxiv.org/abs/2505.22650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22646v1</id>
    <updated>2025-05-28T17:56:53Z</updated>
    <published>2025-05-28T17:56:53Z</published>
    <title>Path-Dependent SDEs: Solutions and Parameter Estimation</title>
    <summary>  We develop a consistent method for estimating the parameters of a rich class
of path-dependent SDEs, called signature SDEs, which can model general
path-dependent phenomena. Path signatures are iterated integrals of a given
path with the property that any sufficiently nice function of the path can be
approximated by a linear functional of its signatures. This is why we model the
drift and diffusion of our signature SDE as linear functions of path
signatures. We provide conditions that ensure the existence and uniqueness of
solutions to a general signature SDE. We then introduce the Expected Signature
Matching Method (ESMM) for linear signature SDEs, which enables inference of
the signature-dependent drift and diffusion coefficients from observed
trajectories. Furthermore, we prove that ESMM is consistent: given sufficiently
many samples and Picard iterations used by the method, the parameters estimated
by the ESMM approach the true parameter with arbitrary precision. Finally, we
demonstrate on a variety of empirical simulations that our ESMM accurately
infers the drift and diffusion parameters from observed trajectories. While
parameter estimation is often restricted by the need for a suitable parametric
model, this work makes progress toward a completely general framework for SDE
parameter estimation, using signature terms to model arbitrary path-independent
and path-dependent processes.
</summary>
    <author>
      <name>Pardis Semnani</name>
    </author>
    <author>
      <name>Vincent Guan</name>
    </author>
    <author>
      <name>Elina Robeva</name>
    </author>
    <author>
      <name>Darrick Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 4 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.22646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60L20, 60L90, 62M99, 62M09" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22637v1</id>
    <updated>2025-05-28T17:53:31Z</updated>
    <published>2025-05-28T17:53:31Z</published>
    <title>Understanding (Un)Reliability of Steering Vectors in Language Models</title>
    <summary>  Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.
</summary>
    <author>
      <name>Joschka Braun</name>
    </author>
    <author>
      <name>Carsten Eickhoff</name>
    </author>
    <author>
      <name>David Krueger</name>
    </author>
    <author>
      <name>Seyed Ali Bahrainian</name>
    </author>
    <author>
      <name>Dmitrii Krasheninnikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 10 figures. Presented at the ICLR 2025 Workshop on
  Foundation Models in the Wild</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.22637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22636v1</id>
    <updated>2025-05-28T17:51:17Z</updated>
    <published>2025-05-28T17:51:17Z</published>
    <title>ObjectClear: Complete Object Removal via Object-Effect Attention</title>
    <summary>  Object removal requires eliminating not only the target object but also its
effects, such as shadows and reflections. However, diffusion-based inpainting
methods often produce artifacts, hallucinate content, alter background, and
struggle to remove object effects accurately. To address this challenge, we
introduce a new dataset for OBject-Effect Removal, named OBER, which provides
paired images with and without object effects, along with precise masks for
both objects and their associated visual artifacts. The dataset comprises
high-quality captured and simulated data, covering diverse object categories
and complex multi-object scenes. Building on OBER, we propose a novel
framework, ObjectClear, which incorporates an object-effect attention mechanism
to guide the model toward the foreground removal regions by learning attention
masks, effectively decoupling foreground removal from background
reconstruction. Furthermore, the predicted attention map enables an
attention-guided fusion strategy during inference, greatly preserving
background details. Extensive experiments demonstrate that ObjectClear
outperforms existing methods, achieving improved object-effect removal quality
and background fidelity, especially in complex scenarios.
</summary>
    <author>
      <name>Jixin Zhao</name>
    </author>
    <author>
      <name>Shangchen Zhou</name>
    </author>
    <author>
      <name>Zhouxia Wang</name>
    </author>
    <author>
      <name>Peiqing Yang</name>
    </author>
    <author>
      <name>Chen Change Loy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://zjx0101.github.io/projects/ObjectClear/</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.22636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22632v1</id>
    <updated>2025-05-28T17:50:20Z</updated>
    <published>2025-05-28T17:50:20Z</published>
    <title>Towards the Efficient Inference by Incorporating Automated Computational
  Phenotypes under Covariate Shift</title>
    <summary>  Collecting gold-standard phenotype data via manual extraction is typically
labor-intensive and slow, whereas automated computational phenotypes (ACPs)
offer a systematic and much faster alternative. However, simply replacing the
gold-standard with ACPs, without acknowledging their differences, could lead to
biased results and misleading conclusions. Motivated by the complexity of
incorporating ACPs while maintaining the validity of downstream analyses, in
this paper, we consider a semi-supervised learning setting that consists of
both labeled data (with gold-standard) and unlabeled data (without
gold-standard), under the covariate shift framework. We develop doubly robust
and semiparametrically efficient estimators that leverage ACPs for general
target parameters in the unlabeled and combined populations. In addition, we
carefully analyze the efficiency gains achieved by incorporating ACPs,
comparing scenarios with and without their inclusion. Notably, we identify that
ACPs for the unlabeled data, instead of for the labeled data, drive the
enhanced efficiency gains. To validate our theoretical findings, we conduct
comprehensive synthetic experiments and apply our method to multiple real-world
datasets, confirming the practical advantages of our approach.
\hfill{\texttt{Code}:
\href{https://github.com/brucejunjin/ICML2025-ACPCS}{\faGithub}}
</summary>
    <author>
      <name>Chao Ying</name>
    </author>
    <author>
      <name>Jun Jin</name>
    </author>
    <author>
      <name>Yi Guo</name>
    </author>
    <author>
      <name>Xiudi Li</name>
    </author>
    <author>
      <name>Muxuan Liang</name>
    </author>
    <author>
      <name>Jiwei Zhao</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2025 ICML</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2505.22632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22618v1</id>
    <updated>2025-05-28T17:39:15Z</updated>
    <published>2025-05-28T17:39:15Z</published>
    <title>Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV
  Cache and Parallel Decoding</title>
    <summary>  Diffusion-based large language models (Diffusion LLMs) have shown promise for
non-autoregressive text generation with parallel decoding capabilities.
However, the practical inference speed of open-sourced Diffusion LLMs often
lags behind autoregressive models due to the lack of Key-Value (KV) Cache and
quality degradation when decoding multiple tokens simultaneously. To bridge
this gap, we introduce a novel block-wise approximate KV Cache mechanism
tailored for bidirectional diffusion models, enabling cache reuse with
negligible performance drop. Additionally, we identify the root cause of
generation quality degradation in parallel decoding as the disruption of token
dependencies under the conditional independence assumption. To address this, we
propose a confidence-aware parallel decoding strategy that selectively decodes
tokens exceeding a confidence threshold, mitigating dependency violations and
maintaining generation quality. Experimental results on LLaDA and Dream models
across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$
throughput} improvement with minimal accuracy loss, closing the performance gap
with autoregressive models and paving the way for practical deployment of
Diffusion LLMs.
</summary>
    <author>
      <name>Chengyue Wu</name>
    </author>
    <author>
      <name>Hao Zhang</name>
    </author>
    <author>
      <name>Shuchen Xue</name>
    </author>
    <author>
      <name>Zhijian Liu</name>
    </author>
    <author>
      <name>Shizhe Diao</name>
    </author>
    <author>
      <name>Ligeng Zhu</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Enze Xie</name>
    </author>
    <link href="http://arxiv.org/abs/2505.22618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
