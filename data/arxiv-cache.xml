<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-01-07T01:02:17Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-01-07T01:02:17Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>130114</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2601.02352v1</id>
    <title>On the temperature of the quantum black hole</title>
    <updated>2026-01-05T18:54:38Z</updated>
    <link href="https://arxiv.org/abs/2601.02352v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02352v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A nontrivial peculiarity of general relativity is that when the horizon region of black holes is rendered harmless, the exterior doubles, resulting in a causally disconnected parallel universe. This intricacy plays a central role in 't Hooft's unitarity arguments, emphasising an exact identification between the physical universe and its duplicate on the other side of the horizon. However, it leads to another tension in the form of a factor of two correction in Hawking's temperature. This discrepancy is concerning because the Rindler temperature is universal and complies with the Bekenstein-Hawking entropy. We demonstrate that the mismatch in the Boltzmann factor gets fixed if the state that forms the corresponding density matrix adopts a generalised thermofield double structure. That leaves room for some interesting discussion.</summary>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T18:54:38Z</published>
    <arxiv:comment>6 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="hep-th"/>
    <author>
      <name>Abram Akal</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02346v1</id>
    <title>Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling</title>
    <updated>2026-01-05T18:44:27Z</updated>
    <link href="https://arxiv.org/abs/2601.02346v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02346v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T18:44:27Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name> Falcon LLM Team</name>
    </author>
    <author>
      <name>Iheb Chaabane</name>
    </author>
    <author>
      <name>Puneesh Khanna</name>
    </author>
    <author>
      <name>Suhail Mohmad</name>
    </author>
    <author>
      <name>Slim Frikha</name>
    </author>
    <author>
      <name>Shi Hu</name>
    </author>
    <author>
      <name>Abdalgader Abubaker</name>
    </author>
    <author>
      <name>Reda Alami</name>
    </author>
    <author>
      <name>Mikhail Lubinets</name>
    </author>
    <author>
      <name>Mohamed El Amine Seddik</name>
    </author>
    <author>
      <name>Hakim Hacid</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02336v1</id>
    <title>The Sequential Monte Carlo goes NUTS: Boosting Gravitational-Wave Inference</title>
    <updated>2026-01-05T18:31:26Z</updated>
    <link href="https://arxiv.org/abs/2601.02336v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02336v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sequential Monte Carlo (SMC) methods have recently been applied to gravitational-wave inference as a powerful alternative to standard sampling techniques, such as Nested Sampling. At the same time, gradient-based Markov Chain Monte Carlo algorithms, most notably the No-U-Turn Sampler (NUTS), provide an efficient way to explore high-dimensional parameter spaces. In this work we present SHARPy, a Bayesian inference framework that combines the parallelism and evidence-estimation capabilities of SMC with the state-of-the-art sampling performance of NUTS. Moreover, SHARPy exploits the local geometric structure of the posterior to further improve efficiency. Built on JAX and accelerated on GPUs, SHARPy performs gravitational-wave inference on binary black-hole events in around ten minutes, yielding posterior samples and Bayesian evidence estimates that are consistent with those obtained through Nested Sampling. This work sets a new milestone in GW inference with likelihood-based methods and paves the way for model comparison tasks to be accomplished in minutes.</summary>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T18:31:26Z</published>
    <arxiv:primary_category term="gr-qc"/>
    <author>
      <name>Gabriele Demasi</name>
    </author>
    <author>
      <name>Giulia Capurri</name>
    </author>
    <author>
      <name>Massimo Lenti</name>
    </author>
    <author>
      <name>Angelo Ricciardone</name>
    </author>
    <author>
      <name>Barbara Patricelli</name>
    </author>
    <author>
      <name>Adriano Frattale Mascioli</name>
    </author>
    <author>
      <name>Lorenzo Piccari</name>
    </author>
    <author>
      <name>Saulo Albuquerque</name>
    </author>
    <author>
      <name>Gianluca M. Guidi</name>
    </author>
    <author>
      <name>Francesco Pannarale</name>
    </author>
    <author>
      <name>Giulia Stratta</name>
    </author>
    <author>
      <name>Walter Del Pozzo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02329v1</id>
    <title>BEDS: Bayesian Emergent Dissipative Structures</title>
    <updated>2026-01-05T18:21:02Z</updated>
    <link href="https://arxiv.org/abs/2601.02329v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02329v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.
  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.
  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T18:21:02Z</published>
    <arxiv:comment>19 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Laurent Caraffa</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02322v1</id>
    <title>Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction</title>
    <updated>2026-01-05T18:13:02Z</updated>
    <link href="https://arxiv.org/abs/2601.02322v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02322v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T18:13:02Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Shuozhi Zuo</name>
    </author>
    <author>
      <name>Yixin Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02320v1</id>
    <title>Estimating Text Temperature</title>
    <updated>2026-01-05T18:09:41Z</updated>
    <link href="https://arxiv.org/abs/2601.02320v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02320v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T18:09:41Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Nikolay Mikhaylovskiy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02314v1</id>
    <title>Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents</title>
    <updated>2026-01-05T18:05:29Z</updated>
    <link href="https://arxiv.org/abs/2601.02314v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02314v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T18:05:29Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Sourena Khanzadeh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02305v1</id>
    <title>On Statistical Inference for Rates of Change in Spatial Processes over Riemannian Manifolds</title>
    <updated>2026-01-05T17:47:24Z</updated>
    <link href="https://arxiv.org/abs/2601.02305v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02305v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Statistical inference for spatial processes from partially realized or scattered data has seen voluminous developments in diverse areas ranging from environmental sciences to business and economics. Inference on the associated rates of change has seen some recent developments. The literature has been restricted to Euclidean domains, where inference is sought on directional derivatives, rates along a chosen direction of interest, at arbitrary locations. Inference for higher order rates, particularly directional curvature has also proved useful in these settings. Modern spatial data often arise from non-Euclidean domains. This manuscript particularly considers spatial processes defined over compact Riemannian manifolds. We develop a comprehensive inferential framework for spatial rates of change for such processes over vector fields. In doing so, we formalize smoothness of process realizations and construct differential processes -- the derivative and curvature processes. We derive conditions for kernels that ensure the existence of these processes and establish validity of the joint multivariate process consisting of the ``parent'' Gaussian process (GP) over the manifold and the associated differential processes. Predictive inference on these rates is devised conditioned on the realized process over the manifold. Manifolds arise as polyhedral meshes in practice. The success of our simulation experiments for assessing derivatives for processes observed over such meshes validate our theoretical findings. By enhancing our understanding of GPs on manifolds, this manuscript unlocks a variety of potential applications in machine learning and statistics where GPs have seen wide usage. We propose a fully model-based approach to inference on the differential processes arising from a spatial process from partially observed or realized data across scattered location on a manifold.</summary>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T17:47:24Z</published>
    <arxiv:primary_category term="math.ST"/>
    <author>
      <name>Didong Li</name>
    </author>
    <author>
      <name>Aritra Halder</name>
    </author>
    <author>
      <name>Sudipto Banerjee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02298v1</id>
    <title>Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)</title>
    <updated>2026-01-05T17:33:16Z</updated>
    <link href="https://arxiv.org/abs/2601.02298v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02298v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T17:33:16Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Mahmoud Elgenedy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.02297v1</id>
    <title>The Polarization and Magnetic Field of the Radio Arc as Observed by ALMA at 100 GHz</title>
    <updated>2026-01-05T17:32:18Z</updated>
    <link href="https://arxiv.org/abs/2601.02297v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.02297v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The unique Galactic Center non-thermal filaments (NTFs) have been a focus of investigations for over 40 years. The most prominent manifestation of the NTFs is a bundle of parallel filaments known as the Radio Arc. Radio polarimetric observations made with the Very Large Array (VLA) at 10 GHz have revealed an alternating magnetic field pattern in the Radio Arc that could either be a result of multiple field systems being encountered along the line of sight or an intrinsic feature of the Radio Arc. These VLA observations were not able to distinguish between these possibilities due to the large rotation measures encountered towards the source. We present ALMA 100 GHz observations of the Radio Arc that are not impacted by significant Faraday effects. The observations reported here represent both the first time that ALMA has been used to study the NTFs and the first time 100 GHz polarimetric observations have been conducted on the Radio Arc. We find a uniformly rotated magnetic field with respect to the NTF filament orientation, with the angle of rotation being constant along the length of each filament. However, we find a systematically different magnetic field orientation in different Radio Arc filaments. We use this field pattern to update our understanding of the line-of-sight structures local to the Radio Arc. We find that the magnetic field inferred from our ALMA observations is likely a result either of confusion from multiple magnetic field systems or because the polarization is centrally concentrated within the NTF filaments.</summary>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-05T17:32:18Z</published>
    <arxiv:primary_category term="astro-ph.GA"/>
    <author>
      <name>Nora Salem</name>
    </author>
    <author>
      <name>Dylan M. Paré</name>
    </author>
    <author>
      <name>Paulo Cortes</name>
    </author>
    <author>
      <name>Mark R. Morris</name>
    </author>
    <author>
      <name>Valentin J. M. Le Gouellec</name>
    </author>
  </entry>
</feed>
