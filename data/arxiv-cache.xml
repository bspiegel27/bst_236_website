<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-12T00:51:13Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-03-11T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">108535</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.07482v1</id>
    <updated>2025-03-10T15:58:43Z</updated>
    <published>2025-03-10T15:58:43Z</published>
    <title>Efficient Membership Inference Attacks by Bayesian Neural Network</title>
    <summary>  Membership Inference Attacks (MIAs) aim to estimate whether a specific data
point was used in the training of a given model. Previous attacks often utilize
multiple reference models to approximate the conditional score distribution,
leading to significant computational overhead. While recent work leverages
quantile regression to estimate conditional thresholds, it fails to capture
epistemic uncertainty, resulting in bias in low-density regions. In this work,
we propose a novel approach - Bayesian Membership Inference Attack (BMIA),
which performs conditional attack through Bayesian inference. In particular, we
transform a trained reference model into Bayesian neural networks by Laplace
approximation, enabling the direct estimation of the conditional score
distribution by probabilistic model parameters. Our method addresses both
epistemic and aleatoric uncertainty with only a reference model, enabling
efficient and powerful MIA. Extensive experiments on five datasets demonstrate
the effectiveness and efficiency of BMIA.
</summary>
    <author>
      <name>Zhenlong Liu</name>
    </author>
    <author>
      <name>Wenyu Jiang</name>
    </author>
    <author>
      <name>Feng Zhou</name>
    </author>
    <author>
      <name>Hongxin Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.07482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07475v1</id>
    <updated>2025-03-10T15:49:58Z</updated>
    <published>2025-03-10T15:49:58Z</published>
    <title>Sample Complexity of Nonparametric Closeness Testing for Continuous
  Distributions and Its Application to Causal Discovery with Hidden Confounding</title>
    <summary>  We study the problem of closeness testing for continuous distributions and
its implications for causal discovery. Specifically, we analyze the sample
complexity of distinguishing whether two multidimensional continuous
distributions are identical or differ by at least $\epsilon$ in terms of
Kullback-Leibler (KL) divergence under non-parametric assumptions. To this end,
we propose an estimator of KL divergence which is based on the von Mises
expansion. Our closeness test attains optimal parametric rates under smoothness
assumptions. Equipped with this test, which serves as a building block of our
causal discovery algorithm to identify the causal structure between two
multidimensional random variables, we establish sample complexity guarantees
for our causal discovery method. To the best of our knowledge, this work is the
first work that provides sample complexity guarantees for distinguishing cause
and effect in multidimensional non-linear models with non-Gaussian continuous
variables in the presence of unobserved confounding.
</summary>
    <author>
      <name>Fateme Jamshidi</name>
    </author>
    <author>
      <name>Sina Akbari</name>
    </author>
    <author>
      <name>Negar Kiyavash</name>
    </author>
    <link href="http://arxiv.org/abs/2503.07475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07465v1</id>
    <updated>2025-03-10T15:42:59Z</updated>
    <published>2025-03-10T15:42:59Z</published>
    <title>YOLOE: Real-Time Seeing Anything</title>
    <summary>  Object detection and segmentation are widely employed in computer vision
applications, yet conventional models like YOLO series, while efficient and
accurate, are limited by predefined categories, hindering adaptability in open
scenarios. Recent open-set methods leverage text prompts, visual cues, or
prompt-free paradigm to overcome this, but often compromise between performance
and efficiency due to high computational demands or deployment complexity. In
this work, we introduce YOLOE, which integrates detection and segmentation
across diverse open prompt mechanisms within a single highly efficient model,
achieving real-time seeing anything. For text prompts, we propose
Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines
pretrained textual embeddings via a re-parameterizable lightweight auxiliary
network and enhances visual-textual alignment with zero inference and
transferring overhead. For visual prompts, we present Semantic-Activated Visual
Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches
to bring improved visual embedding and accuracy with minimal complexity. For
prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.
It utilizes a built-in large vocabulary and specialized embedding to identify
all objects, avoiding costly language model dependency. Extensive experiments
show YOLOE's exceptional zero-shot performance and transferability with high
inference efficiency and low training cost. Notably, on LVIS, with 3$\times$
less training cost and 1.4$\times$ inference speedup, YOLOE-v8-S surpasses
YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6
AP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\times$ less
training time. Code and models are available at
https://github.com/THU-MIG/yoloe.
</summary>
    <author>
      <name>Ao Wang</name>
    </author>
    <author>
      <name>Lihao Liu</name>
    </author>
    <author>
      <name>Hui Chen</name>
    </author>
    <author>
      <name>Zijia Lin</name>
    </author>
    <author>
      <name>Jungong Han</name>
    </author>
    <author>
      <name>Guiguang Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures;</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.07465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07459v1</id>
    <updated>2025-03-10T15:38:44Z</updated>
    <published>2025-03-10T15:38:44Z</published>
    <title>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning</title>
    <summary>  Large Language Models (LLMs) have shown impressive performance on existing
medical question-answering benchmarks. This high performance makes it
increasingly difficult to meaningfully evaluate and differentiate advanced
methods. We present MedAgentsBench, a benchmark that focuses on challenging
medical questions requiring multi-step clinical reasoning, diagnosis
formulation, and treatment planning-scenarios where current models still
struggle despite their strong performance on standard tests. Drawing from seven
established medical datasets, our benchmark addresses three key limitations in
existing evaluations: (1) the prevalence of straightforward questions where
even base models achieve high performance, (2) inconsistent sampling and
evaluation protocols across studies, and (3) lack of systematic analysis of the
interplay between performance, cost, and inference time. Through experiments
with various base models and reasoning methods, we demonstrate that the latest
thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in
complex medical reasoning tasks. Additionally, advanced search-based agent
methods offer promising performance-to-cost ratios compared to traditional
approaches. Our analysis reveals substantial performance gaps between model
families on complex questions and identifies optimal model selections for
different computational constraints. Our benchmark and evaluation framework are
publicly available at https://github.com/gersteinlab/medagents-benchmark.
</summary>
    <author>
      <name>Xiangru Tang</name>
    </author>
    <author>
      <name>Daniel Shao</name>
    </author>
    <author>
      <name>Jiwoong Sohn</name>
    </author>
    <author>
      <name>Jiapeng Chen</name>
    </author>
    <author>
      <name>Jiayi Zhang</name>
    </author>
    <author>
      <name>Jinyu Xiang</name>
    </author>
    <author>
      <name>Fang Wu</name>
    </author>
    <author>
      <name>Yilun Zhao</name>
    </author>
    <author>
      <name>Chenglin Wu</name>
    </author>
    <author>
      <name>Wenqi Shi</name>
    </author>
    <author>
      <name>Arman Cohan</name>
    </author>
    <author>
      <name>Mark Gerstein</name>
    </author>
    <link href="http://arxiv.org/abs/2503.07459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07458v1</id>
    <updated>2025-03-10T15:38:03Z</updated>
    <published>2025-03-10T15:38:03Z</published>
    <title>Causality violation of Schrödinger-Newton equation: direct test on
  the horizon?</title>
    <summary>  We quote a definitive simple proof that neither classical stochastic dynamics
nor quantum dynamics can be nonlinear if we stick to their standard statistical
interpretations. A recently proposed optomechanical test of gravity's
classicality versus quantumness is based on the nonlinear Schr\"odinger-Newton
equation (SNE) which is the nonrelativistic limit of standard semiclassical
gravity. While in typical cosmological applications of semiclassical gravity
the predicted violation of causality is ignored, it cannot be disregarded in
applications of the SNE in high sensitive laboratory tests hoped for the coming
years. We reveal that, in a recently designed experiment, quantum optical
monitoring of massive probes predicts fake action-at-a-distance (acausality) on
a single probe already. The proposed experiment might first include the direct
test of this acausality.
</summary>
    <author>
      <name>Lajos Diósi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5pp, to appear in Journal of Physics: Conference Series (2025),
  proceedings of DICE2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.07458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07453v1</id>
    <updated>2025-03-10T15:31:42Z</updated>
    <published>2025-03-10T15:31:42Z</published>
    <title>Is a Good Foundation Necessary for Efficient Reinforcement Learning? The
  Computational Role of the Base Model in Exploration</title>
    <summary>  Language model alignment (or, reinforcement learning) techniques that
leverage active exploration -- deliberately encouraging the model to produce
diverse, informative responses -- offer the promise of super-human
capabilities. However, current understanding of algorithm design primitives for
computationally efficient exploration with language models is limited. To
better understand how to leverage access to powerful pre-trained generative
models to improve the efficiency of exploration, we introduce a new
computational framework for RL with language models, in which the learner
interacts with the model through a sampling oracle. Focusing on the linear
softmax model parameterization, we provide new results that reveal the
computational-statistical tradeoffs of efficient exploration:
  1. Necessity of coverage: Coverage refers to the extent to which the
pre-trained model covers near-optimal responses -- a form of hidden knowledge.
We show that coverage, while not necessary for data efficiency, lower bounds
the runtime of any algorithm in our framework.
  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,
which obtains optimal data efficiency and is computationally efficient whenever
the pre-trained model enjoys sufficient coverage, matching our lower bound.
SpannerSampling leverages inference-time computation with the pre-trained model
to reduce the effective search space for exploration.
  3. Insufficiency of training-time interventions: We contrast the result above
by showing that training-time interventions that produce proper policies cannot
achieve similar guarantees in polynomial time.
  4. Computational benefits of multi-turn exploration: Finally, we show that
under additional representational assumptions, one can achieve improved runtime
(replacing sequence-level coverage with token-level coverage) through
multi-turn exploration.
</summary>
    <author>
      <name>Dylan J. Foster</name>
    </author>
    <author>
      <name>Zakaria Mhammedi</name>
    </author>
    <author>
      <name>Dhruv Rohatgi</name>
    </author>
    <link href="http://arxiv.org/abs/2503.07453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07435v1</id>
    <updated>2025-03-10T15:18:10Z</updated>
    <published>2025-03-10T15:18:10Z</published>
    <title>Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds</title>
    <summary>  The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,
particularly gait recognition, has recently gathered significant attention due
to their efficiency, resilience to environmental conditions, and
privacy-preserving nature. In this work, we tackle the challenging problem of
Open-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike
most existing research, which assumes a closed-set scenario, our work considers
the more realistic open-set case, where unknown subjects might be present at
inference time, and should be correctly recognized by the system. Point clouds
are well-suited for edge computing applications with resource constraints, but
are more significantly affected by noise and random fluctuations than other
representations, like the more common micro-Doppler signature. This is the
first work addressing open-set gait recognition with sparse point cloud data.
To do so, we propose a novel neural network architecture that combines
supervised classification with unsupervised reconstruction of the point clouds,
creating a robust, rich, and highly regularized latent space of gait features.
To detect unknown subjects at inference time, we introduce a probabilistic
novelty detection algorithm that leverages the structured latent space and
offers a tunable trade-off between inference speed and prediction accuracy.
Along with this paper, we release mmGait10, an original human gait dataset
featuring over five hours of measurements from ten subjects, under varied
walking modalities. Extensive experimental results show that our solution
attains F1-Score improvements by 24% over state-of-the-art methods, on average,
and across multiple openness levels.
</summary>
    <author>
      <name>Riccardo Mazzieri</name>
    </author>
    <author>
      <name>Jacopo Pegoraro</name>
    </author>
    <author>
      <name>Michele Rossi</name>
    </author>
    <link href="http://arxiv.org/abs/2503.07435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07425v1</id>
    <updated>2025-03-10T15:10:40Z</updated>
    <published>2025-03-10T15:10:40Z</published>
    <title>CATPlan: Loss-based Collision Prediction in End-to-End Autonomous
  Driving</title>
    <summary>  In recent years, there has been increased interest in the design, training,
and evaluation of end-to-end autonomous driving (AD) systems. One often
overlooked aspect is the uncertainty of planned trajectories predicted by these
systems, despite awareness of their own uncertainty being key to achieve safety
and robustness. We propose to estimate this uncertainty by adapting loss
prediction from the uncertainty quantification literature. To this end, we
introduce a novel light-weight module, dubbed CATPlan, that is trained to
decode motion and planning embeddings into estimates of the collision loss used
to partially supervise end-to-end AD systems. During inference, these estimates
are interpreted as collision risk. We evaluate CATPlan on the safety-critical,
nerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect
collisions with a $54.8\%$ relative improvement to average precision over a
GMM-based baseline in which the predicted trajectory is compared to the
forecasted trajectories of other road users. Our findings indicate that the
addition of CATPlan can lead to safer end-to-end AD systems and hope that our
work will spark increased interest in uncertainty quantification for such
systems.
</summary>
    <author>
      <name>Ziliang Xiong</name>
    </author>
    <author>
      <name>Shipeng Liu</name>
    </author>
    <author>
      <name>Nathaniel Helgesen</name>
    </author>
    <author>
      <name>Joakim Johnander</name>
    </author>
    <author>
      <name>Per-Erik Forssen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.07425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07418v1</id>
    <updated>2025-03-10T15:05:59Z</updated>
    <published>2025-03-10T15:05:59Z</published>
    <title>AR-Diffusion: Asynchronous Video Generation with Auto-Regressive
  Diffusion</title>
    <summary>  The task of video generation requires synthesizing visually realistic and
temporally coherent video frames. Existing methods primarily use asynchronous
auto-regressive models or synchronous diffusion models to address this
challenge. However, asynchronous auto-regressive models often suffer from
inconsistencies between training and inference, leading to issues such as error
accumulation, while synchronous diffusion models are limited by their reliance
on rigid sequence length. To address these issues, we introduce Auto-Regressive
Diffusion (AR-Diffusion), a novel model that combines the strengths of
auto-regressive and diffusion models for flexible, asynchronous video
generation. Specifically, our approach leverages diffusion to gradually corrupt
video frames in both training and inference, reducing the discrepancy between
these phases. Inspired by auto-regressive generation, we incorporate a
non-decreasing constraint on the corruption timesteps of individual frames,
ensuring that earlier frames remain clearer than subsequent ones. This setup,
together with temporal causal attention, enables flexible generation of videos
with varying lengths while preserving temporal coherence. In addition, we
design two specialized timestep schedulers: the FoPP scheduler for balanced
timestep sampling during training, and the AD scheduler for flexible timestep
differences during inference, supporting both synchronous and asynchronous
generation. Extensive experiments demonstrate the superiority of our proposed
method, which achieves competitive and state-of-the-art results across four
challenging benchmarks.
</summary>
    <author>
      <name>Mingzhen Sun</name>
    </author>
    <author>
      <name>Weining Wang</name>
    </author>
    <author>
      <name>Gen Li</name>
    </author>
    <author>
      <name>Jiawei Liu</name>
    </author>
    <author>
      <name>Jiahui Sun</name>
    </author>
    <author>
      <name>Wanquan Feng</name>
    </author>
    <author>
      <name>Shanshan Lao</name>
    </author>
    <author>
      <name>SiYu Zhou</name>
    </author>
    <author>
      <name>Qian He</name>
    </author>
    <author>
      <name>Jing Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.07418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.07384v1</id>
    <updated>2025-03-10T14:32:56Z</updated>
    <published>2025-03-10T14:32:56Z</published>
    <title>Is My Text in Your AI Model? Gradient-based Membership Inference Test
  applied to LLMs</title>
    <summary>  This work adapts and studies the gradient-based Membership Inference Test
(gMINT) to the classification of text based on LLMs. MINT is a general approach
intended to determine if given data was used for training machine learning
models, and this work focuses on its application to the domain of Natural
Language Processing. Using gradient-based analysis, the MINT model identifies
whether particular data samples were included during the language model
training phase, addressing growing concerns about data privacy in machine
learning. The method was evaluated in seven Transformer-based models and six
datasets comprising over 2.5 million sentences, focusing on text classification
tasks. Experimental results demonstrate MINTs robustness, achieving AUC scores
between 85% and 99%, depending on data size and model architecture. These
findings highlight MINTs potential as a scalable and reliable tool for auditing
machine learning models, ensuring transparency, safeguarding sensitive data,
and fostering ethical compliance in the deployment of AI/NLP technologies.
</summary>
    <author>
      <name>Gonzalo Mancera</name>
    </author>
    <author>
      <name>Daniel de Alcala</name>
    </author>
    <author>
      <name>Julian Fierrez</name>
    </author>
    <author>
      <name>Ruben Tolosana</name>
    </author>
    <author>
      <name>Aythami Morales</name>
    </author>
    <link href="http://arxiv.org/abs/2503.07384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
