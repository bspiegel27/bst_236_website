<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-11-22T00:54:23Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-22T00:54:23Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>126695</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2511.16668v1</id>
    <title>V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</title>
    <updated>2025-11-20T18:59:42Z</updated>
    <link href="https://arxiv.org/abs/2511.16668v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16668v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:59:42Z</published>
    <arxiv:comment>Project Page: https://oahzxl.github.io/VReasonBench</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yang Luo</name>
    </author>
    <author>
      <name>Xuanlei Zhao</name>
    </author>
    <author>
      <name>Baijiong Lin</name>
    </author>
    <author>
      <name>Lingting Zhu</name>
    </author>
    <author>
      <name>Liyao Tang</name>
    </author>
    <author>
      <name>Yuqi Liu</name>
    </author>
    <author>
      <name>Ying-Cong Chen</name>
    </author>
    <author>
      <name>Shengju Qian</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Yang You</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16666v1</id>
    <title>SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</title>
    <updated>2025-11-20T18:59:25Z</updated>
    <link href="https://arxiv.org/abs/2511.16666v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16666v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:59:25Z</published>
    <arxiv:comment>NeurIPS 2025 (Spotlight), Project Page: https://henghuiding.com/SceneDesigner/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhenyuan Qin</name>
    </author>
    <author>
      <name>Xincheng Shuai</name>
    </author>
    <author>
      <name>Henghui Ding</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16655v1</id>
    <title>Solving Spatial Supersensing Without Spatial Supersensing</title>
    <updated>2025-11-20T18:57:05Z</updated>
    <link href="https://arxiv.org/abs/2511.16655v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16655v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:57:05Z</published>
    <arxiv:comment>Tech Report</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Vishaal Udandarao</name>
    </author>
    <author>
      <name>Shyamgopal Karthik</name>
    </author>
    <author>
      <name>Surabhi S. Nath</name>
    </author>
    <author>
      <name>Andreas Hochlehnert</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <author>
      <name>Ameya Prabhu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16654v1</id>
    <title>Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems</title>
    <updated>2025-11-20T18:56:49Z</updated>
    <link href="https://arxiv.org/abs/2511.16654v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16654v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:56:49Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Elias Lumer</name>
    </author>
    <author>
      <name>Alex Cardenas</name>
    </author>
    <author>
      <name>Matt Melich</name>
    </author>
    <author>
      <name>Myles Mason</name>
    </author>
    <author>
      <name>Sara Dieter</name>
    </author>
    <author>
      <name>Vamse Kumar Subbiah</name>
    </author>
    <author>
      <name>Pradeep Honaganahalli Basavaraju</name>
    </author>
    <author>
      <name>Roberto Hernandez</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16649v1</id>
    <title>Domain walls in the scaling regime: Equal Time Correlator and Gravitational Waves</title>
    <updated>2025-11-20T18:52:59Z</updated>
    <link href="https://arxiv.org/abs/2511.16649v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16649v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Domain walls are topological defects that may have formed in the early Universe through the spontaneous breakdown of discrete symmetries, and can be a strong source of gravitational waves (GWs). We perform 3D lattice field theory simulations with CosmoLattice, considering grid sizes $N = 1250$, $2048$ and $4096$, to study the dynamics of the domain wall network and its GW signatures. We first analyze how the network approaches the scaling regime with a constant $\mathcal{O}(1)$ number of domain walls per Hubble volume, including setups with a large initial number of domains as expected in realistic scenarios, and find that scaling is always reached in a few Hubble times after the network formation. To better understand the properties of the scaling regime, we then numerically extract the Equal Time Correlator (ETC) of the energy-momentum tensor of the network, thus determining its characteristic shape for the case of domain walls, and verifying explicitly its functional dependence as predicted by scaling arguments. The ETC can be further extended to the Unequal Time Correlator (UTC) controlling the GW emission by making assumptions on the coherence of the source. By comparison with the actual GW spectrum evaluated by CosmoLattice, we are then able to infer the degree of coherence of the domain wall network. Finally, by performing numerical simulations in different background cosmologies, e.g. radiation domination and kination, we find evidence for a universal ETC at subhorizon scales and hence a universal shape of the GW spectrum in the UV, while the expansion history of the Universe may instead be determined by the IR features of the GW spectrum.</summary>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:52:59Z</published>
    <arxiv:comment>42 pages, 18 figures, 5 tables</arxiv:comment>
    <arxiv:primary_category term="hep-ph"/>
    <author>
      <name>Simone Blasi</name>
    </author>
    <author>
      <name>Alberto Mariotti</name>
    </author>
    <author>
      <name>Aäron Rase</name>
    </author>
    <author>
      <name>Miguel Vanvlasselaer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16648v1</id>
    <title>Lucky Strikes: On the Origins of GW190814 Through Isolated Binary Evolution</title>
    <updated>2025-11-20T18:52:47Z</updated>
    <link href="https://arxiv.org/abs/2511.16648v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16648v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The asymmetric nature of GW190814, particularly its mass ratio ($q \approx 1/10$), has made its astrophysical origin elusive. We explore isolated binary evolution as a potential explanation for GW190814's formation. Using the binary population synthesis code COSMIC, and the backpop sampling technique to map the observed parameters of GW190814 to the initial conditions of Zero Age Main Sequence binary stars while simultaneously inferring the astrophysical prescriptions for common envelope evolution, stable mass transfer and natal kick kinematics that are needed for its formation and eventual merger. We find that the initial conditions for the binary stellar population that forms GW190814 do not stand out significantly from massive star populations observed in the Local Group. Our backpop simulations recover a dominant formation pathway where the first Roche overflow phase includes a common envelope evolution and the second Roche overflow phase remains stable. Our findings suggest that natal kicks imparted during compact object formation play the strongest role in forming GW190814-like systems. Specifically, our models require a low magnitude first natal kick (independent of direction) that prevents the binary from unbinding and a large second natal kick with its direction in the plane of the orbit and toward the binary's center of mass. The second natal kick strength and direction crucially increases the orbital eccentricity, leading to shorter delay times, and thus enabling mergers within a Hubble time. We estimate the chance probability for GW190814-like events that experience such a lucky kick and find that it occurs in $\sim20\%$ of systems if natal kicks are randomly oriented. We discuss the astrophysical implications for the formation of asymmetric GW190814-like systems under the context of binary stellar evolution.</summary>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:52:47Z</published>
    <arxiv:comment>14 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="astro-ph.HE"/>
    <author>
      <name>Ignacio Magaña Hernandez</name>
    </author>
    <author>
      <name>Katelyn Breivik</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16642v1</id>
    <title>TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming</title>
    <updated>2025-11-20T18:49:09Z</updated>
    <link href="https://arxiv.org/abs/2511.16642v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16642v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{https://github.com/zeyuanyin/TRIM}{link}$.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:49:09Z</published>
    <arxiv:comment>NeurIPS 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zeyuan Yin</name>
    </author>
    <author>
      <name>Xiaoming Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16635v1</id>
    <title>SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction</title>
    <updated>2025-11-20T18:41:44Z</updated>
    <link href="https://arxiv.org/abs/2511.16635v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16635v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:41:44Z</published>
    <arxiv:comment>20 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Guolin Huang</name>
    </author>
    <author>
      <name>Wenting Chen</name>
    </author>
    <author>
      <name>Jiaqi Yang</name>
    </author>
    <author>
      <name>Xinheng Lyu</name>
    </author>
    <author>
      <name>Xiaoling Luo</name>
    </author>
    <author>
      <name>Sen Yang</name>
    </author>
    <author>
      <name>Xiaohan Xing</name>
    </author>
    <author>
      <name>Linlin Shen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16628v1</id>
    <title>Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring</title>
    <updated>2025-11-20T18:34:27Z</updated>
    <link href="https://arxiv.org/abs/2511.16628v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16628v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.
  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.</summary>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:34:27Z</published>
    <arxiv:primary_category term="cs.CE"/>
    <author>
      <name>Tammam Bakeer</name>
    </author>
    <author>
      <name>Max Herbers</name>
    </author>
    <author>
      <name>Steffen Marx</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16618v1</id>
    <title>SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</title>
    <updated>2025-11-20T18:18:49Z</updated>
    <link href="https://arxiv.org/abs/2511.16618v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16618v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&amp;$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&amp;$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T18:18:49Z</published>
    <arxiv:comment>11 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haofeng Liu</name>
    </author>
    <author>
      <name>Ziyue Wang</name>
    </author>
    <author>
      <name>Sudhanshu Mishra</name>
    </author>
    <author>
      <name>Mingqi Gao</name>
    </author>
    <author>
      <name>Guanyi Qin</name>
    </author>
    <author>
      <name>Chang Han Low</name>
    </author>
    <author>
      <name>Alex Y. W. Kong</name>
    </author>
    <author>
      <name>Yueming Jin</name>
    </author>
  </entry>
</feed>
