<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-28T00:56:48Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">111219</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.17789v1</id>
    <updated>2025-04-24T17:59:56Z</updated>
    <published>2025-04-24T17:59:56Z</published>
    <title>Token-Shuffle: Towards High-Resolution Image Generation with
  Autoregressive Models</title>
    <summary>  Autoregressive (AR) models, long dominant in language generation, are
increasingly applied to image synthesis but are often considered less
competitive than Diffusion-based models. A primary limitation is the
substantial number of image tokens required for AR models, which constrains
both training and inference efficiency, as well as image resolution. To address
this, we present Token-Shuffle, a novel yet simple method that reduces the
number of image tokens in Transformer. Our key insight is the dimensional
redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),
where low-dimensional visual codes from visual encoder are directly mapped to
high-dimensional language vocabularies. Leveraging this, we consider two key
operations: token-shuffle, which merges spatially local tokens along channel
dimension to decrease the input token number, and token-unshuffle, which
untangles the inferred tokens after Transformer blocks to restore the spatial
arrangement for output. Jointly training with textual prompts, our strategy
requires no additional pretrained text-encoder and enables MLLMs to support
extremely high-resolution image synthesis in a unified next-token prediction
way while maintaining efficient training and inference. For the first time, we
push the boundary of AR text-to-image generation to a resolution of 2048x2048
with gratifying generation performance. In GenAI-benchmark, our 2.7B model
achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen
by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human
evaluations also demonstrate our prominent image generation ability in terms of
text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle
can serve as a foundational design for efficient high-resolution image
generation within MLLMs.
</summary>
    <author>
      <name>Xu Ma</name>
    </author>
    <author>
      <name>Peize Sun</name>
    </author>
    <author>
      <name>Haoyu Ma</name>
    </author>
    <author>
      <name>Hao Tang</name>
    </author>
    <author>
      <name>Chih-Yao Ma</name>
    </author>
    <author>
      <name>Jialiang Wang</name>
    </author>
    <author>
      <name>Kunpeng Li</name>
    </author>
    <author>
      <name>Xiaoliang Dai</name>
    </author>
    <author>
      <name>Yujun Shi</name>
    </author>
    <author>
      <name>Xuan Ju</name>
    </author>
    <author>
      <name>Yushi Hu</name>
    </author>
    <author>
      <name>Artsiom Sanakoyeu</name>
    </author>
    <author>
      <name>Felix Juefei-Xu</name>
    </author>
    <author>
      <name>Ji Hou</name>
    </author>
    <author>
      <name>Junjiao Tian</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <author>
      <name>Tingbo Hou</name>
    </author>
    <author>
      <name>Yen-Cheng Liu</name>
    </author>
    <author>
      <name>Zecheng He</name>
    </author>
    <author>
      <name>Zijian He</name>
    </author>
    <author>
      <name>Matt Feiszli</name>
    </author>
    <author>
      <name>Peizhao Zhang</name>
    </author>
    <author>
      <name>Peter Vajda</name>
    </author>
    <author>
      <name>Sam Tsai</name>
    </author>
    <author>
      <name>Yun Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17763v1</id>
    <updated>2025-04-24T17:26:42Z</updated>
    <published>2025-04-24T17:26:42Z</published>
    <title>Geodesic causality in Kerr spacetimes with $|a|\geq M$</title>
    <summary>  The analytic extension of the Kerr spacetimes into the negative radial region
contains closed causal curves for any non-zero rotation parameter $a$ and mass
parameter $M$. Furthermore, the spacetimes become totally vicious when $|a|&gt;M$,
meaning that through every point there exists a closed timelike curve. Despite
this, we prove that the Kerr spacetimes do not admit any closed null geodesics
when $|a|\geq M$. This result generalises recent findings by one of the
authors, which showed the nonexistence of closed causal geodesics in the case
$|a|&lt;M$. Combining these results, we establish the absence of closed null
geodesics in Kerr spacetimes for any non-zero $a$.
</summary>
    <author>
      <name>Giulio Sanzeni</name>
    </author>
    <author>
      <name>Karim Mosani</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17752v1</id>
    <updated>2025-04-24T17:10:18Z</updated>
    <published>2025-04-24T17:10:18Z</published>
    <title>Disaggregated Deep Learning via In-Physics Computing at Radio Frequency</title>
    <summary>  Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,
rely on deep learning to enable a wide range of intelligent applications,
including object recognition, environment perception, and autonomous
navigation. However, deploying deep learning models directly on the often
resource-constrained edge devices demands significant memory footprints and
computational power for real-time inference using traditional digital computing
architectures. In this paper, we present WISE, a novel computing architecture
for wireless edge networks designed to overcome energy constraints in deep
learning inference. WISE achieves this goal through two key innovations:
disaggregated model access via wireless broadcasting and in-physics computation
of general complex-valued matrix-vector multiplications directly at radio
frequency. Using a software-defined radio platform with wirelessly broadcast
model weights over the air, we demonstrate that WISE achieves 95.7% image
classification accuracy with ultra-low operation power of 6.0 fJ/MAC per
client, corresponding to a computation efficiency of 165.8 TOPS/W. This
approach enables energy-efficient deep learning inference on wirelessly
connected edge devices, achieving more than two orders of magnitude improvement
in efficiency compared to traditional digital computing.
</summary>
    <author>
      <name>Zhihui Gao</name>
    </author>
    <author>
      <name>Sri Krishna Vadlamani</name>
    </author>
    <author>
      <name>Kfir Sulimany</name>
    </author>
    <author>
      <name>Dirk Englund</name>
    </author>
    <author>
      <name>Tingjun Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures. Supplementary Information: 54 pages, 20 figures,
  1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.17752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17717v1</id>
    <updated>2025-04-24T16:19:13Z</updated>
    <published>2025-04-24T16:19:13Z</published>
    <title>Early Detection of Multidrug Resistance Using Multivariate Time Series
  Analysis and Interpretable Patient-Similarity Representations</title>
    <summary>  Background and Objectives: Multidrug Resistance (MDR) is a critical global
health issue, causing increased hospital stays, healthcare costs, and
mortality. This study proposes an interpretable Machine Learning (ML) framework
for MDR prediction, aiming for both accurate inference and enhanced
explainability.
  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing
clinical progression and patient-to-patient interactions. Similarity among
patients is quantified using MTS-based methods: descriptive statistics, Dynamic
Time Warping, and Time Cluster Kernel. These similarity measures serve as
inputs for MDR classification via Logistic Regression, Random Forest, and
Support Vector Machines, with dimensionality reduction and kernel
transformations improving model performance. For explainability, patient
similarity networks are constructed from these metrics. Spectral clustering and
t-SNE are applied to identify MDR-related subgroups and visualize high-risk
clusters, enabling insight into clinically relevant patterns.
  Results: The framework was validated on ICU Electronic Health Records from
the University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms
baseline ML and deep learning models by leveraging graph-based patient
similarity. The approach identifies key risk factors -- prolonged antibiotic
use, invasive procedures, co-infections, and extended ICU stays -- and reveals
clinically meaningful clusters. Code and results are available at
\https://github.com/oscarescuderoarnanz/DM4MTS.
  Conclusions: Patient similarity representations combined with graph-based
analysis provide accurate MDR prediction and interpretable insights. This
method supports early detection, risk factor identification, and patient
stratification, highlighting the potential of explainable ML in critical care.
</summary>
    <author>
      <name>Óscar Escudero-Arnanz</name>
    </author>
    <author>
      <name>Antonio G. Marques</name>
    </author>
    <author>
      <name>Inmaculada Mora-Jiménez</name>
    </author>
    <author>
      <name>Joaquín Álvarez-Rodríguez</name>
    </author>
    <author>
      <name>Cristina Soguero-Ruiz</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17699v2</id>
    <updated>2025-04-25T05:02:28Z</updated>
    <published>2025-04-24T16:08:52Z</published>
    <title>Quadratic Interest Network for Multimodal Click-Through Rate Prediction</title>
    <summary>  Multimodal click-through rate (CTR) prediction is a key technique in
industrial recommender systems. It leverages heterogeneous modalities such as
text, images, and behavioral logs to capture high-order feature interactions
between users and items, thereby enhancing the system's understanding of user
interests and its ability to predict click behavior. The primary challenge in
this field lies in effectively utilizing the rich semantic information from
multiple modalities while satisfying the low-latency requirements of online
inference in real-world applications. To foster progress in this area, the
Multimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop
formulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding:
this task aims to explore multimodal information extraction and item
representation learning methods that enhance recommendation tasks; and (2) Task
2 of Multimodal CTR Prediction: this task aims to explore what multimodal
recommendation model can effectively leverage multimodal embedding features and
achieve better performance. In this paper, we propose a novel model for Task 2,
named Quadratic Interest Network (QIN) for Multimodal CTR Prediction.
Specifically, QIN employs adaptive sparse target attention to extract
multimodal user behavior features, and leverages Quadratic Neural Networks to
capture high-order feature interactions. As a result, QIN achieved an AUC of
0.9798 on the leaderboard and ranked second in the competition. The model code,
training logs, hyperparameter configurations, and checkpoints are available at
https://github.com/salmon1802/QIN.
</summary>
    <author>
      <name>Honghao Li</name>
    </author>
    <author>
      <name>Hanwei Li</name>
    </author>
    <author>
      <name>Jing Zhang</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Ziniu Yu</name>
    </author>
    <author>
      <name>Lei Sang</name>
    </author>
    <author>
      <name>Yiwen Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17699v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17699v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17698v1</id>
    <updated>2025-04-24T16:07:45Z</updated>
    <published>2025-04-24T16:07:45Z</published>
    <title>Self-Supervised Noise Adaptive MRI Denoising via Repetition to
  Repetition (Rep2Rep) Learning</title>
    <summary>  Purpose: This work proposes a novel self-supervised noise-adaptive image
denoising framework, called Repetition to Repetition (Rep2Rep) learning, for
low-field (&lt;1T) MRI applications. Methods: Rep2Rep learning extends the
Noise2Noise framework by training a neural network on two repeated MRI
acquisitions, using one repetition as input and another as target, without
requiring ground-truth data. It incorporates noise-adaptive training, enabling
denoising generalization across varying noise levels and flexible inference
with any number of repetitions. Performance was evaluated on both synthetic
noisy brain MRI and 0.55T prostate MRI data, and compared against supervised
learning and Monte Carlo Stein's Unbiased Risk Estimator (MC-SURE). Results:
Rep2Rep learning outperforms MC-SURE on both synthetic and 0.55T MRI datasets.
On synthetic brain data, it achieved denoising quality comparable to supervised
learning and surpassed MC-SURE, particularly in preserving structural details
and reducing residual noise. On the 0.55T prostate MRI dataset, a reader study
showed radiologists preferred Rep2Rep-denoised 2-average images over 8-average
noisy images. Rep2Rep demonstrated robustness to noise-level discrepancies
between training and inference, supporting its practical implementation.
Conclusion: Rep2Rep learning offers an effective self-supervised denoising for
low-field MRI by leveraging routinely acquired multi-repetition data. Its
noise-adaptivity enables generalization to different SNR regimes without clean
reference images. This makes Rep2Rep learning a promising tool for improving
image quality and scan efficiency in low-field MRI.
</summary>
    <author>
      <name>Nikola Janjušević</name>
    </author>
    <author>
      <name>Jingjia Chen</name>
    </author>
    <author>
      <name>Luke Ginocchio</name>
    </author>
    <author>
      <name>Mary Bruno</name>
    </author>
    <author>
      <name>Yuhui Huang</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <author>
      <name>Hersh Chandarana</name>
    </author>
    <author>
      <name>Li Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 figures, 1 table, supplementary information at end of
  document</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.17698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17695v1</id>
    <updated>2025-04-24T16:03:11Z</updated>
    <published>2025-04-24T16:03:11Z</published>
    <title>PICO: Reconstructing 3D People In Contact with Objects</title>
    <summary>  Recovering 3D Human-Object Interaction (HOI) from single color images is
challenging due to depth ambiguities, occlusions, and the huge variation in
object shape and appearance. Thus, past work requires controlled settings such
as known object shapes and contacts, and tackles only limited object classes.
Instead, we need methods that generalize to natural images and novel object
classes. We tackle this in two main ways: (1) We collect PICO-db, a new dataset
of natural images uniquely paired with dense 3D contact on both body and object
meshes. To this end, we use images from the recent DAMON dataset that are
paired with contacts, but these contacts are only annotated on a canonical 3D
body. In contrast, we seek contact labels on both the body and the object. To
infer these given an image, we retrieve an appropriate 3D object mesh from a
database by leveraging vision foundation models. Then, we project DAMON's body
contact patches onto the object via a novel method needing only 2 clicks per
patch. This minimal human input establishes rich contact correspondences
between bodies and objects. (2) We exploit our new dataset of contact
correspondences in a novel render-and-compare fitting method, called PICO-fit,
to recover 3D body and object meshes in interaction. PICO-fit infers contact
for the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db
for that object, and uses the contact to iteratively fit the 3D body and object
meshes to image evidence via optimization. Uniquely, PICO-fit works well for
many object categories that no existing method can tackle. This is crucial to
enable HOI understanding to scale in the wild. Our data and code are available
at https://pico.is.tue.mpg.de.
</summary>
    <author>
      <name>Alpár Cseke</name>
    </author>
    <author>
      <name>Shashank Tripathi</name>
    </author>
    <author>
      <name>Sai Kumar Dwivedi</name>
    </author>
    <author>
      <name>Arjun Lakshmipathy</name>
    </author>
    <author>
      <name>Agniv Chatterjee</name>
    </author>
    <author>
      <name>Michael J. Black</name>
    </author>
    <author>
      <name>Dimitrios Tzionas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in CVPR'25. Project Page: https://pico.is.tue.mpg.de</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.17695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17685v1</id>
    <updated>2025-04-24T15:55:10Z</updated>
    <published>2025-04-24T15:55:10Z</published>
    <title>Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve
  LLM-level Accuracy in Profile Matching Tasks</title>
    <summary>  This study explores the potential of small language model(SLM) ensembles to
achieve accuracy comparable to proprietary large language models (LLMs). We
propose Ensemble Bayesian Inference (EBI), a novel approach that applies
Bayesian estimation to combine judgments from multiple SLMs, allowing them to
exceed the performance limitations of individual models. Our experiments on
diverse tasks(aptitude assessments and consumer profile analysis in both
Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze
cases where incorporating models with negative Lift values into ensembles
improves overall performance, and we examine the method's efficacy across
different languages. These findings suggest new possibilities for constructing
high-performance AI systems with limited computational resources and for
effectively utilizing models with individually lower performance. Building on
existing research on LLM performance evaluation, ensemble methods, and
open-source LLM utilization, we discuss the novelty and significance of our
approach.
</summary>
    <author>
      <name>Haru-Tada Sato</name>
    </author>
    <author>
      <name>Fuka Matsuzaki</name>
    </author>
    <author>
      <name>Jun-ichiro Takahashi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.17685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17674v1</id>
    <updated>2025-04-24T15:45:05Z</updated>
    <published>2025-04-24T15:45:05Z</published>
    <title>Energy Considerations of Large Language Model Inference and Efficiency
  Optimizations</title>
    <summary>  As large language models (LLMs) scale in size and adoption, their
computational and environmental costs continue to rise. Prior benchmarking
efforts have primarily focused on latency reduction in idealized settings,
often overlooking the diverse real-world inference workloads that shape energy
use. In this work, we systematically analyze the energy implications of common
inference efficiency optimizations across diverse Natural Language Processing
(NLP) and generative Artificial Intelligence (AI) workloads, including
conversational AI and code generation. We introduce a modeling approach that
approximates real-world LLM workflows through a binning strategy for
input-output token distributions and batch size variations. Our empirical
analysis spans software frameworks, decoding strategies, GPU architectures,
online and offline serving settings, and model parallelism configurations. We
show that the effectiveness of inference optimizations is highly sensitive to
workload geometry, software stack, and hardware accelerators, demonstrating
that naive energy estimates based on FLOPs or theoretical GPU utilization
significantly underestimate real-world energy consumption. Our findings reveal
that the proper application of relevant inference efficiency optimizations can
reduce total energy use by up to 73% from unoptimized baselines. These insights
provide a foundation for sustainable LLM deployment and inform energy-efficient
design strategies for future AI infrastructure.
</summary>
    <author>
      <name>Jared Fernandez</name>
    </author>
    <author>
      <name>Clara Na</name>
    </author>
    <author>
      <name>Vashisth Tiwari</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Sasha Luccioni</name>
    </author>
    <author>
      <name>Emma Strubell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.17674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17660v1</id>
    <updated>2025-04-24T15:29:39Z</updated>
    <published>2025-04-24T15:29:39Z</published>
    <title>Effortless, Simulation-Efficient Bayesian Inference using Tabular
  Foundation Models</title>
    <summary>  Simulation-based inference (SBI) offers a flexible and general approach to
performing Bayesian inference: In SBI, a neural network is trained on synthetic
data simulated from a model and used to rapidly infer posterior distributions
for observed data. A key goal for SBI is to achieve accurate inference with as
few simulations as possible, especially for expensive simulators. In this work,
we address this challenge by repurposing recent probabilistic foundation models
for tabular data: We show how tabular foundation models -- specifically TabPFN
-- can be used as pre-trained autoregressive conditional density estimators for
SBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks
(NPE-PF) and show that it is competitive with current SBI approaches in terms
of accuracy for both benchmark tasks and two complex scientific inverse
problems. Crucially, it often substantially outperforms them in terms of
simulation efficiency, sometimes requiring orders of magnitude fewer
simulations. NPE-PF eliminates the need for inference network selection,
training, and hyperparameter tuning. We also show that it exhibits superior
robustness to model misspecification and can be scaled to simulation budgets
that exceed the context size limit of TabPFN. NPE-PF provides a new direction
for SBI, where training-free, general-purpose inference models offer efficient,
easy-to-use, and flexible solutions for a wide range of stochastic inverse
problems.
</summary>
    <author>
      <name>Julius Vetter</name>
    </author>
    <author>
      <name>Manuel Gloeckler</name>
    </author>
    <author>
      <name>Daniel Gedon</name>
    </author>
    <author>
      <name>Jakob H. Macke</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
