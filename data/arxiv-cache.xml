<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-03-01T00:54:36Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-02-28T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">107863</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.20388v1</id>
    <updated>2025-02-27T18:59:08Z</updated>
    <published>2025-02-27T18:59:08Z</published>
    <title>Beyond Next-Token: Next-X Prediction for Autoregressive Visual
  Generation</title>
    <summary>  Autoregressive (AR) modeling, known for its next-token prediction paradigm,
underpins state-of-the-art language and visual generative models.
Traditionally, a ``token'' is treated as the smallest prediction unit, often a
discrete symbol in language or a quantized patch in vision. However, the
optimal token definition for 2D image structures remains an open question.
Moreover, AR models suffer from exposure bias, where teacher forcing during
training leads to error accumulation at inference. In this paper, we propose
xAR, a generalized AR framework that extends the notion of a token to an entity
X, which can represent an individual patch token, a cell (a $k\times k$
grouping of neighboring patches), a subsample (a non-local grouping of distant
patches), a scale (coarse-to-fine resolution), or even a whole image.
Additionally, we reformulate discrete token classification as
\textbf{continuous entity regression}, leveraging flow-matching methods at each
AR step. This approach conditions training on noisy entities instead of ground
truth tokens, leading to Noisy Context Learning, which effectively alleviates
exposure bias. As a result, xAR offers two key advantages: (1) it enables
flexible prediction units that capture different contextual granularity and
spatial structures, and (2) it mitigates exposure bias by avoiding reliance on
teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B
(172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20$\times$ faster
inference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24,
running 2.2$\times$ faster than the previous best-performing model without
relying on vision foundation modules (\eg, DINOv2) or advanced guidance
interval sampling.
</summary>
    <author>
      <name>Sucheng Ren</name>
    </author>
    <author>
      <name>Qihang Yu</name>
    </author>
    <author>
      <name>Ju He</name>
    </author>
    <author>
      <name>Xiaohui Shen</name>
    </author>
    <author>
      <name>Alan Yuille</name>
    </author>
    <author>
      <name>Liang-Chieh Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page at \url{https://oliverrensu.github.io/project/xAR}</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.20388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20385v1</id>
    <updated>2025-02-27T18:57:44Z</updated>
    <published>2025-02-27T18:57:44Z</published>
    <title>rSPDE: tools for statistical modeling using fractional SPDEs</title>
    <summary>  The R software package rSPDE contains methods for approximating Gaussian
random fields based on fractional-order stochastic partial differential
equations (SPDEs). A common example of such fields are Whittle-Mat\'ern fields
on bounded domains in $\mathbb{R}^d$, manifolds, or metric graphs. The package
also implements various other models which are briefly introduced in this
article. Besides the approximation methods, the package contains methods for
simulation, prediction, and statistical inference for such models, as well as
interfaces to INLA, inlabru and MetricGraph. With these interfaces,
fractional-order SPDEs can be used as model components in general latent
Gaussian models, for which full Bayesian inference can be performed, also for
fractional models on metric graphs. This includes estimation of the smoothness
parameter of the fields. This article describes the computational methods used
in the package and summarizes the theoretical basis for these. The main
functions of the package are introduced, and their usage is illustrated through
various examples.
</summary>
    <author>
      <name>David Bolin</name>
    </author>
    <author>
      <name>Alexandre B. Simas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.20385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20364v1</id>
    <updated>2025-02-27T18:35:39Z</updated>
    <published>2025-02-27T18:35:39Z</published>
    <title>Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with
  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix
  Factorization</title>
    <summary>  Agentic Generative AI, powered by Large Language Models (LLMs) with
Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores
(VSs), represents a transformative technology applicable to specialized domains
such as legal systems, research, recommender systems, cybersecurity, and global
security, including proliferation research. This technology excels at inferring
relationships within vast unstructured or semi-structured datasets. The legal
domain here comprises complex data characterized by extensive, interrelated,
and semi-structured knowledge systems with complex relations. It comprises
constitutions, statutes, regulations, and case law. Extracting insights and
navigating the intricate networks of legal documents and their relations is
crucial for effective legal research. Here, we introduce a generative AI system
that integrates RAG, VS, and KG, constructed via Non-Negative Matrix
Factorization (NMF), to enhance legal information retrieval and AI reasoning
and minimize hallucinations. In the legal system, these technologies empower AI
agents to identify and analyze complex connections among cases, statutes, and
legal precedents, uncovering hidden relationships and predicting legal
trends-challenging tasks that are essential for ensuring justice and improving
operational efficiency. Our system employs web scraping techniques to
systematically collect legal texts, such as statutes, constitutional
provisions, and case law, from publicly accessible platforms like Justia. It
bridges the gap between traditional keyword-based searches and contextual
understanding by leveraging advanced semantic representations, hierarchical
relationships, and latent topic discovery. This framework supports legal
document clustering, summarization, and cross-referencing, for scalable,
interpretable, and accurate retrieval for semi-structured data while advancing
computational law and AI.
</summary>
    <author>
      <name>Ryan C. Barron</name>
    </author>
    <author>
      <name>Maksim E. Eren</name>
    </author>
    <author>
      <name>Olga M. Serafimova</name>
    </author>
    <author>
      <name>Cynthia Matuszek</name>
    </author>
    <author>
      <name>Boian S. Alexandrov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.20364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20344v1</id>
    <updated>2025-02-27T18:16:47Z</updated>
    <published>2025-02-27T18:16:47Z</published>
    <title>Sparse Auto-Encoder Interprets Linguistic Features in Large Language
  Models</title>
    <summary>  Large language models (LLMs) excel in tasks that require complex linguistic
abilities, such as reference disambiguation and metaphor
recognition/generation. Although LLMs possess impressive capabilities, their
internal mechanisms for processing and representing linguistic knowledge remain
largely opaque. Previous work on linguistic mechanisms has been limited by
coarse granularity, insufficient causal analysis, and a narrow focus. In this
study, we present a systematic and comprehensive causal investigation using
sparse auto-encoders (SAEs). We extract a wide range of linguistic features
from six dimensions: phonetics, phonology, morphology, syntax, semantics, and
pragmatics. We extract, evaluate, and intervene on these features by
constructing minimal contrast datasets and counterfactual sentence datasets. We
introduce two indices-Feature Representation Confidence (FRC) and Feature
Intervention Confidence (FIC)-to measure the ability of linguistic features to
capture and control linguistic phenomena. Our results reveal inherent
representations of linguistic knowledge in LLMs and demonstrate the potential
for controlling model outputs. This work provides strong evidence that LLMs
possess genuine linguistic knowledge and lays the foundation for more
interpretable and controllable language modeling in future research.
</summary>
    <author>
      <name>Yi Jing</name>
    </author>
    <author>
      <name>Zijun Yao</name>
    </author>
    <author>
      <name>Lingxu Ran</name>
    </author>
    <author>
      <name>Hongzhu Guo</name>
    </author>
    <author>
      <name>Xiaozhi Wang</name>
    </author>
    <author>
      <name>Lei Hou</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
    <link href="http://arxiv.org/abs/2502.20344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20339v1</id>
    <updated>2025-02-27T18:08:16Z</updated>
    <published>2025-02-27T18:08:16Z</published>
    <title>Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners</title>
    <summary>  Recent advancements have demonstrated that the performance of large language
models (LLMs) can be significantly enhanced by scaling computational resources
at test time. A common strategy involves generating multiple Chain-of-Thought
(CoT) trajectories and aggregating their outputs through various selection
mechanisms. This raises a fundamental question: can models with lower
complexity leverage their superior generation throughput to outperform
similarly sized Transformers for a fixed computational budget? To address this
question and overcome the lack of strong subquadratic reasoners, we distill
pure and hybrid Mamba models from pretrained Transformers. Trained on only 8
billion tokens, our distilled models show strong performance and scaling on
mathematical reasoning datasets while being much faster at inference for large
batches and long sequences. Despite the zero-shot performance hit due to
distillation, both pure and hybrid Mamba models can scale their coverage and
accuracy performance past their Transformer teacher models under fixed time
budgets, opening a new direction for scaling inference compute.
</summary>
    <author>
      <name>Daniele Paliotta</name>
    </author>
    <author>
      <name>Junxiong Wang</name>
    </author>
    <author>
      <name>Matteo Pagliardini</name>
    </author>
    <author>
      <name>Kevin Y. Li</name>
    </author>
    <author>
      <name>Aviv Bick</name>
    </author>
    <author>
      <name>J. Zico Kolter</name>
    </author>
    <author>
      <name>Albert Gu</name>
    </author>
    <author>
      <name>François Fleuret</name>
    </author>
    <author>
      <name>Tri Dao</name>
    </author>
    <link href="http://arxiv.org/abs/2502.20339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20330v1</id>
    <updated>2025-02-27T17:59:36Z</updated>
    <published>2025-02-27T17:59:36Z</published>
    <title>Long-Context Inference with Retrieval-Augmented Speculative Decoding</title>
    <summary>  The emergence of long-context large language models (LLMs) offers a promising
alternative to traditional retrieval-augmented generation (RAG) for processing
extensive documents. However, the computational overhead of long-context
inference, particularly in managing key-value (KV) caches, presents significant
efficiency challenges. While Speculative Decoding (SD) traditionally
accelerates inference using smaller draft models, its effectiveness diminishes
substantially in long-context scenarios due to memory-bound KV cache
operations. We present Retrieval-Augmented Speculative Decoding (RAPID), which
leverages RAG for both accelerating and enhancing generation quality in
long-context inference. RAPID introduces the RAG drafter-a draft LLM operating
on shortened retrieval contexts-to speculate on the generation of long-context
target LLMs. Our approach enables a new paradigm where same-scale or even
larger LLMs can serve as RAG drafters while maintaining computational
efficiency. To fully leverage the potentially superior capabilities from
stronger RAG drafters, we develop an inference-time knowledge transfer dynamic
that enriches the target distribution by RAG. Extensive experiments on the
LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates
the strengths of both approaches, achieving significant performance
improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with
more than 2x speedups. Our analyses reveal that RAPID achieves robust
acceleration beyond 32K context length and demonstrates superior generation
quality in real-world applications.
</summary>
    <author>
      <name>Guanzheng Chen</name>
    </author>
    <author>
      <name>Qilong Feng</name>
    </author>
    <author>
      <name>Jinjie Ni</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Michael Qizhe Shieh</name>
    </author>
    <link href="http://arxiv.org/abs/2502.20330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20320v1</id>
    <updated>2025-02-27T17:45:20Z</updated>
    <published>2025-02-27T17:45:20Z</published>
    <title>ACCORD: Application Context-aware Cross-layer Optimization and Resource
  Design for 5G/NextG Machine-centric Applications</title>
    <summary>  Recent advancements in AI and edge computing have accelerated the development
of machine-centric applications (MCAs), such as smart surveillance systems. In
these applications, video cameras and sensors offload inference tasks like
license plate recognition and vehicle tracking to remote servers due to local
computing and energy constraints. However, legacy network solutions, designed
primarily for human-centric applications, struggle to reliably support these
MCAs, which demand heterogeneous and fluctuating QoS (due to diverse
application inference tasks), further challenged by dynamic wireless network
conditions and limited spectrum resources. To tackle these challenges, we
propose an Application Context-aware Cross-layer Optimization and Resource
Design (ACCORD) framework. This innovative framework anticipates the evolving
demands of MCAs in real time, quickly adapting to provide customized QoS and
optimal performance, even for the most dynamic and unpredictable MCAs. This
also leads to improved network resource management and spectrum utilization.
ACCORD operates as a closed feedback-loop system between the application client
and network and consists of two key components: (1) Building Application
Context: It focuses on understanding the specific context of MCA requirements.
Contextual factors include device capabilities, user behavior (e.g., mobility
speed), and network channel conditions. (2) Cross-layer Network Parameter
Configuration: Utilizing a DRL approach, this component leverages the
contextual information to optimize network configuration parameters across
various layers, including PHY, MAC, and RLC, as well as the application layer,
to meet the desired QoS requirement in real-time. Extensive evaluation with the
3GPP-compliant MATLAB 5G toolbox demonstrates the practicality and
effectiveness of our proposed ACCORD framework.
</summary>
    <author>
      <name>Azuka Chiejina</name>
    </author>
    <author>
      <name>Subhramoy Mohanti</name>
    </author>
    <author>
      <name>Vijay K. Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publications at ICC 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.20320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20313v1</id>
    <updated>2025-02-27T17:39:17Z</updated>
    <published>2025-02-27T17:39:17Z</published>
    <title>FlexVAR: Flexible Visual Autoregressive Modeling without Residual
  Prediction</title>
    <summary>  This work challenges the residual prediction paradigm in visual
autoregressive modeling and presents FlexVAR, a new Flexible Visual
AutoRegressive image generation paradigm. FlexVAR facilitates autoregressive
learning with ground-truth prediction, enabling each step to independently
produce plausible images. This simple, intuitive approach swiftly learns visual
distributions and makes the generation process more flexible and adaptable.
Trained solely on low-resolution images ($\leq$ 256px), FlexVAR can: (1)
Generate images of various resolutions and aspect ratios, even exceeding the
resolution of the training images. (2) Support various image-to-image tasks,
including image refinement, in/out-painting, and image expansion. (3) Adapt to
various autoregressive steps, allowing for faster inference with fewer steps or
enhancing image quality with more steps. Our 1.0B model outperforms its VAR
counterpart on the ImageNet 256$\times$256 benchmark. Moreover, when zero-shot
transfer the image generation process with 13 steps, the performance further
improves to 2.08 FID, outperforming state-of-the-art autoregressive models
AiM/VAR by 0.25/0.28 FID and popular diffusion models LDM/DiT by 1.52/0.19 FID,
respectively. When transferring our 1.0B model to the ImageNet 512$\times$512
benchmark in a zero-shot manner, FlexVAR achieves competitive results compared
to the VAR 2.3B model, which is a fully supervised model trained at
512$\times$512 resolution.
</summary>
    <author>
      <name>Siyu Jiao</name>
    </author>
    <author>
      <name>Gengwei Zhang</name>
    </author>
    <author>
      <name>Yinlong Qian</name>
    </author>
    <author>
      <name>Jiancheng Huang</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <author>
      <name>Humphrey Shi</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <author>
      <name>Yunchao Wei</name>
    </author>
    <author>
      <name>Zequn Jie</name>
    </author>
    <link href="http://arxiv.org/abs/2502.20313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20312v1</id>
    <updated>2025-02-27T17:38:38Z</updated>
    <published>2025-02-27T17:38:38Z</published>
    <title>Inferring Black Hole Spin from Interferometric Measurements of the First
  Photon Ring: A Geometric Approach</title>
    <summary>  Accurately inferring black hole spin is crucial for understanding black hole
dynamics and their astrophysical environments. In this work, we outline a
geometric method for spin estimation by using the interferometric shape of the
first photon ring ($n=1$) as an approximation to the critical curve, which,
given an assumed value of the black hole inclination, is then mapped to a spin
value. While future space-based missions will capture a wealth of data on the
first photon ring--including the full angle-dependent diameter, angular
brightness profile, and astrometric offset from the $n=0$ ring--our analysis is
restricted to using only two angle-dependent diameters to compute its shape
asymmetry and infer spin. Focusing on low inclinations and moderate-to-high
spins, we test the method across various emission models, baselines, and noise
sources, including a mock space-based observation. Although the size of the
$n=1$ ring depends on the emission model, its interferometric shape remains a
robust spin probe at low inclinations. We find that the inferred asymmetry of
the $n=1$ image may be biased by the critical curve morphology, and it can be
heavily skewed by the presence of noise, whether astrophysical or instrumental.
In low-noise limits at low viewing inclination, significant contributions from
the n=0 image at short baselines may lead to a downward bias in asymmetry
estimates. While our method can estimate high spins in noise-free time-averaged
images, increasing the noise and astrophysical variability degrades the
resulting constraints, providing only lower bounds on the spin when applied to
synthetic observed data. Remarkably, even using only the ring's asymmetry, we
can establish lower bounds on the spin, underscoring the promise of photon
ring-based spin inference in future space-based very long baseline
interferometry (VLBI) missions, such as the proposed Black Hole Explorer
(BHEX).
</summary>
    <author>
      <name>Lennox S. Keeble</name>
    </author>
    <author>
      <name>Alejandro Cárdenas-Avendaño</name>
    </author>
    <author>
      <name>Daniel C. M. Palumbo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16+2 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.20312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20307v1</id>
    <updated>2025-02-27T17:33:51Z</updated>
    <published>2025-02-27T17:33:51Z</published>
    <title>Mobius: Text to Seamless Looping Video Generation via Latent Shift</title>
    <summary>  We present Mobius, a novel method to generate seamlessly looping videos from
text descriptions directly without any user annotations, thereby creating new
visual materials for the multi-media presentation. Our method repurposes the
pre-trained video latent diffusion model for generating looping videos from
text prompts without any training. During inference, we first construct a
latent cycle by connecting the starting and ending noise of the videos. Given
that the temporal consistency can be maintained by the context of the video
diffusion model, we perform multi-frame latent denoising by gradually shifting
the first-frame latent to the end in each step. As a result, the denoising
context varies in each step while maintaining consistency throughout the
inference process. Moreover, the latent cycle in our method can be of any
length. This extends our latent-shifting approach to generate seamless looping
videos beyond the scope of the video diffusion model's context. Unlike previous
cinemagraphs, the proposed method does not require an image as appearance,
which will restrict the motions of the generated results. Instead, our method
can produce more dynamic motion and better visual quality. We conduct multiple
experiments and comparisons to verify the effectiveness of the proposed method,
demonstrating its efficacy in different scenarios. All the code will be made
available.
</summary>
    <author>
      <name>Xiuli Bi</name>
    </author>
    <author>
      <name>Jianfei Yuan</name>
    </author>
    <author>
      <name>Bo Liu</name>
    </author>
    <author>
      <name>Yong Zhang</name>
    </author>
    <author>
      <name>Xiaodong Cun</name>
    </author>
    <author>
      <name>Chi-Man Pun</name>
    </author>
    <author>
      <name>Bin Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://mobius-diffusion.github.io/ ; GitHub
  repository: https://github.com/YisuiTT/Mobius</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.20307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
