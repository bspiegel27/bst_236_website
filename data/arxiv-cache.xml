<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-22T00:56:15Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-21T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">124178</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.17803v1</id>
    <updated>2025-10-20T17:59:52Z</updated>
    <published>2025-10-20T17:59:52Z</published>
    <title>ConsistEdit: Highly Consistent and Precise Training-free Visual Editing</title>
    <summary>  Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.
</summary>
    <author>
      <name>Zixin Yin</name>
    </author>
    <author>
      <name>Ling-Hao Chen</name>
    </author>
    <author>
      <name>Lionel Ni</name>
    </author>
    <author>
      <name>Xili Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH Asia 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17793v1</id>
    <updated>2025-10-20T17:52:06Z</updated>
    <published>2025-10-20T17:52:06Z</published>
    <title>Foundational Automatic Evaluators: Scaling Multi-Task Generative
  Evaluator Training for Reasoning-Centric Domains</title>
    <summary>  Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.
</summary>
    <author>
      <name>Austin Xu</name>
    </author>
    <author>
      <name>Xuan-Phi Nguyen</name>
    </author>
    <author>
      <name>Yilun Zhou</name>
    </author>
    <author>
      <name>Chien-Sheng Wu</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Shafiq Joty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 9 tables, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17787v1</id>
    <updated>2025-10-20T17:46:37Z</updated>
    <published>2025-10-20T17:46:37Z</published>
    <title>Comprehensive analysis of time-domain overlapping gravitational wave
  transients: A Lensing Study</title>
    <summary>  Next-generation GW detectors will produce a high rate of temporally
overlapping signals from unrelated compact binary coalescences. Such overlaps
can bias parameter estimation (PE) and mimic signatures of other physical
effects, such as gravitational lensing. In this work, we investigate how
overlapping signals can be degenerate with gravitational lensing by focusing on
two scenarios: Type-II strong lensing and microlensing by an isolated
point-mass lens. We simulate quasicircular binary black-hole pairs with
chirp-mass ratios $\mathscr{M}_{\rm B}/\mathscr{M}_{\rm A}\in\{0.5,\,1,\,2\}$,
SNR ratios $\mathrm{SNR}_{\rm B}/\mathrm{SNR}_{\rm A}\in\{0.5,\,1\}$, and
coalescence-time offsets $\Delta t_{\rm c}\in[-0.1,\,0.1]~\mathrm{s}$. Bayesian
PE and fitting-factor studies show that the Type-II lensing hypothesis is
favored over the unlensed quasicircular hypothesis ($\log_{10}\mathscr{B}^{\rm
L}_{\rm U}&gt;1$) only in a small region of the overlapping parameter space with
$\mathscr{M}_{\rm B}/\mathscr{M}_{\rm A}\gtrsim1$ and $|\Delta t_{\rm
c}|\leq0.03~\rm{s}$.. Meanwhile, false evidence for microlensing signatures can
arise because, to a reasonable approximation, the model produces two
superimposed images whose time delay can closely match $|\Delta t_{\rm c}|$.
Overall, the inferred Bayes factor depends on relative chirp-mass ratios,
relative loudness, difference in coalescence times, and also the absolute SNRs
of the overlapping signals. Cumulatively, our results indicate that overlapping
black-hole binaries with nearly equal chirp masses and comparable loudness are
likely to be falsely identified as lensed. Such misidentifications are expected
to become more common as detector sensitivities improve. While our study
focuses on ground-based detectors using appropriate detectability thresholds,
the findings naturally extend to next-generation GW observatories.
</summary>
    <author>
      <name>Nishkal Rao</name>
    </author>
    <author>
      <name>Anuj Mishra</name>
    </author>
    <author>
      <name>Apratim Ganguly</name>
    </author>
    <author>
      <name>Anupreeta More</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 11 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17786v1</id>
    <updated>2025-10-20T17:44:17Z</updated>
    <published>2025-10-20T17:44:17Z</published>
    <title>Inference-Time Compute Scaling For Flow Matching</title>
    <summary>  Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.
</summary>
    <author>
      <name>Adam Stecklov</name>
    </author>
    <author>
      <name>Noah El Rimawi-Fine</name>
    </author>
    <author>
      <name>Mathieu Blanchette</name>
    </author>
    <link href="http://arxiv.org/abs/2510.17786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17777v1</id>
    <updated>2025-10-20T17:35:47Z</updated>
    <published>2025-10-20T17:35:47Z</published>
    <title>SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference</title>
    <summary>  Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.
</summary>
    <author>
      <name>Samir Khaki</name>
    </author>
    <author>
      <name>Junxian Guo</name>
    </author>
    <author>
      <name>Jiaming Tang</name>
    </author>
    <author>
      <name>Shang Yang</name>
    </author>
    <author>
      <name>Yukang Chen</name>
    </author>
    <author>
      <name>Konstantinos N. Plataniotis</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Zhijian Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2510.17777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17774v1</id>
    <updated>2025-10-20T17:34:40Z</updated>
    <published>2025-10-20T17:34:40Z</published>
    <title>A High-Resolution Spectroscopic Survey of Directly Imaged Companion
  Hosts: II. Diversity in C/O Ratios among Host Stars</title>
    <summary>  The era of JWST has enabled measurements of abundances of elements such as C,
O, and even Na, S, K, and Fe in planetary atmospheres to very high precisions
($\sim$0.1 dex). Accurate inference of planet formation using these elemental
abundances require the corresponding abundance measurements for the host star.
We present the second set of results from our high-resolution spectroscopic
survey of directly imaged companion host stars, measuring abundances of 16
elements (including C, O, Na, Mg, Si, S, K and Fe) for five directly imaged
companion host stars. Using both the spectral fitting and the equivalent width
methods, we find solar C/O ratios for HR 2562 (0.58 $\pm$ 0.09), AB Pic (0.50
$\pm$ 0.14), and YSES 1 (0.45 $\pm$ 0.05), and sub-solar C/O ratios for PZ Tel
(0.28 $\pm$ 0.05) and $\beta$ Pic (0.22 $\pm$ 0.06). The $4\sigma$ sub-solar
C/O detections for PZ Tel and $\beta$ Pic highlight the importance of accurate
stellar C/O estimates for constraining planet formation. Subsequently, we
combine our abundances with those from our previous work to measure
population-level average elemental abundances. We find super-solar carbon and
oxygen for this stellar population, indicating that the protoplanetary disks
around these stars were potentially rich in volatiles. We compare stellar C/O
to those of their companions, revealing super-stellar C/O for several objects
that suggest planet-like formation mechanisms. We also compare the C/O of our
directly imaged companion host star population with other planet host stars
using the Kolmogorov-Smirnov Test, which indicates insufficient evidence to
differentiate between the various stellar populations
</summary>
    <author>
      <name>Aneesh Baburaj</name>
    </author>
    <author>
      <name>Quinn M. Konopacky</name>
    </author>
    <author>
      <name>Christopher A. Theissen</name>
    </author>
    <author>
      <name>Roman Gerasimov</name>
    </author>
    <author>
      <name>Kielan K. W. Hoch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 22 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17771v1</id>
    <updated>2025-10-20T17:31:09Z</updated>
    <published>2025-10-20T17:31:09Z</published>
    <title>Seeing but Not Believing: Probing the Disconnect Between Visual
  Attention and Answer Correctness in VLMs</title>
    <summary>  Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.
</summary>
    <author>
      <name>Zhining Liu</name>
    </author>
    <author>
      <name>Ziyi Chen</name>
    </author>
    <author>
      <name>Hui Liu</name>
    </author>
    <author>
      <name>Chen Luo</name>
    </author>
    <author>
      <name>Xianfeng Tang</name>
    </author>
    <author>
      <name>Suhang Wang</name>
    </author>
    <author>
      <name>Joy Zeng</name>
    </author>
    <author>
      <name>Zhenwei Dai</name>
    </author>
    <author>
      <name>Zhan Shi</name>
    </author>
    <author>
      <name>Tianxin Wei</name>
    </author>
    <author>
      <name>Benoit Dumoulin</name>
    </author>
    <author>
      <name>Hanghang Tong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 10 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17761v1</id>
    <updated>2025-10-20T17:13:11Z</updated>
    <published>2025-10-20T17:13:11Z</published>
    <title>QUIJOTE scientific results XIX. New constraints on the synchrotron
  spectral index using a semi-blind component separation method</title>
    <summary>  We introduce a novel approach to estimate the spectral index, $\beta_s$, of
polarised synchrotron emission, combining the moment expansion of CMB
foregrounds and the constrained-ILC method. We reconstruct the maps of the
first two synchrotron moments, combining multi-frequency data, and apply the
`T-T plot' technique between two moment maps to estimate the synchrotron
spectral index. This approach offers a new technique for mapping the foreground
spectral parameters, complementing the model-based parametric component
separation methods. Applying this technique, we derive a new constraint on the
spectral index of polarised synchrotron emission using QUIJOTE MFI wide-survey
11 and 13 GHz data, Wilkinson Microwave Anisotropy Probe (WMAP) data at K and
Ka bands, and Planck LFI 30 GHz data. In the Galactic plane and North Polar
Spur regions, we obtain an inverse-variance-weighted mean synchrotron index of
$\beta_s = -3.11$ with a standard deviation of $0.21$ due to intrinsic scatter,
consistent with previous results based on parametric methods using the same
dataset. We find that the inverse-variance-weighted mean spectral index,
including both statistical and systematic uncertainties, is $\beta_s^{\rm
plane} = -3.05 \pm 0.01$ in the Galactic plane and $\beta_s^{\rm
high\text{-}lat} = -3.13 \pm 0.02$ at high latitudes, indicating a moderate
steepening of the spectral index from low to high Galactic latitudes. Our
analysis indicates that, within the current upper limit on the AME polarisation
fraction, our results are not subject to any appreciable bias. Furthermore, we
infer the spectral index over the entire QUIJOTE survey region, partitioning
the sky into 21 patches. This technique can be further extended to constrain
the synchrotron spectral curvature by reconstructing higher-order moments when
better-quality data become available.
</summary>
    <author>
      <name>Debabrata Adak</name>
    </author>
    <author>
      <name>J. A. Rubiño-Martín</name>
    </author>
    <author>
      <name>R. T. Génova-Santos</name>
    </author>
    <author>
      <name>M. Remazeilles</name>
    </author>
    <author>
      <name>A. Almeida</name>
    </author>
    <author>
      <name>K. Aryan</name>
    </author>
    <author>
      <name>M. Ashdown</name>
    </author>
    <author>
      <name>R. B. Barreiro</name>
    </author>
    <author>
      <name>U. Bose</name>
    </author>
    <author>
      <name>R. Cepeda-Arroita</name>
    </author>
    <author>
      <name>J. M. Casas</name>
    </author>
    <author>
      <name>M. Fernández-Torreiro</name>
    </author>
    <author>
      <name>E. Martínez-Gonzalez</name>
    </author>
    <author>
      <name>F. Poidevin</name>
    </author>
    <author>
      <name>R. Rebolo</name>
    </author>
    <author>
      <name>P. Vielva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 15 figures, 1 table, submitted in A&amp;A</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17759v1</id>
    <updated>2025-10-20T17:12:10Z</updated>
    <published>2025-10-20T17:12:10Z</published>
    <title>VERA-V: Variational Inference Framework for Jailbreaking Vision-Language
  Models</title>
    <summary>  Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.
</summary>
    <author>
      <name>Qilin Liao</name>
    </author>
    <author>
      <name>Anamika Lochab</name>
    </author>
    <author>
      <name>Ruqi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 Figures,</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.17746v1</id>
    <updated>2025-10-20T17:02:29Z</updated>
    <published>2025-10-20T17:02:29Z</published>
    <title>A search for black holes with metal-poor stellar companions: I. Survey
  sample selection and single epoch radial velocity follow-up</title>
    <summary>  Stellar-mass black holes (BHs) above $30 M_\odot$ are predicted to form from
low-metallicity progenitors, but direct detections of such systems in the Milky
Way remain scarce. Motivated by the recent discovery of Gaia BH3, a $33
M_\odot$ BH with a very metal-poor giant companion, we conduct a systematic
search for additional systems. Approximately 900 candidates are identified with
Gaia as having significant deviations from single-star astrometric motion,
evidence of RV variability, and low metallicities inferred from Gaia XP
spectra. We obtain single epoch high-resolution spectra for over 600 of these
sources with Magellan/MIKE and Lick/APF and measure independent RVs with
$\approx 1$ km s$^{-1}$ precision. After removing contaminants such as hot
stars, pulsators, eclipsing binaries, and hierarchical triples, we identify
about 15 promising candidates with large RV amplitudes or offsets from the Gaia
reported values. This program establishes a well-characterized sample of BH
candidates for detailed orbital modeling once Gaia DR4 epoch astrometry and RVs
are released in late 2026; multi-epoch RV follow-up is ongoing. Together, the
Gaia and ground-based data will place new constraints on the demographics of
BHs with metal-poor companions and test theoretical predictions linking low
metallicity to the formation of the most massive stellar remnants.
</summary>
    <author>
      <name>Casey Y. Lam</name>
    </author>
    <author>
      <name>Joshua D. Simon</name>
    </author>
    <author>
      <name>Kareem El-Badry</name>
    </author>
    <author>
      <name>Howard Isaacson</name>
    </author>
    <author>
      <name>Daniel D. Kelson</name>
    </author>
    <author>
      <name>Jessica Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 9 figures, 9 tables, 3 appendices. Submitted to ApJ. Tables
  1 - 3 are available here:
  https://drive.google.com/drive/folders/1vbxLSUFfvu1q9nrJ8MFQ4KOxpQOKvkiJ</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.17746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.17746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
