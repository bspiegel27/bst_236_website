<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2026-01-23T01:03:41Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2026-01-23T01:03:42Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>131453</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2601.15286v1</id>
    <title>Iterative Refinement Improves Compositional Image Generation</title>
    <updated>2026-01-21T18:59:40Z</updated>
    <link href="https://arxiv.org/abs/2601.15286v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15286v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T18:59:40Z</published>
    <arxiv:comment>Project webpage: https://iterative-img-gen.github.io/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shantanu Jaiswal</name>
    </author>
    <author>
      <name>Mihir Prabhudesai</name>
    </author>
    <author>
      <name>Nikash Bhardwaj</name>
    </author>
    <author>
      <name>Zheyang Qin</name>
    </author>
    <author>
      <name>Amir Zadeh</name>
    </author>
    <author>
      <name>Chuan Li</name>
    </author>
    <author>
      <name>Katerina Fragkiadaki</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15284v1</id>
    <title>Walk through Paintings: Egocentric World Models from Internet Priors</title>
    <updated>2026-01-21T18:59:32Z</updated>
    <link href="https://arxiv.org/abs/2601.15284v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15284v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T18:59:32Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Anurag Bagchi</name>
    </author>
    <author>
      <name>Zhipeng Bao</name>
    </author>
    <author>
      <name>Homanga Bharadhwaj</name>
    </author>
    <author>
      <name>Yu-Xiong Wang</name>
    </author>
    <author>
      <name>Pavel Tokmakov</name>
    </author>
    <author>
      <name>Martial Hebert</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15254v1</id>
    <title>Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?</title>
    <updated>2026-01-21T18:36:34Z</updated>
    <link href="https://arxiv.org/abs/2601.15254v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15254v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\ell_1$-regularized estimation and post-selection refitting.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T18:36:34Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Felix Schur</name>
    </author>
    <author>
      <name>Niklas Pfister</name>
    </author>
    <author>
      <name>Peng Ding</name>
    </author>
    <author>
      <name>Sach Mukherjee</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15250v1</id>
    <title>FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion</title>
    <updated>2026-01-21T18:32:27Z</updated>
    <link href="https://arxiv.org/abs/2601.15250v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15250v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T18:32:27Z</published>
    <arxiv:comment>Under Review</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zichen Xi</name>
    </author>
    <author>
      <name>Hao-Xiang Chen</name>
    </author>
    <author>
      <name>Nan Xue</name>
    </author>
    <author>
      <name>Hongyu Yan</name>
    </author>
    <author>
      <name>Qi-Yuan Feng</name>
    </author>
    <author>
      <name>Levent Burak Kara</name>
    </author>
    <author>
      <name>Joaquim Jorge</name>
    </author>
    <author>
      <name>Qun-Ce Xu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15224v1</id>
    <title>PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</title>
    <updated>2026-01-21T17:56:59Z</updated>
    <link href="https://arxiv.org/abs/2601.15224v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15224v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T17:56:59Z</published>
    <arxiv:comment>Website: https://progresslm.github.io/ProgressLM/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jianshu Zhang</name>
    </author>
    <author>
      <name>Chengxuan Qian</name>
    </author>
    <author>
      <name>Haosen Sun</name>
    </author>
    <author>
      <name>Haoran Lu</name>
    </author>
    <author>
      <name>Dingcheng Wang</name>
    </author>
    <author>
      <name>Letian Xue</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15198v1</id>
    <title>Revealing massive black hole astrophysics: The potential of hierarchical inference with extreme mass-ratio inspiral observations</title>
    <updated>2026-01-21T17:16:05Z</updated>
    <link href="https://arxiv.org/abs/2601.15198v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15198v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Gravitational waves from extreme mass-ratio inspirals (EMRIs) will enable sub-percent measurements of massive black hole parameters and provide access to the demographics of compact objects in galactic nuclei. During the LISA mission, multiple EMRIs are expected to be detected, allowing statistical studies of massive black hole populations and their formation pathways. We perform hierarchical Bayesian inference on simulated EMRI catalogues to assess how well LISA could constrain the astrophysical population using parametrised population models. We test our inference framework on a variety of populations, including heterogeneous and homogeneous mixtures of parametrised subpopulations, and scenarios in which the assumed model is deliberately misspecified. Our results show that population parameters governing distributions with sharp features can be tightly constrained. Mixed populations can be disentangled with as few as $\sim20$ detections, and even with model misspecification, the inference retains sensitivity to key population features. These results demonstrate the capabilities and limitations of EMRI population inference, providing guidance for constructing realistic astrophysical population models for LISA analysis.</summary>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T17:16:05Z</published>
    <arxiv:comment>19 pages, 7 figures, 3 tables</arxiv:comment>
    <arxiv:primary_category term="astro-ph.HE"/>
    <author>
      <name>Shashwat Singh</name>
    </author>
    <author>
      <name>Christian E. A. Chapman-Bird</name>
    </author>
    <author>
      <name>Christopher P. L. Berry</name>
    </author>
    <author>
      <name>John Veitch</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15187v1</id>
    <title>The early r-process nucleosynthesis scenarios</title>
    <updated>2026-01-21T17:05:38Z</updated>
    <link href="https://arxiv.org/abs/2601.15187v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15187v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>I compare seven actively studied r-process nucleosynthesis scenarios against observed properties of r-process elements in the early Universe, and conclude that the most likely scenario to contribute to the site of elements below the third r-process peak is the magnetorotational r-process scenario, and that of the third peak is the common envelope jets supernova (CEJSN) r-process scenario. The collapsar and CEJSN r-process scenario might also contribute to the lighter r-process elements, and the binary neutron star (NS-NS) merger r-process scenario might contribute to the third r-process peak. The magnetar, the wind from the newly born NS, and the accretion-induced collapse of a white dwarf r-process scenarios fall short in explaining observations. They might exist, but cannot be major contributors to the r-process in the early Universe. To constrain r-process scenarios in the early Universe, I require that they explain the large scatter in the r-process abundances of very metal-poor stars, account for the correlation between light r-process nucleosynthesis and iron production, and the lack of correlation between the third peak r-process production and iron production, as inferred from very metal-poor stars. I discuss the diversity of the CEJSN r-process scenario and encourage extending its exploration.</summary>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T17:05:38Z</published>
    <arxiv:comment>It will be submitted after the Nuclear Astrophysics Workshop, March 10-13, 2026, Tucson, Arizona</arxiv:comment>
    <arxiv:primary_category term="astro-ph.HE"/>
    <author>
      <name>Noam Soker</name>
      <arxiv:affiliation>Technion, Israel</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15167v1</id>
    <title>DeGAS: Gradient-Based Optimization of Probabilistic Programs without Sampling</title>
    <updated>2026-01-21T16:45:10Z</updated>
    <link href="https://arxiv.org/abs/2601.15167v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15167v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present DeGAS, a differentiable Gaussian approximate semantics for loopless probabilistic programs that enables sample-free, gradient-based optimization in models with both continuous and discrete components. DeGAS evaluates programs under a Gaussian-mixture semantics and replaces measure-zero predicates and discrete branches with a vanishing smoothing, yielding closed-form expressions for posterior and path probabilities. We prove differentiability of these quantities with respect to program parameters, enabling end-to-end optimization via standard automatic differentiation, without Monte Carlo estimators. On thirteen benchmark programs, DeGAS achieves accuracy and runtime competitive with variational inference and MCMC. Importantly, it reliably tackles optimization problems where sampling-based baselines fail to converge due to conditioning involving continuous variables.</summary>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T16:45:10Z</published>
    <arxiv:primary_category term="cs.PL"/>
    <author>
      <name>Francesca Randone</name>
    </author>
    <author>
      <name>Romina Doz</name>
    </author>
    <author>
      <name>Mirco Tribastone</name>
    </author>
    <author>
      <name>Luca Bortolussi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15152v1</id>
    <title>A Theory of transmission spectroscopy of planetary winds: Spectral-line saturation and limits on mass-loss inference</title>
    <updated>2026-01-21T16:22:59Z</updated>
    <link href="https://arxiv.org/abs/2601.15152v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15152v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Transmission spectroscopy is a key technique in the characterization of exoplanet atmospheres and has been widely applied to planets undergoing hydrodynamic escape. While a robust analytic theory exists for transmission spectra of hydrostatic atmospheres, the corresponding interpretation for escaping atmospheres has so far relied on numerical modeling. In this work, we develop a theory of transmission spectroscopy in hydrodynamically escaping atmospheres by coupling the standard transmission geometry to a steady-state, spherically symmetric, isothermal outflow. This approach yields closed-form expressions and allows the optical depth inversion problem to be examined. The analytic solution reveals that transmission spectroscopy of planetary winds naturally separates into two regimes. In an opacity-limited regime, transmission depths retain sensitivity to the atmospheric mass-loss rate. Beyond a critical threshold, however, spectral-line cores become saturated and no longer provide a unique constraint on the mass flux. This transition is marked by a sharp analytic boundary of the form $σ(λ)\times \dot M \le C_{sat}$, where $C_{sat}$ is a constant set by the thermodynamic and geometric properties of the wind. This condition specifies when the inversion between transmission depth and mass-loss rate admits a real solution. Once it is violated, the effective transit radius is no longer controlled by opacity or mass loss, but by the geometric extent of the absorbing wind. These results demonstrate that spectral-line saturation in transmission spectroscopy corresponds to a fundamental loss of invertibility between absorption and atmospheric mass loss, rather than a gradual weakening of sensitivity. The theory provides a physically transparent explanation for why strong transmission line cores often fail to constrain mass-loss rates, while weaker lines and line wings remain diagnostic.</summary>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T16:22:59Z</published>
    <arxiv:comment>Submitted to Astronomy &amp; Astrophysics</arxiv:comment>
    <arxiv:primary_category term="astro-ph.EP"/>
    <author>
      <name>Leonardos Gkouvelis</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.15111v1</id>
    <title>Auditing Language Model Unlearning via Information Decomposition</title>
    <updated>2026-01-21T15:51:19Z</updated>
    <link href="https://arxiv.org/abs/2601.15111v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.15111v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-21T15:51:19Z</published>
    <arxiv:comment>EACL 2026 Main</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Anmol Goel</name>
    </author>
    <author>
      <name>Alan Ritter</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
  </entry>
</feed>
