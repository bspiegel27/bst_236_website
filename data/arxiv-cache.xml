<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-10-29T00:58:03Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-10-28T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">124770</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.23606v1</id>
    <updated>2025-10-27T17:59:57Z</updated>
    <published>2025-10-27T17:59:57Z</published>
    <title>Variational Masked Diffusion Models</title>
    <summary>  Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.
</summary>
    <author>
      <name>Yichi Zhang</name>
    </author>
    <author>
      <name>Alex Schwing</name>
    </author>
    <author>
      <name>Zhizhen Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://riccizz.github.io/VMD</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.23606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23601v1</id>
    <updated>2025-10-27T17:59:14Z</updated>
    <published>2025-10-27T17:59:14Z</published>
    <title>Alita-G: Self-Evolving Generative Agent for Agent Generation</title>
    <summary>  Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.
</summary>
    <author>
      <name>Jiahao Qiu</name>
    </author>
    <author>
      <name>Xuan Qi</name>
    </author>
    <author>
      <name>Hongru Wang</name>
    </author>
    <author>
      <name>Xinzhe Juan</name>
    </author>
    <author>
      <name>Yimin Wang</name>
    </author>
    <author>
      <name>Zelin Zhao</name>
    </author>
    <author>
      <name>Jiayi Geng</name>
    </author>
    <author>
      <name>Jiacheng Guo</name>
    </author>
    <author>
      <name>Peihang Li</name>
    </author>
    <author>
      <name>Jingzhe Shi</name>
    </author>
    <author>
      <name>Shilong Liu</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.23601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23588v1</id>
    <updated>2025-10-27T17:54:08Z</updated>
    <published>2025-10-27T17:54:08Z</published>
    <title>FARMER: Flow AutoRegressive Transformer over Pixels</title>
    <summary>  Directly modeling the explicit likelihood of the raw data distribution is key
topic in the machine learning area, which achieves the scaling successes in
Large Language Models by autoregressive modeling. However, continuous AR
modeling over visual pixel data suffer from extremely long sequences and
high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end
generative framework that unifies Normalizing Flows (NF) and Autoregressive
(AR) models for tractable likelihood estimation and high-quality image
synthesis directly from raw pixels. FARMER employs an invertible autoregressive
flow to transform images into latent sequences, whose distribution is modeled
implicitly by an autoregressive model. To address the redundancy and complexity
in pixel-level modeling, we propose a self-supervised dimension reduction
scheme that partitions NF latent channels into informative and redundant
groups, enabling more effective and efficient AR modeling. Furthermore, we
design a one-step distillation scheme to significantly accelerate inference
speed and introduce a resampling-based classifier-free guidance algorithm to
boost image generation quality. Extensive experiments demonstrate that FARMER
achieves competitive performance compared to existing pixel-based generative
models while providing exact likelihoods and scalable training.
</summary>
    <author>
      <name>Guangting Zheng</name>
    </author>
    <author>
      <name>Qinyu Zhao</name>
    </author>
    <author>
      <name>Tao Yang</name>
    </author>
    <author>
      <name>Fei Xiao</name>
    </author>
    <author>
      <name>Zhijie Lin</name>
    </author>
    <author>
      <name>Jie Wu</name>
    </author>
    <author>
      <name>Jiajun Deng</name>
    </author>
    <author>
      <name>Yanyong Zhang</name>
    </author>
    <author>
      <name>Rui Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Bytedance Seed Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.23588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23582v1</id>
    <updated>2025-10-27T17:51:22Z</updated>
    <published>2025-10-27T17:51:22Z</published>
    <title>Cosmic magnification on multi-catalogue Herschel submillimetre galaxies</title>
    <summary>  {Submillimetre galaxies (SMGs) are excellent background sources for
magnification-bias studies, but the limited sky coverage in the submillimetre
(sub-mm) band constrains their statistical power. Beyond H-ATLAS, Herschel
produced additional sub-mm catalogues, though not optimised for spatial
statistical lensing analyses.} {Our goal is to refine cosmological constraints
from SMG magnification bias by exploiting the full sub-mm sky surveyed by
Herschel.} {We expanded the SMG sample by incorporating other Herschel
catalogues overlapping SDSS spectroscopic lenses. Random catalogues were
generated via kernel density estimation to compute cross-correlations, and
Markov Chain Monte Carlo methods were applied to infer astrophysical and
cosmological parameters for each catalogue and for the combined dataset.} {We
report the first detection of magnification bias in SMGs beyond H-ATLAS,
reinforcing the robustness of this observable. Individual Herschel catalogues
yield reasonable central values for $\Omega_m$ and $\sigma_8$, although with
large uncertainties. The combined analysis, dominated by the more powerful
H-ATLAS sample, gives results consistent with $\Lambda$CDM: $\Omega_m =
0.30^{+0.05}_{-0.07}$, $\sigma_8 = 0.80 (+/- 0.07)$, and $h &lt; 0.80$, in better
agreement with \textit{Planck} 2018 than previous non-tomographic studies.}
{SMGs are promising tracers for magnification bias, but the narrow sub-mm
coverage remains a major limitation. Wider surveys optimised for lensing would
enable cross-correlations on larger scales, yielding tighter cosmological
constraints.}
</summary>
    <author>
      <name>R. Fernandez-Fernandez</name>
    </author>
    <author>
      <name>M. M. Cueli</name>
    </author>
    <author>
      <name>J. Gonz√°lez-Nuevo</name>
    </author>
    <author>
      <name>L. Bonavera</name>
    </author>
    <author>
      <name>D. Crespo</name>
    </author>
    <author>
      <name>E. Goitia</name>
    </author>
    <author>
      <name>J. M. Casas</name>
    </author>
    <author>
      <name>J. A. Cano</name>
    </author>
    <author>
      <name>M. Migliaccio</name>
    </author>
    <link href="http://arxiv.org/abs/2510.23582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23569v1</id>
    <updated>2025-10-27T17:38:17Z</updated>
    <published>2025-10-27T17:38:17Z</published>
    <title>EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT</title>
    <summary>  Egocentric video reasoning centers on an unobservable agent behind the camera
who dynamically shapes the environment, requiring inference of hidden
intentions and recognition of fine-grained interactions. This core challenge
limits current multimodal large language models MLLMs, which excel at visible
event reasoning but lack embodied, first-person understanding. To bridge this
gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust
egocentric reasoning capabilities through spatio-temporal chain-of-thought
supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,
a large-scale egocentric QA dataset constructed from 13M diverse egocentric
video clips. This dataset features multi-minute segments annotated with
detailed CoT rationales and dense hand-object grounding. Second, we employ SFT
on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning
RFT to further enhance spatio-temporal localization. Experimental results show
that EgoThinker outperforms existing methods across multiple egocentric
benchmarks, while achieving substantial improvements in fine-grained
spatio-temporal localization tasks. Full code and data are released at
https://github.com/InternRobotics/EgoThinker.
</summary>
    <author>
      <name>Baoqi Pei</name>
    </author>
    <author>
      <name>Yifei Huang</name>
    </author>
    <author>
      <name>Jilan Xu</name>
    </author>
    <author>
      <name>Yuping He</name>
    </author>
    <author>
      <name>Guo Chen</name>
    </author>
    <author>
      <name>Fei Wu</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Jiangmiao Pang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NeurIPS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.23569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23564v2</id>
    <updated>2025-10-28T03:22:35Z</updated>
    <published>2025-10-27T17:35:15Z</published>
    <title>ReCode: Unify Plan and Action for Universal Granularity Control</title>
    <summary>  Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.
</summary>
    <author>
      <name>Zhaoyang Yu</name>
    </author>
    <author>
      <name>Jiayi Zhang</name>
    </author>
    <author>
      <name>Huixue Su</name>
    </author>
    <author>
      <name>Yufan Zhao</name>
    </author>
    <author>
      <name>Yifan Wu</name>
    </author>
    <author>
      <name>Mingyi Deng</name>
    </author>
    <author>
      <name>Jinyu Xiang</name>
    </author>
    <author>
      <name>Yizhang Lin</name>
    </author>
    <author>
      <name>Lingxiao Tang</name>
    </author>
    <author>
      <name>Yingchao Li</name>
    </author>
    <author>
      <name>Yuyu Luo</name>
    </author>
    <author>
      <name>Bang Liu</name>
    </author>
    <author>
      <name>Chenglin Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2510.23564v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23564v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23559v1</id>
    <updated>2025-10-27T17:31:42Z</updated>
    <published>2025-10-27T17:31:42Z</published>
    <title>KongNet: A Multi-headed Deep Learning Model for Detection and
  Classification of Nuclei in Histopathology Images</title>
    <summary>  Accurate detection and classification of nuclei in histopathology images are
critical for diagnostic and research applications. We present KongNet, a
multi-headed deep learning architecture featuring a shared encoder and
parallel, cell-type-specialised decoders. Through multi-task learning, each
decoder jointly predicts nuclei centroids, segmentation masks, and contours,
aided by Spatial and Channel Squeeze-and-Excitation (SCSE) attention modules
and a composite loss function. We validate KongNet in three Grand Challenges.
The proposed model achieved first place on track 1 and second place on track 2
during the MONKEY Challenge. Its lightweight variant (KongNet-Det) secured
first place in the 2025 MIDOG Challenge. KongNet pre-trained on the MONKEY
dataset and fine-tuned on the PUMA dataset ranked among the top three in the
PUMA Challenge without further optimisation. Furthermore, KongNet established
state-of-the-art performance on the publicly available PanNuke and CoNIC
datasets. Our results demonstrate that the specialised multi-decoder design is
highly effective for nuclei detection and classification across diverse tissue
and stain types. The pre-trained model weights along with the inference code
have been publicly released to support future research.
</summary>
    <author>
      <name>Jiaqi Lv</name>
    </author>
    <author>
      <name>Esha Sadia Nasir</name>
    </author>
    <author>
      <name>Kesi Xu</name>
    </author>
    <author>
      <name>Mostafa Jahanifar</name>
    </author>
    <author>
      <name>Brinder Singh Chohan</name>
    </author>
    <author>
      <name>Behnaz Elhaminia</name>
    </author>
    <author>
      <name>Shan E Ahmed Raza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Medical Image Analysis, currently under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.23559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23550v1</id>
    <updated>2025-10-27T17:27:18Z</updated>
    <published>2025-10-27T17:27:18Z</published>
    <title>Bayesian Nonlinear PDE Inference via Gaussian Process Collocation with
  Application to the Richards Equation</title>
    <summary>  The estimation of unknown parameters in nonlinear partial differential
equations (PDEs) offers valuable insights across a wide range of scientific
domains. In this work, we focus on estimating plant root parameters in the
Richards equation, which is essential for understanding the soil-plant system
in agricultural studies. Since conventional methods are computationally
intensive and often yield unstable estimates, we develop a new Gaussian process
collocation method for efficient Bayesian inference. Unlike existing Gaussian
process-based approaches, our method constructs an approximate posterior
distribution using samples drawn from a Gaussian process model fitted to the
observed data, which does not require any structural assumption about the
underlying PDE. Further, we propose to use an importance sampling procedure to
correct for the discrepancy between the approximate and true posterior
distributions. As an alternative, we also devise a prior-guided Bayesian
optimization algorithm leveraging the approximate posterior. Simulation studies
demonstrate that our method yields robust estimates under various settings.
Finally, we apply our method on a real agricultural data set and estimate the
plant root parameters with uncertainty quantification.
</summary>
    <author>
      <name>Yumo Yang</name>
    </author>
    <author>
      <name>Anass Ben Bouazza</name>
    </author>
    <author>
      <name>Xuejun Dong</name>
    </author>
    <author>
      <name>Quan Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.23550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23540v1</id>
    <updated>2025-10-27T17:14:20Z</updated>
    <published>2025-10-27T17:14:20Z</published>
    <title>The causal interpretation of panel vector autoregressions</title>
    <summary>  This paper discusses the different contemporaneous causal interpretations of
Panel Vector Autoregressions (PVAR). I show that the interpretation of PVARs
depends on the distribution of the causing variable, and can range from average
treatment effects, to average causal responses, to a combination of the two. If
the researcher is willing to postulate a no residual autocorrelation
assumption, and some units can be thought of as controls, PVAR can identify
average treatment effects on the treated. This method complements the toolkits
already present in the literature, such as staggered-DiD, or LP-DiD, as it
formulates assumptions in the residuals, and not in the outcome variables. Such
a method features a notable advantage: it allows units to be ``sparsely''
treated, capturing the impact of interventions on the innovation component of
the outcome variables. I provide an example related to the evaluation of the
effects of natural disasters economic activity at the weekly frequency in the
US.I conclude by discussing solutions to potential violations of the SUTVA
assumption arising from interference.
</summary>
    <author>
      <name>Raimondo Pala</name>
    </author>
    <link href="http://arxiv.org/abs/2510.23540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.23534v1</id>
    <updated>2025-10-27T17:10:43Z</updated>
    <published>2025-10-27T17:10:43Z</published>
    <title>Direct Debiased Machine Learning via Bregman Divergence Minimization</title>
    <summary>  We develop a direct debiased machine learning framework comprising Neyman
targeted estimation and generalized Riesz regression. Our framework unifies
Riesz regression for automatic debiased machine learning, covariate balancing,
targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In
many problems involving causal effects or structural models, the parameters of
interest depend on regression functions. Plugging regression functions
estimated by machine learning methods into the identifying equations can yield
poor performance because of first-stage bias. To reduce such bias, debiased
machine learning employs Neyman orthogonal estimating equations. Debiased
machine learning typically requires estimation of the Riesz representer and the
regression function. For this problem, we develop a direct debiased machine
learning framework with an end-to-end algorithm. We formulate estimation of the
nuisance parameters, the regression function and the Riesz representer, as
minimizing the discrepancy between Neyman orthogonal scores computed with known
and unknown nuisance parameters, which we refer to as Neyman targeted
estimation. Neyman targeted estimation includes Riesz representer estimation,
and we measure discrepancies using the Bregman divergence. The Bregman
divergence encompasses various loss functions as special cases, where the
squared loss yields Riesz regression and the Kullback-Leibler divergence yields
entropy balancing. We refer to this Riesz representer estimation as generalized
Riesz regression. Neyman targeted estimation also yields TMLE as a special case
for regression function estimation. Furthermore, for specific pairs of models
and Riesz representer estimation methods, we can automatically obtain the
covariate balancing property without explicitly solving the covariate balancing
objective.
</summary>
    <author>
      <name>Masahiro Kato</name>
    </author>
    <link href="http://arxiv.org/abs/2510.23534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.23534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
