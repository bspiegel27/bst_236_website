<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-09T00:58:35Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-09T00:58:36Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>127969</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.05964v1</id>
    <title>Training-Time Action Conditioning for Efficient Real-Time Chunking</title>
    <updated>2025-12-05T18:57:28Z</updated>
    <link href="https://arxiv.org/abs/2512.05964v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05964v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T18:57:28Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Kevin Black</name>
    </author>
    <author>
      <name>Allen Z. Ren</name>
    </author>
    <author>
      <name>Michael Equi</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05955v1</id>
    <title>SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</title>
    <updated>2025-12-05T18:51:03Z</updated>
    <link href="https://arxiv.org/abs/2512.05955v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05955v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T18:51:03Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Haowen Liu</name>
    </author>
    <author>
      <name>Shaoxiong Yao</name>
    </author>
    <author>
      <name>Haonan Chen</name>
    </author>
    <author>
      <name>Jiawei Gao</name>
    </author>
    <author>
      <name>Jiayuan Mao</name>
    </author>
    <author>
      <name>Jia-Bin Huang</name>
    </author>
    <author>
      <name>Yilun Du</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05950v1</id>
    <title>Impugan: Learning Conditional Generative Models for Robust Data Imputation</title>
    <updated>2025-12-05T18:46:33Z</updated>
    <link href="https://arxiv.org/abs/2512.05950v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05950v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T18:46:33Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Zalish Mahmud</name>
    </author>
    <author>
      <name>Anantaa Kotal</name>
    </author>
    <author>
      <name>Aritran Piplai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05940v1</id>
    <title>Designing an Optimal Sensor Network via Minimizing Information Loss</title>
    <updated>2025-12-05T18:38:30Z</updated>
    <link href="https://arxiv.org/abs/2512.05940v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05940v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that "minimize information loss" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T18:38:30Z</published>
    <arxiv:comment>37 pages, 15 figures. Accepted to Bayesian Analysis</arxiv:comment>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Daniel Waxman</name>
    </author>
    <author>
      <name>Fernando Llorente</name>
    </author>
    <author>
      <name>Katia Lamer</name>
    </author>
    <author>
      <name>Petar M. Djurić</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05933v1</id>
    <title>Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech</title>
    <updated>2025-12-05T18:19:36Z</updated>
    <link href="https://arxiv.org/abs/2512.05933v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05933v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current speech-language models (SLMs) typically use a cascade of speech encoder and large language model, treating speech understanding as a single black box. They analyze the content of speech well but reason weakly about other aspects, especially under sparse supervision. Thus, we argue for explicit reasoning over speech states and actions with modular and transparent decisions. Inspired by cognitive science we adopt a modular perspective and a world model view in which the system learns forward dynamics over latent states. We factorize speech understanding into four modules that communicate through a causal graph, establishing a cognitive state search space. Guided by posterior traces from this space, an instruction-tuned language model produces a concise causal analysis and a user-facing response, enabling counterfactual interventions and interpretability under partial supervision. We present the first graph based modular speech model for explicit reasoning and we will open source the model and data to promote the development of advanced speech understanding.</summary>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T18:19:36Z</published>
    <arxiv:primary_category term="eess.AS"/>
    <author>
      <name>Xuanru Zhou</name>
    </author>
    <author>
      <name>Jiachen Lian</name>
    </author>
    <author>
      <name>Henry Hong</name>
    </author>
    <author>
      <name>Xinyi Yang</name>
    </author>
    <author>
      <name>Gopala Anumanchipalli</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05916v1</id>
    <title>KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity</title>
    <updated>2025-12-05T17:51:10Z</updated>
    <link href="https://arxiv.org/abs/2512.05916v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05916v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T17:51:10Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Damien Lesens</name>
    </author>
    <author>
      <name>Beheshteh T. Rakhshan</name>
    </author>
    <author>
      <name>Guillaume Rabusseau</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05909v1</id>
    <title>Learning the Cosmic Web: Graph-based Classification of Simulated Galaxies by their Dark Matter Environments</title>
    <updated>2025-12-05T17:44:39Z</updated>
    <link href="https://arxiv.org/abs/2512.05909v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05909v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a novel graph-based machine learning classifier for identifying the dark matter cosmic web environments of galaxies. Large galaxy surveys offer comprehensive statistical views of how galaxy properties are shaped by large-scale structure, but this requires robust classifications of galaxies' cosmic web environments. Using stellar mass-selected IllustrisTNG-300 galaxies, we apply a three-stage, simulation-based framework to link galaxies to the total (mainly dark) underlying matter distribution. Here, we apply the following three steps: First, we assign the positions of simulated galaxies to a void, wall, filament, or cluster environment using the T-Web classification of the underlying matter distribution. Second, we construct a Delaunay triangulation of the galaxy distribution to summarise the local geometric structure with ten graph metrics for each galaxy. Third, we train a graph attention network (GAT) on each galaxy's graph metrics to predict its cosmic web environment. For galaxies with stellar mass $\mathrm{&gt;10^9 M_{\odot}}$, our GAT+ model achieves an accuracy of $85\,\%$, outperforming graph-agnostic multilayer perceptrons and graph convolutional networks. Our results demonstrate that graph-based representations of galaxy positions provide a powerful and physically meaningful way to infer dark matter environments. We plan to apply this simulation-based graph modelling to investigate how the properties of observed galaxies from the Dark Energy Spectroscopic Instrument (DESI) survey are influenced by their dark matter environments.</summary>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T17:44:39Z</published>
    <arxiv:comment>15 pages, 7 figures, 9 tables, submitted to Royal Astronomical Society Techniques and Instruments</arxiv:comment>
    <arxiv:primary_category term="astro-ph.GA"/>
    <author>
      <name>Dakshesh Kololgi</name>
    </author>
    <author>
      <name>Krishna Naidoo</name>
    </author>
    <author>
      <name>Amelie Saintonge</name>
    </author>
    <author>
      <name>Ofer Lahav</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05902v1</id>
    <title>Quantitatively mapping the Eady model onto a two-layer quasi-geostrophic model</title>
    <updated>2025-12-05T17:33:23Z</updated>
    <link href="https://arxiv.org/abs/2512.05902v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05902v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The two-layer quasigeostrophic model (2LQG) and the Eady model are two idealized systems illustrating the baroclinic instability of atmospheric jets and ocean currents. The two setups share many ingredients -- background vertically sheared zonal flow of density-stratified fluid in a rapidly rotating frame -- while differing in complexity and dimensionality. The Eady model has a continuous vertical direction, with baroclinic turbulence induced by boundary potential vorticity (PV) gradients at top and bottom. By contrast, the 2LQG sytem typically models baroclinic instability induced by interior PV gradients. This distinction challenges our ability to clearly identify a couple of 'modes' through which the Eady dynamics could be inferred from a simpler 2LQG system. In the present study, we show that this difficulty can be circumvented in the turbulent regime arising for weak bottom drag. Namely, guided by the common organization of both systems into a gas of coherent vortices, we identify a quantitative mapping between the Eady and the 2LQG models. The mapping allows for parameter-free predictions of the eddy diffusivity of the Eady model based on the knowledge of the 2LQG diffusivity. We illustrate these results using numerical simulations of the Eady and 2LQG models with linear or quadratic bottom drag.</summary>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T17:33:23Z</published>
    <arxiv:comment>18 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="physics.flu-dyn"/>
    <author>
      <name>Julie Meunier</name>
    </author>
    <author>
      <name>Basile Gallet</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05883v1</id>
    <title>The Bayesian Way: Uncertainty, Learning, and Statistical Reasoning</title>
    <updated>2025-12-05T16:59:25Z</updated>
    <link href="https://arxiv.org/abs/2512.05883v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05883v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper offers a comprehensive introduction to Bayesian inference, combining historical context, theoretical foundations, and core analytical examples. Beginning with Bayes' theorem and the philosophical distinctions between Bayesian and frequentist approaches, we develop the inferential framework for estimation, interval construction, hypothesis testing, and prediction. Through canonical models, we illustrate how prior information and observed data are formally integrated to yield posterior distributions. We also explore key concepts including loss functions, credible intervals, Bayes factors, identifiability, and asymptotic behavior. While emphasizing analytical tractability in classical settings, we outline modern extensions that rely on simulation-based methods and discuss challenges related to prior specification and model evaluation. Though focused on foundational ideas, this paper sets the stage for applying Bayesian methods in contemporary domains such as hierarchical modeling, nonparametrics, and structured applications in time series, spatial data, networks, and political science. The goal is to provide a rigorous yet accessible entry point for students and researchers seeking to adopt a Bayesian perspective in statistical practice.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T16:59:25Z</published>
    <arxiv:comment>56 pages, 1 table, 0 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Juan Sosa</name>
    </author>
    <author>
      <name>Carlos A. Martínez</name>
    </author>
    <author>
      <name>Danna Cruz</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05881v1</id>
    <title>DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints</title>
    <updated>2025-12-05T16:55:54Z</updated>
    <link href="https://arxiv.org/abs/2512.05881v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05881v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-05T16:55:54Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Rahul Golder</name>
    </author>
    <author>
      <name>Bimol Nath Roy</name>
    </author>
    <author>
      <name>M. M. Faruque Hasan</name>
    </author>
  </entry>
</feed>
