<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-06T00:55:52Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-06T00:55:52Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>127895</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.05117v1</id>
    <title>The Universal Weight Subspace Hypothesis</title>
    <updated>2025-12-04T18:59:58Z</updated>
    <link href="https://arxiv.org/abs/2512.05117v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05117v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:59:58Z</published>
    <arxiv:comment>37 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Prakhar Kaushik</name>
    </author>
    <author>
      <name>Shravan Chaudhari</name>
    </author>
    <author>
      <name>Ankit Vaidya</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <author>
      <name>Alan Yuille</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05113v1</id>
    <title>Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</title>
    <updated>2025-12-04T18:59:53Z</updated>
    <link href="https://arxiv.org/abs/2512.05113v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05113v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:59:53Z</published>
    <arxiv:comment>WACV 2025. Project page: https://chien90190.github.io/splannequin/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hao-Jen Chien</name>
    </author>
    <author>
      <name>Yi-Chuan Huang</name>
    </author>
    <author>
      <name>Chung-Ho Wu</name>
    </author>
    <author>
      <name>Wei-Lun Chao</name>
    </author>
    <author>
      <name>Yu-Lun Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05107v1</id>
    <title>STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</title>
    <updated>2025-12-04T18:59:22Z</updated>
    <link href="https://arxiv.org/abs/2512.05107v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05107v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -&gt; Preference -&gt; Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:59:22Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Feng Xu</name>
    </author>
    <author>
      <name>Guangyao Zhai</name>
    </author>
    <author>
      <name>Xin Kong</name>
    </author>
    <author>
      <name>Tingzhong Fu</name>
    </author>
    <author>
      <name>Daniel F. N. Gordon</name>
    </author>
    <author>
      <name>Xueli An</name>
    </author>
    <author>
      <name>Benjamin Busam</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05106v1</id>
    <title>NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</title>
    <updated>2025-12-04T18:59:18Z</updated>
    <link href="https://arxiv.org/abs/2512.05106v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05106v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. φ-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, φ-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, φ-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:59:18Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yu Zeng</name>
    </author>
    <author>
      <name>Charles Ochoa</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <author>
      <name>Vitor Guizilini</name>
    </author>
    <author>
      <name>Rowan McAllister</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05105v1</id>
    <title>Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning</title>
    <updated>2025-12-04T18:59:18Z</updated>
    <link href="https://arxiv.org/abs/2512.05105v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05105v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:59:18Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Purbesh Mitra</name>
    </author>
    <author>
      <name>Sennur Ulukus</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05103v1</id>
    <title>TV2TV: A Unified Framework for Interleaved Language and Video Generation</title>
    <updated>2025-12-04T18:59:09Z</updated>
    <link href="https://arxiv.org/abs/2512.05103v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05103v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:59:09Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xiaochuang Han</name>
    </author>
    <author>
      <name>Youssef Emad</name>
    </author>
    <author>
      <name>Melissa Hall</name>
    </author>
    <author>
      <name>John Nguyen</name>
    </author>
    <author>
      <name>Karthik Padthe</name>
    </author>
    <author>
      <name>Liam Robbins</name>
    </author>
    <author>
      <name>Amir Bar</name>
    </author>
    <author>
      <name>Delong Chen</name>
    </author>
    <author>
      <name>Michal Drozdzal</name>
    </author>
    <author>
      <name>Maha Elbayad</name>
    </author>
    <author>
      <name>Yushi Hu</name>
    </author>
    <author>
      <name>Shang-Wen Li</name>
    </author>
    <author>
      <name>Sreya Dutta Roy</name>
    </author>
    <author>
      <name>Jakob Verbeek</name>
    </author>
    <author>
      <name>XuDong Wang</name>
    </author>
    <author>
      <name>Marjan Ghazvininejad</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <author>
      <name>Emily Dinan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05078v1</id>
    <title>Improving Posterior Inference of Galaxy Properties with Image-Based Conditional Flow Matching</title>
    <updated>2025-12-04T18:44:14Z</updated>
    <link href="https://arxiv.org/abs/2512.05078v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05078v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Estimating physical properties of galaxies from wide-field surveys remains a central challenge in astrophysics. While spectroscopy provides precise measurements, it is observationally expensive, and photometry discards morphological information that correlates with mass, star formation history, metallicity, and dust. We present a conditional flow matching (CFM) framework that leverages pixel-level imaging alongside photometry to improve posterior inference of galaxy properties. Using $\sim10^5$ SDSS galaxies, we compare models trained on photometry alone versus photometry plus images. The image+photometry model outperforms the photometry-only model in posterior inference and more reliably recovers known scaling relations. Morphological information also helps mitigate the dust--age degeneracy. Our results highlight the potential of integrating morphology into photometric SED fitting pipelines, opening a pathway towards more accurate and physically informed constraints on galaxy properties.</summary>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:44:14Z</published>
    <arxiv:comment>Accepted at NeurIPS 2025 ML4PS workshop</arxiv:comment>
    <arxiv:primary_category term="astro-ph.IM"/>
    <author>
      <name>Mikaeel Yunus</name>
    </author>
    <author>
      <name>John F. Wu</name>
    </author>
    <author>
      <name>Benne W. Holwerda</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05073v1</id>
    <title>David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?</title>
    <updated>2025-12-04T18:37:29Z</updated>
    <link href="https://arxiv.org/abs/2512.05073v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05073v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:37:29Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shashwat Shankar</name>
    </author>
    <author>
      <name>Subhranshu Pandey</name>
    </author>
    <author>
      <name>Innocent Dengkhw Mochahari</name>
    </author>
    <author>
      <name>Bhabesh Mali</name>
    </author>
    <author>
      <name>Animesh Basak Chowdhury</name>
    </author>
    <author>
      <name>Sukanta Bhattacharjee</name>
    </author>
    <author>
      <name>Chandan Karfa</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05070v1</id>
    <title>Control Consistency Losses for Diffusion Bridges</title>
    <updated>2025-12-04T18:31:39Z</updated>
    <link href="https://arxiv.org/abs/2512.05070v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05070v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:31:39Z</published>
    <arxiv:comment>Frontiers in Probabilistic Inference: Sampling Meets Learning Workshop at NeurIPS 2025 (Oral)</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Samuel Howard</name>
    </author>
    <author>
      <name>Nikolas Nüsken</name>
    </author>
    <author>
      <name>Jakiw Pidstrigach</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.05060v1</id>
    <title>4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer</title>
    <updated>2025-12-04T18:15:27Z</updated>
    <link href="https://arxiv.org/abs/2512.05060v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.05060v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-04T18:15:27Z</published>
    <arxiv:comment>Code: https://github.com/hustvl/4DLangVGGT, Webpage: https://hustvl.github.io/4DLangVGGT</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xianfeng Wu</name>
    </author>
    <author>
      <name>Yajing Bai</name>
    </author>
    <author>
      <name>Minghan Li</name>
    </author>
    <author>
      <name>Xianzu Wu</name>
    </author>
    <author>
      <name>Xueqi Zhao</name>
    </author>
    <author>
      <name>Zhongyuan Lai</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
  </entry>
</feed>
