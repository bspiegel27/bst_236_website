<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-05-07T00:56:01Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-05-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">111721</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.02835v1</id>
    <updated>2025-05-05T17:59:50Z</updated>
    <published>2025-05-05T17:59:50Z</published>
    <title>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement
  Learning</title>
    <summary>  Multimodal Reward Models (MRMs) play a crucial role in enhancing the
performance of Multimodal Large Language Models (MLLMs). While recent
advancements have primarily focused on improving the model structure and
training data of MRMs, there has been limited exploration into the
effectiveness of long-term reasoning capabilities for reward modeling and how
to activate these capabilities in MRMs. In this paper, we explore how
Reinforcement Learning (RL) can be used to improve reward modeling.
Specifically, we reformulate the reward modeling problem as a rule-based RL
task. However, we observe that directly applying existing RL algorithms, such
as Reinforce++, to reward modeling often leads to training instability or even
collapse due to the inherent limitations of these algorithms. To address this
issue, we propose the StableReinforce algorithm, which refines the training
loss, advantage estimation strategy, and reward design of existing RL methods.
These refinements result in more stable training dynamics and superior
performance. To facilitate MRM training, we collect 200K preference data from
diverse datasets. Our reward model, R1-Reward, trained using the
StableReinforce algorithm on this dataset, significantly improves performance
on multimodal reward modeling benchmarks. Compared to previous SOTA models,
R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$
improvement on the Multimodal Reward Bench. Moreover, with more inference
compute, R1-Reward's performance is further enhanced, highlighting the
potential of RL algorithms in optimizing MRMs.
</summary>
    <author>
      <name>Yi-Fan Zhang</name>
    </author>
    <author>
      <name>Xingyu Lu</name>
    </author>
    <author>
      <name>Xiao Hu</name>
    </author>
    <author>
      <name>Chaoyou Fu</name>
    </author>
    <author>
      <name>Bin Wen</name>
    </author>
    <author>
      <name>Tianke Zhang</name>
    </author>
    <author>
      <name>Changyi Liu</name>
    </author>
    <author>
      <name>Kaiyu Jiang</name>
    </author>
    <author>
      <name>Kaibing Chen</name>
    </author>
    <author>
      <name>Kaiyu Tang</name>
    </author>
    <author>
      <name>Haojie Ding</name>
    </author>
    <author>
      <name>Jiankang Chen</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Zhang Zhang</name>
    </author>
    <author>
      <name>Tingting Gao</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Home page: https://github.com/yfzhang114/r1_reward</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02825v2</id>
    <updated>2025-05-06T10:17:58Z</updated>
    <published>2025-05-05T17:51:56Z</published>
    <title>Towards Application-Specific Evaluation of Vision Models: Case Studies
  in Ecology and Biology</title>
    <summary>  Computer vision methods have demonstrated considerable potential to
streamline ecological and biological workflows, with a growing number of
datasets and models becoming available to the research community. However,
these resources focus predominantly on evaluation using machine learning
metrics, with relatively little emphasis on how their application impacts
downstream analysis. We argue that models should be evaluated using
application-specific metrics that directly represent model performance in the
context of its final use case. To support this argument, we present two
disparate case studies: (1) estimating chimpanzee abundance and density with
camera trap distance sampling when using a video-based behaviour classifier and
(2) estimating head rotation in pigeons using a 3D posture estimator. We show
that even models with strong machine learning performance (e.g., 87% mAP) can
yield data that leads to discrepancies in abundance estimates compared to
expert-derived data. Similarly, the highest-performing models for posture
estimation do not produce the most accurate inferences of gaze direction in
pigeons. Motivated by these findings, we call for researchers to integrate
application-specific metrics in ecological/biological datasets, allowing for
models to be benchmarked in the context of their downstream application and to
facilitate better integration of models into application workflows.
</summary>
    <author>
      <name>Alex Hoi Hang Chan</name>
    </author>
    <author>
      <name>Otto Brookes</name>
    </author>
    <author>
      <name>Urs Waldmann</name>
    </author>
    <author>
      <name>Hemal Naik</name>
    </author>
    <author>
      <name>Iain D. Couzin</name>
    </author>
    <author>
      <name>Majid Mirmehdi</name>
    </author>
    <author>
      <name>Noël Adiko Houa</name>
    </author>
    <author>
      <name>Emmanuelle Normand</name>
    </author>
    <author>
      <name>Christophe Boesch</name>
    </author>
    <author>
      <name>Lukas Boesch</name>
    </author>
    <author>
      <name>Mimi Arandjelovic</name>
    </author>
    <author>
      <name>Hjalmar Kühl</name>
    </author>
    <author>
      <name>Tilo Burghardt</name>
    </author>
    <author>
      <name>Fumihiro Kano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CVPR Workshop, CV4Animals 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02811v1</id>
    <updated>2025-05-05T17:39:35Z</updated>
    <published>2025-05-05T17:39:35Z</published>
    <title>Knowing You Don't Know: Learning When to Continue Search in Multi-round
  RAG through Self-Practicing</title>
    <summary>  Retrieval Augmented Generation (RAG) has shown strong capability in enhancing
language models' knowledge and reducing AI generative hallucinations, driving
its widespread use. However, complex tasks requiring multi-round retrieval
remain challenging, and early attempts tend to be overly optimistic without a
good sense of self-skepticism. Current multi-round RAG systems may continue
searching even when enough information has already been retrieved, or they may
provide incorrect answers without having sufficient information or knowledge.
Existing solutions either require large amounts of expensive human-labeled
process supervision data or lead to subpar performance.
  This paper aims to address these limitations by introducing a new framework,
\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and
multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system
self-practice multi-round retrieval, augmenting existing question-answer pairs
with intermediate inner monologue reasoning steps to generate synthetic
training data. For each pair, the system may explore multiple retrieval paths,
which are labeled as successful if they reach the correct answer and
unsuccessful otherwise. Using this data, we train a lightweight information
sufficiency Critic. At inference time, the Critic evaluates whether the RAG
system has retrieved sufficient information at each round, guiding retrieval
decisions and improving system-level self-awareness through in-context
reinforcement learning.
  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an
effective multi-round RAG solution. Furthermore, this framework is
system-efficient, adding a lightweight component to RAG without requiring
modifications to existing LLMs or search engines, and data-efficient,
eliminating the need for costly human-annotated mid-step retrieval process
supervision data.
</summary>
    <author>
      <name>Diji Yang</name>
    </author>
    <author>
      <name>Linda Zeng</name>
    </author>
    <author>
      <name>Jinmeng Rao</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 48th International ACM SIGIR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02787v1</id>
    <updated>2025-05-05T17:02:13Z</updated>
    <published>2025-05-05T17:02:13Z</published>
    <title>Unsupervised training of keypoint-agnostic descriptors for flexible
  retinal image registration</title>
    <summary>  Current color fundus image registration approaches are limited, among other
things, by the lack of labeled data, which is even more significant in the
medical domain, motivating the use of unsupervised learning. Therefore, in this
work, we develop a novel unsupervised descriptor learning method that does not
rely on keypoint detection. This enables the resulting descriptor network to be
agnostic to the keypoint detector used during the registration inference.
  To validate this approach, we perform an extensive and comprehensive
comparison on the reference public retinal image registration dataset.
Additionally, we test our method with multiple keypoint detectors of varied
nature, even proposing some novel ones. Our results demonstrate that the
proposed approach offers accurate registration, not incurring in any
performance loss versus supervised methods. Additionally, it demonstrates
accurate performance regardless of the keypoint detector used. Thus, this work
represents a notable step towards leveraging unsupervised learning in the
medical domain.
</summary>
    <author>
      <name>David Rivas-Villar</name>
    </author>
    <author>
      <name>Álvaro S. Hervella</name>
    </author>
    <author>
      <name>José Rouco</name>
    </author>
    <author>
      <name>Jorge Novo</name>
    </author>
    <link href="http://arxiv.org/abs/2505.02787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02781v1</id>
    <updated>2025-05-05T16:47:29Z</updated>
    <published>2025-05-05T16:47:29Z</published>
    <title>Local Markov Equivalence and Local Causal Discovery for Identifying
  Controlled Direct Effects</title>
    <summary>  Understanding and identifying controlled direct effects (CDEs) is crucial
across numerous scientific domains, including public health. While existing
methods can identify these effects from causal directed acyclic graphs (DAGs),
the true underlying structure is often unknown in practice. Essential graphs,
which represent a Markov equivalence class of DAGs characterized by the same
set of d-separations, provide a more practical and realistic alternative.
However, learning the full essential graph is computationally intensive and
typically depends on strong, untestable assumptions. In this work, we
characterize a local class of graphs, defined relative to a target variable,
that share a specific subset of d-separations, and introduce a graphical
representation of this class, called the local essential graph (LEG). We then
present LocPC, a novel algorithm designed to recover the LEG from an observed
distribution using only local conditional independence tests. Building on
LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG
that is sufficient to identify a CDE, bypassing the need of retrieving the full
essential graph. Compared to global methods, our algorithms require less
conditional independence tests and operate under weaker assumptions while
maintaining theoretical guarantees.
</summary>
    <author>
      <name>Timothée Loranchet</name>
    </author>
    <author>
      <name>Charles K. Assaad</name>
    </author>
    <link href="http://arxiv.org/abs/2505.02781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02773v1</id>
    <updated>2025-05-05T16:36:46Z</updated>
    <published>2025-05-05T16:36:46Z</published>
    <title>Can Transformers help us perform parameter estimation of overlapping
  signals in gravitational wave detectors?</title>
    <summary>  Overlapping signals represent one of the major data analysis challenges in
next-generation gravitational wave detectors. We leverage Transformers and
Normalizing Flows, state-of-the-art machine learning algorithms, to address the
parameter estimation of overlapping binary black hole mergers in the Einstein
Telescope (ET). Our proposed model combines a Transformer-based "Knowledge
Extractor Neural Network" (KENN) with a Normalizing Flow (HYPERION) to perform
rapid and unbiased inference over multiple overlapping black hole binary
events. The choice of architecture leverages the strength of Transformers in
capturing complex and long-range temporal structures in the strain time series
data, while Normalizing Flows provide a powerful framework to sample posterior
distributions. We demonstrate the effectiveness and robustness of our model
over simulated gravitational wave signals, showing that it maintains the same
level of accuracy regardless of the correlation level in the data. Moreover our
model provides estimates of chirp mass and coalescence times within &lt;10-20%
from the true simulated value. The results obtained are promising and show how
this approach might represent a first step toward a deep-learning based
inference pipeline for ET and other future gravitational wave detectors.
</summary>
    <author>
      <name>Lucia Papalini</name>
    </author>
    <author>
      <name>Federico De Santi</name>
    </author>
    <author>
      <name>Massimiliano Razzano</name>
    </author>
    <author>
      <name>Ik Siong Heng</name>
    </author>
    <author>
      <name>Elena Cuoco</name>
    </author>
    <link href="http://arxiv.org/abs/2505.02773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02754v1</id>
    <updated>2025-05-05T16:07:28Z</updated>
    <published>2025-05-05T16:07:28Z</published>
    <title>Debiased inference in error-in-variable problems with non-Gaussian
  measurement error</title>
    <summary>  We consider drawing statistical inferences based on data subject to
non-Gaussian measurement error. Unlike most existing methods developed under
the assumption of Gaussian measurement error, the proposed strategy exploits
hypercomplex numbers to reduce bias in naive estimation that ignores
non-Gaussian measurement error. We apply this new method to several widely
applicable parametric regression models with error-prone covariates, and kernel
density estimation using error-contaminated data. The efficacy of this method
in bias reduction is demonstrated in simulation studies and a real-life
application in sports analytics.
</summary>
    <author>
      <name>Nicholas W. Woolsey</name>
    </author>
    <author>
      <name>Xianzheng Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.02754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62G08, 62J02, secondary 62F12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02743v1</id>
    <updated>2025-05-05T15:50:52Z</updated>
    <published>2025-05-05T15:50:52Z</published>
    <title>Cooperative Bayesian and variance networks disentangle aleatoric and
  epistemic uncertainties</title>
    <summary>  Real-world data contains aleatoric uncertainty - irreducible noise arising
from imperfect measurements or from incomplete knowledge about the data
generation process. Mean variance estimation (MVE) networks can learn this type
of uncertainty but require ad-hoc regularization strategies to avoid
overfitting and are unable to predict epistemic uncertainty (model
uncertainty). Conversely, Bayesian neural networks predict epistemic
uncertainty but are notoriously difficult to train due to the approximate
nature of Bayesian inference. We propose to cooperatively train a variance
network with a Bayesian neural network and demonstrate that the resulting model
disentangles aleatoric and epistemic uncertainties while improving the mean
estimation. We demonstrate the effectiveness and scalability of this method
across a diverse range of datasets, including a time-dependent heteroscedastic
regression dataset we created where the aleatoric uncertainty is known. The
proposed method is straightforward to implement, robust, and adaptable to
various model architectures.
</summary>
    <author>
      <name>Jiaxiang Yi</name>
    </author>
    <author>
      <name>Miguel A. Bessa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02705v1</id>
    <updated>2025-05-05T14:57:43Z</updated>
    <published>2025-05-05T14:57:43Z</published>
    <title>Multi-View Learning with Context-Guided Receptance for Image Denoising</title>
    <summary>  Image denoising is essential in low-level vision applications such as
photography and automated driving. Existing methods struggle with
distinguishing complex noise patterns in real-world scenes and consume
significant computational resources due to reliance on Transformer-based
models. In this work, the Context-guided Receptance Weighted Key-Value (\M)
model is proposed, combining enhanced multi-view feature integration with
efficient sequence modeling. Our approach introduces the Context-guided Token
Shift (CTS) paradigm, which effectively captures local spatial dependencies and
enhance the model's ability to model real-world noise distributions.
Additionally, the Frequency Mix (FMix) module extracting frequency-domain
features is designed to isolate noise in high-frequency spectra, and is
integrated with spatial representations through a multi-view learning process.
To improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is
adopted, enabling full pixel-sequence interaction with linear complexity while
overcoming the causal selection constraints. The model is validated on multiple
real-world image denoising datasets, outperforming the existing
state-of-the-art methods quantitatively and reducing inference time up to 40\%.
Qualitative results further demonstrate the ability of our model to restore
fine details in various scenes.
</summary>
    <author>
      <name>Binghong Chen</name>
    </author>
    <author>
      <name>Tingting Chai</name>
    </author>
    <author>
      <name>Wei Jiang</name>
    </author>
    <author>
      <name>Yuanrong Xu</name>
    </author>
    <author>
      <name>Guanglu Zhou</name>
    </author>
    <author>
      <name>Xiangqian Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI 2025, code will be available at
  https://github.com/Seeker98/CRWKV</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02703v1</id>
    <updated>2025-05-05T14:57:02Z</updated>
    <published>2025-05-05T14:57:02Z</published>
    <title>Structure Causal Models and LLMs Integration in Medical Visual Question
  Answering</title>
    <summary>  Medical Visual Question Answering (MedVQA) aims to answer medical questions
according to medical images. However, the complexity of medical data leads to
confounders that are difficult to observe, so bias between images and questions
is inevitable. Such cross-modal bias makes it challenging to infer medically
meaningful answers. In this work, we propose a causal inference framework for
the MedVQA task, which effectively eliminates the relative confounding effect
between the image and the question to ensure the precision of the
question-answering (QA) session. We are the first to introduce a novel causal
graph structure that represents the interaction between visual and textual
elements, explicitly capturing how different questions influence visual
features. During optimization, we apply the mutual information to discover
spurious correlations and propose a multi-variable resampling front-door
adjustment method to eliminate the relative confounding effect, which aims to
align features based on their true causal relevance to the question-answering
task. In addition, we also introduce a prompt strategy that combines multiple
prompt forms to improve the model's ability to understand complex medical data
and answer accurately. Extensive experiments on three MedVQA datasets
demonstrate that 1) our method significantly improves the accuracy of MedVQA,
and 2) our method achieves true causal correlations in the face of complex
medical data.
</summary>
    <author>
      <name>Zibo Xu</name>
    </author>
    <author>
      <name>Qiang Li</name>
    </author>
    <author>
      <name>Weizhi Nie</name>
    </author>
    <author>
      <name>Weijie Wang</name>
    </author>
    <author>
      <name>Anan Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMI.2025.3564320</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMI.2025.3564320" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE TMI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
