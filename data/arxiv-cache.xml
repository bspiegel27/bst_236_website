<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-12-04T00:58:16Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-12-04T00:58:16Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>127712</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.03044v1</id>
    <title>Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling</title>
    <updated>2025-12-02T18:59:53Z</updated>
    <link href="https://arxiv.org/abs/2512.03044v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.03044v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T18:59:53Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Yueru Jia</name>
    </author>
    <author>
      <name>Jiaming Liu</name>
    </author>
    <author>
      <name>Shengbang Liu</name>
    </author>
    <author>
      <name>Rui Zhou</name>
    </author>
    <author>
      <name>Wanhe Yu</name>
    </author>
    <author>
      <name>Yuyang Yan</name>
    </author>
    <author>
      <name>Xiaowei Chi</name>
    </author>
    <author>
      <name>Yandong Guo</name>
    </author>
    <author>
      <name>Boxin Shi</name>
    </author>
    <author>
      <name>Shanghang Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.03024v1</id>
    <title>TokenPowerBench: Benchmarking the Power Consumption of LLM Inference</title>
    <updated>2025-12-02T18:50:17Z</updated>
    <link href="https://arxiv.org/abs/2512.03024v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.03024v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T18:50:17Z</published>
    <arxiv:comment>Accepted by the AAAI'26 Conference Main Track</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Chenxu Niu</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Jie Li</name>
    </author>
    <author>
      <name>Yongjian Zhao</name>
    </author>
    <author>
      <name>Tongyang Wang</name>
    </author>
    <author>
      <name>Xi Wang</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.03019v1</id>
    <title>Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge</title>
    <updated>2025-12-02T18:46:47Z</updated>
    <link href="https://arxiv.org/abs/2512.03019v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.03019v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T18:46:47Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Hamid Dadkhahi</name>
    </author>
    <author>
      <name>Firas Trabelsi</name>
    </author>
    <author>
      <name>Parker Riley</name>
    </author>
    <author>
      <name>Juraj Juraska</name>
    </author>
    <author>
      <name>Mehdi Mirzazadeh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.03018v1</id>
    <title>AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry</title>
    <updated>2025-12-02T18:46:25Z</updated>
    <link href="https://arxiv.org/abs/2512.03018v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.03018v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The boundary representation (B-Rep) is the standard data structure used in Computer-Aided Design (CAD) for defining solid models. Despite recent progress, directly generating B-Reps end-to-end with precise geometry and watertight topology remains a challenge. This paper presents AutoBrep, a novel Transformer model that autoregressively generates B-Reps with high quality and validity. AutoBrep employs a unified tokenization scheme that encodes both geometric and topological characteristics of a B-Rep model as a sequence of discrete tokens. Geometric primitives (i.e., surfaces and curves) are encoded as latent geometry tokens, and their structural relationships are defined as special topological reference tokens. Sequence order in AutoBrep naturally follows a breadth first traversal of the B-Rep face adjacency graph. At inference time, neighboring faces and edges along with their topological structure are progressively generated. Extensive experiments demonstrate the advantages of our unified representation when coupled with next-token prediction for B-Rep generation. AutoBrep outperforms baselines with better quality and watertightness. It is also highly scalable to complex solids with good fidelity and inference speed. We further show that autocompleting B-Reps is natively supported through our unified tokenization, enabling user-controllable CAD generation with minimal changes. Code is available at https://github.com/AutodeskAILab/AutoBrep.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T18:46:25Z</published>
    <arxiv:comment>Accepted to Siggraph Asia 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xiang Xu</name>
    </author>
    <author>
      <name>Pradeep Kumar Jayaraman</name>
    </author>
    <author>
      <name>Joseph G. Lambourne</name>
    </author>
    <author>
      <name>Yilin Liu</name>
    </author>
    <author>
      <name>Durvesh Malpure</name>
    </author>
    <author>
      <name>Pete Meltzer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.03014v1</id>
    <title>Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks</title>
    <updated>2025-12-02T18:41:10Z</updated>
    <link href="https://arxiv.org/abs/2512.03014v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.03014v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T18:41:10Z</published>
    <arxiv:comment>NeurIPS 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Matthew Dutson</name>
    </author>
    <author>
      <name>Nathan Labiosa</name>
    </author>
    <author>
      <name>Yin Li</name>
    </author>
    <author>
      <name>Mohit Gupta</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.03010v1</id>
    <title>SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting</title>
    <updated>2025-12-02T18:35:54Z</updated>
    <link href="https://arxiv.org/abs/2512.03010v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.03010v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T18:35:54Z</published>
    <arxiv:comment>Project page: https://lfranke.github.io/surffill</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Svenja Strobel</name>
    </author>
    <author>
      <name>Matthias Innmann</name>
    </author>
    <author>
      <name>Bernhard Egger</name>
    </author>
    <author>
      <name>Marc Stamminger</name>
    </author>
    <author>
      <name>Linus Franke</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.02996v1</id>
    <title>Structured Clifford+T Circuits for Efficient Generation of Quantum Chaos</title>
    <updated>2025-12-02T18:20:50Z</updated>
    <link href="https://arxiv.org/abs/2512.02996v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.02996v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We investigate the emergence of quantum chaos and unitary T-design behavior in derandomized Clifford+T circuits using causal cover architectures. Motivated by the need for deterministic constructions that can exhibit chaotic behavior across diverse quantum hardware platforms, we explore deterministic Clifford circuit architectures (random Clifford circuits with causal cover, bitonic sorting networks, and permutation-based routing circuits) to drive quantum circuits toward Wigner-Dyson (WD) entanglement spectrum statistics and OTOC decay.Our experiments demonstrate that causal connectivity, not circuit depth or randomness, is a critical feature that drives circuits to chaos. We show that initializing with n T-states and adding a second T-layer after a causally covered Clifford evolution yields consistent OTOC decay and WD statistics. This also enables deeper understanding of the circuit structures that generate complex entanglement behavior. Notably, our work suggests polylogarithmic-depth deterministic circuits suffice to approximate chaotic behavior, highlighting that causal connectivity is sufficient for operator spreading to induce Wigner-Dyson entanglement statistics and OTOC decay.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T18:20:50Z</published>
    <arxiv:comment>6 pages, 4 figures, IEEE QCE 2025</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Asim Sharma</name>
    </author>
    <author>
      <name>Avah Banerjee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.02968v1</id>
    <title>Flexible Gravitational-Wave Parameter Estimation with Transformers</title>
    <updated>2025-12-02T17:49:08Z</updated>
    <link href="https://arxiv.org/abs/2512.02968v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.02968v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Gravitational-wave data analysis relies on accurate and efficient methods to extract physical information from noisy detector signals, yet the increasing rate and complexity of observations represent a growing challenge. Deep learning provides a powerful alternative to traditional inference, but existing neural models typically lack the flexibility to handle variations in data analysis settings. Such variations accommodate imperfect observations or are required for specialized tests, and could include changes in detector configurations, overall frequency ranges, or localized cuts. We introduce a flexible transformer-based architecture paired with a training strategy that enables adaptation to diverse analysis settings at inference time. Applied to parameter estimation, we demonstrate that a single flexible model -- called Dingo-T1 -- can (i) analyze 48 gravitational-wave events from the third LIGO-Virgo-KAGRA Observing Run under a wide range of analysis configurations, (ii) enable systematic studies of how detector and frequency configurations impact inferred posteriors, and (iii) perform inspiral-merger-ringdown consistency tests probing general relativity. Dingo-T1 also improves median sample efficiency on real events from a baseline of 1.4% to 4.2%. Our approach thus demonstrates flexible and scalable inference with a principled framework for handling missing or incomplete data -- key capabilities for current and next-generation observatories.</summary>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T17:49:08Z</published>
    <arxiv:comment>8+11 pages, 3+7 figures</arxiv:comment>
    <arxiv:primary_category term="gr-qc"/>
    <author>
      <name>Annalena Kofler</name>
    </author>
    <author>
      <name>Maximilian Dax</name>
    </author>
    <author>
      <name>Stephen R. Green</name>
    </author>
    <author>
      <name>Jonas Wildberger</name>
    </author>
    <author>
      <name>Nihar Gupte</name>
    </author>
    <author>
      <name>Jakob H. Macke</name>
    </author>
    <author>
      <name>Jonathan Gair</name>
    </author>
    <author>
      <name>Alessandra Buonanno</name>
    </author>
    <author>
      <name>Bernhard Sch√∂lkopf</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.02960v1</id>
    <title>Conservation of Momentum and Energy in the Lorenz-Abraham-Dirac Equation of Motion</title>
    <updated>2025-12-02T17:36:13Z</updated>
    <link href="https://arxiv.org/abs/2512.02960v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.02960v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>After a brief review of the modified causal Lorentz-Abraham classical equation of motion for an extended charged sphere and its limit to the mass-renormalized modified causal Lorentz-Abraham-Dirac equation of motion as the radius of the charged sphere approaches zero, a streamlined derivation is given for the conditions on the external force required for these modified equations of motion to satisfy conservation of momentum and energy.</summary>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T17:36:13Z</published>
    <arxiv:comment>8 pages</arxiv:comment>
    <arxiv:primary_category term="physics.class-ph"/>
    <author>
      <name>Arthur D. Yaghjian</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.02954v1</id>
    <title>The suppression of the matter power spectrum: strong feedback from X-ray gas mass fractions, kSZ effect profiles, and galaxy-galaxy lensing</title>
    <updated>2025-12-02T17:31:28Z</updated>
    <link href="https://arxiv.org/abs/2512.02954v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.02954v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Baryon feedback redistributes gas relative to the underlying dark matter distribution and suppresses the matter power spectrum on small scales, but the amplitude and scale dependence of this effect are uncertain. We constrain the impact of baryon feedback on the matter power spectrum by jointly analysing X-ray gas mass fractions from the eROSITA and HSC-XXL samples and SDSS/DESI+ACT kinetic Sunyaev-Zel'dovich (kSZ) effect profiles; the samples are characterised with galaxy-galaxy lensing and together span group and cluster masses at $0&lt;z&lt;1$. Using the baryonification framework, our joint eROSITA and kSZ model gives precise constraints on the suppression of the matter power spectrum: $10 \pm 2\%$ at $k=1~h~\mathrm{Mpc}^{-1}$. The inferred gas profiles are more extended and the power suppression is stronger than predicted by the fiducial models of recent hydrodynamical simulation suites, including FLAMINGO and BAHAMAS. The HSC-XXL gas mass fractions, which the fiducial simulations were calibrated to reproduce, prefer more moderate power suppression than the kSZ and eROSITA data: $5 \pm 4\%$ at $k=1~h~\mathrm{Mpc}^{-1}$. With a simulated LSST Year 1 weak lensing analysis, we demonstrate a framework for next-generation surveys: calibrating feedback models with multi-wavelength gas observables to recover the small-scale statistical power of cosmic shear.</summary>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-02T17:31:28Z</published>
    <arxiv:comment>21 pages, submitted to MNRAS</arxiv:comment>
    <arxiv:primary_category term="astro-ph.CO"/>
    <author>
      <name>Jared Siegel</name>
    </author>
    <author>
      <name>Leah Bigwood</name>
    </author>
    <author>
      <name>Alexandra Amon</name>
    </author>
    <author>
      <name>Jamie McCullough</name>
    </author>
    <author>
      <name>Masaya Yamamoto</name>
    </author>
    <author>
      <name>Ian G. McCarthy</name>
    </author>
    <author>
      <name>Matthieu Schaller</name>
    </author>
    <author>
      <name>Aurel Schneider</name>
    </author>
    <author>
      <name>Joop Schaye</name>
    </author>
  </entry>
</feed>
