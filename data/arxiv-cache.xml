<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-11-25T00:56:19Z -->
<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/p6EyKyP4fMY/66n9Ev8E79F7KlI</id>
  <title>arXiv Query: search_query=all:causal OR all:inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-25T00:56:19Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:causal+OR+all:inference&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>126779</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2511.17454v1</id>
    <title>Illustrator's Depth: Monocular Layer Index Prediction for Image Decomposition</title>
    <updated>2025-11-21T17:56:43Z</updated>
    <link href="https://arxiv.org/abs/2511.17454v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17454v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Illustrator's Depth, a novel definition of depth that addresses a key challenge in digital content creation: decomposing flat images into editable, ordered layers. Inspired by an artist's compositional process, illustrator's depth infers a layer index to each pixel, forming an interpretable image decomposition through a discrete, globally consistent ordering of elements optimized for editability. We also propose and train a neural network using a curated dataset of layered vector graphics to predict layering directly from raster inputs. Our layer index inference unlocks a range of powerful downstream applications. In particular, it significantly outperforms state-of-the-art baselines for image vectorization while also enabling high-fidelity text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive depth-aware editing. By reframing depth from a physical quantity to a creative abstraction, illustrator's depth prediction offers a new foundation for editable image decomposition.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:56:43Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Nissim Maruani</name>
    </author>
    <author>
      <name>Peiying Zhang</name>
    </author>
    <author>
      <name>Siddhartha Chaudhuri</name>
    </author>
    <author>
      <name>Matthew Fisher</name>
    </author>
    <author>
      <name>Nanxuan Zhao</name>
    </author>
    <author>
      <name>Vladimir G. Kim</name>
    </author>
    <author>
      <name>Pierre Alliez</name>
    </author>
    <author>
      <name>Mathieu Desbrun</name>
    </author>
    <author>
      <name>Wang Yifan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17438v1</id>
    <title>Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models</title>
    <updated>2025-11-21T17:35:27Z</updated>
    <link href="https://arxiv.org/abs/2511.17438v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17438v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:35:27Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Jesse Wheeler</name>
    </author>
    <author>
      <name>Aaron J. Abkemeier</name>
    </author>
    <author>
      <name>Edward L. Ionides</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17433v1</id>
    <title>The Iberian Blackout: A Black Swan or a Gray Rhino? A Thorough Power System Analysis</title>
    <updated>2025-11-21T17:30:34Z</updated>
    <link href="https://arxiv.org/abs/2511.17433v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17433v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>On April 28, 2025, the Iberian power system suffered a full blackout. It was the first documented overvoltage-driven cascade in Europe. The event sparked debate about root causes, including high renewables output, low inertia, and operator actions. This paper presents a thorough power system analysis of the incident to sort signal from noise and explain, step by step, how the blackout unfolded. Specifically, we (i) reconstruct the timeline and causal chain of the incident, (ii) present and summarize contributing factors using factual findings from incident reports, (iii) reproduce the blackout on an IEEE test system, (iv) analyze the incident from a system-theoretic, voltage-control perspective, and (v) translate our analysis into practical, technical measures that aim to mitigate and prevent similar incidents.</summary>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:30:34Z</published>
    <arxiv:primary_category term="eess.SY"/>
    <author>
      <name>Abdallah Alalem Albustami</name>
    </author>
    <author>
      <name>Ahmad F. Taha</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17418v1</id>
    <title>MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing</title>
    <updated>2025-11-21T17:17:12Z</updated>
    <link href="https://arxiv.org/abs/2511.17418v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17418v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application.</summary>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:17:12Z</published>
    <arxiv:primary_category term="cs.AR"/>
    <author>
      <name>Houji Zhou</name>
    </author>
    <author>
      <name>Ling Yang</name>
    </author>
    <author>
      <name>Zhiwei Zhou</name>
    </author>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>Xiangshui Miao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17415v1</id>
    <title>Bayesian Bridge Gaussian Process Regression</title>
    <updated>2025-11-21T17:13:23Z</updated>
    <link href="https://arxiv.org/abs/2511.17415v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17415v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The performance of Gaussian Process (GP) regression is often hampered by the curse of dimensionality, which inflates computational cost and reduces predictive power in high-dimensional problems. Variable selection is thus crucial for building efficient and accurate GP models. Inspired by Bayesian bridge regression, we propose the Bayesian Bridge Gaussian Process Regression (B\textsuperscript{2}GPR) model. This framework places $\ell_q$-norm constraints on key GP parameters to automatically induce sparsity and identify active variables. We formulate two distinct versions: one for $q=2$ using conjugate Gaussian priors, and another for $0&lt;q&lt;2$ that employs constrained flat priors, leading to non-standard, norm-constrained posterior distributions. To enable posterior inference, we design a Gibbs sampling algorithm that integrates Spherical Hamiltonian Monte Carlo (SphHMC) to efficiently sample from the constrained posteriors when $0&lt;q&lt;2$. Simulations and a real-data application confirm that B\textsuperscript{2}GPR offers superior variable selection and prediction compared to alternative approaches.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:13:23Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Minshen Xu</name>
    </author>
    <author>
      <name>Shiwei Lan</name>
    </author>
    <author>
      <name>Lulu Kang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17413v1</id>
    <title>Quasar clustering and duty cycle measurements at $0\leq z\leq 4$ with the Gaia-unWISE Catalog</title>
    <updated>2025-11-21T17:11:55Z</updated>
    <link href="https://arxiv.org/abs/2511.17413v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17413v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We measure the two-point correlation function of a uniformly selected, all-sky sample of $\sim$1.3 million quasars with magnitudes $G\leq20.5$ from the Gaia--unWISE Quasar Catalog (Quaia) over the redshift range $0 \leq z \leq 4$ to trace the evolution of the quasar clustering strength across cosmic time. We find a steady increase in the correlation length $r_0$ with redshift, i.e. $r_0 = 6.8 \pm 0.2\,h^{-1}\mathrm{Mpc}$ at $0 \leq z &lt; 1$, $r_0=8.0 \pm 0.2\,h^{-1}\mathrm{Mpc}$ at $1 \leq z &lt; 2$, $r_0=10.8 \pm 0.2\,h^{-1}\mathrm{Mpc}$ at $2 \leq z &lt; 3$, and $r_0=13.9 \pm 1.2\,h^{-1}\mathrm{Mpc}$ at $3 \leq z &lt; 4$, and slopes consistent with $γ\approx 2$. Our measurements suggest a slightly weaker clustering signal at $z&gt;3$ than previous studies, and thus we find a smooth, monotonic rise in clustering strength. Using a bias-halo mass relation and a step-function for the halo occupation distribution, we infer characteristic minimum halo masses of quasar hosts of $\log_{10}(M_{\mathrm{min}}/M_\odot) \approx 12.8$ across all redshifts. Combining these with the observed quasar number densities yields duty cycles that rise from $f_{\mathrm{duty}} \approx 2\%$ to $\approx 7\%$ with increasing redshift, corresponding to integrated quasar lifetimes of $t_{\rm QSO}\sim10^8$~years. These results suggest that both the characteristic halo mass of active quasars and their typical lifetimes have remained remarkably stable over more than $12 \sim$ Gyr of cosmic time, implying a self-regulated growth process largely independent of epoch.</summary>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:11:55Z</published>
    <arxiv:comment>11 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="astro-ph.GA"/>
    <author>
      <name>Mariona Giner Mascarell</name>
    </author>
    <author>
      <name>Anna-Christina Eilers</name>
    </author>
    <author>
      <name>Kate Storey-Fisher</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17411v1</id>
    <title>SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding</title>
    <updated>2025-11-21T17:09:43Z</updated>
    <link href="https://arxiv.org/abs/2511.17411v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17411v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $π_0$-FAST and $π_{0.5}$, while it uses 20$\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:09:43Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Nikolay Nikolov</name>
    </author>
    <author>
      <name>Giuliano Albanese</name>
    </author>
    <author>
      <name>Sombit Dey</name>
    </author>
    <author>
      <name>Aleksandar Yanev</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <author>
      <name>Jan-Nico Zaech</name>
    </author>
    <author>
      <name>Danda Pani Paudel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17408v1</id>
    <title>That's not natural: The Impact of Off-Policy Training Data on Probe Performance</title>
    <updated>2025-11-21T17:08:48Z</updated>
    <link href="https://arxiv.org/abs/2511.17408v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17408v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:08:48Z</published>
    <arxiv:comment>10 pages, EurIPS 2025 Workshop on Private AI Governance</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Nathalie Kirch</name>
    </author>
    <author>
      <name>Samuel Dower</name>
    </author>
    <author>
      <name>Adrians Skapars</name>
    </author>
    <author>
      <name>Ekdeep Singh Lubana</name>
    </author>
    <author>
      <name>Dmitrii Krasheninnikov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17407v1</id>
    <title>A Unified Causal Framework for Nonlinear Electrodynamics Black Hole from Courant-Hilbert Approach: Thermodynamics and Singularity</title>
    <updated>2025-11-21T17:08:37Z</updated>
    <link href="https://arxiv.org/abs/2511.17407v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17407v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We develop a unified framework for analyzing black hole thermodynamics and spacetime structure in Einstein gravity coupled to causal nonlinear electrodynamics (NED) in asymptotically anti--de Sitter backgrounds. The electromagnetic sector is governed by a Generalized Nonlinear Electrodynamics (GNED) Lagrangian obtained from a root-$T\bar T$ deformation constructed via the Courant--Hilbert approach, ensuring both duality invariance and causal propagation. This theory contains ModMax, Generalized Born-Infeld (GBI), and self-dual logarithmic electrodynamics as continuous limits. Within this framework we obtain exact charged AdS black hole solutions and perform a detailed study of their thermodynamic properties, including mass, temperature, entropy, and free energy. The resulting phase structure exhibits van der~Waals-type transitions between small and large black holes and features a characteristic swallowtail in the free energy at the critical point.
  We further investigate the internal geometry and show that the nature of the central singularity is determined by the matter fields sourcing the spacetime. Analysis of the Kretschmann scalar demonstrates how gravity and electromagnetism jointly control curvature blow-up in charged and ModMax black holes, with gravity providing the leading divergence. In contrast, (GBI) and logarithmic (NED) models imprint distinct singularity profiles governed by the specific functional form of their Lagrangians. Examining the near-origin behavior of the metric allows us to identify the parameter ranges that support an event horizon and to determine when the solution instead becomes a naked singularity. Overall, the comparison across models reveals that different causal NED theories produce identifiable signatures in both the strength of the singularity and the conditions for horizon formation.</summary>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:08:37Z</published>
    <arxiv:comment>37 pages, 7 figures</arxiv:comment>
    <arxiv:primary_category term="hep-th"/>
    <author>
      <name>H. Babaei-Aghbolagh</name>
    </author>
    <author>
      <name>Komeil Babaei Velni</name>
    </author>
    <author>
      <name>Song He</name>
    </author>
    <author>
      <name>Fateme Isapour</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.17401v1</id>
    <title>Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment</title>
    <updated>2025-11-21T17:00:48Z</updated>
    <link href="https://arxiv.org/abs/2511.17401v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.17401v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-21T17:00:48Z</published>
    <arxiv:comment>37 pages, 9 figures, and 7 tables</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Xiaoshan Zhou</name>
    </author>
    <author>
      <name>Carol C. Menassa</name>
    </author>
    <author>
      <name>Vineet R. Kamat</name>
    </author>
  </entry>
</feed>
