<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-04-03T00:52:57Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-04-02T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">109835</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.24391v1</id>
    <updated>2025-03-31T17:59:58Z</updated>
    <published>2025-03-31T17:59:58Z</published>
    <title>Easi3R: Estimating Disentangled Motion from DUSt3R Without Training</title>
    <summary>  Recent advances in DUSt3R have enabled robust estimation of dense point
clouds and camera parameters of static scenes, leveraging Transformer network
architectures and direct supervision on large-scale 3D datasets. In contrast,
the limited scale and diversity of available 4D datasets present a major
bottleneck for training a highly generalizable 4D model. This constraint has
driven conventional 4D methods to fine-tune 3D models on scalable dynamic video
data with additional geometric priors such as optical flow and depths. In this
work, we take an opposite path and introduce Easi3R, a simple yet efficient
training-free method for 4D reconstruction. Our approach applies attention
adaptation during inference, eliminating the need for from-scratch pre-training
or network fine-tuning. We find that the attention layers in DUSt3R inherently
encode rich information about camera and object motion. By carefully
disentangling these attention maps, we achieve accurate dynamic region
segmentation, camera pose estimation, and 4D dense point map reconstruction.
Extensive experiments on real-world dynamic videos demonstrate that our
lightweight attention adaptation significantly outperforms previous
state-of-the-art methods that are trained or finetuned on extensive dynamic
datasets. Our code is publicly available for research purpose at
https://easi3r.github.io/
</summary>
    <author>
      <name>Xingyu Chen</name>
    </author>
    <author>
      <name>Yue Chen</name>
    </author>
    <author>
      <name>Yuliang Xiu</name>
    </author>
    <author>
      <name>Andreas Geiger</name>
    </author>
    <author>
      <name>Anpei Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Page: https://easi3r.github.io/ Code:
  https://github.com/Inception3D/Easi3R</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24388v1</id>
    <updated>2025-03-31T17:59:52Z</updated>
    <published>2025-03-31T17:59:52Z</published>
    <title>RIG: Synergizing Reasoning and Imagination in End-to-End Generalist
  Policy</title>
    <summary>  Reasoning before action and imagining potential outcomes (i.e., world models)
are essential for embodied agents operating in complex open-world environments.
Yet, prior work either incorporates only one of these abilities in an
end-to-end agent or integrates multiple specialized models into an agent
system, limiting the learning efficiency and generalization of the policy.
Thus, this paper makes the first attempt to synergize Reasoning and Imagination
in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end
manner, we construct a data pipeline that progressively integrates and enriches
the content of imagination and reasoning in the trajectories collected from
existing agents. The joint learning of reasoning and next image generation
explicitly models the inherent correlation between reasoning, action, and
dynamics of environments, and thus exhibits more than $17\times$ sample
efficiency improvements and generalization in comparison with previous works.
During inference, RIG first reasons about the next action, produces potential
action, and then predicts the action outcomes, which offers the agent a chance
to review and self-correct based on the imagination before taking real actions.
Experimental results show that the synergy of reasoning and imagination not
only improves the robustness, generalization, and interoperability of
generalist policy but also enables test-time scaling to enhance overall
performance.
</summary>
    <author>
      <name>Zhonghan Zhao</name>
    </author>
    <author>
      <name>Wenwei Zhang</name>
    </author>
    <author>
      <name>Haian Huang</name>
    </author>
    <author>
      <name>Kuikun Liu</name>
    </author>
    <author>
      <name>Jianfei Gao</name>
    </author>
    <author>
      <name>Gaoang Wang</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.24388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24377v1</id>
    <updated>2025-03-31T17:58:07Z</updated>
    <published>2025-03-31T17:58:07Z</published>
    <title>Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for
  Large Language Models</title>
    <summary>  Recent advancements in Large Language Models (LLMs) have significantly
enhanced their ability to perform complex reasoning tasks, transitioning from
fast and intuitive thinking (System 1) to slow and deep reasoning (System 2).
While System 2 reasoning improves task accuracy, it often incurs substantial
computational costs due to its slow thinking nature and inefficient or
unnecessary reasoning behaviors. In contrast, System 1 reasoning is
computationally efficient but leads to suboptimal performance. Consequently, it
is critical to balance the trade-off between performance (benefits) and
computational costs (budgets), giving rise to the concept of reasoning economy.
In this survey, we provide a comprehensive analysis of reasoning economy in
both the post-training and test-time inference stages of LLMs, encompassing i)
the cause of reasoning inefficiency, ii) behavior analysis of different
reasoning patterns, and iii) potential solutions to achieve reasoning economy.
By offering actionable insights and highlighting open challenges, we aim to
shed light on strategies for improving the reasoning economy of LLMs, thereby
serving as a valuable resource for advancing research in this evolving area. We
also provide a public repository to continually track developments in this
fast-evolving field.
</summary>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Hongru Wang</name>
    </author>
    <author>
      <name>Boyang Xue</name>
    </author>
    <author>
      <name>Jianhui Pang</name>
    </author>
    <author>
      <name>Shudong Liu</name>
    </author>
    <author>
      <name>Yi Chen</name>
    </author>
    <author>
      <name>Jiahao Qiu</name>
    </author>
    <author>
      <name>Derek Fai Wong</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Kam-Fai Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Progress; Paper list Repo:
  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24374v1</id>
    <updated>2025-03-31T17:53:05Z</updated>
    <published>2025-03-31T17:53:05Z</published>
    <title>ERUPT: Efficient Rendering with Unposed Patch Transformer</title>
    <summary>  This work addresses the problem of novel view synthesis in diverse scenes
from small collections of RGB images. We propose ERUPT (Efficient Rendering
with Unposed Patch Transformer) a state-of-the-art scene reconstruction model
capable of efficient scene rendering using unposed imagery. We introduce
patch-based querying, in contrast to existing pixel-based queries, to reduce
the compute required to render a target view. This makes our model highly
efficient both during training and at inference, capable of rendering at 600
fps on commercial hardware. Notably, our model is designed to use a learned
latent camera pose which allows for training using unposed targets in datasets
with sparse or inaccurate ground truth camera pose. We show that our approach
can generalize on large real-world data and introduce a new benchmark dataset
(MSVS-1M) for latent view synthesis using street-view imagery collected from
Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense
imagery and precise metadata, ERUPT can render novel views of arbitrary scenes
with as few as five unposed input images. ERUPT achieves better rendered image
quality than current state-of-the-art methods for unposed image synthesis
tasks, reduces labeled data requirements by ~95\% and decreases computational
requirements by an order of magnitude, providing efficient novel view synthesis
for diverse real-world scenes.
</summary>
    <author>
      <name>Maxim V. Shugaev</name>
    </author>
    <author>
      <name>Vincent Chen</name>
    </author>
    <author>
      <name>Maxim Karrenbach</name>
    </author>
    <author>
      <name>Kyle Ashley</name>
    </author>
    <author>
      <name>Bridget Kennedy</name>
    </author>
    <author>
      <name>Naresh P. Cuntoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24368v1</id>
    <updated>2025-03-31T17:47:42Z</updated>
    <published>2025-03-31T17:47:42Z</published>
    <title>Adapting Vision Foundation Models for Real-time Ultrasound Image
  Segmentation</title>
    <summary>  We propose a novel approach that adapts hierarchical vision foundation models
for real-time ultrasound image segmentation. Existing ultrasound segmentation
methods often struggle with adaptability to new tasks, relying on costly manual
annotations, while real-time approaches generally fail to match
state-of-the-art performance. To overcome these limitations, we introduce an
adaptive framework that leverages the vision foundation model Hiera to extract
multi-scale features, interleaved with DINOv2 representations to enhance visual
expressiveness. These enriched features are then decoded to produce precise and
robust segmentation. We conduct extensive evaluations on six public datasets
and one in-house dataset, covering both cardiac and thyroid ultrasound
segmentation. Experiments show that our approach outperforms state-of-the-art
methods across multiple datasets and excels with limited supervision,
surpassing nnUNet by over 20\% on average in the 1\% and 10\% data settings.
Our method achieves $\sim$77 FPS inference speed with TensorRT on a single GPU,
enabling real-time clinical applications.
</summary>
    <author>
      <name>Xiaoran Zhang</name>
    </author>
    <author>
      <name>Eric Z. Chen</name>
    </author>
    <author>
      <name>Lin Zhao</name>
    </author>
    <author>
      <name>Xiao Chen</name>
    </author>
    <author>
      <name>Yikang Liu</name>
    </author>
    <author>
      <name>Boris Maihe</name>
    </author>
    <author>
      <name>James S. Duncan</name>
    </author>
    <author>
      <name>Terrence Chen</name>
    </author>
    <author>
      <name>Shanhui Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2503.24368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24367v1</id>
    <updated>2025-03-31T17:46:37Z</updated>
    <published>2025-03-31T17:46:37Z</published>
    <title>The structure and topology of an amorphous metal-organic framework</title>
    <summary>  Amorphous metal-organic frameworks are an important emerging materials class
that combine the attractive physical properties of the amorphous state with the
versatility of metal-organic framework (MOF) chemistry. The structures of
amorphous MOFs have largely been inferred by drawing analogies to crystalline
polymorphs and inorganic glasses, but ultimately the validity of such
structural models has been challenging to establish either experimentally or
computationally. Here we use a unified data-driven approach, combining
experimental scattering data and active machine learning for interatomic
potentials, to determine the structure of an amorphous zeolitic imidazolate
framework (a-ZIF) -- the canonical amorphous MOF. Our results reveal clear
differences between the structure of a-ZIF and that of other amorphous
tetrahedral networks, allowing us to invalidate the long-standing assumption
that these inorganic and hybrid glasses are topologically equivalent. To this
end, we introduce a systematic notation for the network topology of amorphous
solids, building a bridge to the successful use of topology analysis in
crystalline MOFs and to materials informatics. Our work provides insights into
the structure and topology of the archetypal amorphous MOF and opens up new
avenues for modelling and understanding amorphous framework materials more
generally.
</summary>
    <author>
      <name>Thomas C. Nicholas</name>
    </author>
    <author>
      <name>Daniel F. Thomas du Toit</name>
    </author>
    <author>
      <name>Louise A. M. Rosset</name>
    </author>
    <author>
      <name>Davide M. Proserpio</name>
    </author>
    <author>
      <name>Andrew L. Goodwin</name>
    </author>
    <author>
      <name>Volker L. Deringer</name>
    </author>
    <link href="http://arxiv.org/abs/2503.24367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24364v1</id>
    <updated>2025-03-31T17:43:36Z</updated>
    <published>2025-03-31T17:43:36Z</published>
    <title>Query and Conquer: Execution-Guided SQL Generation</title>
    <summary>  We propose a novel approach for generating complex outputs that significantly
improves accuracy in text-to-SQL tasks. Our method leverages execution results
to select the most semantically consistent query from multiple candidates,
enabling smaller, cost-effective models to surpass computationally intensive
reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference
cost by as much as 30 times. It integrates effortlessly with existing models,
offering a practical and scalable pathway to state-of-the-art SQL generation.
</summary>
    <author>
      <name>≈Åukasz Borchmann</name>
    </author>
    <author>
      <name>Marek Wydmuch</name>
    </author>
    <link href="http://arxiv.org/abs/2503.24364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24322v1</id>
    <updated>2025-03-31T17:08:57Z</updated>
    <published>2025-03-31T17:08:57Z</published>
    <title>NoProp: Training Neural Networks without Back-propagation or
  Forward-propagation</title>
    <summary>  The canonical deep learning approach for learning requires computing a
gradient term at each layer by back-propagating the error signal from the
output towards each learnable parameter. Given the stacked structure of neural
networks, where each layer builds on the representation of the layer below,
this approach leads to hierarchical representations. More abstract features
live on the top layers of the model, while features on lower layers are
expected to be less abstract. In contrast to this, we introduce a new learning
method named NoProp, which does not rely on either forward or backwards
propagation. Instead, NoProp takes inspiration from diffusion and flow matching
methods, where each layer independently learns to denoise a noisy target. We
believe this work takes a first step towards introducing a new family of
gradient-free learning methods, that does not learn hierarchical
representations -- at least not in the usual sense. NoProp needs to fix the
representation at each layer beforehand to a noised version of the target,
learning a local denoising process that can then be exploited at inference. We
demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100
image classification benchmarks. Our results show that NoProp is a viable
learning algorithm which achieves superior accuracy, is easier to use and
computationally more efficient compared to other existing back-propagation-free
methods. By departing from the traditional gradient based learning paradigm,
NoProp alters how credit assignment is done within the network, enabling more
efficient distributed learning as well as potentially impacting other
characteristics of the learning process.
</summary>
    <author>
      <name>Qinyu Li</name>
    </author>
    <author>
      <name>Yee Whye Teh</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.24322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24320v1</id>
    <updated>2025-03-31T17:07:37Z</updated>
    <published>2025-03-31T17:07:37Z</published>
    <title>Can Test-Time Scaling Improve World Foundation Model?</title>
    <summary>  World foundation models, which simulate the physical world by predicting
future states from current observations and inputs, have become central to many
applications in physical intelligence, including autonomous driving and
robotics. However, these models require substantial computational resources for
pretraining and are further constrained by available data during post-training.
As such, scaling computation at test time emerges as both a critical and
practical alternative to traditional model enlargement or re-training. In this
work, we introduce SWIFT, a test-time scaling framework tailored for WFMs.
SWIFT integrates our extensible WFM evaluation toolkit with process-level
inference strategies, including fast tokenization, probability-based Top-K
pruning, and efficient beam search. Empirical results on the COSMOS model
demonstrate that test-time scaling exists even in a compute-optimal way. Our
findings reveal that test-time scaling laws hold for WFMs and that SWIFT
provides a scalable and effective pathway for improving WFM inference without
retraining or increasing model size. The code is available at
https://github.com/Mia-Cong/SWIFT.git.
</summary>
    <author>
      <name>Wenyan Cong</name>
    </author>
    <author>
      <name>Hanqing Zhu</name>
    </author>
    <author>
      <name>Peihao Wang</name>
    </author>
    <author>
      <name>Bangya Liu</name>
    </author>
    <author>
      <name>Dejia Xu</name>
    </author>
    <author>
      <name>Kevin Wang</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <author>
      <name>Yan Wang</name>
    </author>
    <author>
      <name>Zhiwen Fan</name>
    </author>
    <author>
      <name>Zhangyang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2503.24320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24311v1</id>
    <updated>2025-03-31T16:57:04Z</updated>
    <published>2025-03-31T16:57:04Z</published>
    <title>Selective Inference in Graphical Models via Maximum Likelihood</title>
    <summary>  The graphical lasso is a widely used algorithm for fitting undirected
Gaussian graphical models. However, for inference on functionals of edge values
in the learned graph, standard tools lack formal statistical guarantees, such
as control of the type I error rate. In this paper, we introduce a selective
inference method for asymptotically valid inference after graphical lasso
selection with added randomization. We obtain a selective likelihood,
conditional on the event of selection, through a change of variable on the
known density of the randomization variables. Our method enables interval
estimation and hypothesis testing for a wide range of functionals of edge
values in the learned graph using the conditional maximum likelihood estimate.
Our numerical studies show that introducing a small amount of randomization:
(i) greatly increases power and yields substantially shorter intervals compared
to other conditional inference methods, including data splitting; (ii) ensures
intervals of bounded length in high-dimensional settings where data splitting
is infeasible due to insufficient samples for inference; (iii) enables
inference for a wide range of inferential targets in the learned graph,
including measures of node influence and connectivity between nodes.
</summary>
    <author>
      <name>Sofia Guglielmini</name>
    </author>
    <author>
      <name>Gerda Claeskens</name>
    </author>
    <author>
      <name>Snigdha Panigrahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 2 Figures, 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
