<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-09-09T00:53:30Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-09-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">120399</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2509.05291v1</id>
    <updated>2025-09-05T17:56:24Z</updated>
    <published>2025-09-05T17:56:24Z</published>
    <title>Crosscoding Through Time: Tracking Emergence &amp; Consolidation Of
  Linguistic Representations Throughout LLM Pretraining</title>
    <summary>  Large language models (LLMs) learn non-trivial abstractions during
pretraining, like detecting irregular plural noun subjects. However, it is not
well understood when and how specific linguistic abilities emerge as
traditional evaluation methods such as benchmarking fail to reveal how models
acquire concepts and capabilities. To bridge this gap and better understand
model training at the concept level, we use sparse crosscoders to discover and
align features across model checkpoints. Using this approach, we track the
evolution of linguistic features during pretraining. We train crosscoders
between open-sourced checkpoint triplets with significant performance and
representation shifts, and introduce a novel metric, Relative Indirect Effects
(RelIE), to trace training stages at which individual features become causally
important for task performance. We show that crosscoders can detect feature
emergence, maintenance, and discontinuation during pretraining. Our approach is
architecture-agnostic and scalable, offering a promising path toward more
interpretable and fine-grained analysis of representation learning throughout
pretraining.
</summary>
    <author>
      <name>Deniz Bayazit</name>
    </author>
    <author>
      <name>Aaron Mueller</name>
    </author>
    <author>
      <name>Antoine Bosselut</name>
    </author>
    <link href="http://arxiv.org/abs/2509.05291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05284v1</id>
    <updated>2025-09-05T17:52:31Z</updated>
    <published>2025-09-05T17:52:31Z</published>
    <title>Causal mechanism and mediation analysis for macroeconomics dynamics: a
  bridge of Granger and Sims causality</title>
    <summary>  This paper introduces a novel concept of impulse response decomposition to
disentangle the dynamic contributions of the mediator variables in the
transmission of structural shocks. We justify our decomposition by drawing on
causal mediation analysis and demonstrating its equivalence to the average
mediation effect. Our result establishes a formal link between Sims and Granger
causality. Sims causality captures the total effect, while Granger causality
corresponds to the mediation effect. We construct a dynamic mediation index
that quantifies the evolving role of mediator variables in shock propagation.
Applying our framework to studies of the transmission channels of US monetary
policy, we find that investor sentiment explains approximately 60% of the peak
aggregate output response in three months following a policy shock, while
expected default risk contributes negligibly across all horizons.
</summary>
    <author>
      <name>Jean-Marie Dufour</name>
    </author>
    <author>
      <name>Endong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2509.05284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05276v1</id>
    <updated>2025-09-05T17:34:00Z</updated>
    <published>2025-09-05T17:34:00Z</published>
    <title>SpikingBrain Technical Report: Spiking Brain-inspired Large Models</title>
    <summary>  Mainstream Transformer-based large language models face major efficiency
bottlenecks: training computation scales quadratically with sequence length,
and inference memory grows linearly, limiting long-context processing. Building
large models on non-NVIDIA platforms also poses challenges for stable and
efficient training. To address this, we introduce SpikingBrain, a family of
brain-inspired models designed for efficient long-context training and
inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three
aspects: (1) Model Architecture: linear and hybrid-linear attention
architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an
efficient, conversion-based training pipeline and a dedicated spike coding
framework; (3) System Engineering: customized training frameworks, operator
libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,
and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the
feasibility of large-scale LLM development on non-NVIDIA platforms.
SpikingBrain achieves performance comparable to open-source Transformer
baselines while using only about 150B tokens for continual pre-training. Our
models significantly improve long-sequence training efficiency and deliver
inference with (partially) constant memory and event-driven spiking behavior.
For example, SpikingBrain-7B attains over 100x speedup in Time to First Token
for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX
C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4
percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling
low-power operation. Overall, this work demonstrates the potential of
brain-inspired mechanisms to drive the next generation of efficient and
scalable large model design.
</summary>
    <author>
      <name>Yuqi Pan</name>
    </author>
    <author>
      <name>Yupeng Feng</name>
    </author>
    <author>
      <name>Jinghao Zhuang</name>
    </author>
    <author>
      <name>Siyu Ding</name>
    </author>
    <author>
      <name>Zehao Liu</name>
    </author>
    <author>
      <name>Bohan Sun</name>
    </author>
    <author>
      <name>Yuhong Chou</name>
    </author>
    <author>
      <name>Han Xu</name>
    </author>
    <author>
      <name>Xuerui Qiu</name>
    </author>
    <author>
      <name>Anlin Deng</name>
    </author>
    <author>
      <name>Anjie Hu</name>
    </author>
    <author>
      <name>Peng Zhou</name>
    </author>
    <author>
      <name>Man Yao</name>
    </author>
    <author>
      <name>Jibin Wu</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <author>
      <name>Guoliang Sun</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <author>
      <name>Guoqi Li</name>
    </author>
    <link href="http://arxiv.org/abs/2509.05276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05241v1</id>
    <updated>2025-09-05T16:57:54Z</updated>
    <published>2025-09-05T16:57:54Z</published>
    <title>Deep Learning-Enhanced for Amine Emission Monitoring and Performance
  Analysis in Industrial Carbon Capture Plants</title>
    <summary>  We present data driven deep learning models for forecasting and monitoring
amine emissions and key performance parameters in amine-based post-combustion
carbon capture systems. Using operational data from the CESAR1 solvent campaign
at Technology Center Mongstad, four DL architectures such as Basic Long
Short-Term Memory (LSTM), Stacked LSTM, Bi-directional LSTM, and Convolutional
LSTM were developed to capture time-dependent process behavior. For emission
prediction, models were designed for 2-amino-2-methyl-1-propanol (AMP) and
Piperazine emissions measured via FTIR and IMR-MS methods. System performance
models target four critical parameters: CO$_2$ product flow, absorber outlet
temperature, depleted flue gas outlet temperature, and RFCC stripper bottom
temperature. These models achieved high predictive accuracy exceeding 99% and
effectively tracked both steady trends and abrupt fluctuations. Additionally,
we conducted causal impact analysis to evaluate how operational variables
influence emissions and system performance. Eight input variables were
systematically perturbed within $\pm$20% of nominal values to simulate
deviations and assess their impact. This analysis revealed that adjusting
specific operational parameters, such as lean solvent temperature and water
wash conditions, can significantly reduce amine emissions and enhance system
performance. This study highlights ML not only as a predictive tool but also as
a decision support system for optimizing carbon capture operations under steady
state and dynamic conditions. By enabling real time monitoring, scenario
testing, and operational optimization, the developed ML framework offers a
practical pathway for mitigating environmental impacts. This work represents a
step toward intelligent, data-driven control strategies that enhance the
efficiency, stability, and sustainability of carbon capture and storage
technologies.
</summary>
    <author>
      <name>Lokendra Poudel</name>
    </author>
    <author>
      <name>David Tincher</name>
    </author>
    <author>
      <name>Duy-Nhat Phan</name>
    </author>
    <author>
      <name>Rahul Bhowmik</name>
    </author>
    <link href="http://arxiv.org/abs/2509.05241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05238v1</id>
    <updated>2025-09-05T16:54:26Z</updated>
    <published>2025-09-05T16:54:26Z</published>
    <title>Uncertain but Useful: Leveraging CNN Variability into Data Augmentation</title>
    <summary>  Deep learning (DL) is rapidly advancing neuroimaging by achieving
state-of-the-art performance with reduced computation times. Yet the numerical
stability of DL models -- particularly during training -- remains
underexplored. While inference with DL is relatively stable, training
introduces additional variability primarily through iterative stochastic
optimization. We investigate this training-time variability using FastSurfer, a
CNN-based whole-brain segmentation pipeline. Controlled perturbations are
introduced via floating point perturbations and random seeds. We find that: (i)
FastSurfer exhibits higher variability compared to that of a traditional
neuroimaging pipeline, suggesting that DL inherits and is particularly
susceptible to sources of instability present in its predecessors; (ii)
ensembles generated with perturbations achieve performance similar to an
unperturbed baseline; and (iii) variability effectively produces ensembles of
numerical model families that can be repurposed for downstream applications. As
a proof of concept, we demonstrate that numerical ensembles can be used as a
data augmentation strategy for brain age regression. These findings position
training-time variability not only as a reproducibility concern but also as a
resource that can be harnessed to improve robustness and enable new
applications in neuroimaging.
</summary>
    <author>
      <name>Inés Gonzalez-Pepe</name>
    </author>
    <author>
      <name>Vinuyan Sivakolunthu</name>
    </author>
    <author>
      <name>Yohan Chatelain</name>
    </author>
    <author>
      <name>Tristan Glatard</name>
    </author>
    <link href="http://arxiv.org/abs/2509.05238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05226v1</id>
    <updated>2025-09-05T16:40:13Z</updated>
    <published>2025-09-05T16:40:13Z</published>
    <title>Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware
  Chain-of-Thought Distillation</title>
    <summary>  Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose
output for simpler problems. We present a framework for difficulty-aware
reasoning that teaches models to dynamically adjust reasoning depth based on
problem complexity. Remarkably, we show that models can be endowed with such
dynamic inference pathways without any architectural modifications; we simply
post-train on data that is carefully curated to include chain-of-thought traces
that are proportional in length to problem difficulty. Our analysis reveals
that post-training via supervised fine-tuning (SFT) primarily captures patterns
like reasoning length and format, while direct preference optimization (DPO)
preserves reasoning accuracy, with their combination reducing length and
maintaining or improving performance. Both quantitative metrics and qualitative
assessments confirm that models can learn to "think proportionally", reasoning
minimally on simple problems while maintaining depth for complex ones.
</summary>
    <author>
      <name>Abdul Waheed</name>
    </author>
    <author>
      <name>Chancharik Mitra</name>
    </author>
    <author>
      <name>Laurie Z. Wang</name>
    </author>
    <author>
      <name>Deva Ramanan</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.05226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05224v1</id>
    <updated>2025-09-05T16:37:37Z</updated>
    <published>2025-09-05T16:37:37Z</published>
    <title>Reshetnyak Majorisation and discrete upper curvature bounds for
  Lorentzian length spaces</title>
    <summary>  We present an analogue to the Majorisation Theorem of Reshetnyak in the
setting of Lorentzian length spaces with upper curvature bounds: given two
future-directed timelike rectifiable curves $\alpha$ and $\beta$ with the same
endpoints in a Lorentzian length space $X$, there exists a convex region in
$\mathbb{L}^2(K)$ bounded by two future-directed causal curves $\bar \alpha$
and $\bar \beta$ with the same endpoints and a 1-anti-Lipschitz map from that
region into $X$ such that $\bar \alpha$ and $\bar \beta$ are respectively
mapped $\tau$-length-preservingly onto $\alpha$ and $\beta$. A special case of
this theorem leads to an interesting characterisation of upper curvature bounds
via four-point configurations which is truly suitable for a discrete setting.
</summary>
    <author>
      <name>Tobias Beran</name>
    </author>
    <author>
      <name>Felix Rott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.05224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="53C50, 53C23, 53B30, 51K10, 53C80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05221v1</id>
    <updated>2025-09-05T16:32:26Z</updated>
    <published>2025-09-05T16:32:26Z</published>
    <title>A functional tensor model for dynamic multilayer networks with common
  invariant subspaces and the RKHS estimation</title>
    <summary>  Dynamic multilayer networks are frequently used to describe the structure and
temporal evolution of multiple relationships among common entities, with
applications in fields such as sociology, economics, and neuroscience. However,
exploration of analytical methods for these complex data structures remains
limited. We propose a functional tensor-based model for dynamic multilayer
networks, with the key feature of capturing the shared structure among common
vertices across all layers, while simultaneously accommodating smoothly varying
temporal dynamics and layer-specific heterogeneity. The proposed model and its
embeddings can be applied to various downstream network inference tasks,
including dimensionality reduction, vertex community detection, analysis of
network evolution periodicity, visualization of dynamic network evolution
patterns, and evaluation of inter-layer similarity. We provide an estimation
algorithm based on functional tensor Tucker decomposition and the reproducing
kernel Hilbert space framework, with an effective initialization strategy to
improve computational efficiency. The estimation procedure can be extended to
address more generalized functional tensor problems, as well as to handle
missing data or unaligned observations. We validate our method on simulated
data and two real-world cases: the dynamic Citi Bike trip network and an
international food trade dynamic multilayer network, with each layer
corresponding to a different product.
</summary>
    <author>
      <name>Runshi Tang</name>
    </author>
    <author>
      <name>Runbing Zheng</name>
    </author>
    <author>
      <name>Anru R. Zhang</name>
    </author>
    <author>
      <name>Carey E. Priebe</name>
    </author>
    <link href="http://arxiv.org/abs/2509.05221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05199v1</id>
    <updated>2025-09-05T15:58:49Z</updated>
    <published>2025-09-05T15:58:49Z</published>
    <title>Triadic Fusion of Cognitive, Functional, and Causal Dimensions for
  Explainable LLMs: The TAXAL Framework</title>
    <summary>  Large Language Models (LLMs) are increasingly being deployed in high-risk
domains where opacity, bias, and instability undermine trust and
accountability. Traditional explainability methods, focused on surface outputs,
do not capture the reasoning pathways, planning logic, and systemic impacts of
agentic LLMs.
  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a
triadic fusion framework that unites three complementary dimensions: cognitive
(user understanding), functional (practical utility), and causal (faithful
reasoning). TAXAL provides a unified, role-sensitive foundation for designing,
evaluating, and deploying explanations in diverse sociotechnical settings.
  Our analysis synthesizes existing methods, ranging from post-hoc attribution
and dialogic interfaces to explanation-aware prompting, and situates them
within the TAXAL triadic fusion model. We further demonstrate its applicability
through case studies in law, education, healthcare, and public services,
showing how explanation strategies adapt to institutional constraints and
stakeholder roles.
  By combining conceptual clarity with design patterns and deployment pathways,
TAXAL advances explainability as a technical and sociotechnical practice,
supporting trustworthy and context-sensitive LLM applications in the era of
agentic AI.
</summary>
    <author>
      <name>David Herrera-Poyatos</name>
    </author>
    <author>
      <name>Carlos Peláez-González</name>
    </author>
    <author>
      <name>Cristina Zuheros</name>
    </author>
    <author>
      <name>Virilo Tejedor</name>
    </author>
    <author>
      <name>Rosana Montes</name>
    </author>
    <author>
      <name>Francisco Herrera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 9 tables and 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.05199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.05186v1</id>
    <updated>2025-09-05T15:35:04Z</updated>
    <published>2025-09-05T15:35:04Z</published>
    <title>Probabilistic operator learning: generative modeling and uncertainty
  quantification for foundation models of differential equations</title>
    <summary>  In-context operator networks (ICON) are a class of operator learning methods
based on the novel architectures of foundation models. Trained on a diverse set
of datasets of initial and boundary conditions paired with corresponding
solutions to ordinary and partial differential equations (ODEs and PDEs), ICON
learns to map example condition-solution pairs of a given differential equation
to an approximation of its solution operator. Here, we present a probabilistic
framework that reveals ICON as implicitly performing Bayesian inference, where
it computes the mean of the posterior predictive distribution over solution
operators conditioned on the provided context, i.e., example condition-solution
pairs. The formalism of random differential equations provides the
probabilistic framework for describing the tasks ICON accomplishes while also
providing a basis for understanding other multi-operator learning methods. This
probabilistic perspective provides a basis for extending ICON to
\emph{generative} settings, where one can sample from the posterior predictive
distribution of solution operators. The generative formulation of ICON
(GenICON) captures the underlying uncertainty in the solution operator, which
enables principled uncertainty quantification in the solution predictions in
operator learning.
</summary>
    <author>
      <name>Benjamin J. Zhang</name>
    </author>
    <author>
      <name>Siting Liu</name>
    </author>
    <author>
      <name>Stanley J. Osher</name>
    </author>
    <author>
      <name>Markos A. Katsoulakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First two authors contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.05186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.05186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
