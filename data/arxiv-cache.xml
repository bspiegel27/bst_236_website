<?xml version="1.0" encoding="UTF-8"?>
<!-- Last updated: 2025-08-22T00:55:31Z -->
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acausal%20inference%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:causal inference&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/SsRksLzRb0LuVhSmQ+N8WbAs+f8</id>
  <updated>2025-08-21T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">119324</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2508.14890v1</id>
    <updated>2025-08-20T17:59:01Z</updated>
    <published>2025-08-20T17:59:01Z</published>
    <title>Estimating Initial Mass of Gaia-Enceladus Dwarf Galaxy with Chemical
  Evolution Model</title>
    <summary>  This work investigates the initial mass and chemical evolution history of the
Gaia-Enceladus dwarf galaxy. We combine spectroscopic data from APOGEE with
astrometric data from Gaia DR3 to identify Gaia-Enceladus candidate stars via a
machine-learning pipeline using t-SNE and HDBSCAN. By focusing on kinematic and
chemical parameters, especially $\mathrm{[Fe/H]}$, $\mathrm{[Mg/Fe]}$,
$\mathrm{[Al/Fe]}$, and $\mathrm{[Mn/Fe]}$, we uncover a population of
metal-poor, high-eccentricity stars that align with literature criteria for
Gaia-Enceladus debris. We then apply the \textit{OMEGA+} chemical evolution
model, incorporating MCMC fitting of the observed abundance trends in the
$\mathrm{[Mg/Fe]\times[Fe/H]}$ plane. Our best-fitting model indicates a gas
mass of $4.93_{-0.72}^{+0.32}\times10^9\,{M_{\odot}}$ for Gaia-Enceladus,
placing it at the higher end of previously suggested mass ranges. The model
scenario suggests a short star formation timescale, substantial outflows, and a
rapid build-up of metals mainly driven by core-collapse supernovae, with a
lesser contribution from Type~Ia supernovae. Comparison with observational data
in other chemical planes (e.g., $\mathrm{[Mg/Mn]\times[Al/Fe]}$) supports this
scenario, emphasizing a distinct evolution path relative to the Milky Way.
Additionally, our results provide indirect evidence that star formation in
Gaia-Enceladus likely ceased within the first 4 Gyr, consistent with earlier
inferences of an early merger event. These findings highlight the power of
chemical evolution modeling in reconstructing the origin and mass of ancient
accreted systems. Overall, we show that Gaia-Enceladus, through a rapid star
formation and strong outflows, contributed a significant fraction of the
metal-poor stellar halo of the Milky Way.
</summary>
    <author>
      <name>Olcay Plevne</name>
    </author>
    <author>
      <name>Furkan Akbaba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in The Astrophysical Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.14890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14846v1</id>
    <updated>2025-08-20T16:57:55Z</updated>
    <published>2025-08-20T16:57:55Z</published>
    <title>Modeling tails of escaping gas in exoplanet atmospheres with Harmonica</title>
    <summary>  Exoplanets that reside close to their host stars, and therefore receive
substantial amounts of X-ray and ultraviolet radiation, are prone to suffer
from strong atmospheric escape. This can lead to the creation of an envelope of
escaping gas along the planet's orbital trajectory, often referred to as a
tail. When transiting in front of their host star, these tails can not only
produce larger depths in the transit light curves, but also introduce
significant asymmetries between ingress and egress. Using the publicly
available software Harmonica, we present a method to model the light curves of
transiting planets surrounded by extended envelopes of escaping gas, and
subsequently infer the shape and size of the latter. We apply this method to
the JWST NIRISS/SOSS observations of HAT-P-18b, which show pronounced helium
tail features in its spectroscopic light curve of the metastable helium triplet
at 10830 \r{A}. Our model reveals that, in order to fit the observed light
curve of HAT-P-18b, the planet must possess a trailing helium tail of
$15.79^{+1.14}_{-1.05}$ planetary radii. We carry out injection-recovery tests
to validate the effectiveness of the proposed methodology. We demonstrate that,
with sufficient precision, we would be able to fit a multi-layer envelope to
the data, which would provide insight into the relative radial variations in
the opacity profile.
</summary>
    <author>
      <name>Carlos Gascón</name>
    </author>
    <author>
      <name>Mercedes López-Morales</name>
    </author>
    <author>
      <name>Shreyas Vissapragada</name>
    </author>
    <author>
      <name>Morgan MacLeod</name>
    </author>
    <author>
      <name>Hannah R. Wakeford</name>
    </author>
    <author>
      <name>David Grant</name>
    </author>
    <author>
      <name>Ignasi Ribas</name>
    </author>
    <author>
      <name>Guillem Anglada-Escudé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures, 3 tables. Accepted to APJL</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.14846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14828v1</id>
    <updated>2025-08-20T16:22:51Z</updated>
    <published>2025-08-20T16:22:51Z</published>
    <title>Long Chain-of-Thought Reasoning Across Languages</title>
    <summary>  Scaling inference through long chains-of-thought (CoTs) has unlocked
impressive reasoning capabilities in large language models (LLMs), yet the
reasoning process remains almost exclusively English-centric. We construct
translated versions of two popular English reasoning datasets, fine-tune Qwen
2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT
generation across French, Japanese, Latvian, and Swahili. Our experiments
reveal three key findings. First, the efficacy of using English as a pivot
language varies by language: it provides no benefit for French, improves
performance when used as the reasoning language for Japanese and Latvian, and
proves insufficient for Swahili where both task comprehension and reasoning
remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but
does not eliminate the cross-lingual performance gap. A lightweight fine-tune
using only 1k traces still improves performance by over 30\% in Swahili. Third,
data quality versus scale trade-offs are language dependent: small, carefully
curated datasets suffice for English and French, whereas larger but noisier
corpora prove more effective for Swahili and Latvian. Together, these results
clarify when and why long CoTs transfer across languages and provide translated
datasets to foster equitable multilingual reasoning research.
</summary>
    <author>
      <name>Josh Barua</name>
    </author>
    <author>
      <name>Seun Eisape</name>
    </author>
    <author>
      <name>Kayo Yin</name>
    </author>
    <author>
      <name>Alane Suhr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SCALR @ COLM 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.14828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14816v1</id>
    <updated>2025-08-20T16:09:04Z</updated>
    <published>2025-08-20T16:09:04Z</published>
    <title>Joint estimation of asymmetric community numbers in directed networks</title>
    <summary>  Community detection in directed networks is a central task in network
analysis. Unlike undirected networks, directed networks encode inherently
asymmetric relationships, giving rise to sender and receiver roles that may
each follow distinct community organizations with possibly different numbers of
communities. Estimating these two community counts simultaneously is therefore
considerably more challenging than in the undirected setting, yet it is
essential for faithful model specification and reliable downstream inference.
This work addresses this challenge within the stochastic co-block model (ScBM),
a powerful statistical framework for capturing asymmetric relational structures
inherent in directed networks. We propose a novel goodness-of-fit test based on
the deviation of the largest singular value of a normalized residual matrix
from the constant value 2. We show that the upper bound of this test statistic
converges to zero under the null hypothesis, while this statistic goes to
infinity if the true model has finer communities than hypothesized. Leveraging
this tail bounds behavior, we develop an efficient sequential testing algorithm
that lexicographically explores candidate community number pairs. To enhance
robustness in practical settings, we further introduce a ratio-based variant
that detects the transition point in the test statistic sequence. We rigorously
show both algorithms' consistency in recovering the true sender and receiver
community counts under ScBM. Numerical experiments demonstrate the accuracy and
robustness of our methods in estimating community numbers across diverse ScBM
settings. %To our knowledge, this work presents the first theoretically
guaranteed approach for jointly estimating the numbers of sender and receiver
communities within the ScBM framework, providing a critical tool for reliable
directed network analysis.
</summary>
    <author>
      <name>Huan Qing</name>
    </author>
    <link href="http://arxiv.org/abs/2508.14816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14812v1</id>
    <updated>2025-08-20T16:03:56Z</updated>
    <published>2025-08-20T16:03:56Z</published>
    <title>Repeating Words for Video-Language Retrieval with Coarse-to-Fine
  Objectives</title>
    <summary>  The explosive growth of video streaming presents challenges in achieving high
accuracy and low training costs for video-language retrieval. However, existing
methods rely on large-scale pre-training to improve video retrieval
performance, resulting in significant computational demands. Additionally, the
fine-grained information in videos and texts remains underexplored. To
alleviate these problems, we propose a novel framework to learn fine-grained
features for better alignment and introduce an inference pipeline to improve
performance without additional training. Specifically, we employ coarse-to-fine
objectives to understand the semantic information of video-text pairs,
including contrastive and matching learning. The fine-grained data used for
training is obtained through the Granularity-Aware Representation module, which
is designed based on similarity analysis between video frames and words in
captions. Furthermore, we observe that the repetition of keywords in the
original captions, referred to as "Repetition", can enhance retrieval
performance and improve alignment between video and text. Based on this
insight, we propose a novel and effective inference pipeline that incorporates
a voting mechanism and a new Matching Entropy metric to achieve better
retrieval performance without requiring additional pre-training. Experimental
results on four benchmarks demonstrate that the proposed method outperforms
previous approaches. Additionally, our inference pipeline achieves significant
performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT
dataset and a 1.6% increase on the DiDeMo dataset.
</summary>
    <author>
      <name>Haoyu Zhao</name>
    </author>
    <author>
      <name>Jiaxi Gu</name>
    </author>
    <author>
      <name>Shicong Wang</name>
    </author>
    <author>
      <name>Xing Zhang</name>
    </author>
    <author>
      <name>Hang Xu</name>
    </author>
    <author>
      <name>Zuxuan Wu</name>
    </author>
    <author>
      <name>Yu-Gang Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.14812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14770v1</id>
    <updated>2025-08-20T15:19:08Z</updated>
    <published>2025-08-20T15:19:08Z</published>
    <title>Non-Existent Outcomes in Research on Inequality: A Causal Approach</title>
    <summary>  Scholars of social stratification often study exposures that shape life
outcomes. But some outcomes (such as wage) only exist for some people (such as
those who are employed). We show how a common practice -- dropping cases with
non-existent outcomes -- can obscure causal effects when a treatment affects
both outcome existence and outcome values. The effects of both beneficial and
harmful treatments can be underestimated. Drawing on existing approaches for
principal stratification, we show how to study (1) the average effect on
whether an outcome exists and (2) the average effect on the outcome among the
latent subgroup whose outcome would exist in either treatment condition. To
extend our approach to the selection-on-observables settings common in applied
research, we develop a framework involving regression and simulation to enable
principal stratification estimates that adjust for measured confounders. We
illustrate through an empirical example about the effects of parenthood on
labor market outcomes.
</summary>
    <author>
      <name>Ian Lundberg</name>
    </author>
    <author>
      <name>Soonhong Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.14770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62D20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14748v1</id>
    <updated>2025-08-20T14:48:44Z</updated>
    <published>2025-08-20T14:48:44Z</published>
    <title>Cross-Modality Controlled Molecule Generation with Diffusion Language
  Model</title>
    <summary>  Current SMILES-based diffusion models for molecule generation typically
support only unimodal constraint. They inject conditioning signals at the start
of the training process and require retraining a new model from scratch
whenever the constraint changes. However, real-world applications often involve
multiple constraints across different modalities, and additional constraints
may emerge over the course of a study. This raises a challenge: how to extend a
pre-trained diffusion model not only to support cross-modality constraints but
also to incorporate new ones without retraining. To tackle this problem, we
propose the Cross-Modality Controlled Molecule Generation with Diffusion
Language Model (CMCM-DLM), demonstrated by two distinct cross modalities:
molecular structure and chemical properties. Our approach builds upon a
pre-trained diffusion model, incorporating two trainable modules, the Structure
Control Module (SCM) and the Property Control Module (PCM), and operates in two
distinct phases during the generation process. In Phase I, we employs the SCM
to inject structural constraints during the early diffusion steps, effectively
anchoring the molecular backbone. Phase II builds on this by further
introducing PCM to guide the later stages of inference to refine the generated
molecules, ensuring their chemical properties match the specified targets.
Experimental results on multiple datasets demonstrate the efficiency and
adaptability of our approach, highlighting CMCM-DLM's significant advancement
in molecular generation for drug discovery applications.
</summary>
    <author>
      <name>Yunzhe Zhang</name>
    </author>
    <author>
      <name>Yifei Wang</name>
    </author>
    <author>
      <name>Khanh Vinh Nguyen</name>
    </author>
    <author>
      <name>Pengyu Hong</name>
    </author>
    <link href="http://arxiv.org/abs/2508.14748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14746v1</id>
    <updated>2025-08-20T14:43:04Z</updated>
    <published>2025-08-20T14:43:04Z</published>
    <title>MissionHD: Data-Driven Refinement of Reasoning Graph Structure through
  Hyperdimensional Causal Path Encoding and Decoding</title>
    <summary>  Reasoning graphs from Large Language Models (LLMs) are often misaligned with
downstream visual tasks such as video anomaly detection (VAD). Existing Graph
Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less
graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly
optimizes graph structure using downstream task data, and propose MissionHD, a
hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses
an efficient encode-decode process to refine the graph, guided by the
downstream task signal. Experiments on challenging VAD and VAR benchmarks show
significant performance improvements when using our refined graphs, validating
our approach as an effective pre-processing step.
</summary>
    <author>
      <name>Sanggeon Yun</name>
    </author>
    <author>
      <name>Raheeb Hassan</name>
    </author>
    <author>
      <name>Ryozo Masukawa</name>
    </author>
    <author>
      <name>Mohsen Imani</name>
    </author>
    <link href="http://arxiv.org/abs/2508.14746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14735v1</id>
    <updated>2025-08-20T14:30:34Z</updated>
    <published>2025-08-20T14:30:34Z</published>
    <title>Evaluating Multilingual and Code-Switched Alignment in LLMs via
  Synthetic Natural Language Inference</title>
    <summary>  Large language models (LLMs) are increasingly applied in multilingual
contexts, yet their capacity for consistent, logically grounded alignment
across languages remains underexplored. We present a controlled evaluation
framework for multilingual natural language inference (NLI) that generates
synthetic, logic-based premise-hypothesis pairs and translates them into a
typologically diverse set of languages. This design enables precise control
over semantic relations and allows testing in both monolingual and
mixed-language (code-switched) conditions. Surprisingly, code-switching does
not degrade, and can even improve, performance, suggesting that
translation-induced lexical variation may serve as a regularization signal. We
validate semantic preservation through embedding-based similarity analyses and
cross-lingual alignment visualizations, confirming the fidelity of translated
pairs. Our findings expose both the potential and the brittleness of current
LLM cross-lingual reasoning, and identify code-switching as a promising lever
for improving multilingual robustness. Code available at:
https://github.com/KurbanIntelligenceLab/nli-stress-testing
</summary>
    <author>
      <name>Samir Abdaljalil</name>
    </author>
    <author>
      <name>Erchin Serpedin</name>
    </author>
    <author>
      <name>Khalid Qaraqe</name>
    </author>
    <author>
      <name>Hasan Kurban</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.14735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.14713v1</id>
    <updated>2025-08-20T13:43:49Z</updated>
    <published>2025-08-20T13:43:49Z</published>
    <title>Long-Context Speech Synthesis with Context-Aware Memory</title>
    <summary>  In long-text speech synthesis, current approaches typically convert text to
speech at the sentence-level and concatenate the results to form
pseudo-paragraph-level speech. These methods overlook the contextual coherence
of paragraphs, leading to reduced naturalness and inconsistencies in style and
timbre across the long-form speech. To address these issues, we propose a
Context-Aware Memory (CAM)-based long-context Text-to-Speech (TTS) model. The
CAM block integrates and retrieves both long-term memory and local context
details, enabling dynamic memory updates and transfers within long paragraphs
to guide sentence-level speech synthesis. Furthermore, the prefix mask enhances
the in-context learning ability by enabling bidirectional attention on prefix
tokens while maintaining unidirectional generation. Experimental results
demonstrate that the proposed method outperforms baseline and state-of-the-art
long-context methods in terms of prosody expressiveness, coherence and context
inference cost across paragraph-level speech.
</summary>
    <author>
      <name>Zhipeng Li</name>
    </author>
    <author>
      <name>Xiaofen Xing</name>
    </author>
    <author>
      <name>Jingyuan Xing</name>
    </author>
    <author>
      <name>Hangrui Hu</name>
    </author>
    <author>
      <name>Heng Lu</name>
    </author>
    <author>
      <name>Xiangmin Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Interspeech25</arxiv:comment>
    <link href="http://arxiv.org/abs/2508.14713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2508.14713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
